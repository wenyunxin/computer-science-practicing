{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75c7c30-7c89-47b3-8b87-57d29135d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f8c2ba-b990-4204-bf7f-df356114a1e3",
   "metadata": {},
   "source": [
    "## Fancier Output Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f619fa7-a201-4725-9fcd-fa9d952f0541",
   "metadata": {},
   "source": [
    "three ways of writing values:\n",
    "- expression statements\n",
    "- print() function\n",
    "- using the write() method of file objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e24154-fa4d-4f24-b31b-240bcd24e124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x103d0df60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The standard output file can be referenced as sys.stdout\n",
    "import sys\n",
    "sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886adafe-b6d0-464a-a946-f40de29ef536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Results of the 2016 Referendum'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# several ways to format output, first is to use formatted string literals\n",
    "year = 2016\n",
    "event = 'Referendum'\n",
    "f'Results of the {year} {event}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd50671f-850c-4ffb-94bc-a0dbecb96546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'42572654 YES votes 49.67%'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second is to use str.format() method of strings\n",
    "yes_votes = 42_572_654\n",
    "no_votes = 43_132_495\n",
    "percentage = yes_votes / (yes_votes + no_votes)\n",
    "'{:-8} YES votes {:2.2%}'.format(yes_votes, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7c3d0a9-87e0-473f-8c3d-bd93e6e08bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"'Hello world.'\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Hello world.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'0.14285714285714285'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of x is 32.5, and y is 40000...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello, world\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, world\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"'hello, world\\\\n'\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello, world\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"(32.5, 40000, ('spam', 'eggs'))\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conver any value to a string with the repr() or str() functions\n",
    "# str() function is meant to return representations of values that are fairly human-readable\n",
    "# repr() is meant to generate representations that can be read by the interpreter\n",
    "# For objects which don't have a particular representation for human consumption, str() function will return the same value as repr()\n",
    "# Many values, such as numbers or structures like lists and dictionaries, have the same representation using either function\n",
    "# string, in particular, has two distinct representations\n",
    "s = 'Hello world.'\n",
    "\n",
    "str(s)\n",
    "repr(s)\n",
    "eval(repr(s))\n",
    "\n",
    "str(1/7)\n",
    "\n",
    "x = 10 * 3.25\n",
    "y = 200 * 200\n",
    "s = 'The value of x is ' + repr(x) + ', and y is ' + repr(y) + '...'\n",
    "print(s)\n",
    "\n",
    "hello = 'hello, world\\n'\n",
    "# don't have a particular representation for human consumption?\n",
    "hello\n",
    "print(hello)\n",
    "hellos = repr(hello)\n",
    "hellos\n",
    "print(hellos)\n",
    "\n",
    "repr((x, y, ('spam', 'eggs')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42fda7a8-e399-40f8-9650-cdaa26fb1d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of pi is approximately 3.142.\n",
      "Sjoerd     ==>        4127\n",
      "Jack       ==>        4098\n",
      "Dcab       ==>        7678\n",
      "My hovercraft is full of eels\n",
      "My hovercraft is full of 'eels'\n",
      "Debugging bugs='roaches' 13 living room\n"
     ]
    }
   ],
   "source": [
    "# formatted string literals (f-strings) include the value of Python expressions inside a string\n",
    "import math\n",
    "print(f'The value of pi is approximately {math.pi:.3f}.')\n",
    "\n",
    "# Passing an integer after the : will cause the field to be a minimum number of characters wide\n",
    "# This is useful for making columns line up\n",
    "table = {'Sjoerd': 4127, 'Jack': 4098, 'Dcab': 7678}\n",
    "for name, phone in table.items():\n",
    "    print(f'{name:10} ==>  {phone:10d}')\n",
    "\n",
    "# Other modifiers can be used to convert the value before it is formatted\n",
    "# !a applies ascii(), !s applies str(), !r applies repr()\n",
    "animals = 'eels'\n",
    "print(f'My hovercraft is full of {animals}')\n",
    "print(f'My hovercraft is full of {animals!r}')\n",
    "\n",
    "# The = specifier can be used to expand an expression to the text of the expression\n",
    "bugs = 'roaches'\n",
    "count = 13\n",
    "area = 'living room'\n",
    "print(f'Debugging {bugs=} {count} {area}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc2cf616-0d34-4787-8472-3b4374cb9d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are the knights who say \"Ni!\"\n",
      "spam and eggs\n",
      "eggs and spam\n",
      "This spam is absolutely horrible.\n",
      "The story of Bill, Manfred, and Georg.\n",
      "Jack: 4098; Sjoerd: 4127; Dcab: 8637678\n",
      "Jack: 4098; Sjoerd: 4127; Dcab: 8637678\n",
      "Help on built-in function vars in module builtins:\n",
      "\n",
      "vars(...)\n",
      "    vars([object]) -> dictionary\n",
      "    \n",
      "    Without arguments, equivalent to locals().\n",
      "    With an argument, equivalent to object.__dict__.\n",
      "\n",
      " 1   1    1\n",
      " 2   4    8\n",
      " 3   9   27\n",
      " 4  16   64\n",
      " 5  25  125\n",
      " 6  36  216\n",
      " 7  49  343\n",
      " 8  64  512\n",
      " 9  81  729\n",
      "10 100 1000\n"
     ]
    }
   ],
   "source": [
    "# basic usage of str.format()\n",
    "print('We are the {} who say \"{}!\"'.format('knights', 'Ni'))\n",
    "\n",
    "# A number in the braces can be used to refer to the position of the object passed into the str.format()\n",
    "print('{0} and {1}'.format('spam', 'eggs'))\n",
    "print('{1} and {0}'.format('spam', 'eggs'))\n",
    "\n",
    "# keyword arguments used in str.format() method\n",
    "print('This {food} is {adjective}.'.format(\n",
    "    food='spam', adjective='absolutely horrible'))\n",
    "\n",
    "# positional and keyword arguments can be combined\n",
    "print('The story of {0}, {1}, and {other}.'.format('Bill', 'Manfred',\n",
    "                                                   other='Georg'))\n",
    "\n",
    "# passing the dict and using square brackets to access the keys\n",
    "table = {'Sjoerd': 4127, 'Jack': 4098, 'Dcab': 8637678}\n",
    "print('Jack: {0[Jack]:d}; Sjoerd: {0[Sjoerd]:d}; '\n",
    "      'Dcab: {0[Dcab]:d}'.format(table))\n",
    "\n",
    "# passing the dict as keyword arguments with the ** notation\n",
    "table = {'Sjoerd': 4127, 'Jack': 4098, 'Dcab': 8637678}\n",
    "print('Jack: {Jack:d}; Sjoerd: {Sjoerd:d}; Dcab: {Dcab:d}'.format(**table))\n",
    "\n",
    "help(vars)\n",
    "\n",
    "for x in range(1, 11):\n",
    "    print('{0:2d} {1:3d} {2:4d}'.format(x, x**2, x**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6402dbc4-6bda-48c4-8ca0-706d55283e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1   1    1\n",
      " 2   4    8\n",
      " 3   9   27\n",
      " 4  16   64\n",
      " 5  25  125\n",
      " 6  36  216\n",
      " 7  49  343\n",
      " 8  64  512\n",
      " 9  81  729\n",
      "10 100 1000\n"
     ]
    }
   ],
   "source": [
    "# formatted manually\n",
    "for x in range(1, 11):\n",
    "    print(repr(x).rjust(2), repr(x**2).rjust(3), repr(x**3).rjust(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bd6a690-36b4-4e17-a9a2-1fef6157725c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00012'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'-003.14'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'3.14159265359'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# str.zfill() pads a numeric string on the left with zeros\n",
    "'12'.zfill(5)\n",
    "'-3.14'.zfill(7)\n",
    "'3.14159265359'.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2de707-c115-43c3-a4e5-b7dba014e13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of pi is approximately 3.142.\n"
     ]
    }
   ],
   "source": [
    "# the % operator can be used for string formatting, known as string interpolation\n",
    "print('The value of pi is approximately %5.3f.' % math.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0581417d-48b3-4719-bccb-98e31a15bf43",
   "metadata": {},
   "source": [
    "More information can be found in the [printf-style String Formatting](https://docs.python.org/3/library/stdtypes.html#old-string-formatting) section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba449fc0-8d6d-474c-b3d0-acf93aac69d8",
   "metadata": {},
   "source": [
    "## Reading and Writing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83c8ff6a-790c-4455-a4b1-d17440f85e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='build.txt' mode='r' encoding='utf-8'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open() returns a file object, and is most commonly used with two positional arguments and one keyword argument\n",
    "# open(filename, mode, encoding=None)\n",
    "f = open('build.txt', 'r', encoding=\"utf-8\")\n",
    "f\n",
    "\n",
    "# if not using the with keyword, call f.close() to close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43650250-f079-4d60-a011-33d3287a3ddd",
   "metadata": {},
   "source": [
    "Normally, files are opened in *text mode*. The mode argument describes the way in which the file will be used\n",
    "- `r` the file will only be read\n",
    "- `w` for only writing (an existing file with the same name will be erased)\n",
    "- `a` opens the file for appending, any data written to the file is automatically added to the end\n",
    "- `r+` opens the file for both reading and writing\n",
    "- the mode argument is optional, `r` will be assumed if it's omitted\n",
    "\n",
    "Appending a `b` to the mode opens the file in *binary mode*. Binary mode data is read and written as [bytes](https://docs.python.org/3/library/stdtypes.html#bytes) objects. You can not specify *encoding* when opening file in binary mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b5b6348-d9c4-4758-a929-b7e6a35b177b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"foreign  [Music]  researcher and founding member of open  AI Andre carpathy  [Applause]  hi everyone  I'm happy to be here to tell you about  the state of gbt and more generally  about the rapidly growing ecosystem of  large language models  so I would like to partition the talk  into two parts  in the first part I would like to tell  you about how we train GPT assistance  and then in the second part we will we  are going to take a look at how we can  use these assistants effectively for  your applications  so first let's take a look at the  emerging recipe for how to train these  assistants and keep in mind that this is  all very new and still rapidly evolving  but so far the recipe looks something  like this  now this is kind of a complicated slide  so I'm going to go through it piece by  piece but roughly speaking we have four  major stages free training supervised  fine tuning reward modeling  reinforcement learning and they follow  each other serially  now in each stage we have a data set  that is that powers that stage we have  an algorithm that for our purposes will  be a objective and a and over for uh for  training the neural network and then we  have a resulting model and then there's  some notes on the bottom so the first  stage we're going to start with is the  pre-training stage now this stage is  kind of special in this diagram and this  diagram is not to scale because this  stage is where all of the computational  work basically happens this is 99 of the  training uh compute time and also flops  and so this is where we are dealing with  internet scale data sets with thousands  of gpus in the store computer and also  months of training potentially the other  three stages are fine-tuning stages that  are much more along the lines of a small  few number of gpus and hours or days  so let's take a look at the pre-training  stage to achieve a base model  first we're going to gather a large  amount of data here's an example of what  we call a data mixture that comes from  this uh paper that was released by meta  where they released this llama based  model now you can see roughly the kinds  of data sets that enter into these  collections so we have common crawl  which is just a web scrape C4 which is  also a common crawl and then some high  quality data sets as well so for example  GitHub Wikipedia books archive stack  exchange and so on these are all mixed  up together and then they are sampled  According to some given proportions and  that forms the training set for the  neural net for the GPT  now before we can actually train on this  data we need to go through one more  pre-processing step and that is  tokenization and this is basically a  translation of the raw text that we  scrape from the internet into sequences  of integers because that's the native  representation over which gpts function  now this is a lossless kind of  translation between pieces of text and  tokens and integers and there are a  number of algorithms for this stage  typically for example you could use  something like byte pair encoding which  iteratively merges little text chunks  and groups them into tokens  and so here I'm showing some example  chunks of these tokens and then this is  the raw integer sequence that will  actually feed into a Transformer  now here I'm showing two uh sort of like  examples for hyper parameters that  govern this stage so gpt4 we did not  release too much information about how  it was trained and so on so I'm using  gpt3's numbers but gpt3 is of course a  little bit old by now about three years  ago but llama is a fairly recent model  from meta so these are roughly the  orders of magnitude that we're dealing  with when we're doing pre-training  the vocabulary size is usually a couple  10 000 tokens the context length is  usually something like two thousand four  thousand or nowadays even 100 000 and  this governs the maximum number of  integers that the GPT will look at when  it's trying to predict the next integer  in a sequence  um you can see that roughly the number  of parameters is say 65 billion for  llama now even though llama has only 65b  parameters compared to gvt3's 175  billion parameters llama is a  significantly more powerful model and  intuitively that's because the model is  trained for significantly longer in this  case 1.4 trillion tokens instead of just  300 billion tokens so you shouldn't  judge the power of a model just by the  number of parameters that it contains  below I'm showing some tables of rough  number of rough hyper parameters that  typically go into specifying the  Transformer neural network so the number  of heads the dimension size number of  layers and so on and on the bottom I'm  showing some training hyper parameters  so for example to train the 65b model  meta used 2000 gpus roughly 21 days  train of training and and roughly  several million dollars and so that's  the rough orders of magnitude that you  should have in mind for the pre-training  stage  now when we're actually pre-chaining  what happens roughly speaking we are  going to take our tokens and we're going  to lay them out into Data batches so we  have these arrays that will feed into  the Transformer and these arrays are B  the batch size and these are all  independent examples stacked up in rows  and B by T T being the maximum context  length so in my picture I only have 10.  the context length so this could be two  thousand four thousand Etc so these are  extremely long rows and what we do is we  take these documents and we pack them  into rows and we delimit them with these  Special end of text tokens uh basically  it's on the Transformer where a new  document begins  and so here I have a few examples of  documents and then I've stretched them  out into uh into this input  now we're going to feed all of these  numbers into Transformer and let me let  me just focus on a single particular  cell but the same thing will happen at  every every cell in this diagram so  let's look at the green cell the green  cell is going to take a look at all of  the tokens before it so all of the  tokens in yellow and we're going to feed  that entire context into the Transformer  neural network  and the Transformer is going to try to  predict the next token in the sequence  in this case in red  now the Transformer I don't have too  much time to unfortunately go into the  full details of this neural network  architecture is just a large blob of  neural net stuff for our purposes and  it's got several 10 billion parameters  typically or something like that and of  course as they tune these parameters  you're getting slightly different uh  predicted distributions for every single  one of these cells  and so for example if our vocabulary  size is 50 257 tokens then we're going  to have that many numbers because we  need to specify a probability  distribution for what comes next so  basically we have a probability for  whatever may follow now in this specific  example for the specific cell 513 will  come next and so we can use this as a  source of supervision to update our  Transformers weights and so we're  applying this basically on every single  cell in parallel and we keep swap  swapping batches and we're trying to get  the Transformer to make the correct  predictions over what token comes next  in a sequence  so let me show you more concretely what  this looks like when you train one of  these models this is actually coming  from a New York Times and they trained a  small GPT on Shakespeare and so here's a  small snippet of Shakespeare and they  trained a GPT on it now in the beginning  at initialization the GPT starts with  completely random weights so you're just  getting completely random outputs as  well  but over time as you train uh the GPT  longer and longer you are getting more  and more coherent and consistent uh sort  of samples from the model and the way  you sample from it of course is you uh  predict what comes next you sample from  that distribution and you keep feeding  that back into the process and you can  basically uh sample large sequences and  so by the end you see that the  Transformer has learned about words and  where to put spaces and where to put  commas and so on and so we're making  more and more consistent predictions  over time  these are the kinds of plots that you're  looking at when you're doing model  pre-training effectively we're looking  at the loss function over time as you  train and low loss means that our  Transformer is predicting the correct uh  is giving a higher probability to the  correct next integer in a sequence  now what are we going to do with this  model once we've trained it after a  month well the first thing that we  noticed we the field is that these  models basically in the process of  language modeling learn very powerful  General representations and it's  possible to very efficiently fine-tune  them for any arbitrary Downstream task  you might be interested in so as an  example if you're interested in  sentiment classification the approach  used to be that you collect a bunch of  positives and negatives and then you  train some kind of an NLP model for for  that but the new approach is ignore  sentiment classification  go off and do large language model  pre-training train the large Transformer  and then you can only you may only have  a few examples and you can very  efficiently fine tune your model for  that task and so this works very well in  practice and the reason for this is that  basically the Transformer is forced to  multitask a huge amount of tasks in the  language modeling task because just just  in terms of predicting the next token  it's forced to understand a lot about  the structure of the of the text and all  the different concepts they're in  so that was GPT one now around the time  of gpt2 people noticed that actually  even better than fine tuning you can  actually prompt these models very  effectively so these are language models  and they want to complete documents so  you can actually trick them into  performing tasks just by arranging these  fake documents so in this example for uh  for example we have some passage and  then we sort of like do QA qaqa this is  called a few shot prompt and then we do  q and then as the Transformer is trying  to complete the document it's actually  answering our question and so this is an  example of prompt engineering a base  model making the belief that it's sort  of imitating a document and getting it  to perform a task  and so this kicked off I think the era  of I would say prompting over  fine-tuning and seeing that this  actually can work extremely well on a  lot of problems even without training  any neural networks fine-tuning or so on  now since then we've seen an entire  evolutionary tree of Base models that  everyone has trained not all of these  models are available for example the  gpt4 base model was never released the  gpt4 model that you might be interacting  with over API is not a base model it's  an assistant model and we're going to  cover how to get those in a bit gpt3  based model is available via the API  under the name DaVinci and gpt2 base  model is available even as weights on  our GitHub repo but currently the best  available base model probably is the  Llama series from meta although it is  not commercially licensed  now one thing to point out is base  models are not assistance they don't  want to answer to they don't want to  make answers to your questions they just  want to complete documents so if you  tell them write a poem about the brand  cheese it will just you know it will  answer questions with more questions  it's just completing what it thinks as a  document  however you can prompt them in a  specific way for base models that that  is more likely to work so as an example  here's a poem about bread and cheese and  in that case it will auto-complete  correctly  you can even trick base models into  being assistance and the way you would  do this is you would create like a  specific few shot prompt that makes it  look like there's some kind of a  document between a human and assistant  and they're exchanging sort of um  information and then at the bottom you  sort of put your query at the end and  the base model will sort of like  condition itself into being like a  helpful assistant and kind of answer but  this is not very reliable and doesn't  work super well in practice although it  can be done  so instead we have a different path to  make actual gbt assistance not just base  model document completers and so that  takes us into supervised fine-tuning so  in the supervised fine tuning stage we  are going to collect small but high  quality data sets and in this case we're  going to ask human contractors to gather  data of the format of the form prompt  and ideal response and we're going to  collect lots of these typically tens of  thousands or something like that  and then we're going to still do  language modeling on this data so  nothing change algorithmically we're  just swapping out a training set so it  used to be internet documents which is a  high quantity local  four basically QA prompt response kind  of data and that is low quantity high  quality  so we still do language modeling and  then after training we get an sfd model  and you can actually deploy these models  and they are actual assistants and they  work to some extent let me show you what  an example demonstration might look like  so here's something that a human  contractor might come up with here's  some random prompt can you write a short  introduction about the relevance of the  term monopsony or something like that  and then the contractor also writes out  an ideal response and when they write  out these responses they are following  extensive labeling documentations and  they're being asked to be helpful  truthful and harmless and those labeling  instructions here you probably can't  read it another can I but they're long  and this is just people following  instructions and trying to complete  these prompts  so that's what the data set looks like  and you can train these models and this  works to some extent  now you can actually continue the  pipeline from here on and go into rlhf  reinforcement learning from Human  feedback that consists of both reward  modeling and reinforcement learning so  let me cover that and then I'll come  back to why you may want to go through  the extra steps and how that compares to  just sft models  so in the reward modeling step what  we're going to do is we're now going to  shift our data collection to be of the  form of comparisons so here's an example  of what our data set will look like  I have the same prompt identical prompt  on the top which is asking the assistant  to write a program or function that  checks if a given string is a palindrome  and then what we do is we take the sft  model which we've already trained and we  create multiple completions so in this  case we have three completions that the  model has created  and then we ask people to rank these  completions so if you stare at this for  a while and by the way these are very  difficult things to do to compare some  of these predictions and this can take  people even hours for a single prompt  completion pairs but let's say we  decided that one of these is much better  than the others and so on so we rank  them  then we can follow that with something  that looks very much kind of like a  binary classification on all the  possible pairs between these completions  so what we do now is we lay out our  prompt in rows and the prompts is  identical across all three rows here so  it's all the same prompt but the  completion of this vary and so the  yellow tokens are coming from the sft  model then what we do is we append  another special reward uh readout token  at the end and we basically only  supervise the Transformer at this single  green token and the Transformer will  predict some reward for how good that  completion is for that prompt  and so basically it makes a guess about  the quality of each completion and then  once it makes a guess for every one of  them we also have the ground truth which  is telling us the ranking of them and so  we can actually enforce that some of  these numbers should be much higher than  others and so on WE formulate this into  a loss function and we train our model  to make reward predictions that are  consistent with the ground truth coming  from the comparisons from all these  contractors so that's how we train our  reward model and that allows us to score  how good a completion is for a prompt  once we have a reward model this is we  can't deploy this because this is not  very useful as an assistant by itself  but it's very useful for the reinforce  reinforcement learning stage that  follows now because we have a reward  model we can score the quality of any  arbitrary completion for any given  prompt so what we do during  reinforcement learning is we basically  get again a large collection of prompts  and now we do reinforcement learning  with respect to the reward model so  here's what that looks like  we we take a single prompt we lay it out  in rows and now we use the sft we use  the basically the model we'd like to  train which is initialized at sft model  to create some completions in yellow  and then we append the reward token  again and we read off the reward  according to the reward model which is  now kept fixed it doesn't change anymore  and now the reward model tells us the  quality of every single completion for  each for these prompts and so what we  can do is we can now just basically  apply the same language modeling loss  function but we're currently training on  the yellow tokens and we are weighing  the language modeling objective by the  rewards indicated by the reward model so  as an example in the first row the  reward model said that this is a fairly  high score in completion and so all the  tokens that we happen to sample on the  first row are going to get reinforced  and they're going to get higher  probabilities for the future  conversely on the second row the reward  model really did not like this  completion negative 1.2 and so therefore  every single token that we sampled in  that second row is going to get a  slightly higher probability for the  future and we do this over and over on  many prompts on many batches and  basically we get a policy which creates  yellow tokens here and it basically all  of them all of the completions here will  score high according to the reward model  that we trained in the previous stage  so that's how we train uh that's what  the rohf pipeline is  now and then at the end you get a model  that you could deploy and so and as an  example chat GPT is an rlhf model but  some other models that you might come  across like for example of the kuna 13B  and so on these are sft models so we  have base models sft models and rlh  models and that's kind of like the state  of things there  now why would you want to do rlhf so one  answer that is kind of not that exciting  is that it just works better so this  comes from the instructor gbt paper  according to these experiments a while  ago now these PPO models are rlhf and we  see that they are basically just  preferred in a lot of comparisons when  we give them to humans so humans just  prefer out basically tokens that come  from our religion models compared to sft  models compared to base model that is  prompted to be an assistant and so it  just works better but you might ask why  why does it work better and I don't  think that there's a single like amazing  answer that the community has really  like agreed on but I will just offer one  uh one reason potentially and it has to  do with the asymmetry between how easy  computationally it is to compare versus  generate so let's take an example of  generating a haiku suppose I ask a model  to write a haiku about paper clips if  you're a contractor trying to give train  data then imagine being a contractor  collecting basically data for the sft  stage how are you supposed to create a  nice haiku for a paperclip you might  just not be very good at that but if I  give you a few examples of haikus you  might be able to appreciate some of  these haikus a lot more than others and  so judging which one of these is good is  much easier task and so basically this  asymmetry makes it so that comparisons  are a better way to potentially leverage  yourself as a human and your judgment to  create a slightly better model  now our religious models are not  strictly an improvement on the base  models in some cases so in particular  we've noticed for example that they lose  some entropy so that means that they  give More PT results they can output  lower variations uh like they can output  samples with lower variation than base  model so base model has lots of entropy  and we'll go with lots of diverse  outputs  so for example one one kind of place  where I still prefer to use a base model  is in the setup where  you basically have N Things and you want  to generate more things like it and so  here is an example that I just cooked up  I want to generate cool Pokemon names I  gave it seven Pokemon names and I asked  the base model to complete the document  and it gave me a lot more Pokemon names  uh these are fictitious I tried to look  them up I don't believe they're actual  Pokemons and uh this is the kind of task  that I think base model would be good at  because it still has lots of entropy it  will give you lots of diverse cool kind  of more things that look like whatever  you give it before  so this is what uh this is number having  all said all that these are kind of like  the assistant models that are probably  available to you at this point uh that  there's a team at Berkeley that ranked a  lot of the available assistant models  and gave them basically ELO ratings So  currently some of the best models of  course are gpt4 by far I would say  followed by Claude gvt 3.5 and then a  number of models some of these might be  available as weights like the kuna koala  Etc and the first three rows here are  all they're all rohf models and all of  the other models to my knowledge are sft  models I believe  okay so that's how we train these models  on the high level now I'm going to  switch gears and let's look at how we  can best apply and the GPT assistant  model to your problems now I would like  to work  in a setting of a concrete example so  let's  let's work with a concrete example here  let's say that you are working on an  article or a blog post and you're going  to write this sentence at the end  California's population is 53 times that  of Alaska so for some reason you want to  compare the populations of these two  states  think about the rich internal monologue  and Tool use and how much work actually  goes computationally in your brain to  generate this one final sentence so  here's maybe what that could look like  in your brain okay for this next step  let me blog uh for my blog let me  compare these two populations  okay first I'm going to obviously need  to get both of these populations now I  know that I probably don't know these  populations off the top of my head so  I'm kind of like aware of what I know or  don't know of my self-knowledge right  so I go I do some tool use and I go to  Wikipedia and I look up California's  population and Alaska's population  now I know that I should divide the two  but again I know that dividing three 9.2  by 0.74 is very unlikely to succeed  that's not the kind of thing that I can  do in my head and so therefore I'm gonna  rely on the calculator so I'm going to  use a calculator punch it in and see  that the output is roughly 53.  and then maybe I do some reflection and  Sanity checks in my brain so does 53  make sense well that's quite quite a  large fraction but then California is  the most popular state so maybe that  looks okay  so then I have all the information I  might need and now I get to the sort of  creative portion of writing so I might  start to write something like California  has 53 x times greater and then I think  to myself that's actually like really  awkward phrasing so let me actually  delete that and let me try again and so  as I'm writing I have this separate  process almost inspecting what I'm  writing and judging whether it looks  good or not  and then maybe I delete and maybe I  reframe it and then maybe I'm happy with  what comes out  so basically long story short a ton  happens under the hood in terms of your  internal monologue when you create  sentences like this but what does a  sentence like this look like when we are  training a GPT on it  From gpt's perspective this is just a  sequence of tokens  so GPT when it's reading or generating  these tokens it just goes  and each chunk is roughly the same  amount of computational work for each  token and these Transformers are not  very shallow networks they have about 80  layers of reasoning but 80 is still not  like too much and so this Transformer is  going to do its best to imitate but of  course the process here looks very very  different from the process that you took  so in particular in our final artifacts  in the data sets that we create and then  eventually feed to llms all of that  internal dialogue is completely stripped  and uh and unlike you the GPT will look  at every single token and spend the same  amount of compute on every one of them  and so you can't expect it to actually  like well you can't expect it to do to  uh sort of do too much work per token  so and also in particular basically  these Transformers are just like token  simulators so they don't know what they  don't know like they just imitate the  next token they don't know what they're  good at or not good at they just tried  their best imitate the next token they  don't reflect in the loop they don't  sanity check anything they don't correct  their mistakes along the way by default  they just uh sample token sequences  they don't have separate inner monologue  streams in their head right that are  evaluating what's happening now they do  have some sort of cognitive advantages I  would say and that is that they do  actually have a very large fact-based  knowledge across a vast number of areas  because they have say several 10 billion  parameters so it's a lot of storage for  a lot of facts  but and they also I think have a  relatively large and perfect working  memory so whatever fixed into the  whatever fits into the context window is  immediately available to the Transformer  through its internal self-attention  mechanism and so it's kind of like um  Perfect Memory but it's got that finite  size but the Transformer has a very  direct access to it and so it can like  losslessly remember anything that is  inside its context window  so that's kind of how I would compare  those two and the reason I bring all of  this up is because I think to a large  extent prompting is just making up for  this sort of cognitive difference  between these two kind of architectures  um like our brains here and llm brains  you can look at it that way almost  so here's one thing that people found  for example works pretty well in  practice especially if your tasks  require reasoning you can't expect the  Transformer to make to to do too much  reasoning per token and so you have to  really spread out the reasoning across  more and more tokens so for example you  can't give a Transformer a very  complicated question and expect it to  get the answer in a single token there's  just not enough time for it these  Transformers need tokens to think quote  unquote I like to say sometimes and so  this is some of the things that work  well you may for example have a few shot  prompt that shows the Transformer that  it should like show its work when it's  answering a question when it's answering  the question and if you give a few  examples the Transformer will imitate  that template and it will just end up  working out better in terms of its  evaluation  Additionally you can elicit this kind of  behavior from the Transformer by saying  let's think step by step because this  condition is the transformer into sort  of like showing its work and because it  kind of snaps into a mode of showing its  work it's going to do less computational  work per token and so it's more likely  to succeed as a result because it's  making  um slower reasoning over time  here's another example this one is  called self-consistency we saw that we  had the ability to start writing and  then if it didn't work out I can try  again and I can try multiple times and  uh and maybe  um select the one that worked best so in  these kinds of approaches you may sample  not just once but you may sample  multiple times and then have some  process for finding the ones that aren't  good and then keeping just those samples  or doing a majority vote or something  like that so basically these  Transformers in the process as they  predict the next token just like you  they can get unlucky and they can they  could sample and not a very good token  and they can go down sort of like a  blind alley in terms of reasoning and so  unlike you they cannot recover from that  they are stuck with every single token  they sample and so they will continue  the sequence even if they even know that  this sequence is not going to work out  so give them the ability to look back  inspect or try to find uh try to  basically sample around it  here's one technique also you could it  turns out that actually llms like they  know when they've screwed up so as an  example  say you asked the model to generate a  poem that does not rhyme and it might  give you a poem but it actually Rhymes  but it turns out that especially for the  bigger models like gpt4 you can just ask  it did you meet the assignment and  actually gpt4 knows very well that it  did not meet the assignment it just kind  of got unlucky in its sampling and so it  will tell you no I didn't actually meet  the assignment here's let me try again  but without you prompting it it doesn't  even like it doesn't know it doesn't  know to revisit and uh and so on so you  have to make up for that in your prompt  you have to get it to check if you don't  ask it to check it's not going to check  by itself it's just a token simulator  I think more generally a lot of these  techniques fall into the bucket of what  I would say recreating our system too so  you might be familiar with the system  one system to thinking for humans system  one is a fast automatic process and I  think kind of corresponds to like an llm  just sampling tokens  and system two is the slower deliberate  planning sort of part of your brain  and so this is a paper actually from  just last week because this space is  pretty quickly evolving it's called tree  of thought and in tree of thought uh the  authors of this paper propose  maintaining multiple completions for any  given prompt and then they are also  scoring them along the way and keeping  the ones that are going well if that  makes sense and so a lot of people are  like really playing around with kind of  um prompt engineering to um to basically  bring back some of these abilities that  we sort of have in our brain for llms  now one thing I would like to note here  is that this is not just a prompt this  is actually prompts that are together  used with some python glue code because  you don't you actually have to maintain  multiple prompts and you also have to do  some tree search algorithm here um to  like figure out which prompt to expand  Etc so it's a symbiosis of python glue  code and individual prompts that are  called in a while loop or in a bigger  algorithm  I also think there's a really cool  parallel here to alphago alphago has a  policy for placing the next Stone when  it plays go and its policy was trained  originally by imitating humans but in  addition to this policy it also does  Monte Carlo tree search and basically it  will play out a number of possibilities  in its head and evaluate all of them and  only keep the ones that work well and so  I think this is kind of an equivalent of  alphago but for text  if that makes sense  so just like tree of thought I think  more generally people are starting to  like really Explore More General  techniques of not just the simple  question answer prompt but something  that looks a lot more like python blue  code stringing together many problems so  on the right I have an example from this  paper called react where they structure  the answer to a prompt as a sequence of  thought action observation thought  action observation and it's a full  rollout a kind of a thinking process to  answer the query and in these actions  the model is also allowed to to lose  on the left I have an example of rogpt  and now rgbt by the way became as a  project that I think got a lot of hype  recently and I think uh but I think I  still find it kind of inspirationally  interesting it's um it's a project that  allows an llm to sort of keep task list  and continue to recursively break down  tasks and I don't think this currently  works very well and I would not advise  people to use it in Practical  applications I just think it's something  to generally take inspiration from in  terms of where this is going I think  over time  so that's kind of like giving our model  system to thinking the next thing that I  find kind of interesting is  this following survey would say almost  psychological Quirk of llms is that llms  don't want to succeed they want to  imitate  you want to succeed and you should ask  for it  so what I mean by that is  when Transformers are trained they have  training sets and there can be an entire  spectrum of performance qualities in  their training data so for example there  could be some kind of a prompt for some  Physics question or something like that  and there could be a student solution  that is completely wrong but there can  also be an expert answer that is  extremely right and Transformers can't  tell the difference between like look I  mean they know they know about low  quality Solutions and high quality  Solutions but by default they want to  imitate all of it because they're just  trained on language modeling and so at  test time you actually have to ask for a  good performance so in this example in  this paper it's um they tried various  prompts and let's think step by step was  very powerful because it's sort of like  spread out the reasoning of remaining  tokens but what worked even better is  let's work this out in a step-by-step  way to be sure we have the right answer  and so it's kind of like a conditioning  on getting a right answer and this  actually makes the Transformer work  better because the Transformer doesn't  have to now hedge its probability Mass  on low quality Solutions as ridiculous  as that sounds and so basically don't be  feel free to ask for a strong solution  say something like you are a leading  expert on this topic pretend you have  iq120 Etc but don't try to ask for too  much IQ because if you ask for IQ of  like 400 you might be out of data  distribution or even worse you could be  in data distribution for some like  sci-fi stuff and it will start to like  take on some sci-fi or like role-playing  or something like that so you have to  find like the right amount of IQ I think  it's got some U-shaped curve there  next up  as we saw when we are trying to solve  problems we know we are good at and what  we're not good at and we Lean On Tools  computationally you want to do the same  potentially with your uh llms so in  particular we may want to give them  calculators code interpreters and so on  the ability to do search and uh there's  a lot of techniques for doing that one  thing to keep in mind again is that  these Transformers by default may not  know what they not don't know so you may  even want to tell the transformer in the  prompt you are not very good at mental  arithmetic whenever you need to do very  large number addition multiplication or  whatever instead use this calculator  here's how you use the calculator use  this token combination etc etc so you  have to actually like spell it out  because the model by default doesn't  know what it's good at or not good at  necessarily just like you and I you and  I might be  next up I think something that is very  interesting is we went from a world that  was retrieval only all the way the  pendulum has swung to The Other Extreme  where it's memory only in lens but  actually there's this entire space in  between of these retrieval augmented  models and this works extremely well in  practice  as I mentioned the context window of a  transformer is its working memory if you  can load the working memory with any  information that is relevant to the task  the model will work extremely well  because it can immediately access all  that memory  and so I think a lot of people are  really interested in  uh basically retrieval augmented  generation and on the bottom I have like  an example of llama index which is one  sort of data connector to lots of  different types of data and you can uh  all you can make it you can index all of  that data and you can make it accessible  to llms and the emerging recipe there is  you take relevant documents you split  them up into chunks you embed all of  them and you basically get embedding  vectors that represent that data you  store that in a vector store and then at  test time you make some kind of a query  to your vector store and you fetch  chunks that might be relevant to your  task and you stuff them into the prompt  and then you generate so this can work  quite well in practice so this is I  think similar to when you're and I solve  problems you can do everything from your  memory and Transformers have very large  and extensive memory but also it really  helps to reference some primary  documents so when you whenever you find  yourself going back to a textbook to  find something or whenever you find  yourself going back to the documentation  of a library to look something up the  transform transformers definitely when I  do that too you you have a mem you have  some memory over how some documentation  of a library works but it's much better  to look it up so the same same applies  here  next I wanted to briefly talk about  constraint prompting I also find this  very interesting  uh this is basically techniques for  um  forcing a certain template in the  outputs of llms so guidance is one  example from Microsoft actually and uh  here we are enforcing that the output  from the llm will be Json and this will  actually guarantee that the output will  take on this form because they go in and  they mess with the probabilities of all  the different tokens that come out of  the Transformer and if they clamp those  tokens and then the Transformer is only  filling in the blanks here and then you  can enforce additional restrictions on  what could go into those blanks so this  might be really helpful and I think this  kind of constraint sampling is also  extremely interesting  I also wanted to say a few words about  fine tuning it is the case that you can  get really far with prompt engineering  but it's also possible to think about  fine-tuning your models now fine-tuning  models means that you are actually going  to change the weights of the model  it is becoming a lot more accessible to  do this in practice and that's because  of a number of techniques that have been  developed and have libraries for uh very  recently so for example parameter  efficient fine-tuning techniques like  Laura make sure that you're only Trend  you're only training small Spar species  of your model so most of the model is  kept clamped at the base model and some  pieces of it are allowed to change and  this still works pretty well empirically  and makes it much cheaper to sort of  tune only small pieces of your model  uh there's also it also means that  because most of your model is clamped  you can use very low Precision inference  for computing those parts because they  are not going to be updated by gradient  descent and so that makes everything a  lot more efficient as well and in  addition we have a number of Open Source  high quality based models currently as I  mentioned I think llama is quite nice  although it is not commercially licensed  I believe right now  some things to keep in mind is that  basically fine tuning is is a lot more  technically involved it requires a lot  more I think technical expertise to do  to do right it requires human data  contractors for data sets and or  synthetic data pipelines that can be  pretty complicated this will definitely  slow down your iteration cycle by a lot  and I would say on a high level sft is  achievable because it is just your  continuing language modeling task It's  relatively straightforward but rlhref I  would say is very much research  territory and is even much harder to get  to work and so I would probably not  advise that someone just tries to roll  their own rlh of implementation these  things are pretty unstable very  difficult to train uh not something that  is I think very beginner friendly right  now and it's also potentially likely  also to to change pretty rapidly still  so I think these are my sort of default  recommendations right now  I would break up your task into two  major parts number one achieve your top  performance and number two optimize your  performance in that order  number one the best performance will  currently come from GT4 model it is the  most capable of by far  use prompts that are very detailed they  have lots of task contents relative  relevant information and instructions  think along the lines of what would you  tell a task contractor if they can't  email you back but then also keep in  mind that a task contractor is a human  and they have inner monologue and  they're very clever Etc llms do not  possess those qualities so make sure to  think through the psychology of the llm  almost and cater prompts to that  retrieve and add any relevant context  and information to these prompts  basically refer to a lot of the prompt  engineering techniques some of them are  highlighted in the slides above but also  this is a very large space and I would  just advise you to look for prompt  engineering techniques online there's a  lot to cover there  experiment with few shop examples what  this refers to is you don't just want to  tell you want to show whenever it's  possible so give it examples of  everything that helps it really  understand what you mean if you can  tools and plugins to offload a tasks  that are difficult for llms natively uh  and then think about not just a single  prompt and answer think about potential  chains and reflection and how you glue  them together and how you could  potentially make multiple samples and so  on  finally if you think you've squeezed out  prompt engineering which I think you  should stick with for a while look at  some potentially fine-tuning a model to  your application but expect this to be a  lot more slower and evolved and then  there's an expert fragile research Zone  here and I would say that is rlhf which  currently does work a bit better than  sft if you can get it to work but again  this is uh pretty involved I would say  and to optimize your costs try to  explore lower capacity models or shorter  prompts and so on  I also wanted to say a few words about  the use cases in which I think llms are  currently well suited for so in  particular note that there's a large  number of limitations to llms today and  uh so I would keep that definitely in  mind for all your applications models  and this by the way could be an entire  talk so I don't have time to cover it in  full detail models may be biased they  may fabricate hallucinate information  they may have reasoning errors they may  struggle in entire classes of  applications they have knowledge cutoffs  so they might not know any information  above say September 2021 they are  susceptible to a large range of attacks  which are sort of like coming out on  Twitter daily including prompt injection  jailbreak attacks data poisoning attacks  and so on so my recommendation right now  is use llms in low stakes applications  combine them with always with human  oversight use them as a source of  inspiration and suggestions and think  co-pilots instead of completely  autonomous agents that are just like  performing a task somewhere it's just  not clear that the models are there  right now  so I wanted to close by saying that gpt4  is an amazing artifact I'm very thankful  that it exists and it's it's beautiful  it has a ton of knowledge across so many  areas it can do math code and so on and  in addition there's this thriving  ecosystem of everything else that is  being built and incorporated into into  the ecosystem uh some of some of the  things these things I've talked about  and all of this power is accessible at  your fingertips  so here's everything that's needed in  terms of code to ask GPT for a question  to prompt it and get a response in this  case I said uh can you say something to  inspire the audience of Microsoft build  2023  and I just punched this into Python and  verbatim gpt4 said the following  uh and by the way I did not know that  they used this trick in the keynote so I  thought I was being clever but  but it is really good at this it says  ladies and gentlemen Invaders and  Trailblazers Microsoft build 2023  welcome to the Gathering of Brilliant  Minds like no other you are The  Architects of the future The Visionaries  molding the digital realm in which  Humanity thrives embrace the Limitless  possibilities of Technologies and let  your ideas soar as high as your  imagination together let's create a more  connected remarkable and inclusive world  for generations to come get ready to  unleash your creativity canvas the  unknown and turn dreams into reality  your Journey Begins today  okay\\n外国\\n\\n[音乐]\\n\\n研究员和开放人工智能创始成员\\n\\n安德烈·卡帕西\\n\\n[鼓掌]\\n\\n大家好，\\n\\n我很高兴能在这里向大家介绍\\n\\nGBT 的现状，以及更广泛的\\n\\n关于快速发展的大型语言模型生态系统的信息，\\n\\n\\n所以我想 将演讲\\n\\n分为两个部分，\\n\\n在第一部分中，我想告诉\\n\\n您我们如何训练 GPT 辅助，\\n\\n然后在第二部分中，我们将\\n\\n看看如何在\\n\\n\\n您的应用程序中有效地使用这些助手，\\n\\n所以首先 让我们看一下\\n\\n如何训练这些助手的新兴配方，\\n\\n请记住，这都是\\n\\n非常新的，并且仍在快速发展，\\n\\n但到目前为止，配方看起来\\n\\n像这样，\\n\\n现在这是一张复杂的幻灯片，\\n\\n所以我要 一点一点地进行，\\n\\n但粗略地说，我们有四个\\n\\n主要阶段：免费训练、监督微调、奖励\\n\\n建模、\\n\\n强化学习，它们按顺序相互跟随，\\n\\n\\n现在在每个阶段，我们都有一个数据集，\\n\\n为该阶段提供动力，我们有\\n\\n一个算法， 就我们的目的而言，这将\\n\\n是\\n\\n训练神经网络的目标，然后我们将\\n\\n得到一个结果模型，然后\\n\\n在底部有一些注释，因此\\n\\n我们要开始的第一个阶段是\\n\\n预训练阶段 现在这个阶段\\n\\n在这个图中有点特殊，这个\\n\\n图不是按比例的，因为这个\\n\\n阶段是所有计算\\n\\n工作基本上发生的地方，这是\\n\\n训练呃计算时间的 99，还有失败\\n\\n，所以这就是我们正在处理的地方\\n\\n互联网规模的数据集，\\n\\n商店计算机中包含数千个 GPU，并且\\n\\n可能需要数月的训练，其他\\n\\n三个阶段是微调阶段，\\n\\n更多的是沿着\\n\\n少量 GPU 和数小时或数天的路线，\\n\\n所以让我们看一下 在预训练\\n\\n阶段，为了首先实现基本模型，\\n\\n我们将收集\\n\\n大量数据，这是\\n\\n我们所谓的数据混合的示例，该示例来自\\n\\nMeta 发布的这篇呃论文，\\n\\n他们在该论文中发布了基于 llama 的模型\\n\\n现在，您可以粗略地看到\\n\\n进入这些集合的数据集类型，\\n\\n因此我们有常见的爬行，\\n\\n这只是一个网络抓取 C4，这也是\\n\\n一个常见的爬行，然后还有一些高质量\\n\\n的数据集，例如\\n\\nGitHub 维基百科图书档案 堆栈\\n\\n交换等等这些都\\n\\n混合在一起，然后\\n\\n根据给定的比例对它们进行采样，\\n\\n形成\\n\\nGPT 神经网络的训练集，然后\\n\\n我们才能真正训练这些\\n\\n数据，我们需要再经历一次\\n\\n预处理步骤，即\\n\\n标记化，这基本上是\\n\\n我们\\n\\n从互联网上抓取的\\n\\n原始文本到整数序列的翻译，因为这是\\n\\ngpts 现在运行的本机表示，\\n\\n这是文本片段\\n\\n和文本片段之间的无损翻译。\\n\\n令牌和整数，\\n\\n此阶段有许多算法，\\n\\n例如，您通常可以使用\\n\\n字节对编码之类的算法，它\\n\\n迭代地合并小文本块\\n\\n并将它们分组为令牌，\\n\\n因此在这里我展示了\\n\\n这些令牌的一些示例块，然后 这是\\n\\n\\n实际输入到 Transformer 的原始整数序列，\\n\\n现在在这里我展示了两个类似的\\n\\n示例，用于\\n\\n控制这个阶段的超参数，所以 gpt4 我们没有\\n\\n发布太多关于\\n\\n它是如何训练的信息等等 我使用的是\\n\\ngpt3 的数字，但 gpt3 当然\\n\\n在大约三年前有点旧了，\\n\\n但 llama 是来自 meta 的一个相当新的模型，\\n\\n所以这些大致是\\n\\n我们在\\n\\n做 pre 时处理的数量级。  - 训练\\n\\n词汇量通常是几个\\n\\n10 000 个标记，上下文长度通常是\\n\\n\\n204000 个，现在甚至是 100000 个，\\n\\n这决定了\\n\\nGPT 在\\n\\n尝试预测下一个整数时将查看的最大整数数量\\n\\n在序列中，\\n\\n您可以看到 llama 现在的参数数量大致为\\n\\n650 亿个，\\n\\n尽管 llama\\n\\n与 gvt3 的 1750\\n\\n亿个参数相比只有 65b 个参数，但 llama 是一个\\n\\n更强大的模型，\\n\\n直观上这是因为该模型\\n\\n经过了显着的训练 在这种\\n\\n情况下，更长的时间是 1.4 万亿个令牌，而不是\\n\\n3000 亿个令牌，因此您不应该\\n\\n仅通过模型包含的参数数量来判断模型的能力，\\n\\n\\n下面我将显示一些通常会出现的\\n\\n粗略超参数的粗略数量表\\n\\n指定\\n\\nTransformer 神经网络，\\n\\n头数、维度大小、\\n\\n层数等，在底部，我\\n\\n显示了一些训练超参数，\\n\\n例如训练 65b 模型元，\\n\\n使用 2000 GPU 大约 21 天\\n\\n的训练 大约\\n\\n几百万美元，所以这是\\n\\n你现在在\\n\\n预训练阶段应该记住的粗略数量级，\\n\\n\\n当我们实际上预链接发生的\\n\\n事情时，粗略地说，我们\\n\\n将拿走我们的代币，我们\\n\\n将它们放入数据批次中，这样我们\\n\\n就有这些数组将输入到\\n\\nTransformer 中，这些数组是 B\\n\\n批次大小，这些都是按\\n\\n行堆叠的独立示例，\\n\\nB 乘 T T 是最大上下文\\n\\n长度，所以在我的图片中 我只有 10.\\n\\n上下文长度，所以这可能是\\n\\n两千四千等等，所以这些是\\n\\n非常长的行，我们所做的就是\\n\\n获取这些文档，将它们打包\\n\\n成行，然后用这些\\n\\n特殊的文本结尾标记来分隔它们 基本上\\n\\n它是在 Transformer 上\\n\\n开始新文档的，\\n\\n所以这里我有一些\\n\\n文档示例，然后我将它们\\n\\n延伸到这个输入中，\\n\\n现在我们将把所有这些\\n\\n数字输入 Transformer，让我让\\n\\n我只关注一个特定的\\n\\n单元格，但\\n\\n该图中的每个单元格都会发生同样的事情，所以\\n\\n让我们看一下绿色单元格，绿色\\n\\n单元格将查看\\n\\n它之前的所有标记，因此其中的所有\\n\\n标记 黄色，我们将把\\n\\n整个上下文输入 Transformer\\n\\n神经网络，\\n\\nTransformer 将尝试\\n\\n预测序列中的下一个标记，\\n\\n在这种情况下为红色，\\n\\n现在 Transformer 不幸的是我没有\\n\\n太多时间去 深入了解\\n\\n这个神经网络架构的全部细节\\n\\n只是\\n\\n为了我们的目的而设计的一大块神经网络东西，\\n\\n它通常有几百亿个参数\\n\\n或类似的东西，\\n\\n当然，当他们调整这些参数时，\\n\\n你会得到稍微不同的呃\\n\\n预测分布 对于\\n\\n这些单元中的每一个，\\n\\n例如，如果我们的词汇量\\n\\n大小是 50 257 个标记，那么我们将\\n\\n拥有那么多数字，因为我们\\n\\n需要指定\\n\\n接下来发生的情况的概率分布，所以\\n\\n基本上我们有一个概率\\n\\n现在在这个具体\\n\\n示例中，接下来将出现特定单元格 513，\\n\\n因此我们可以使用它作为\\n\\n监督源来更新我们的\\n\\nTransformers 权重，因此我们\\n\\n基本上并行地将其应用于每个\\n\\n单元格，并且我们不断交换\\n\\n交换批次 我们正在尝试让\\n\\nTransformer 对序列中\\n\\n接下来出现的标记做出正确的预测，\\n\\n\\n所以让我更具体地向您展示\\n\\n当您训练其中一个\\n\\n模型时这实际上是\\n\\n来自《纽约时报》的模型，并且 他们\\n\\n在莎士比亚上训练了一个小 GPT，所以这是\\n\\n莎士比亚的一小段，他们\\n\\n现在在初始化开始时训练了一个 GPT，\\n\\nGPT 以\\n\\n完全随机的权重开始，所以你也\\n\\n得到完全随机的输出，\\n\\n\\n但随着时间的推移 你训练呃 GPT 的时间\\n\\n越来越长，你会从模型中得到\\n\\n越来越连贯和一致的\\n\\n样本，\\n\\n你从中采样的方式当然是你呃\\n\\n预测接下来会发生什么，你从\\n\\n这个分布中采样，然后你继续提供\\n\\n它 回到这个过程，你\\n\\n基本上可以对大序列进行采样，\\n\\n所以到最后你会看到\\n\\nTransformer 已经学会了单词、在\\n\\n哪里放置空格、在哪里放置\\n\\n逗号等等，所以我们正在做出\\n\\n越来越一致的预测\\n\\n随着时间的推移，\\n\\n这些是您在\\n\\n\\n有效地进行模型预训练时所看到的图表类型，我们在训练\\n\\n时会随着时间的推移查看损失函数，\\n\\n低损失意味着我们的\\n\\nTransformer 正在预测正确的\\n\\n呃 为序列中正确的下一个整数提供更高的概率\\n\\n\\n现在，\\n\\n一旦我们在一个月后训练好该模型，我们将如何处理该模型，\\n\\n我们\\n\\n注意到我们领域的第一件事是这些\\n\\n模型基本上处于以下过程：\\n\\n语言建模学习非常强大的\\n\\n通用表示，并且\\n\\n可以非常有效地\\n\\n针对您可能感兴趣的任何下游任务对它们进行微调，\\n\\n\\n例如，如果您对\\n\\n情感分类感兴趣，\\n\\n过去的方法是收集一堆\\n\\n正面和负面，然后你\\n\\n为此训练某种 NLP 模型，\\n\\n但新方法是忽略\\n\\n情感分类\\n\\n并进行大型语言模型\\n\\n预训练，训练大型 Transformer\\n\\n，然后你只能有\\n\\n几个例子 你可以非常\\n\\n有效地微调\\n\\n该任务的模型，因此这在实践中非常有效，其\\n\\n原因是\\n\\n基本上 Transformer 被迫\\n\\n在语言建模任务中执行大量任务，\\n\\n因为仅就以下方面而言：\\n\\n预测下一个标记，\\n\\n它被迫了解很多关于\\n\\n文本的结构以及\\n\\n它们所包含的所有不同概念，\\n\\n所以这是 GPT 一个现在大约在\\n\\ngpt2 的时间人们注意到，实际上\\n\\n甚至比微调更好\\n\\n非常有效地提示这些模型，\\n\\n所以这些是语言模型\\n\\n，它们想要完成文档，这样\\n\\n你实际上可以\\n\\n通过排列这些\\n\\n假文档来欺骗它们执行任务，所以在这个例子中，\\n\\n例如，我们有一些段落，\\n\\n然后我们有点像这样做 QA qaqa 这被\\n\\n称为“一些镜头提示”，然后我们执行\\n\\nq，然后当 Transformer 尝试\\n\\n完成文档时，它实际上是在\\n\\n回答我们的问题，所以这是一个\\n\\n提示工程基础模型的示例，人们\\n\\n相信它是在\\n\\n模仿 一个文档并让它\\n\\n执行一项任务，\\n\\n所以这拉开了我想说的时代，我想说的是，\\n\\n\\n\\n\\n即使没有训练\\n\\n任何神经网络微调，这实际上也可以很好地解决很多问题 等等，\\n\\n从那时起，我们已经看到了每个人都训练过的\\n\\n基本模型的完整进化树，但\\n\\n并非所有这些\\n\\n模型都可用，例如，\\n\\ngpt4 基本模型从未发布过，\\n\\n您可能通过 API 与之交互的 gpt4 模型\\n\\n则不可用 一个基本模型，它是\\n\\n一个辅助模型，我们将\\n\\n介绍如何获得这些\\n\\n基于 gpt3 的模型可以\\n\\n通过名为 DaVinci 的 API 获得，而 gpt2 基本\\n\\n模型甚至可以作为\\n\\n我们 GitHub 存储库上的权重使用，但目前是最好的\\n\\n可用的基本模型可能是\\n\\n来自meta的Llama系列，尽管它\\n\\n没有商业许可\\n\\n现在需要指出的是基本\\n\\n模型并不能提供帮助，他们\\n\\n不想回答他们不想\\n\\n回答你的问题，他们只是\\n\\n想要 完成文件，所以如果你\\n\\n告诉他们写一首关于该品牌\\n\\n奶酪的诗，你就会知道它会\\n\\n回答更多问题，它\\n\\n只是完成它认为作为\\n\\n文件的内容，\\n\\n但是你可以以\\n\\n特定的方式提示他们基本模型\\n\\n更有可能起作用，因此作为一个例子，\\n\\n这里有一首关于面包和奶酪的诗，\\n\\n在这种情况下，它将正确自动完成，\\n\\n\\n您甚至可以欺骗基础模型来\\n\\n提供帮助，而您\\n\\n这样做的方式是您将创建像\\n\\n特定的几个 镜头提示\\n\\n看起来就像\\n\\n人类和助手之间有某种文档\\n\\n，他们正在交换某种嗯\\n\\n信息，然后在底部你将\\n\\n查询放在最后，\\n\\n基本模型将有点像\\n\\n条件 它本身就像一个\\n\\n有用的助手和某种答案，但这\\n\\n不是很可靠，\\n\\n并且在实践中效果不佳，尽管\\n\\n可以这样做，因此\\n\\n我们有不同的路径来\\n\\n使实际的 GBT 帮助不仅仅是基本\\n\\n模型文档完成器和 因此，这\\n\\n使我们进入监督微调阶段，因此\\n\\n在监督微调阶段，我们\\n\\n将收集少量但高质量\\n\\n的数据集，在这种情况下，我们\\n\\n将要求人工承包商收集表单提示\\n\\n格式的数据，\\n\\n并 理想的响应，我们将\\n\\n收集大量这样的数据，通常是\\n\\n数万个或类似的东西，\\n\\n然后我们仍然会对\\n\\n这些数据进行语言建模，因此\\n\\n算法上不会发生任何变化，我们\\n\\n只是交换训练集，以便它\\n\\n使用 作为互联网文档，这是一种\\n\\n大量的本地\\n\\n四基本上 QA 提示响应类型的\\n\\n数据，并且是低数量\\n\\n高质量的，\\n\\n所以我们仍然进行语言建模，\\n\\n然后在训练之后我们得到一个 sfd 模型\\n\\n，你可以实际部署这些模型，\\n\\n它们是 实际的助理，他们\\n\\n在某种程度上工作让我向您\\n\\n展示示例演示可能是什么样子\\n\\n所以这是人类\\n\\n承包商可能想出的东西这是\\n\\n一些随机提示您可以写一个关于术语垄断或类似术语\\n\\n的相关性的简短介绍吗\\n\\n\\n然后承包商也会写出\\n\\n一个理想的回复，当他们\\n\\n写出这些回复时，他们会遵循\\n\\n大量的标签文件，并被\\n\\n要求提供帮助、\\n\\n真实且无害，而\\n\\n这里的标签说明您可能无法\\n\\n阅读其他可以 我但是它们很长，\\n\\n这只是人们遵循\\n\\n指示并尝试完成\\n\\n这些提示，\\n\\n这就是数据集的样子，\\n\\n您可以训练这些模型，这\\n\\n在某种程度上有效，\\n\\n现在您实际上可以\\n\\n从这里继续管道， 进入来自\\n\\n人类反馈的 rlhf 强化学习，\\n\\n其中包括奖励\\n\\n建模和强化学习，所以\\n\\n让我介绍这一点，然后我会\\n\\n回到为什么你可能想要执行\\n\\n额外的步骤，以及它与 sft 模型相比如何，\\n\\n\\n所以 奖励建模步骤\\n\\n我们要做的就是将\\n\\n我们的数据收集转变为\\n\\n比较的形式，所以这里是\\n\\n我们的数据集的一个例子\\n\\n我有相同的提示相同的提示\\n\\ntop 要求助手\\n\\n编写一个程序或函数来\\n\\n检查给定的字符串是否是回文\\n\\n，然后我们要做的是采用\\n\\n我们已经训练过的 sft 模型并\\n\\n创建多个完成，因此在本\\n\\n例中我们有三个\\n\\n模型创建的完成结果，\\n\\n然后我们要求人们对这些\\n\\n完成情况进行排名，因此，如果您盯着它\\n\\n一段时间，顺便说一下，\\n\\n比较其中一些\\n\\n预测是非常困难的，这可能会花费\\n\\n人们甚至几个小时的时间 单个提示\\n\\n完成对，但假设我们\\n\\n认为其中一个比\\n\\n其他更好，依此类推，因此我们对它们进行排名，\\n\\n\\n然后我们可以用\\n\\n看起来非常像对这些之间\\n\\n所有可能的对进行二元分类的东西来跟踪\\n\\n\\n所以我们现在要做的就是将\\n\\n提示按行布局，并且\\n\\n这里所有三行的提示都是相同的，所以提示\\n\\n都是相同的，但是\\n\\n完成的情况有所不同，所以\\n\\n黄色标记来自 sft\\n\\n模型，那么我们的 要做的是我们在最后附加\\n\\n另一个特殊奖励呃读出令牌\\n\\n，我们基本上只\\n\\n在这个绿色令牌上监督 Transformer，\\n\\n并且 Transformer 会\\n\\n预测一些奖励，以判断\\n\\n该提示的完成程度\\n\\n，因此基本上它会猜测\\n\\n每个完成的质量，然后\\n\\n一旦它对每个完成进行猜测，\\n\\n我们也有基本事实，它\\n\\n告诉我们它们的排名，因此\\n\\n我们实际上可以强制其中一些\\n\\n数字应该比其他数字高得多，\\n\\n依此类推 我们将其公式化为\\n\\n损失函数，并训练我们的模型\\n\\n做出奖励预测，该预测\\n\\n\\n与所有这些承包商的比较得出的基本事实一致，\\n\\n这就是我们训练\\n\\n奖励模型的方式，并让我们能够对\\n\\n完成情况进行评分 对于提示，\\n\\n一旦我们有了奖励模型，我们就\\n\\n无法部署它，因为这\\n\\n本身作为助手不是很有用，\\n\\n但对于现在接下来的强化\\n\\n强化学习阶段非常有用，\\n\\n因为我们有一个\\n\\n可以评分的奖励模型 任何\\n\\n给定\\n\\n提示的任意完成的质量，所以我们在\\n\\n强化学习期间所做的就是我们基本上\\n\\n再次得到大量提示，\\n\\n现在我们\\n\\n针对奖励模型进行强化学习，所以\\n\\n这就是\\n\\n我们采取的单一提示 提示我们将其排成行，\\n\\n现在我们使用 sft 我们\\n\\n基本上使用我们想要\\n\\n训练的模型，该模型在 sft 模型中初始化\\n\\n以创建一些黄色的完成\\n\\n，然后我们再次附加奖励令牌\\n\\n，我们读出\\n\\n根据\\n\\n现在保持固定的奖励模型进行奖励，它不再改变\\n\\n，现在奖励模型告诉我们\\n\\n每个提示的每个完成的质量\\n\\n，所以我们\\n\\n可以做的是我们现在可以基本上\\n\\n应用相同的 语言建模损失\\n\\n函数，但我们目前正在对\\n\\n黄色标记进行训练，并且我们正在通过\\n\\n\\n奖励模型指示的奖励来权衡语言建模目标，因此\\n\\n作为第一行的示例，\\n\\n奖励模型表示这是一个相当\\n\\n高的分数 因此，\\n\\n我们在第一行中碰巧采样的所有令牌都\\n\\n将得到强化，\\n\\n并且它们将在未来获得更高的\\n\\n概率，\\n\\n相反，在第二行中，奖励\\n\\n模型确实不喜欢这种\\n\\n完成负数 1.2，所以 因此，\\n\\n我们在第二行中采样的每个令牌\\n\\n\\n未来的概率都会稍高一些，\\n\\n我们在许多批次的许多提示中一遍又一遍地执行此操作，\\n\\n\\n基本上我们得到了一个\\n\\n在这里创建黄色令牌的策略，它基本上全部\\n\\n根据我们在前一阶段训练的奖励模型，这里所有的完成都会\\n\\n得分很高，这\\n\\n\\n就是我们训练的方式，呃，这就是\\n\\n\\n现在的 rohf 管道，最后你会得到一个\\n\\n可以部署的模型，所以 举个\\n\\n例子，聊天 GPT 是 rlhf 模型，但\\n\\n您可能会遇到的其他一些模型，\\n\\n例如 kuna 13B 等，\\n\\n这些都是 sft 模型，因此我们\\n\\n有基本模型 sft 模型和 rlh\\n\\n模型，这有点像状态\\n\\n\\n现在为什么你想做 rlhf 所以一个\\n\\n不太令人兴奋的答案是\\n\\n它效果更好所以这\\n\\n来自导师 GBT 论文\\n\\n根据不久前的这些实验\\n\\n现在这些 PPO 模型是 rlhf 并且我们\\n\\n看到当我们把它们给人类时，它们基本上只是\\n\\n在很多比较中被首选，\\n\\n所以人类只是更\\n\\n喜欢\\n\\n来自我们宗教模型的基本令牌，与 sft\\n\\n模型相比，与被\\n\\n提示成为助手的基本模型相比，所以它\\n\\n只是 效果更好，但你可能会问为什么\\n\\n它效果更好，我不\\n\\n认为有一个像\\n\\n社区真正\\n\\n同意的令人惊奇的答案，但我只会提供\\n\\n一个可能的原因，它与 比较\\n\\n与生成在计算上的容易程度之间存在不对称性，\\n\\n\\n所以让我们以\\n\\n生成俳句为例假设我要求一个模型\\n\\n写一篇关于回形针的俳句，如果\\n\\n你是一个试图提供火车\\n\\n数据的承包商，那么想象一下你是一个\\n\\n基本上收集的承包商 sft 阶段的数据\\n\\n你应该如何为回形针创建一个\\n\\n漂亮的俳句 你可能\\n\\n不太擅长，但如果我\\n\\n给你一些俳句的例子，你\\n\\n可能会比其中一些\\n\\n俳句更欣赏\\n\\n因此，判断其中哪一个是好的是\\n\\n更容易的任务，因此基本上这种\\n\\n不对称使得比较成为\\n\\n一种更好的方式，可以潜在地利用\\n\\n自己作为一个人的能力，并且你的判断力可以\\n\\n创建一个稍微更好的模型，\\n\\n现在我们的宗教模型不是\\n\\n\\n在某些情况下，严格地对基本模型进行了改进，因此\\n\\n我们特别注意到，例如它们丢失了\\n\\n一些熵，这意味着它们\\n\\n给出了更多 PT 结果，它们可以输出\\n\\n更低的变化，就像它们可以输出\\n\\n比基本模型具有更低变化的样本一样\\n\\n所以基础模型有很多熵\\n\\n，我们会使用很多不同的\\n\\n输出，\\n\\n例如，\\n\\n我仍然更喜欢使用基础模型的一种地方\\n\\n是在设置中，\\n\\n你基本上有 N 个东西，并且你想要\\n\\n生成更多 类似的东西，所以\\n\\n这是我刚刚编写的一个例子，\\n\\n我想生成很酷的神奇宝贝名称，我\\n\\n给了它七个神奇宝贝名称，我要求\\n\\n基本模型完成文档，\\n\\n它给了我更多神奇宝贝名称，\\n\\n呃这些都是虚构的 我试着\\n\\n查找它们，我不相信它们是真正的\\n\\n口袋妖怪，呃，这是\\n\\n我认为基本模型擅长的任务，\\n\\n因为它仍然有很多熵，它\\n\\n会给你很多不同的很酷的东西\\n\\n更多的东西看起来像\\n\\n你之前给的任何东西，\\n\\n所以这就是呃这是数字，\\n\\n所有这些都说这些有点像\\n\\n\\n你现在可能可以使用的助理模型呃，\\n\\n伯克利有一个团队排名\\n\\n很多可用的辅助模型，\\n\\n并给了它们基本上 ELO 评级 所以\\n\\n目前一些最好的模型当然\\n\\n是 gpt4 我想说其次\\n\\n是 Claude gvt 3.5 然后是\\n\\n许多模型 其中一些可能可以作为\\n\\n权重使用，例如 kuna 考拉\\n\\n等，这里的前三行都是\\n\\nrohf 模型，据\\n\\n我所知，所有其他模型都是 sft\\n\\n模型，我相信\\n\\n没问题，所以这就是我们在高水平上训练这些模型的方式，\\n\\n现在我要\\n\\n切换齿轮 让我们看看如何\\n\\n最好地应用 GPT 助手\\n\\n模型来解决您的问题现在我想\\n\\n\\n在一个具体示例的设置中工作所以让我们\\n\\n\\n在这里使用一个具体示例\\n\\n假设您正在写一篇\\n\\n文章或一个 博客文章，你要\\n\\n在最后写下这句话\\n\\n加利福尼亚州的人口是\\n\\n阿拉斯加的 53 倍，所以出于某种原因你想\\n\\n比较这两个州的人口，\\n\\n\\n想想丰富的内心独白\\n\\n和工具的使用以及实际工作量\\n\\n在你的大脑中进行计算以\\n\\n生成最后一个句子，所以\\n\\n这可能是\\n\\n你大脑中的样子好吧，下一步\\n\\n让我写博客呃我的博客让我\\n\\n比较这两个群体\\n\\n好吧首先我显然需要\\n\\n现在我\\n\\n知道我可能不知道这些\\n\\n人群，所以\\n\\n我有点像意识到我知道什么或\\n\\n不知道我的自我认知，\\n\\n所以我去我 使用一些工具，我访问\\n\\n维基百科，查找加利福尼亚州的\\n\\n人口和阿拉斯加的人口，\\n\\n现在我知道我应该将两者相除，\\n\\n但我再次知道，将 3 9.2\\n\\n除以 0.74 不太可能成功，\\n\\n这不是我想要的那种事情 我可以\\n\\n在脑海中完成，因此我将\\n\\n依赖计算器，因此我将\\n\\n使用计算器输入它并查看\\n\\n输出大约为 53。\\n\\n然后也许我会\\n\\n在我的大脑中进行一些反思和健全性检查 所以 53\\n\\n很有意义吗？这是一个相当\\n\\n大的分数，但加利福尼亚是\\n\\n最受欢迎的州，所以也许\\n\\n看起来不错，所以\\n\\n然后我有了我\\n\\n可能需要的所有信息，现在我开始\\n\\n写作的创造性部分，所以我可能\\n\\n开始写一些像加州\\n\\n有 53 倍大的东西，然后我认为这\\n\\n实际上是非常\\n\\n尴尬的措辞，所以让我实际上\\n\\n删除它并让我再试一次，所以\\n\\n当我写的时候，我有一个单独的\\n\\n过程几乎检查我的内容 我正在\\n\\n写作并判断它是否看起来\\n\\n不错，\\n\\n然后也许我会删除它，也许我会\\n\\n重新构建它，然后也许我对结果感到满意\\n\\n\\n所以基本上是长话短说，在\\n\\n你的内心独白方面，在幕后发生了很多事情\\n\\n你创建\\n\\n这样的句子，但是\\n\\n当我们在\\n\\n其上训练 GPT 时，这样的句子是什么样的？\\n\\n从 gpt 的角度来看，这只是一个\\n\\n标记序列，\\n\\n所以 GPT 当它读取或生成\\n\\n这些标记时，它就会运行，\\n\\n每个块大致是\\n\\n每个 token 的计算工作量相同，\\n\\n这些 Transformer 并不是\\n\\n很浅的网络，它们有大约 80\\n\\n层推理，但 80 层仍然\\n\\n不算太多，所以这个 Transformer\\n\\n将尽力模仿，但\\n\\n当然这里的过程看起来\\n\\n与您采取的过程非常不同，\\n\\n特别是在\\n\\n我们创建的数据集中的最终工件中，然后\\n\\n最终将其提供给 llms，所有\\n\\n内部对话都被完全剥离\\n\\n，呃，与您不同的是，GPT 会查看\\n\\n每一个标记 并且\\n\\n在每一个上花费相同的计算量，\\n\\n所以你不能指望它实际上会\\n\\n很好，你不能指望它对\\n\\n每个令牌做太多的工作，\\n\\n所以特别是基本上\\n\\n这些 Transformer 就像令牌\\n\\n模拟器一样，所以他们不知道自己\\n\\n不知道什么，就像他们只是模仿\\n\\n下一个令牌一样，他们不知道自己\\n\\n擅长或不擅长什么，他们只是\\n\\n尽力模仿他们不擅长的下一个令牌\\n\\n不会在循环中反映出来，他们不会进行\\n\\n理智检查，默认情况下他们不会纠正\\n\\n错误，\\n\\n他们只是呃样本标记序列，\\n\\n\\n他们的头脑中没有单独的内部独白流来\\n\\n评估现在正在发生的事情\\n\\n我想说的是，他们确实有某种认知优势，那\\n\\n就是他们\\n\\n实际上拥有\\n\\n跨大量领域的大量基于事实的知识，\\n\\n因为他们有数百亿个\\n\\n参数，因此需要大量存储空间来存储\\n\\n大量数据。 事实，\\n\\n但我认为它们也有一个\\n\\n相对较大和完美的工作\\n\\n记忆，所以无论固定\\n\\n在上下文窗口中的什么内容，都可以\\n\\n\\n通过其内部的自我注意\\n\\n机制立即供 Transformer 使用，所以它有点像\\n\\n完美记忆，但它是 得到了有限的\\n\\n大小，但是 Transformer 可以非常\\n\\n直接地访问它，因此它可以\\n\\n无损地记住\\n\\n其上下文窗口内的任何内容，\\n\\n这就是我比较\\n\\n这两者的方式，我提出所有\\n\\n这些的原因是因为我 我认为在\\n\\n很大程度上，提示只是弥补\\n\\n这两种架构之间的认知差异，\\n\\n\\n就像我们的大脑和 llm 的大脑，\\n\\n你几乎可以这样看，\\n\\n所以这是人们发现的一件事，\\n\\n例如，效果很好 在\\n\\n实践中，特别是如果您的任务\\n\\n需要推理，您不能期望\\n\\nTransformer 对\\n\\n每个令牌进行太多推理，因此您必须\\n\\n真正将推理分散到\\n\\n越来越多的令牌上，例如您\\n\\n不能给 Transformer 一个非常\\n\\n复杂的问题，并期望它能\\n\\n在一个令牌中得到答案，\\n\\n只是没有足够的时间，这些\\n\\n变形金刚需要令牌来思考\\n\\n引用不引用我有时喜欢说，所以\\n\\n这是一些效果\\n\\n很好的东西，例如你可以 有一些镜头\\n\\n提示，向 Transformer 表明\\n\\n它应该在回答问题时展示其工作，\\n\\n\\n如果您给出一些\\n\\n示例，Transformer 将模仿\\n\\n该模板，并且最终会\\n\\n表现得更好\\n\\n\\n此外，您可以\\n\\n通过说让我们\\n\\n一步一步思考来从 Transformer 中引出这种行为，因为这种\\n\\n条件是 Transformer\\n\\n类似于显示其工作，并且因为它会\\n\\n突然进入显示其工作的模式，因此它会\\n\\n\\n每个令牌做的计算工作更少，因此它更有可能\\n\\n成功，因为\\n\\n\\n随着时间的推移，它会使推理速度变慢，这是\\n\\n另一个例子，这被\\n\\n称为自我一致性，我们看到我们\\n\\n有能力开始编写，\\n\\n然后如果没有的话' 不解决我可以再试\\n\\n一次，我可以尝试多次，\\n\\n呃，也许\\n\\n嗯，选择最有效的一个，所以在\\n\\n这些方法中，你可能\\n\\n不仅仅采样一次，但你可能采样\\n\\n多次，然后有一些\\n\\n过程来找到 那些\\n\\n不好的，然后只保留这些样本\\n\\n或进行多数投票或\\n\\n类似的事情，所以基本上这些\\n\\n变形金刚在这个过程中，因为他们\\n\\n像你一样预测下一个令牌，\\n\\n他们可能会运气不好，他们\\n\\n可以采样而不是一个 非常好的令牌\\n\\n，他们在推理方面可能会像死胡同一样走下去，\\n\\n所以\\n\\n与你不同的是，他们无法从中恢复过来，他们\\n\\n被困在他们采样的每一个令牌上\\n\\n，所以他们会继续这个\\n\\n序列，即使他们甚至知道\\n\\n这个 序列不会成功，\\n\\n所以让他们能够回顾\\n\\n检查或尝试找到呃尝试\\n\\n基本上围绕它进行采样这是\\n\\n一种技术，你也可以事实\\n\\n证明，实际上llms就像他们\\n\\n知道什么时候搞砸了一样 举个\\n\\n例子，\\n\\n假设你要求模型生成一首\\n\\n不押韵的诗，它可能\\n\\n会给你一首诗，但它实际上押韵，\\n\\n但事实证明，特别是对于\\n\\n像 gpt4 这样的更大的模型，你可以只问\\n\\n它你是否满足了作业并且\\n\\n实际上 gpt4 非常清楚它\\n\\n没有满足任务要求，它\\n\\n只是在采样中有点不幸，所以它\\n\\n会告诉你不，我实际上没有满足\\n\\n任务要求，让我再试一次，\\n\\n但如果没有你提示它它\\n\\n甚至不会 就像它不知道它不\\n\\n知道重新访问等等所以你\\n\\n必须在提示中弥补这一点\\n\\n你必须让它检查如果你不\\n\\n要求它检查它就不会检查\\n\\n就其本身而言，它只是一个令牌模拟器，\\n\\n我认为更一般地说，这些技术中的很多都属于\\n\\n\\n我所说的重新创建我们的系统的范畴，因此\\n\\n您可能会熟悉系统\\n\\n一系统，以人类的角度思考系统\\n\\n一是一个快速自动过程 我\\n\\n认为有点像法学硕士，\\n\\n只是对令牌进行采样，\\n\\n而系统二是你大脑中较慢的深思熟虑的\\n\\n计划，\\n\\n所以这是一篇实际上是\\n\\n上周的论文，因为这个空间正在\\n\\n快速发展，它被称为\\n\\n思想树 在思想树上，呃，\\n\\n这篇论文的作者建议\\n\\n为任何\\n\\n给定的提示维护多个完成，然后他们还会\\n\\n一路上对它们进行评分，并保留\\n\\n那些进展顺利的（如果\\n\\n有意义的话），所以很多人\\n\\n真的很喜欢\\n\\n\\n\\n\\n现在，我想在这里指出的一件事\\n\\n是，这不仅仅是一个提示，这\\n\\n实际上是提示\\n\\n与一些 python 胶水代码一起使用，因为\\n\\n你实际上不需要维护\\n\\n多个提示，而且你还必须\\n\\n在这里做一些树搜索算法，\\n\\n比如找出要扩展的提示\\n\\n等等，所以它是 python 胶水代码的共生\\n\\n和\\n\\n在 while 循环或更大的算法中调用的单独提示\\n\\n\\n我还认为这里与 alphago 有一个非常酷的\\n\\n相似之处 alphago 有一个\\n\\n在下围棋时放置下一颗棋子的策略，\\n\\n它的策略\\n\\n最初是通过模仿人类来训练的，但\\n\\n除此之外 对于这个策略，它也进行\\n\\n蒙特卡罗树搜索，基本上它\\n\\n会在头脑中发挥出多种可能性\\n\\n并评估所有这些可能性，\\n\\n只保留那些工作良好的，所以\\n\\n我认为这有点类似于\\n\\nalphago，但对于 文本，\\n\\n如果这有意义的话，\\n\\n就像思想树一样，我认为\\n\\n更普遍的是，人们开始真正\\n\\n喜欢探索更通用的\\n\\n技术，不仅仅是简单的\\n\\n问题答案提示，而且\\n\\n看起来更像是Python蓝色\\n\\n代码，将许多问题串在一起，\\n\\n等等 右边我有一个\\n\\n名为“反应”的论文的例子，其中他们将\\n\\n提示的答案构建为一系列\\n\\n思想行动观察思想\\n\\n行动观察，这是一个全面的\\n\\n推出，一种回答查询的思维过程，\\n\\n在这些行动中\\n\\n模型 也允许\\n\\n在左边输掉我有一个 rogpt 的例子，顺便说一句，\\n\\n现在 rgbt 变成了一个\\n\\n项目，我认为最近得到了很多炒作，\\n\\n我想呃，但我认为我\\n\\n仍然发现它有点鼓舞人心，很\\n\\n有趣 嗯，这是一个\\n\\n允许 llm 保留任务列表\\n\\n并继续递归地分解\\n\\n任务的项目，我认为这目前\\n\\n效果不是很好，我不会建议\\n\\n人们在实际应用中使用它，\\n\\n我只是认为它是\\n\\n通常会从事情的发展方向中获得灵感，\\n\\n我认为随着时间的推移，这\\n\\n\\n有点像让我们的模型\\n\\n系统思考下一件我\\n\\n觉得有趣的事情是\\n\\n下面的调查会说几乎\\n\\n心理上的 llms 的怪癖是 llms\\n\\n不 不想成功，他们想\\n\\n模仿\\n\\n你，想要成功，你应该提出\\n\\n要求，\\n\\n所以我的意思是，\\n\\n当变形金刚接受训练时，他们有\\n\\n训练集，并且他们的训练数据中可能有完整的\\n\\n性能质量范围，\\n\\n因此对于 例如，\\n\\n可能会出现某种\\n\\n物理问题或类似问题的提示，\\n\\n并且可能存在完全错误的学生解决方案，\\n\\n但\\n\\n也可能存在\\n\\n非常正确的专家答案，而变形金刚无法区分\\n\\n之间的区别 看起来我的\\n\\n意思是他们知道他们知道低\\n\\n质量解决方案和高质量\\n\\n解决方案，但默认情况下他们想要\\n\\n模仿所有这些，因为他们只是接受\\n\\n过语言建模培训，所以在\\n\\n测试时你实际上必须要求\\n\\n良好的性能，所以在 本文中的这个例子\\n\\n是，嗯，他们尝试了各种\\n\\n提示，让我们认为一步一步\\n\\n非常强大，因为它有点像\\n\\n展开剩余标记的推理，\\n\\n但更好的是\\n\\n让我们以一步一步的方式解决这个问题\\n\\n确保我们有正确的答案，\\n\\n所以这有点像\\n\\n获得正确答案的条件，这\\n\\n实际上使 Transformer 工作得\\n\\n更好，因为 Transformer\\n\\n现在不必\\n\\n在低质量解决方案上对冲其概率质量，\\n\\n就像那样荒谬 听起来，所以基本上不要\\n\\n随意要求一个强有力的解决方案，\\n\\n比如你是\\n\\n这个话题的领先专家，假装你有智商\\n\\n120等，但不要试图要求\\n\\n太多的智商，因为如果你要求的智商\\n\\n比如 400，你可能会脱离数据\\n\\n分发，或者更糟糕的是，你可能会\\n\\n参与一些科幻小说之类的数据分发，\\n\\n它会开始喜欢\\n\\n接受一些科幻小说或角色扮演\\n\\n或类似的东西，所以你必须\\n\\n找到合适的智商量 我认为\\n\\n\\n接下来会有一些 U 形曲线，\\n\\n正如我们在尝试解决\\n\\n我们知道自己擅长和\\n\\n不擅长的问题时所看到的那样，并且我们依靠计算工具\\n\\n希望对\\n\\n您的 LLMS 做同样的事情，所以\\n\\n特别是我们可能希望为他们提供\\n\\n计算器、代码解释器等\\n\\n进行搜索的能力，并且有\\n\\n很多技术可以做到这一点，\\n\\n需要再次记住的是，\\n\\n这些 默认情况下，变压器可能不\\n\\n知道他们不知道什么，因此您\\n\\n甚至可能想在提示中告诉变压器\\n\\n您不太擅长\\n\\n心算，每当您需要进行非常\\n\\n大的数字加法乘法或\\n\\n其他任何操作时，请使用这里的计算器\\n\\n你如何使用计算器使用\\n\\n这个标记组合等等所以你必须\\n\\n真正喜欢拼写它\\n\\n因为默认情况下的模型不\\n\\n知道它擅长或不擅长什么，\\n\\n就像你和我一样，你和\\n\\n我可能是\\n\\n下一个 我认为非常\\n\\n有趣的是，我们从一个只进行检索的世界一路\\n\\n\\n摆动到了另一个极端，\\n\\n在那里它的记忆仅存在于镜头中，但\\n\\n实际上在\\n\\n这些检索增强\\n\\n模型之间存在着整个空间，并且这个作品 在实践中非常好，\\n\\n\\n正如我提到的，变压器的上下文窗口\\n\\n是它的工作内存，如果您\\n\\n可以使用与任务相关的任何信息加载工作内存，则\\n\\n\\n模型将工作得非常好，\\n\\n因为它可以立即访问所有\\n\\n内存\\n\\n，所以我 我认为很多人\\n\\n真的对\\n\\n呃基本上检索增强\\n\\n生成感兴趣，在底部我有\\n\\n一个 llama 索引的例子，它是一种\\n\\n连接到许多\\n\\n不同类型数据的数据连接器，你可以呃\\n\\n你可以做的一切 可以索引所有\\n\\n这些数据，并且可以使其可供\\n\\nllms 和新兴配方访问，您可以\\n\\n获取相关文档，将\\n\\n它们分成多个块，然后嵌入所有这些数据\\n\\n，基本上得到了\\n\\n表示数据的嵌入向量，您将\\n\\n其存储在 矢量存储，然后在\\n\\n测试时，您\\n\\n对矢量存储进行某种查询，并获取\\n\\n可能与您的任务相关的块，并将\\n\\n它们填充到提示中，\\n\\n然后生成，因此这\\n\\n在实践中可以很好地工作，所以这 我\\n\\n认为类似于当你和我解决\\n\\n问题时，你可以凭记忆做所有事情，\\n\\n变形金刚有非常大\\n\\n和广泛的记忆，但它也确实\\n\\n有助于参考一些主要\\n\\n文档，所以当你发现\\n\\n自己回到教科书时\\n\\n查找某些内容，或者每当您发现\\n\\n自己要返回\\n\\n库的文档以查找\\n\\n转换变压器时，绝对当我也这样\\n\\n做时，您有一个记忆，您对\\n\\n\\n库的某些文档如何工作有一些记忆，但它要好得多\\n\\n查找它，所以这同样适用于\\n\\n\\n接下来我想简单地谈谈\\n\\n约束提示我也发现这\\n\\n非常有趣\\n\\n呃这基本上是\\n\\n\\n在 llms 的输出中强制使用某个模板的技术\\n\\n所以指导\\n\\n实际上是来自微软的一个例子 呃，\\n\\n这里我们强制\\n\\nllm 的输出将是 Json，这\\n\\n实际上保证了输出将\\n\\n采用这种形式，因为它们进入并\\n\\n扰乱了\\n\\n从 Transformer 出来的所有不同令牌的概率，\\n\\n并且如果 他们夹紧这些\\n\\n标记，然后 Transformer 仅\\n\\n填充此处的空白，然后您\\n\\n可以对\\n\\n可以进入这些空白的内容实施额外的限制，因此这\\n\\n可能非常有用，我认为这种\\n\\n约束采样也\\n\\n非常有趣，\\n\\n我也想要 就\\n\\n微调而言，您可以在\\n\\n快速工程方面取得很大进展，\\n\\n但也可以考虑\\n\\n微调您的模型，现在微调\\n\\n模型意味着您实际上将\\n\\n更改模型的权重 模型\\n\\n\\n在实践中做到这一点变得更加容易，这是因为\\n\\n\\n最近已经开发了许多技术并拥有用于呃的库，\\n\\n例如\\n\\n像 Laura 这样的参数高效微调技术，\\n\\n确保您只 趋势是，\\n\\n您只训练模型的小型 Spar 种类，\\n\\n因此大部分模型都\\n\\n被固定在基础模型上，并且\\n\\n允许更改其中的某些部分，\\n\\n这在经验上仍然工作得很好，\\n\\n并且使得仅进行调整的成本要低得多\\n\\n模型的小部分，\\n\\n呃，这也意味着，\\n\\n因为模型的大部分都是固定的，所以\\n\\n您可以使用非常低的精度推理\\n\\n来计算这些部分，因为它们\\n\\n不会通过梯度下降进行更新\\n\\n，因此这使得一切变得\\n\\n更加高效 此外，正如\\n\\n\\n我提到的，目前我们有许多基于开源高质量的模型，\\n\\n我认为 llama 非常好，\\n\\n尽管它没有商业许可，\\n\\n我相信现在需要\\n\\n记住的一些事情是，\\n\\n基本上微调是一个 涉及更多的\\n\\n技术，它需要更多\\n\\n的技术，我认为\\n\\n要做正确的事情需要技术专业知识，它需要人类数据\\n\\n承包商来处理数据集和/或\\n\\n合成数据管道，这可能\\n\\n非常复杂，这肯定会\\n\\n大大减慢您的迭代周期，\\n\\n我会 说在高水平上 sft 是\\n\\n可以实现的，因为它只是你\\n\\n持续的语言建模任务它\\n\\n相对简单，但 rlhref 我\\n\\n想说这是一个非常多的研究\\n\\n领域，甚至更难\\n\\n开始工作，所以我可能不会\\n\\n建议有人只是尝试 推出\\n\\n自己的 rlh 实现，这些\\n\\n东西非常不稳定，非常\\n\\n难以训练，呃，\\n\\n我认为现在对初学者非常友好，\\n\\n而且它也可能会\\n\\n很快发生变化，\\n\\n所以我认为这些是我的默认\\n\\n建议 现在，\\n\\n我会将您的任务分为两个\\n\\n主要部分，第一部分实现您的最佳\\n\\n性能，第二部分按此顺序优化您的\\n\\n性能第一，\\n\\n\\n目前最好的性能来自 GT4 模型，它是\\n\\n迄今为止最有能力\\n\\n使用提示的 非常详细，他们\\n\\n有很多任务内容，相关的\\n\\n相关信息和说明，就像\\n\\n\\n如果任务承包商无法给\\n\\n您回复电子邮件，您会告诉他们什么一样，但也要\\n\\n记住，任务承包商是人\\n\\n，他们有内在的想法。 独白，\\n\\n他们非常聪明等等，LLM不\\n\\n具备这些品质，所以一定要仔细\\n\\n思考LLM的心理，\\n\\n并提供提示，\\n\\n检索并在这些提示中添加任何相关的上下文\\n\\n和信息，这些提示\\n\\n基本上指的是很多提示\\n\\n工程技术其中一些\\n\\n在上面的幻灯片中突出显示，但这\\n\\n是一个非常大的空间，我\\n\\n只是建议您在网上寻找快速的\\n\\n工程技术，那里\\n\\n有很多内容要涵盖，\\n\\n用几个商店示例进行实验，\\n\\n这指的是您不知道的内容 不仅仅是想\\n\\n告诉你想尽可能地展示，\\n\\n所以给它提供\\n\\n所有的例子，帮助它真正\\n\\n理解你的意思，如果你可以使用\\n\\n工具和插件来卸载\\n\\n对 llms 来说很难的任务，\\n\\n然后考虑不仅仅是 一个\\n\\n提示和答案考虑潜在的\\n\\n链和反射，以及如何将\\n\\n它们粘合在一起以及如何\\n\\n可能制作多个样本等等\\n\\n\\n最后，如果您认为自己已经挤出了\\n\\n提示工程，我认为您\\n\\n应该坚持一段时间看看 在\\n\\n某些方面可能会根据\\n\\n您的应用程序对模型进行微调，但预计这会\\n\\n慢得多并且不断发展，然后\\n\\n\\n这里有一个专家脆弱的研究区，我会说这是 rlhf，如果可以的话，它\\n\\n目前确实比 sft 工作得更好一些\\n\\n让它工作，但\\n\\n我想说的是，这非常复杂，\\n\\n为了优化你的成本，尝试\\n\\n探索较低容量的模型或更短的\\n\\n提示等等，\\n\\n我还想就\\n\\n我认为 llms 目前的用例说几句话\\n\\n非常适合，所以\\n\\n特别要注意的是，\\n\\n今天的 llms 有很多限制，\\n\\n所以我会在\\n\\n所有应用程序模型中牢记这一点，\\n\\n顺便说一句，这可能是一个完整的\\n\\n演讲，所以我没有时间\\n\\n详细介绍模型可能有偏见 他们\\n\\n可能编造幻觉信息\\n\\n他们可能有推理错误 他们可能\\n\\n在整个类别的\\n\\n应用程序中陷入困境 他们有知识限制，\\n\\n因此他们可能不知道上述任何信息\\n\\n说 2021 年 9 月 他们很\\n\\n容易受到大范围的影响\\n\\n有点像\\n\\nTwitter 上每天都会出现的攻击，包括即时注入\\n\\n越狱攻击、数据中毒攻击\\n\\n等，所以我现在的建议\\n\\n是在低风险应用程序中使用 llms，将\\n\\n它们与人工监督结合起来，将\\n\\n它们作为\\n\\n灵感和建议的来源 并认为\\n\\n副驾驶而不是完全\\n\\n自主的代理就像在\\n\\n某个地方执行任务一样，只是\\n\\n\\n现在还不清楚模型是否在那里，\\n\\n所以我想说 gpt4\\n\\n是一个令人惊叹的神器，我非常感谢\\n\\n它的存在 它很漂亮，\\n\\n它拥有跨多个领域的大量知识，\\n\\n可以编写数学代码等等，\\n\\n此外，还有一个蓬勃发展的生态\\n\\n系统，其中包括正在构建的所有其他内容，\\n\\n并将其纳入\\n\\n生态系统中，其中一些\\n\\n东西是这些 我已经讨论过的事情，\\n\\n所有这些功能都触手可及，\\n\\n\\n所以这里是向\\n\\nGPT 询问问题以\\n\\n提示它并获得响应所需的一切代码，在这种\\n\\n情况下我说，呃，你能说一些\\n\\n启发性的话吗？  Microsoft build 2023 的观众，\\n\\n\\n我刚刚将其输入 Python 并\\n\\n逐字 gpt4 说了以下内容，呃，顺便说一句，\\n\\n我不知道\\n\\n他们在主题演讲中使用了这个技巧，所以我\\n\\n认为我很聪明，\\n\\n但它真的很擅长\\n\\n女士们、先生们，入侵者和\\n\\n开拓者，微软构建 2023\\n\\n欢迎来到杰出思想的聚会，\\n\\n你们是\\n\\n未来的建筑师，\\n\\n塑造\\n\\n人类繁荣发展的数字领域的远见者，拥抱\\n\\n技术的无限可能性，让\\n\\n您的想法飞翔 发挥你的\\n\\n想象力，让我们一起为子孙后代创造一个更加\\n\\n互联、非凡、包容的世界，\\n\\n准备好\\n\\n释放你的创造力，画布\\n\\n未知的事物，将梦想变成现实，\\n\\n你的旅程从今天开始\\n\\n好吗？\\n\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it is good practice to use the with keyword when dealing with file objects\n",
    "with open('build.txt', encoding=\"utf-8\") as f:\n",
    "    read_data = f.read()\n",
    "\n",
    "read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5d9e989-b30b-43d7-b012-9f936ca831f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"foreign  [Music]  researcher and founding member of open  AI Andre carpathy  [Applause]  hi everyone  I'm happy to be here to tell you about  the state of gbt and more generally  about the rapidly growing ecosystem of  large language models  so I would like to partition the talk  into two parts  in the first part I would like to tell  you about how we train GPT assistance  and then in the second part we will we  are going to take a look at how we can  use these assistants effectively for  your applications  so first let's take a look at the  emerging recipe for how to train these  assistants and keep in mind that this is  all very new and still rapidly evolving  but so far the recipe looks something  like this  now this is kind of a complicated slide  so I'm going to go through it piece by  piece but roughly speaking we have four  major stages free training supervised  fine tuning reward modeling  reinforcement learning and they follow  each other serially  now in each stage we have a data set  that is that powers that stage we have  an algorithm that for our purposes will  be a objective and a and over for uh for  training the neural network and then we  have a resulting model and then there's  some notes on the bottom so the first  stage we're going to start with is the  pre-training stage now this stage is  kind of special in this diagram and this  diagram is not to scale because this  stage is where all of the computational  work basically happens this is 99 of the  training uh compute time and also flops  and so this is where we are dealing with  internet scale data sets with thousands  of gpus in the store computer and also  months of training potentially the other  three stages are fine-tuning stages that  are much more along the lines of a small  few number of gpus and hours or days  so let's take a look at the pre-training  stage to achieve a base model  first we're going to gather a large  amount of data here's an example of what  we call a data mixture that comes from  this uh paper that was released by meta  where they released this llama based  model now you can see roughly the kinds  of data sets that enter into these  collections so we have common crawl  which is just a web scrape C4 which is  also a common crawl and then some high  quality data sets as well so for example  GitHub Wikipedia books archive stack  exchange and so on these are all mixed  up together and then they are sampled  According to some given proportions and  that forms the training set for the  neural net for the GPT  now before we can actually train on this  data we need to go through one more  pre-processing step and that is  tokenization and this is basically a  translation of the raw text that we  scrape from the internet into sequences  of integers because that's the native  representation over which gpts function  now this is a lossless kind of  translation between pieces of text and  tokens and integers and there are a  number of algorithms for this stage  typically for example you could use  something like byte pair encoding which  iteratively merges little text chunks  and groups them into tokens  and so here I'm showing some example  chunks of these tokens and then this is  the raw integer sequence that will  actually feed into a Transformer  now here I'm showing two uh sort of like  examples for hyper parameters that  govern this stage so gpt4 we did not  release too much information about how  it was trained and so on so I'm using  gpt3's numbers but gpt3 is of course a  little bit old by now about three years  ago but llama is a fairly recent model  from meta so these are roughly the  orders of magnitude that we're dealing  with when we're doing pre-training  the vocabulary size is usually a couple  10 000 tokens the context length is  usually something like two thousand four  thousand or nowadays even 100 000 and  this governs the maximum number of  integers that the GPT will look at when  it's trying to predict the next integer  in a sequence  um you can see that roughly the number  of parameters is say 65 billion for  llama now even though llama has only 65b  parameters compared to gvt3's 175  billion parameters llama is a  significantly more powerful model and  intuitively that's because the model is  trained for significantly longer in this  case 1.4 trillion tokens instead of just  300 billion tokens so you shouldn't  judge the power of a model just by the  number of parameters that it contains  below I'm showing some tables of rough  number of rough hyper parameters that  typically go into specifying the  Transformer neural network so the number  of heads the dimension size number of  layers and so on and on the bottom I'm  showing some training hyper parameters  so for example to train the 65b model  meta used 2000 gpus roughly 21 days  train of training and and roughly  several million dollars and so that's  the rough orders of magnitude that you  should have in mind for the pre-training  stage  now when we're actually pre-chaining  what happens roughly speaking we are  going to take our tokens and we're going  to lay them out into Data batches so we  have these arrays that will feed into  the Transformer and these arrays are B  the batch size and these are all  independent examples stacked up in rows  and B by T T being the maximum context  length so in my picture I only have 10.  the context length so this could be two  thousand four thousand Etc so these are  extremely long rows and what we do is we  take these documents and we pack them  into rows and we delimit them with these  Special end of text tokens uh basically  it's on the Transformer where a new  document begins  and so here I have a few examples of  documents and then I've stretched them  out into uh into this input  now we're going to feed all of these  numbers into Transformer and let me let  me just focus on a single particular  cell but the same thing will happen at  every every cell in this diagram so  let's look at the green cell the green  cell is going to take a look at all of  the tokens before it so all of the  tokens in yellow and we're going to feed  that entire context into the Transformer  neural network  and the Transformer is going to try to  predict the next token in the sequence  in this case in red  now the Transformer I don't have too  much time to unfortunately go into the  full details of this neural network  architecture is just a large blob of  neural net stuff for our purposes and  it's got several 10 billion parameters  typically or something like that and of  course as they tune these parameters  you're getting slightly different uh  predicted distributions for every single  one of these cells  and so for example if our vocabulary  size is 50 257 tokens then we're going  to have that many numbers because we  need to specify a probability  distribution for what comes next so  basically we have a probability for  whatever may follow now in this specific  example for the specific cell 513 will  come next and so we can use this as a  source of supervision to update our  Transformers weights and so we're  applying this basically on every single  cell in parallel and we keep swap  swapping batches and we're trying to get  the Transformer to make the correct  predictions over what token comes next  in a sequence  so let me show you more concretely what  this looks like when you train one of  these models this is actually coming  from a New York Times and they trained a  small GPT on Shakespeare and so here's a  small snippet of Shakespeare and they  trained a GPT on it now in the beginning  at initialization the GPT starts with  completely random weights so you're just  getting completely random outputs as  well  but over time as you train uh the GPT  longer and longer you are getting more  and more coherent and consistent uh sort  of samples from the model and the way  you sample from it of course is you uh  predict what comes next you sample from  that distribution and you keep feeding  that back into the process and you can  basically uh sample large sequences and  so by the end you see that the  Transformer has learned about words and  where to put spaces and where to put  commas and so on and so we're making  more and more consistent predictions  over time  these are the kinds of plots that you're  looking at when you're doing model  pre-training effectively we're looking  at the loss function over time as you  train and low loss means that our  Transformer is predicting the correct uh  is giving a higher probability to the  correct next integer in a sequence  now what are we going to do with this  model once we've trained it after a  month well the first thing that we  noticed we the field is that these  models basically in the process of  language modeling learn very powerful  General representations and it's  possible to very efficiently fine-tune  them for any arbitrary Downstream task  you might be interested in so as an  example if you're interested in  sentiment classification the approach  used to be that you collect a bunch of  positives and negatives and then you  train some kind of an NLP model for for  that but the new approach is ignore  sentiment classification  go off and do large language model  pre-training train the large Transformer  and then you can only you may only have  a few examples and you can very  efficiently fine tune your model for  that task and so this works very well in  practice and the reason for this is that  basically the Transformer is forced to  multitask a huge amount of tasks in the  language modeling task because just just  in terms of predicting the next token  it's forced to understand a lot about  the structure of the of the text and all  the different concepts they're in  so that was GPT one now around the time  of gpt2 people noticed that actually  even better than fine tuning you can  actually prompt these models very  effectively so these are language models  and they want to complete documents so  you can actually trick them into  performing tasks just by arranging these  fake documents so in this example for uh  for example we have some passage and  then we sort of like do QA qaqa this is  called a few shot prompt and then we do  q and then as the Transformer is trying  to complete the document it's actually  answering our question and so this is an  example of prompt engineering a base  model making the belief that it's sort  of imitating a document and getting it  to perform a task  and so this kicked off I think the era  of I would say prompting over  fine-tuning and seeing that this  actually can work extremely well on a  lot of problems even without training  any neural networks fine-tuning or so on  now since then we've seen an entire  evolutionary tree of Base models that  everyone has trained not all of these  models are available for example the  gpt4 base model was never released the  gpt4 model that you might be interacting  with over API is not a base model it's  an assistant model and we're going to  cover how to get those in a bit gpt3  based model is available via the API  under the name DaVinci and gpt2 base  model is available even as weights on  our GitHub repo but currently the best  available base model probably is the  Llama series from meta although it is  not commercially licensed  now one thing to point out is base  models are not assistance they don't  want to answer to they don't want to  make answers to your questions they just  want to complete documents so if you  tell them write a poem about the brand  cheese it will just you know it will  answer questions with more questions  it's just completing what it thinks as a  document  however you can prompt them in a  specific way for base models that that  is more likely to work so as an example  here's a poem about bread and cheese and  in that case it will auto-complete  correctly  you can even trick base models into  being assistance and the way you would  do this is you would create like a  specific few shot prompt that makes it  look like there's some kind of a  document between a human and assistant  and they're exchanging sort of um  information and then at the bottom you  sort of put your query at the end and  the base model will sort of like  condition itself into being like a  helpful assistant and kind of answer but  this is not very reliable and doesn't  work super well in practice although it  can be done  so instead we have a different path to  make actual gbt assistance not just base  model document completers and so that  takes us into supervised fine-tuning so  in the supervised fine tuning stage we  are going to collect small but high  quality data sets and in this case we're  going to ask human contractors to gather  data of the format of the form prompt  and ideal response and we're going to  collect lots of these typically tens of  thousands or something like that  and then we're going to still do  language modeling on this data so  nothing change algorithmically we're  just swapping out a training set so it  used to be internet documents which is a  high quantity local  four basically QA prompt response kind  of data and that is low quantity high  quality  so we still do language modeling and  then after training we get an sfd model  and you can actually deploy these models  and they are actual assistants and they  work to some extent let me show you what  an example demonstration might look like  so here's something that a human  contractor might come up with here's  some random prompt can you write a short  introduction about the relevance of the  term monopsony or something like that  and then the contractor also writes out  an ideal response and when they write  out these responses they are following  extensive labeling documentations and  they're being asked to be helpful  truthful and harmless and those labeling  instructions here you probably can't  read it another can I but they're long  and this is just people following  instructions and trying to complete  these prompts  so that's what the data set looks like  and you can train these models and this  works to some extent  now you can actually continue the  pipeline from here on and go into rlhf  reinforcement learning from Human  feedback that consists of both reward  modeling and reinforcement learning so  let me cover that and then I'll come  back to why you may want to go through  the extra steps and how that compares to  just sft models  so in the reward modeling step what  we're going to do is we're now going to  shift our data collection to be of the  form of comparisons so here's an example  of what our data set will look like  I have the same prompt identical prompt  on the top which is asking the assistant  to write a program or function that  checks if a given string is a palindrome  and then what we do is we take the sft  model which we've already trained and we  create multiple completions so in this  case we have three completions that the  model has created  and then we ask people to rank these  completions so if you stare at this for  a while and by the way these are very  difficult things to do to compare some  of these predictions and this can take  people even hours for a single prompt  completion pairs but let's say we  decided that one of these is much better  than the others and so on so we rank  them  then we can follow that with something  that looks very much kind of like a  binary classification on all the  possible pairs between these completions  so what we do now is we lay out our  prompt in rows and the prompts is  identical across all three rows here so  it's all the same prompt but the  completion of this vary and so the  yellow tokens are coming from the sft  model then what we do is we append  another special reward uh readout token  at the end and we basically only  supervise the Transformer at this single  green token and the Transformer will  predict some reward for how good that  completion is for that prompt  and so basically it makes a guess about  the quality of each completion and then  once it makes a guess for every one of  them we also have the ground truth which  is telling us the ranking of them and so  we can actually enforce that some of  these numbers should be much higher than  others and so on WE formulate this into  a loss function and we train our model  to make reward predictions that are  consistent with the ground truth coming  from the comparisons from all these  contractors so that's how we train our  reward model and that allows us to score  how good a completion is for a prompt  once we have a reward model this is we  can't deploy this because this is not  very useful as an assistant by itself  but it's very useful for the reinforce  reinforcement learning stage that  follows now because we have a reward  model we can score the quality of any  arbitrary completion for any given  prompt so what we do during  reinforcement learning is we basically  get again a large collection of prompts  and now we do reinforcement learning  with respect to the reward model so  here's what that looks like  we we take a single prompt we lay it out  in rows and now we use the sft we use  the basically the model we'd like to  train which is initialized at sft model  to create some completions in yellow  and then we append the reward token  again and we read off the reward  according to the reward model which is  now kept fixed it doesn't change anymore  and now the reward model tells us the  quality of every single completion for  each for these prompts and so what we  can do is we can now just basically  apply the same language modeling loss  function but we're currently training on  the yellow tokens and we are weighing  the language modeling objective by the  rewards indicated by the reward model so  as an example in the first row the  reward model said that this is a fairly  high score in completion and so all the  tokens that we happen to sample on the  first row are going to get reinforced  and they're going to get higher  probabilities for the future  conversely on the second row the reward  model really did not like this  completion negative 1.2 and so therefore  every single token that we sampled in  that second row is going to get a  slightly higher probability for the  future and we do this over and over on  many prompts on many batches and  basically we get a policy which creates  yellow tokens here and it basically all  of them all of the completions here will  score high according to the reward model  that we trained in the previous stage  so that's how we train uh that's what  the rohf pipeline is  now and then at the end you get a model  that you could deploy and so and as an  example chat GPT is an rlhf model but  some other models that you might come  across like for example of the kuna 13B  and so on these are sft models so we  have base models sft models and rlh  models and that's kind of like the state  of things there  now why would you want to do rlhf so one  answer that is kind of not that exciting  is that it just works better so this  comes from the instructor gbt paper  according to these experiments a while  ago now these PPO models are rlhf and we  see that they are basically just  preferred in a lot of comparisons when  we give them to humans so humans just  prefer out basically tokens that come  from our religion models compared to sft  models compared to base model that is  prompted to be an assistant and so it  just works better but you might ask why  why does it work better and I don't  think that there's a single like amazing  answer that the community has really  like agreed on but I will just offer one  uh one reason potentially and it has to  do with the asymmetry between how easy  computationally it is to compare versus  generate so let's take an example of  generating a haiku suppose I ask a model  to write a haiku about paper clips if  you're a contractor trying to give train  data then imagine being a contractor  collecting basically data for the sft  stage how are you supposed to create a  nice haiku for a paperclip you might  just not be very good at that but if I  give you a few examples of haikus you  might be able to appreciate some of  these haikus a lot more than others and  so judging which one of these is good is  much easier task and so basically this  asymmetry makes it so that comparisons  are a better way to potentially leverage  yourself as a human and your judgment to  create a slightly better model  now our religious models are not  strictly an improvement on the base  models in some cases so in particular  we've noticed for example that they lose  some entropy so that means that they  give More PT results they can output  lower variations uh like they can output  samples with lower variation than base  model so base model has lots of entropy  and we'll go with lots of diverse  outputs  so for example one one kind of place  where I still prefer to use a base model  is in the setup where  you basically have N Things and you want  to generate more things like it and so  here is an example that I just cooked up  I want to generate cool Pokemon names I  gave it seven Pokemon names and I asked  the base model to complete the document  and it gave me a lot more Pokemon names  uh these are fictitious I tried to look  them up I don't believe they're actual  Pokemons and uh this is the kind of task  that I think base model would be good at  because it still has lots of entropy it  will give you lots of diverse cool kind  of more things that look like whatever  you give it before  so this is what uh this is number having  all said all that these are kind of like  the assistant models that are probably  available to you at this point uh that  there's a team at Berkeley that ranked a  lot of the available assistant models  and gave them basically ELO ratings So  currently some of the best models of  course are gpt4 by far I would say  followed by Claude gvt 3.5 and then a  number of models some of these might be  available as weights like the kuna koala  Etc and the first three rows here are  all they're all rohf models and all of  the other models to my knowledge are sft  models I believe  okay so that's how we train these models  on the high level now I'm going to  switch gears and let's look at how we  can best apply and the GPT assistant  model to your problems now I would like  to work  in a setting of a concrete example so  let's  let's work with a concrete example here  let's say that you are working on an  article or a blog post and you're going  to write this sentence at the end  California's population is 53 times that  of Alaska so for some reason you want to  compare the populations of these two  states  think about the rich internal monologue  and Tool use and how much work actually  goes computationally in your brain to  generate this one final sentence so  here's maybe what that could look like  in your brain okay for this next step  let me blog uh for my blog let me  compare these two populations  okay first I'm going to obviously need  to get both of these populations now I  know that I probably don't know these  populations off the top of my head so  I'm kind of like aware of what I know or  don't know of my self-knowledge right  so I go I do some tool use and I go to  Wikipedia and I look up California's  population and Alaska's population  now I know that I should divide the two  but again I know that dividing three 9.2  by 0.74 is very unlikely to succeed  that's not the kind of thing that I can  do in my head and so therefore I'm gonna  rely on the calculator so I'm going to  use a calculator punch it in and see  that the output is roughly 53.  and then maybe I do some reflection and  Sanity checks in my brain so does 53  make sense well that's quite quite a  large fraction but then California is  the most popular state so maybe that  looks okay  so then I have all the information I  might need and now I get to the sort of  creative portion of writing so I might  start to write something like California  has 53 x times greater and then I think  to myself that's actually like really  awkward phrasing so let me actually  delete that and let me try again and so  as I'm writing I have this separate  process almost inspecting what I'm  writing and judging whether it looks  good or not  and then maybe I delete and maybe I  reframe it and then maybe I'm happy with  what comes out  so basically long story short a ton  happens under the hood in terms of your  internal monologue when you create  sentences like this but what does a  sentence like this look like when we are  training a GPT on it  From gpt's perspective this is just a  sequence of tokens  so GPT when it's reading or generating  these tokens it just goes  and each chunk is roughly the same  amount of computational work for each  token and these Transformers are not  very shallow networks they have about 80  layers of reasoning but 80 is still not  like too much and so this Transformer is  going to do its best to imitate but of  course the process here looks very very  different from the process that you took  so in particular in our final artifacts  in the data sets that we create and then  eventually feed to llms all of that  internal dialogue is completely stripped  and uh and unlike you the GPT will look  at every single token and spend the same  amount of compute on every one of them  and so you can't expect it to actually  like well you can't expect it to do to  uh sort of do too much work per token  so and also in particular basically  these Transformers are just like token  simulators so they don't know what they  don't know like they just imitate the  next token they don't know what they're  good at or not good at they just tried  their best imitate the next token they  don't reflect in the loop they don't  sanity check anything they don't correct  their mistakes along the way by default  they just uh sample token sequences  they don't have separate inner monologue  streams in their head right that are  evaluating what's happening now they do  have some sort of cognitive advantages I  would say and that is that they do  actually have a very large fact-based  knowledge across a vast number of areas  because they have say several 10 billion  parameters so it's a lot of storage for  a lot of facts  but and they also I think have a  relatively large and perfect working  memory so whatever fixed into the  whatever fits into the context window is  immediately available to the Transformer  through its internal self-attention  mechanism and so it's kind of like um  Perfect Memory but it's got that finite  size but the Transformer has a very  direct access to it and so it can like  losslessly remember anything that is  inside its context window  so that's kind of how I would compare  those two and the reason I bring all of  this up is because I think to a large  extent prompting is just making up for  this sort of cognitive difference  between these two kind of architectures  um like our brains here and llm brains  you can look at it that way almost  so here's one thing that people found  for example works pretty well in  practice especially if your tasks  require reasoning you can't expect the  Transformer to make to to do too much  reasoning per token and so you have to  really spread out the reasoning across  more and more tokens so for example you  can't give a Transformer a very  complicated question and expect it to  get the answer in a single token there's  just not enough time for it these  Transformers need tokens to think quote  unquote I like to say sometimes and so  this is some of the things that work  well you may for example have a few shot  prompt that shows the Transformer that  it should like show its work when it's  answering a question when it's answering  the question and if you give a few  examples the Transformer will imitate  that template and it will just end up  working out better in terms of its  evaluation  Additionally you can elicit this kind of  behavior from the Transformer by saying  let's think step by step because this  condition is the transformer into sort  of like showing its work and because it  kind of snaps into a mode of showing its  work it's going to do less computational  work per token and so it's more likely  to succeed as a result because it's  making  um slower reasoning over time  here's another example this one is  called self-consistency we saw that we  had the ability to start writing and  then if it didn't work out I can try  again and I can try multiple times and  uh and maybe  um select the one that worked best so in  these kinds of approaches you may sample  not just once but you may sample  multiple times and then have some  process for finding the ones that aren't  good and then keeping just those samples  or doing a majority vote or something  like that so basically these  Transformers in the process as they  predict the next token just like you  they can get unlucky and they can they  could sample and not a very good token  and they can go down sort of like a  blind alley in terms of reasoning and so  unlike you they cannot recover from that  they are stuck with every single token  they sample and so they will continue  the sequence even if they even know that  this sequence is not going to work out  so give them the ability to look back  inspect or try to find uh try to  basically sample around it  here's one technique also you could it  turns out that actually llms like they  know when they've screwed up so as an  example  say you asked the model to generate a  poem that does not rhyme and it might  give you a poem but it actually Rhymes  but it turns out that especially for the  bigger models like gpt4 you can just ask  it did you meet the assignment and  actually gpt4 knows very well that it  did not meet the assignment it just kind  of got unlucky in its sampling and so it  will tell you no I didn't actually meet  the assignment here's let me try again  but without you prompting it it doesn't  even like it doesn't know it doesn't  know to revisit and uh and so on so you  have to make up for that in your prompt  you have to get it to check if you don't  ask it to check it's not going to check  by itself it's just a token simulator  I think more generally a lot of these  techniques fall into the bucket of what  I would say recreating our system too so  you might be familiar with the system  one system to thinking for humans system  one is a fast automatic process and I  think kind of corresponds to like an llm  just sampling tokens  and system two is the slower deliberate  planning sort of part of your brain  and so this is a paper actually from  just last week because this space is  pretty quickly evolving it's called tree  of thought and in tree of thought uh the  authors of this paper propose  maintaining multiple completions for any  given prompt and then they are also  scoring them along the way and keeping  the ones that are going well if that  makes sense and so a lot of people are  like really playing around with kind of  um prompt engineering to um to basically  bring back some of these abilities that  we sort of have in our brain for llms  now one thing I would like to note here  is that this is not just a prompt this  is actually prompts that are together  used with some python glue code because  you don't you actually have to maintain  multiple prompts and you also have to do  some tree search algorithm here um to  like figure out which prompt to expand  Etc so it's a symbiosis of python glue  code and individual prompts that are  called in a while loop or in a bigger  algorithm  I also think there's a really cool  parallel here to alphago alphago has a  policy for placing the next Stone when  it plays go and its policy was trained  originally by imitating humans but in  addition to this policy it also does  Monte Carlo tree search and basically it  will play out a number of possibilities  in its head and evaluate all of them and  only keep the ones that work well and so  I think this is kind of an equivalent of  alphago but for text  if that makes sense  so just like tree of thought I think  more generally people are starting to  like really Explore More General  techniques of not just the simple  question answer prompt but something  that looks a lot more like python blue  code stringing together many problems so  on the right I have an example from this  paper called react where they structure  the answer to a prompt as a sequence of  thought action observation thought  action observation and it's a full  rollout a kind of a thinking process to  answer the query and in these actions  the model is also allowed to to lose  on the left I have an example of rogpt  and now rgbt by the way became as a  project that I think got a lot of hype  recently and I think uh but I think I  still find it kind of inspirationally  interesting it's um it's a project that  allows an llm to sort of keep task list  and continue to recursively break down  tasks and I don't think this currently  works very well and I would not advise  people to use it in Practical  applications I just think it's something  to generally take inspiration from in  terms of where this is going I think  over time  so that's kind of like giving our model  system to thinking the next thing that I  find kind of interesting is  this following survey would say almost  psychological Quirk of llms is that llms  don't want to succeed they want to  imitate  you want to succeed and you should ask  for it  so what I mean by that is  when Transformers are trained they have  training sets and there can be an entire  spectrum of performance qualities in  their training data so for example there  could be some kind of a prompt for some  Physics question or something like that  and there could be a student solution  that is completely wrong but there can  also be an expert answer that is  extremely right and Transformers can't  tell the difference between like look I  mean they know they know about low  quality Solutions and high quality  Solutions but by default they want to  imitate all of it because they're just  trained on language modeling and so at  test time you actually have to ask for a  good performance so in this example in  this paper it's um they tried various  prompts and let's think step by step was  very powerful because it's sort of like  spread out the reasoning of remaining  tokens but what worked even better is  let's work this out in a step-by-step  way to be sure we have the right answer  and so it's kind of like a conditioning  on getting a right answer and this  actually makes the Transformer work  better because the Transformer doesn't  have to now hedge its probability Mass  on low quality Solutions as ridiculous  as that sounds and so basically don't be  feel free to ask for a strong solution  say something like you are a leading  expert on this topic pretend you have  iq120 Etc but don't try to ask for too  much IQ because if you ask for IQ of  like 400 you might be out of data  distribution or even worse you could be  in data distribution for some like  sci-fi stuff and it will start to like  take on some sci-fi or like role-playing  or something like that so you have to  find like the right amount of IQ I think  it's got some U-shaped curve there  next up  as we saw when we are trying to solve  problems we know we are good at and what  we're not good at and we Lean On Tools  computationally you want to do the same  potentially with your uh llms so in  particular we may want to give them  calculators code interpreters and so on  the ability to do search and uh there's  a lot of techniques for doing that one  thing to keep in mind again is that  these Transformers by default may not  know what they not don't know so you may  even want to tell the transformer in the  prompt you are not very good at mental  arithmetic whenever you need to do very  large number addition multiplication or  whatever instead use this calculator  here's how you use the calculator use  this token combination etc etc so you  have to actually like spell it out  because the model by default doesn't  know what it's good at or not good at  necessarily just like you and I you and  I might be  next up I think something that is very  interesting is we went from a world that  was retrieval only all the way the  pendulum has swung to The Other Extreme  where it's memory only in lens but  actually there's this entire space in  between of these retrieval augmented  models and this works extremely well in  practice  as I mentioned the context window of a  transformer is its working memory if you  can load the working memory with any  information that is relevant to the task  the model will work extremely well  because it can immediately access all  that memory  and so I think a lot of people are  really interested in  uh basically retrieval augmented  generation and on the bottom I have like  an example of llama index which is one  sort of data connector to lots of  different types of data and you can uh  all you can make it you can index all of  that data and you can make it accessible  to llms and the emerging recipe there is  you take relevant documents you split  them up into chunks you embed all of  them and you basically get embedding  vectors that represent that data you  store that in a vector store and then at  test time you make some kind of a query  to your vector store and you fetch  chunks that might be relevant to your  task and you stuff them into the prompt  and then you generate so this can work  quite well in practice so this is I  think similar to when you're and I solve  problems you can do everything from your  memory and Transformers have very large  and extensive memory but also it really  helps to reference some primary  documents so when you whenever you find  yourself going back to a textbook to  find something or whenever you find  yourself going back to the documentation  of a library to look something up the  transform transformers definitely when I  do that too you you have a mem you have  some memory over how some documentation  of a library works but it's much better  to look it up so the same same applies  here  next I wanted to briefly talk about  constraint prompting I also find this  very interesting  uh this is basically techniques for  um  forcing a certain template in the  outputs of llms so guidance is one  example from Microsoft actually and uh  here we are enforcing that the output  from the llm will be Json and this will  actually guarantee that the output will  take on this form because they go in and  they mess with the probabilities of all  the different tokens that come out of  the Transformer and if they clamp those  tokens and then the Transformer is only  filling in the blanks here and then you  can enforce additional restrictions on  what could go into those blanks so this  might be really helpful and I think this  kind of constraint sampling is also  extremely interesting  I also wanted to say a few words about  fine tuning it is the case that you can  get really far with prompt engineering  but it's also possible to think about  fine-tuning your models now fine-tuning  models means that you are actually going  to change the weights of the model  it is becoming a lot more accessible to  do this in practice and that's because  of a number of techniques that have been  developed and have libraries for uh very  recently so for example parameter  efficient fine-tuning techniques like  Laura make sure that you're only Trend  you're only training small Spar species  of your model so most of the model is  kept clamped at the base model and some  pieces of it are allowed to change and  this still works pretty well empirically  and makes it much cheaper to sort of  tune only small pieces of your model  uh there's also it also means that  because most of your model is clamped  you can use very low Precision inference  for computing those parts because they  are not going to be updated by gradient  descent and so that makes everything a  lot more efficient as well and in  addition we have a number of Open Source  high quality based models currently as I  mentioned I think llama is quite nice  although it is not commercially licensed  I believe right now  some things to keep in mind is that  basically fine tuning is is a lot more  technically involved it requires a lot  more I think technical expertise to do  to do right it requires human data  contractors for data sets and or  synthetic data pipelines that can be  pretty complicated this will definitely  slow down your iteration cycle by a lot  and I would say on a high level sft is  achievable because it is just your  continuing language modeling task It's  relatively straightforward but rlhref I  would say is very much research  territory and is even much harder to get  to work and so I would probably not  advise that someone just tries to roll  their own rlh of implementation these  things are pretty unstable very  difficult to train uh not something that  is I think very beginner friendly right  now and it's also potentially likely  also to to change pretty rapidly still  so I think these are my sort of default  recommendations right now  I would break up your task into two  major parts number one achieve your top  performance and number two optimize your  performance in that order  number one the best performance will  currently come from GT4 model it is the  most capable of by far  use prompts that are very detailed they  have lots of task contents relative  relevant information and instructions  think along the lines of what would you  tell a task contractor if they can't  email you back but then also keep in  mind that a task contractor is a human  and they have inner monologue and  they're very clever Etc llms do not  possess those qualities so make sure to  think through the psychology of the llm  almost and cater prompts to that  retrieve and add any relevant context  and information to these prompts  basically refer to a lot of the prompt  engineering techniques some of them are  highlighted in the slides above but also  this is a very large space and I would  just advise you to look for prompt  engineering techniques online there's a  lot to cover there  experiment with few shop examples what  this refers to is you don't just want to  tell you want to show whenever it's  possible so give it examples of  everything that helps it really  understand what you mean if you can  tools and plugins to offload a tasks  that are difficult for llms natively uh  and then think about not just a single  prompt and answer think about potential  chains and reflection and how you glue  them together and how you could  potentially make multiple samples and so  on  finally if you think you've squeezed out  prompt engineering which I think you  should stick with for a while look at  some potentially fine-tuning a model to  your application but expect this to be a  lot more slower and evolved and then  there's an expert fragile research Zone  here and I would say that is rlhf which  currently does work a bit better than  sft if you can get it to work but again  this is uh pretty involved I would say  and to optimize your costs try to  explore lower capacity models or shorter  prompts and so on  I also wanted to say a few words about  the use cases in which I think llms are  currently well suited for so in  particular note that there's a large  number of limitations to llms today and  uh so I would keep that definitely in  mind for all your applications models  and this by the way could be an entire  talk so I don't have time to cover it in  full detail models may be biased they  may fabricate hallucinate information  they may have reasoning errors they may  struggle in entire classes of  applications they have knowledge cutoffs  so they might not know any information  above say September 2021 they are  susceptible to a large range of attacks  which are sort of like coming out on  Twitter daily including prompt injection  jailbreak attacks data poisoning attacks  and so on so my recommendation right now  is use llms in low stakes applications  combine them with always with human  oversight use them as a source of  inspiration and suggestions and think  co-pilots instead of completely  autonomous agents that are just like  performing a task somewhere it's just  not clear that the models are there  right now  so I wanted to close by saying that gpt4  is an amazing artifact I'm very thankful  that it exists and it's it's beautiful  it has a ton of knowledge across so many  areas it can do math code and so on and  in addition there's this thriving  ecosystem of everything else that is  being built and incorporated into into  the ecosystem uh some of some of the  things these things I've talked about  and all of this power is accessible at  your fingertips  so here's everything that's needed in  terms of code to ask GPT for a question  to prompt it and get a response in this  case I said uh can you say something to  inspire the audience of Microsoft build  2023  and I just punched this into Python and  verbatim gpt4 said the following  uh and by the way I did not know that  they used this trick in the keynote so I  thought I was being clever but  but it is really good at this it says  ladies and gentlemen Invaders and  Trailblazers Microsoft build 2023  welcome to the Gathering of Brilliant  Minds like no other you are The  Architects of the future The Visionaries  molding the digital realm in which  Humanity thrives embrace the Limitless  possibilities of Technologies and let  your ideas soar as high as your  imagination together let's create a more  connected remarkable and inclusive world  for generations to come get ready to  unleash your creativity canvas the  unknown and turn dreams into reality  your Journey Begins today  okay\\n外国\\n\\n[音乐]\\n\\n研究员和开放人工智能创始成员\\n\\n安德烈·卡帕西\\n\\n[鼓掌]\\n\\n大家好，\\n\\n我很高兴能在这里向大家介绍\\n\\nGBT 的现状，以及更广泛的\\n\\n关于快速发展的大型语言模型生态系统的信息，\\n\\n\\n所以我想 将演讲\\n\\n分为两个部分，\\n\\n在第一部分中，我想告诉\\n\\n您我们如何训练 GPT 辅助，\\n\\n然后在第二部分中，我们将\\n\\n看看如何在\\n\\n\\n您的应用程序中有效地使用这些助手，\\n\\n所以首先 让我们看一下\\n\\n如何训练这些助手的新兴配方，\\n\\n请记住，这都是\\n\\n非常新的，并且仍在快速发展，\\n\\n但到目前为止，配方看起来\\n\\n像这样，\\n\\n现在这是一张复杂的幻灯片，\\n\\n所以我要 一点一点地进行，\\n\\n但粗略地说，我们有四个\\n\\n主要阶段：免费训练、监督微调、奖励\\n\\n建模、\\n\\n强化学习，它们按顺序相互跟随，\\n\\n\\n现在在每个阶段，我们都有一个数据集，\\n\\n为该阶段提供动力，我们有\\n\\n一个算法， 就我们的目的而言，这将\\n\\n是\\n\\n训练神经网络的目标，然后我们将\\n\\n得到一个结果模型，然后\\n\\n在底部有一些注释，因此\\n\\n我们要开始的第一个阶段是\\n\\n预训练阶段 现在这个阶段\\n\\n在这个图中有点特殊，这个\\n\\n图不是按比例的，因为这个\\n\\n阶段是所有计算\\n\\n工作基本上发生的地方，这是\\n\\n训练呃计算时间的 99，还有失败\\n\\n，所以这就是我们正在处理的地方\\n\\n互联网规模的数据集，\\n\\n商店计算机中包含数千个 GPU，并且\\n\\n可能需要数月的训练，其他\\n\\n三个阶段是微调阶段，\\n\\n更多的是沿着\\n\\n少量 GPU 和数小时或数天的路线，\\n\\n所以让我们看一下 在预训练\\n\\n阶段，为了首先实现基本模型，\\n\\n我们将收集\\n\\n大量数据，这是\\n\\n我们所谓的数据混合的示例，该示例来自\\n\\nMeta 发布的这篇呃论文，\\n\\n他们在该论文中发布了基于 llama 的模型\\n\\n现在，您可以粗略地看到\\n\\n进入这些集合的数据集类型，\\n\\n因此我们有常见的爬行，\\n\\n这只是一个网络抓取 C4，这也是\\n\\n一个常见的爬行，然后还有一些高质量\\n\\n的数据集，例如\\n\\nGitHub 维基百科图书档案 堆栈\\n\\n交换等等这些都\\n\\n混合在一起，然后\\n\\n根据给定的比例对它们进行采样，\\n\\n形成\\n\\nGPT 神经网络的训练集，然后\\n\\n我们才能真正训练这些\\n\\n数据，我们需要再经历一次\\n\\n预处理步骤，即\\n\\n标记化，这基本上是\\n\\n我们\\n\\n从互联网上抓取的\\n\\n原始文本到整数序列的翻译，因为这是\\n\\ngpts 现在运行的本机表示，\\n\\n这是文本片段\\n\\n和文本片段之间的无损翻译。\\n\\n令牌和整数，\\n\\n此阶段有许多算法，\\n\\n例如，您通常可以使用\\n\\n字节对编码之类的算法，它\\n\\n迭代地合并小文本块\\n\\n并将它们分组为令牌，\\n\\n因此在这里我展示了\\n\\n这些令牌的一些示例块，然后 这是\\n\\n\\n实际输入到 Transformer 的原始整数序列，\\n\\n现在在这里我展示了两个类似的\\n\\n示例，用于\\n\\n控制这个阶段的超参数，所以 gpt4 我们没有\\n\\n发布太多关于\\n\\n它是如何训练的信息等等 我使用的是\\n\\ngpt3 的数字，但 gpt3 当然\\n\\n在大约三年前有点旧了，\\n\\n但 llama 是来自 meta 的一个相当新的模型，\\n\\n所以这些大致是\\n\\n我们在\\n\\n做 pre 时处理的数量级。  - 训练\\n\\n词汇量通常是几个\\n\\n10 000 个标记，上下文长度通常是\\n\\n\\n204000 个，现在甚至是 100000 个，\\n\\n这决定了\\n\\nGPT 在\\n\\n尝试预测下一个整数时将查看的最大整数数量\\n\\n在序列中，\\n\\n您可以看到 llama 现在的参数数量大致为\\n\\n650 亿个，\\n\\n尽管 llama\\n\\n与 gvt3 的 1750\\n\\n亿个参数相比只有 65b 个参数，但 llama 是一个\\n\\n更强大的模型，\\n\\n直观上这是因为该模型\\n\\n经过了显着的训练 在这种\\n\\n情况下，更长的时间是 1.4 万亿个令牌，而不是\\n\\n3000 亿个令牌，因此您不应该\\n\\n仅通过模型包含的参数数量来判断模型的能力，\\n\\n\\n下面我将显示一些通常会出现的\\n\\n粗略超参数的粗略数量表\\n\\n指定\\n\\nTransformer 神经网络，\\n\\n头数、维度大小、\\n\\n层数等，在底部，我\\n\\n显示了一些训练超参数，\\n\\n例如训练 65b 模型元，\\n\\n使用 2000 GPU 大约 21 天\\n\\n的训练 大约\\n\\n几百万美元，所以这是\\n\\n你现在在\\n\\n预训练阶段应该记住的粗略数量级，\\n\\n\\n当我们实际上预链接发生的\\n\\n事情时，粗略地说，我们\\n\\n将拿走我们的代币，我们\\n\\n将它们放入数据批次中，这样我们\\n\\n就有这些数组将输入到\\n\\nTransformer 中，这些数组是 B\\n\\n批次大小，这些都是按\\n\\n行堆叠的独立示例，\\n\\nB 乘 T T 是最大上下文\\n\\n长度，所以在我的图片中 我只有 10.\\n\\n上下文长度，所以这可能是\\n\\n两千四千等等，所以这些是\\n\\n非常长的行，我们所做的就是\\n\\n获取这些文档，将它们打包\\n\\n成行，然后用这些\\n\\n特殊的文本结尾标记来分隔它们 基本上\\n\\n它是在 Transformer 上\\n\\n开始新文档的，\\n\\n所以这里我有一些\\n\\n文档示例，然后我将它们\\n\\n延伸到这个输入中，\\n\\n现在我们将把所有这些\\n\\n数字输入 Transformer，让我让\\n\\n我只关注一个特定的\\n\\n单元格，但\\n\\n该图中的每个单元格都会发生同样的事情，所以\\n\\n让我们看一下绿色单元格，绿色\\n\\n单元格将查看\\n\\n它之前的所有标记，因此其中的所有\\n\\n标记 黄色，我们将把\\n\\n整个上下文输入 Transformer\\n\\n神经网络，\\n\\nTransformer 将尝试\\n\\n预测序列中的下一个标记，\\n\\n在这种情况下为红色，\\n\\n现在 Transformer 不幸的是我没有\\n\\n太多时间去 深入了解\\n\\n这个神经网络架构的全部细节\\n\\n只是\\n\\n为了我们的目的而设计的一大块神经网络东西，\\n\\n它通常有几百亿个参数\\n\\n或类似的东西，\\n\\n当然，当他们调整这些参数时，\\n\\n你会得到稍微不同的呃\\n\\n预测分布 对于\\n\\n这些单元中的每一个，\\n\\n例如，如果我们的词汇量\\n\\n大小是 50 257 个标记，那么我们将\\n\\n拥有那么多数字，因为我们\\n\\n需要指定\\n\\n接下来发生的情况的概率分布，所以\\n\\n基本上我们有一个概率\\n\\n现在在这个具体\\n\\n示例中，接下来将出现特定单元格 513，\\n\\n因此我们可以使用它作为\\n\\n监督源来更新我们的\\n\\nTransformers 权重，因此我们\\n\\n基本上并行地将其应用于每个\\n\\n单元格，并且我们不断交换\\n\\n交换批次 我们正在尝试让\\n\\nTransformer 对序列中\\n\\n接下来出现的标记做出正确的预测，\\n\\n\\n所以让我更具体地向您展示\\n\\n当您训练其中一个\\n\\n模型时这实际上是\\n\\n来自《纽约时报》的模型，并且 他们\\n\\n在莎士比亚上训练了一个小 GPT，所以这是\\n\\n莎士比亚的一小段，他们\\n\\n现在在初始化开始时训练了一个 GPT，\\n\\nGPT 以\\n\\n完全随机的权重开始，所以你也\\n\\n得到完全随机的输出，\\n\\n\\n但随着时间的推移 你训练呃 GPT 的时间\\n\\n越来越长，你会从模型中得到\\n\\n越来越连贯和一致的\\n\\n样本，\\n\\n你从中采样的方式当然是你呃\\n\\n预测接下来会发生什么，你从\\n\\n这个分布中采样，然后你继续提供\\n\\n它 回到这个过程，你\\n\\n基本上可以对大序列进行采样，\\n\\n所以到最后你会看到\\n\\nTransformer 已经学会了单词、在\\n\\n哪里放置空格、在哪里放置\\n\\n逗号等等，所以我们正在做出\\n\\n越来越一致的预测\\n\\n随着时间的推移，\\n\\n这些是您在\\n\\n\\n有效地进行模型预训练时所看到的图表类型，我们在训练\\n\\n时会随着时间的推移查看损失函数，\\n\\n低损失意味着我们的\\n\\nTransformer 正在预测正确的\\n\\n呃 为序列中正确的下一个整数提供更高的概率\\n\\n\\n现在，\\n\\n一旦我们在一个月后训练好该模型，我们将如何处理该模型，\\n\\n我们\\n\\n注意到我们领域的第一件事是这些\\n\\n模型基本上处于以下过程：\\n\\n语言建模学习非常强大的\\n\\n通用表示，并且\\n\\n可以非常有效地\\n\\n针对您可能感兴趣的任何下游任务对它们进行微调，\\n\\n\\n例如，如果您对\\n\\n情感分类感兴趣，\\n\\n过去的方法是收集一堆\\n\\n正面和负面，然后你\\n\\n为此训练某种 NLP 模型，\\n\\n但新方法是忽略\\n\\n情感分类\\n\\n并进行大型语言模型\\n\\n预训练，训练大型 Transformer\\n\\n，然后你只能有\\n\\n几个例子 你可以非常\\n\\n有效地微调\\n\\n该任务的模型，因此这在实践中非常有效，其\\n\\n原因是\\n\\n基本上 Transformer 被迫\\n\\n在语言建模任务中执行大量任务，\\n\\n因为仅就以下方面而言：\\n\\n预测下一个标记，\\n\\n它被迫了解很多关于\\n\\n文本的结构以及\\n\\n它们所包含的所有不同概念，\\n\\n所以这是 GPT 一个现在大约在\\n\\ngpt2 的时间人们注意到，实际上\\n\\n甚至比微调更好\\n\\n非常有效地提示这些模型，\\n\\n所以这些是语言模型\\n\\n，它们想要完成文档，这样\\n\\n你实际上可以\\n\\n通过排列这些\\n\\n假文档来欺骗它们执行任务，所以在这个例子中，\\n\\n例如，我们有一些段落，\\n\\n然后我们有点像这样做 QA qaqa 这被\\n\\n称为“一些镜头提示”，然后我们执行\\n\\nq，然后当 Transformer 尝试\\n\\n完成文档时，它实际上是在\\n\\n回答我们的问题，所以这是一个\\n\\n提示工程基础模型的示例，人们\\n\\n相信它是在\\n\\n模仿 一个文档并让它\\n\\n执行一项任务，\\n\\n所以这拉开了我想说的时代，我想说的是，\\n\\n\\n\\n\\n即使没有训练\\n\\n任何神经网络微调，这实际上也可以很好地解决很多问题 等等，\\n\\n从那时起，我们已经看到了每个人都训练过的\\n\\n基本模型的完整进化树，但\\n\\n并非所有这些\\n\\n模型都可用，例如，\\n\\ngpt4 基本模型从未发布过，\\n\\n您可能通过 API 与之交互的 gpt4 模型\\n\\n则不可用 一个基本模型，它是\\n\\n一个辅助模型，我们将\\n\\n介绍如何获得这些\\n\\n基于 gpt3 的模型可以\\n\\n通过名为 DaVinci 的 API 获得，而 gpt2 基本\\n\\n模型甚至可以作为\\n\\n我们 GitHub 存储库上的权重使用，但目前是最好的\\n\\n可用的基本模型可能是\\n\\n来自meta的Llama系列，尽管它\\n\\n没有商业许可\\n\\n现在需要指出的是基本\\n\\n模型并不能提供帮助，他们\\n\\n不想回答他们不想\\n\\n回答你的问题，他们只是\\n\\n想要 完成文件，所以如果你\\n\\n告诉他们写一首关于该品牌\\n\\n奶酪的诗，你就会知道它会\\n\\n回答更多问题，它\\n\\n只是完成它认为作为\\n\\n文件的内容，\\n\\n但是你可以以\\n\\n特定的方式提示他们基本模型\\n\\n更有可能起作用，因此作为一个例子，\\n\\n这里有一首关于面包和奶酪的诗，\\n\\n在这种情况下，它将正确自动完成，\\n\\n\\n您甚至可以欺骗基础模型来\\n\\n提供帮助，而您\\n\\n这样做的方式是您将创建像\\n\\n特定的几个 镜头提示\\n\\n看起来就像\\n\\n人类和助手之间有某种文档\\n\\n，他们正在交换某种嗯\\n\\n信息，然后在底部你将\\n\\n查询放在最后，\\n\\n基本模型将有点像\\n\\n条件 它本身就像一个\\n\\n有用的助手和某种答案，但这\\n\\n不是很可靠，\\n\\n并且在实践中效果不佳，尽管\\n\\n可以这样做，因此\\n\\n我们有不同的路径来\\n\\n使实际的 GBT 帮助不仅仅是基本\\n\\n模型文档完成器和 因此，这\\n\\n使我们进入监督微调阶段，因此\\n\\n在监督微调阶段，我们\\n\\n将收集少量但高质量\\n\\n的数据集，在这种情况下，我们\\n\\n将要求人工承包商收集表单提示\\n\\n格式的数据，\\n\\n并 理想的响应，我们将\\n\\n收集大量这样的数据，通常是\\n\\n数万个或类似的东西，\\n\\n然后我们仍然会对\\n\\n这些数据进行语言建模，因此\\n\\n算法上不会发生任何变化，我们\\n\\n只是交换训练集，以便它\\n\\n使用 作为互联网文档，这是一种\\n\\n大量的本地\\n\\n四基本上 QA 提示响应类型的\\n\\n数据，并且是低数量\\n\\n高质量的，\\n\\n所以我们仍然进行语言建模，\\n\\n然后在训练之后我们得到一个 sfd 模型\\n\\n，你可以实际部署这些模型，\\n\\n它们是 实际的助理，他们\\n\\n在某种程度上工作让我向您\\n\\n展示示例演示可能是什么样子\\n\\n所以这是人类\\n\\n承包商可能想出的东西这是\\n\\n一些随机提示您可以写一个关于术语垄断或类似术语\\n\\n的相关性的简短介绍吗\\n\\n\\n然后承包商也会写出\\n\\n一个理想的回复，当他们\\n\\n写出这些回复时，他们会遵循\\n\\n大量的标签文件，并被\\n\\n要求提供帮助、\\n\\n真实且无害，而\\n\\n这里的标签说明您可能无法\\n\\n阅读其他可以 我但是它们很长，\\n\\n这只是人们遵循\\n\\n指示并尝试完成\\n\\n这些提示，\\n\\n这就是数据集的样子，\\n\\n您可以训练这些模型，这\\n\\n在某种程度上有效，\\n\\n现在您实际上可以\\n\\n从这里继续管道， 进入来自\\n\\n人类反馈的 rlhf 强化学习，\\n\\n其中包括奖励\\n\\n建模和强化学习，所以\\n\\n让我介绍这一点，然后我会\\n\\n回到为什么你可能想要执行\\n\\n额外的步骤，以及它与 sft 模型相比如何，\\n\\n\\n所以 奖励建模步骤\\n\\n我们要做的就是将\\n\\n我们的数据收集转变为\\n\\n比较的形式，所以这里是\\n\\n我们的数据集的一个例子\\n\\n我有相同的提示相同的提示\\n\\ntop 要求助手\\n\\n编写一个程序或函数来\\n\\n检查给定的字符串是否是回文\\n\\n，然后我们要做的是采用\\n\\n我们已经训练过的 sft 模型并\\n\\n创建多个完成，因此在本\\n\\n例中我们有三个\\n\\n模型创建的完成结果，\\n\\n然后我们要求人们对这些\\n\\n完成情况进行排名，因此，如果您盯着它\\n\\n一段时间，顺便说一下，\\n\\n比较其中一些\\n\\n预测是非常困难的，这可能会花费\\n\\n人们甚至几个小时的时间 单个提示\\n\\n完成对，但假设我们\\n\\n认为其中一个比\\n\\n其他更好，依此类推，因此我们对它们进行排名，\\n\\n\\n然后我们可以用\\n\\n看起来非常像对这些之间\\n\\n所有可能的对进行二元分类的东西来跟踪\\n\\n\\n所以我们现在要做的就是将\\n\\n提示按行布局，并且\\n\\n这里所有三行的提示都是相同的，所以提示\\n\\n都是相同的，但是\\n\\n完成的情况有所不同，所以\\n\\n黄色标记来自 sft\\n\\n模型，那么我们的 要做的是我们在最后附加\\n\\n另一个特殊奖励呃读出令牌\\n\\n，我们基本上只\\n\\n在这个绿色令牌上监督 Transformer，\\n\\n并且 Transformer 会\\n\\n预测一些奖励，以判断\\n\\n该提示的完成程度\\n\\n，因此基本上它会猜测\\n\\n每个完成的质量，然后\\n\\n一旦它对每个完成进行猜测，\\n\\n我们也有基本事实，它\\n\\n告诉我们它们的排名，因此\\n\\n我们实际上可以强制其中一些\\n\\n数字应该比其他数字高得多，\\n\\n依此类推 我们将其公式化为\\n\\n损失函数，并训练我们的模型\\n\\n做出奖励预测，该预测\\n\\n\\n与所有这些承包商的比较得出的基本事实一致，\\n\\n这就是我们训练\\n\\n奖励模型的方式，并让我们能够对\\n\\n完成情况进行评分 对于提示，\\n\\n一旦我们有了奖励模型，我们就\\n\\n无法部署它，因为这\\n\\n本身作为助手不是很有用，\\n\\n但对于现在接下来的强化\\n\\n强化学习阶段非常有用，\\n\\n因为我们有一个\\n\\n可以评分的奖励模型 任何\\n\\n给定\\n\\n提示的任意完成的质量，所以我们在\\n\\n强化学习期间所做的就是我们基本上\\n\\n再次得到大量提示，\\n\\n现在我们\\n\\n针对奖励模型进行强化学习，所以\\n\\n这就是\\n\\n我们采取的单一提示 提示我们将其排成行，\\n\\n现在我们使用 sft 我们\\n\\n基本上使用我们想要\\n\\n训练的模型，该模型在 sft 模型中初始化\\n\\n以创建一些黄色的完成\\n\\n，然后我们再次附加奖励令牌\\n\\n，我们读出\\n\\n根据\\n\\n现在保持固定的奖励模型进行奖励，它不再改变\\n\\n，现在奖励模型告诉我们\\n\\n每个提示的每个完成的质量\\n\\n，所以我们\\n\\n可以做的是我们现在可以基本上\\n\\n应用相同的 语言建模损失\\n\\n函数，但我们目前正在对\\n\\n黄色标记进行训练，并且我们正在通过\\n\\n\\n奖励模型指示的奖励来权衡语言建模目标，因此\\n\\n作为第一行的示例，\\n\\n奖励模型表示这是一个相当\\n\\n高的分数 因此，\\n\\n我们在第一行中碰巧采样的所有令牌都\\n\\n将得到强化，\\n\\n并且它们将在未来获得更高的\\n\\n概率，\\n\\n相反，在第二行中，奖励\\n\\n模型确实不喜欢这种\\n\\n完成负数 1.2，所以 因此，\\n\\n我们在第二行中采样的每个令牌\\n\\n\\n未来的概率都会稍高一些，\\n\\n我们在许多批次的许多提示中一遍又一遍地执行此操作，\\n\\n\\n基本上我们得到了一个\\n\\n在这里创建黄色令牌的策略，它基本上全部\\n\\n根据我们在前一阶段训练的奖励模型，这里所有的完成都会\\n\\n得分很高，这\\n\\n\\n就是我们训练的方式，呃，这就是\\n\\n\\n现在的 rohf 管道，最后你会得到一个\\n\\n可以部署的模型，所以 举个\\n\\n例子，聊天 GPT 是 rlhf 模型，但\\n\\n您可能会遇到的其他一些模型，\\n\\n例如 kuna 13B 等，\\n\\n这些都是 sft 模型，因此我们\\n\\n有基本模型 sft 模型和 rlh\\n\\n模型，这有点像状态\\n\\n\\n现在为什么你想做 rlhf 所以一个\\n\\n不太令人兴奋的答案是\\n\\n它效果更好所以这\\n\\n来自导师 GBT 论文\\n\\n根据不久前的这些实验\\n\\n现在这些 PPO 模型是 rlhf 并且我们\\n\\n看到当我们把它们给人类时，它们基本上只是\\n\\n在很多比较中被首选，\\n\\n所以人类只是更\\n\\n喜欢\\n\\n来自我们宗教模型的基本令牌，与 sft\\n\\n模型相比，与被\\n\\n提示成为助手的基本模型相比，所以它\\n\\n只是 效果更好，但你可能会问为什么\\n\\n它效果更好，我不\\n\\n认为有一个像\\n\\n社区真正\\n\\n同意的令人惊奇的答案，但我只会提供\\n\\n一个可能的原因，它与 比较\\n\\n与生成在计算上的容易程度之间存在不对称性，\\n\\n\\n所以让我们以\\n\\n生成俳句为例假设我要求一个模型\\n\\n写一篇关于回形针的俳句，如果\\n\\n你是一个试图提供火车\\n\\n数据的承包商，那么想象一下你是一个\\n\\n基本上收集的承包商 sft 阶段的数据\\n\\n你应该如何为回形针创建一个\\n\\n漂亮的俳句 你可能\\n\\n不太擅长，但如果我\\n\\n给你一些俳句的例子，你\\n\\n可能会比其中一些\\n\\n俳句更欣赏\\n\\n因此，判断其中哪一个是好的是\\n\\n更容易的任务，因此基本上这种\\n\\n不对称使得比较成为\\n\\n一种更好的方式，可以潜在地利用\\n\\n自己作为一个人的能力，并且你的判断力可以\\n\\n创建一个稍微更好的模型，\\n\\n现在我们的宗教模型不是\\n\\n\\n在某些情况下，严格地对基本模型进行了改进，因此\\n\\n我们特别注意到，例如它们丢失了\\n\\n一些熵，这意味着它们\\n\\n给出了更多 PT 结果，它们可以输出\\n\\n更低的变化，就像它们可以输出\\n\\n比基本模型具有更低变化的样本一样\\n\\n所以基础模型有很多熵\\n\\n，我们会使用很多不同的\\n\\n输出，\\n\\n例如，\\n\\n我仍然更喜欢使用基础模型的一种地方\\n\\n是在设置中，\\n\\n你基本上有 N 个东西，并且你想要\\n\\n生成更多 类似的东西，所以\\n\\n这是我刚刚编写的一个例子，\\n\\n我想生成很酷的神奇宝贝名称，我\\n\\n给了它七个神奇宝贝名称，我要求\\n\\n基本模型完成文档，\\n\\n它给了我更多神奇宝贝名称，\\n\\n呃这些都是虚构的 我试着\\n\\n查找它们，我不相信它们是真正的\\n\\n口袋妖怪，呃，这是\\n\\n我认为基本模型擅长的任务，\\n\\n因为它仍然有很多熵，它\\n\\n会给你很多不同的很酷的东西\\n\\n更多的东西看起来像\\n\\n你之前给的任何东西，\\n\\n所以这就是呃这是数字，\\n\\n所有这些都说这些有点像\\n\\n\\n你现在可能可以使用的助理模型呃，\\n\\n伯克利有一个团队排名\\n\\n很多可用的辅助模型，\\n\\n并给了它们基本上 ELO 评级 所以\\n\\n目前一些最好的模型当然\\n\\n是 gpt4 我想说其次\\n\\n是 Claude gvt 3.5 然后是\\n\\n许多模型 其中一些可能可以作为\\n\\n权重使用，例如 kuna 考拉\\n\\n等，这里的前三行都是\\n\\nrohf 模型，据\\n\\n我所知，所有其他模型都是 sft\\n\\n模型，我相信\\n\\n没问题，所以这就是我们在高水平上训练这些模型的方式，\\n\\n现在我要\\n\\n切换齿轮 让我们看看如何\\n\\n最好地应用 GPT 助手\\n\\n模型来解决您的问题现在我想\\n\\n\\n在一个具体示例的设置中工作所以让我们\\n\\n\\n在这里使用一个具体示例\\n\\n假设您正在写一篇\\n\\n文章或一个 博客文章，你要\\n\\n在最后写下这句话\\n\\n加利福尼亚州的人口是\\n\\n阿拉斯加的 53 倍，所以出于某种原因你想\\n\\n比较这两个州的人口，\\n\\n\\n想想丰富的内心独白\\n\\n和工具的使用以及实际工作量\\n\\n在你的大脑中进行计算以\\n\\n生成最后一个句子，所以\\n\\n这可能是\\n\\n你大脑中的样子好吧，下一步\\n\\n让我写博客呃我的博客让我\\n\\n比较这两个群体\\n\\n好吧首先我显然需要\\n\\n现在我\\n\\n知道我可能不知道这些\\n\\n人群，所以\\n\\n我有点像意识到我知道什么或\\n\\n不知道我的自我认知，\\n\\n所以我去我 使用一些工具，我访问\\n\\n维基百科，查找加利福尼亚州的\\n\\n人口和阿拉斯加的人口，\\n\\n现在我知道我应该将两者相除，\\n\\n但我再次知道，将 3 9.2\\n\\n除以 0.74 不太可能成功，\\n\\n这不是我想要的那种事情 我可以\\n\\n在脑海中完成，因此我将\\n\\n依赖计算器，因此我将\\n\\n使用计算器输入它并查看\\n\\n输出大约为 53。\\n\\n然后也许我会\\n\\n在我的大脑中进行一些反思和健全性检查 所以 53\\n\\n很有意义吗？这是一个相当\\n\\n大的分数，但加利福尼亚是\\n\\n最受欢迎的州，所以也许\\n\\n看起来不错，所以\\n\\n然后我有了我\\n\\n可能需要的所有信息，现在我开始\\n\\n写作的创造性部分，所以我可能\\n\\n开始写一些像加州\\n\\n有 53 倍大的东西，然后我认为这\\n\\n实际上是非常\\n\\n尴尬的措辞，所以让我实际上\\n\\n删除它并让我再试一次，所以\\n\\n当我写的时候，我有一个单独的\\n\\n过程几乎检查我的内容 我正在\\n\\n写作并判断它是否看起来\\n\\n不错，\\n\\n然后也许我会删除它，也许我会\\n\\n重新构建它，然后也许我对结果感到满意\\n\\n\\n所以基本上是长话短说，在\\n\\n你的内心独白方面，在幕后发生了很多事情\\n\\n你创建\\n\\n这样的句子，但是\\n\\n当我们在\\n\\n其上训练 GPT 时，这样的句子是什么样的？\\n\\n从 gpt 的角度来看，这只是一个\\n\\n标记序列，\\n\\n所以 GPT 当它读取或生成\\n\\n这些标记时，它就会运行，\\n\\n每个块大致是\\n\\n每个 token 的计算工作量相同，\\n\\n这些 Transformer 并不是\\n\\n很浅的网络，它们有大约 80\\n\\n层推理，但 80 层仍然\\n\\n不算太多，所以这个 Transformer\\n\\n将尽力模仿，但\\n\\n当然这里的过程看起来\\n\\n与您采取的过程非常不同，\\n\\n特别是在\\n\\n我们创建的数据集中的最终工件中，然后\\n\\n最终将其提供给 llms，所有\\n\\n内部对话都被完全剥离\\n\\n，呃，与您不同的是，GPT 会查看\\n\\n每一个标记 并且\\n\\n在每一个上花费相同的计算量，\\n\\n所以你不能指望它实际上会\\n\\n很好，你不能指望它对\\n\\n每个令牌做太多的工作，\\n\\n所以特别是基本上\\n\\n这些 Transformer 就像令牌\\n\\n模拟器一样，所以他们不知道自己\\n\\n不知道什么，就像他们只是模仿\\n\\n下一个令牌一样，他们不知道自己\\n\\n擅长或不擅长什么，他们只是\\n\\n尽力模仿他们不擅长的下一个令牌\\n\\n不会在循环中反映出来，他们不会进行\\n\\n理智检查，默认情况下他们不会纠正\\n\\n错误，\\n\\n他们只是呃样本标记序列，\\n\\n\\n他们的头脑中没有单独的内部独白流来\\n\\n评估现在正在发生的事情\\n\\n我想说的是，他们确实有某种认知优势，那\\n\\n就是他们\\n\\n实际上拥有\\n\\n跨大量领域的大量基于事实的知识，\\n\\n因为他们有数百亿个\\n\\n参数，因此需要大量存储空间来存储\\n\\n大量数据。 事实，\\n\\n但我认为它们也有一个\\n\\n相对较大和完美的工作\\n\\n记忆，所以无论固定\\n\\n在上下文窗口中的什么内容，都可以\\n\\n\\n通过其内部的自我注意\\n\\n机制立即供 Transformer 使用，所以它有点像\\n\\n完美记忆，但它是 得到了有限的\\n\\n大小，但是 Transformer 可以非常\\n\\n直接地访问它，因此它可以\\n\\n无损地记住\\n\\n其上下文窗口内的任何内容，\\n\\n这就是我比较\\n\\n这两者的方式，我提出所有\\n\\n这些的原因是因为我 我认为在\\n\\n很大程度上，提示只是弥补\\n\\n这两种架构之间的认知差异，\\n\\n\\n就像我们的大脑和 llm 的大脑，\\n\\n你几乎可以这样看，\\n\\n所以这是人们发现的一件事，\\n\\n例如，效果很好 在\\n\\n实践中，特别是如果您的任务\\n\\n需要推理，您不能期望\\n\\nTransformer 对\\n\\n每个令牌进行太多推理，因此您必须\\n\\n真正将推理分散到\\n\\n越来越多的令牌上，例如您\\n\\n不能给 Transformer 一个非常\\n\\n复杂的问题，并期望它能\\n\\n在一个令牌中得到答案，\\n\\n只是没有足够的时间，这些\\n\\n变形金刚需要令牌来思考\\n\\n引用不引用我有时喜欢说，所以\\n\\n这是一些效果\\n\\n很好的东西，例如你可以 有一些镜头\\n\\n提示，向 Transformer 表明\\n\\n它应该在回答问题时展示其工作，\\n\\n\\n如果您给出一些\\n\\n示例，Transformer 将模仿\\n\\n该模板，并且最终会\\n\\n表现得更好\\n\\n\\n此外，您可以\\n\\n通过说让我们\\n\\n一步一步思考来从 Transformer 中引出这种行为，因为这种\\n\\n条件是 Transformer\\n\\n类似于显示其工作，并且因为它会\\n\\n突然进入显示其工作的模式，因此它会\\n\\n\\n每个令牌做的计算工作更少，因此它更有可能\\n\\n成功，因为\\n\\n\\n随着时间的推移，它会使推理速度变慢，这是\\n\\n另一个例子，这被\\n\\n称为自我一致性，我们看到我们\\n\\n有能力开始编写，\\n\\n然后如果没有的话' 不解决我可以再试\\n\\n一次，我可以尝试多次，\\n\\n呃，也许\\n\\n嗯，选择最有效的一个，所以在\\n\\n这些方法中，你可能\\n\\n不仅仅采样一次，但你可能采样\\n\\n多次，然后有一些\\n\\n过程来找到 那些\\n\\n不好的，然后只保留这些样本\\n\\n或进行多数投票或\\n\\n类似的事情，所以基本上这些\\n\\n变形金刚在这个过程中，因为他们\\n\\n像你一样预测下一个令牌，\\n\\n他们可能会运气不好，他们\\n\\n可以采样而不是一个 非常好的令牌\\n\\n，他们在推理方面可能会像死胡同一样走下去，\\n\\n所以\\n\\n与你不同的是，他们无法从中恢复过来，他们\\n\\n被困在他们采样的每一个令牌上\\n\\n，所以他们会继续这个\\n\\n序列，即使他们甚至知道\\n\\n这个 序列不会成功，\\n\\n所以让他们能够回顾\\n\\n检查或尝试找到呃尝试\\n\\n基本上围绕它进行采样这是\\n\\n一种技术，你也可以事实\\n\\n证明，实际上llms就像他们\\n\\n知道什么时候搞砸了一样 举个\\n\\n例子，\\n\\n假设你要求模型生成一首\\n\\n不押韵的诗，它可能\\n\\n会给你一首诗，但它实际上押韵，\\n\\n但事实证明，特别是对于\\n\\n像 gpt4 这样的更大的模型，你可以只问\\n\\n它你是否满足了作业并且\\n\\n实际上 gpt4 非常清楚它\\n\\n没有满足任务要求，它\\n\\n只是在采样中有点不幸，所以它\\n\\n会告诉你不，我实际上没有满足\\n\\n任务要求，让我再试一次，\\n\\n但如果没有你提示它它\\n\\n甚至不会 就像它不知道它不\\n\\n知道重新访问等等所以你\\n\\n必须在提示中弥补这一点\\n\\n你必须让它检查如果你不\\n\\n要求它检查它就不会检查\\n\\n就其本身而言，它只是一个令牌模拟器，\\n\\n我认为更一般地说，这些技术中的很多都属于\\n\\n\\n我所说的重新创建我们的系统的范畴，因此\\n\\n您可能会熟悉系统\\n\\n一系统，以人类的角度思考系统\\n\\n一是一个快速自动过程 我\\n\\n认为有点像法学硕士，\\n\\n只是对令牌进行采样，\\n\\n而系统二是你大脑中较慢的深思熟虑的\\n\\n计划，\\n\\n所以这是一篇实际上是\\n\\n上周的论文，因为这个空间正在\\n\\n快速发展，它被称为\\n\\n思想树 在思想树上，呃，\\n\\n这篇论文的作者建议\\n\\n为任何\\n\\n给定的提示维护多个完成，然后他们还会\\n\\n一路上对它们进行评分，并保留\\n\\n那些进展顺利的（如果\\n\\n有意义的话），所以很多人\\n\\n真的很喜欢\\n\\n\\n\\n\\n现在，我想在这里指出的一件事\\n\\n是，这不仅仅是一个提示，这\\n\\n实际上是提示\\n\\n与一些 python 胶水代码一起使用，因为\\n\\n你实际上不需要维护\\n\\n多个提示，而且你还必须\\n\\n在这里做一些树搜索算法，\\n\\n比如找出要扩展的提示\\n\\n等等，所以它是 python 胶水代码的共生\\n\\n和\\n\\n在 while 循环或更大的算法中调用的单独提示\\n\\n\\n我还认为这里与 alphago 有一个非常酷的\\n\\n相似之处 alphago 有一个\\n\\n在下围棋时放置下一颗棋子的策略，\\n\\n它的策略\\n\\n最初是通过模仿人类来训练的，但\\n\\n除此之外 对于这个策略，它也进行\\n\\n蒙特卡罗树搜索，基本上它\\n\\n会在头脑中发挥出多种可能性\\n\\n并评估所有这些可能性，\\n\\n只保留那些工作良好的，所以\\n\\n我认为这有点类似于\\n\\nalphago，但对于 文本，\\n\\n如果这有意义的话，\\n\\n就像思想树一样，我认为\\n\\n更普遍的是，人们开始真正\\n\\n喜欢探索更通用的\\n\\n技术，不仅仅是简单的\\n\\n问题答案提示，而且\\n\\n看起来更像是Python蓝色\\n\\n代码，将许多问题串在一起，\\n\\n等等 右边我有一个\\n\\n名为“反应”的论文的例子，其中他们将\\n\\n提示的答案构建为一系列\\n\\n思想行动观察思想\\n\\n行动观察，这是一个全面的\\n\\n推出，一种回答查询的思维过程，\\n\\n在这些行动中\\n\\n模型 也允许\\n\\n在左边输掉我有一个 rogpt 的例子，顺便说一句，\\n\\n现在 rgbt 变成了一个\\n\\n项目，我认为最近得到了很多炒作，\\n\\n我想呃，但我认为我\\n\\n仍然发现它有点鼓舞人心，很\\n\\n有趣 嗯，这是一个\\n\\n允许 llm 保留任务列表\\n\\n并继续递归地分解\\n\\n任务的项目，我认为这目前\\n\\n效果不是很好，我不会建议\\n\\n人们在实际应用中使用它，\\n\\n我只是认为它是\\n\\n通常会从事情的发展方向中获得灵感，\\n\\n我认为随着时间的推移，这\\n\\n\\n有点像让我们的模型\\n\\n系统思考下一件我\\n\\n觉得有趣的事情是\\n\\n下面的调查会说几乎\\n\\n心理上的 llms 的怪癖是 llms\\n\\n不 不想成功，他们想\\n\\n模仿\\n\\n你，想要成功，你应该提出\\n\\n要求，\\n\\n所以我的意思是，\\n\\n当变形金刚接受训练时，他们有\\n\\n训练集，并且他们的训练数据中可能有完整的\\n\\n性能质量范围，\\n\\n因此对于 例如，\\n\\n可能会出现某种\\n\\n物理问题或类似问题的提示，\\n\\n并且可能存在完全错误的学生解决方案，\\n\\n但\\n\\n也可能存在\\n\\n非常正确的专家答案，而变形金刚无法区分\\n\\n之间的区别 看起来我的\\n\\n意思是他们知道他们知道低\\n\\n质量解决方案和高质量\\n\\n解决方案，但默认情况下他们想要\\n\\n模仿所有这些，因为他们只是接受\\n\\n过语言建模培训，所以在\\n\\n测试时你实际上必须要求\\n\\n良好的性能，所以在 本文中的这个例子\\n\\n是，嗯，他们尝试了各种\\n\\n提示，让我们认为一步一步\\n\\n非常强大，因为它有点像\\n\\n展开剩余标记的推理，\\n\\n但更好的是\\n\\n让我们以一步一步的方式解决这个问题\\n\\n确保我们有正确的答案，\\n\\n所以这有点像\\n\\n获得正确答案的条件，这\\n\\n实际上使 Transformer 工作得\\n\\n更好，因为 Transformer\\n\\n现在不必\\n\\n在低质量解决方案上对冲其概率质量，\\n\\n就像那样荒谬 听起来，所以基本上不要\\n\\n随意要求一个强有力的解决方案，\\n\\n比如你是\\n\\n这个话题的领先专家，假装你有智商\\n\\n120等，但不要试图要求\\n\\n太多的智商，因为如果你要求的智商\\n\\n比如 400，你可能会脱离数据\\n\\n分发，或者更糟糕的是，你可能会\\n\\n参与一些科幻小说之类的数据分发，\\n\\n它会开始喜欢\\n\\n接受一些科幻小说或角色扮演\\n\\n或类似的东西，所以你必须\\n\\n找到合适的智商量 我认为\\n\\n\\n接下来会有一些 U 形曲线，\\n\\n正如我们在尝试解决\\n\\n我们知道自己擅长和\\n\\n不擅长的问题时所看到的那样，并且我们依靠计算工具\\n\\n希望对\\n\\n您的 LLMS 做同样的事情，所以\\n\\n特别是我们可能希望为他们提供\\n\\n计算器、代码解释器等\\n\\n进行搜索的能力，并且有\\n\\n很多技术可以做到这一点，\\n\\n需要再次记住的是，\\n\\n这些 默认情况下，变压器可能不\\n\\n知道他们不知道什么，因此您\\n\\n甚至可能想在提示中告诉变压器\\n\\n您不太擅长\\n\\n心算，每当您需要进行非常\\n\\n大的数字加法乘法或\\n\\n其他任何操作时，请使用这里的计算器\\n\\n你如何使用计算器使用\\n\\n这个标记组合等等所以你必须\\n\\n真正喜欢拼写它\\n\\n因为默认情况下的模型不\\n\\n知道它擅长或不擅长什么，\\n\\n就像你和我一样，你和\\n\\n我可能是\\n\\n下一个 我认为非常\\n\\n有趣的是，我们从一个只进行检索的世界一路\\n\\n\\n摆动到了另一个极端，\\n\\n在那里它的记忆仅存在于镜头中，但\\n\\n实际上在\\n\\n这些检索增强\\n\\n模型之间存在着整个空间，并且这个作品 在实践中非常好，\\n\\n\\n正如我提到的，变压器的上下文窗口\\n\\n是它的工作内存，如果您\\n\\n可以使用与任务相关的任何信息加载工作内存，则\\n\\n\\n模型将工作得非常好，\\n\\n因为它可以立即访问所有\\n\\n内存\\n\\n，所以我 我认为很多人\\n\\n真的对\\n\\n呃基本上检索增强\\n\\n生成感兴趣，在底部我有\\n\\n一个 llama 索引的例子，它是一种\\n\\n连接到许多\\n\\n不同类型数据的数据连接器，你可以呃\\n\\n你可以做的一切 可以索引所有\\n\\n这些数据，并且可以使其可供\\n\\nllms 和新兴配方访问，您可以\\n\\n获取相关文档，将\\n\\n它们分成多个块，然后嵌入所有这些数据\\n\\n，基本上得到了\\n\\n表示数据的嵌入向量，您将\\n\\n其存储在 矢量存储，然后在\\n\\n测试时，您\\n\\n对矢量存储进行某种查询，并获取\\n\\n可能与您的任务相关的块，并将\\n\\n它们填充到提示中，\\n\\n然后生成，因此这\\n\\n在实践中可以很好地工作，所以这 我\\n\\n认为类似于当你和我解决\\n\\n问题时，你可以凭记忆做所有事情，\\n\\n变形金刚有非常大\\n\\n和广泛的记忆，但它也确实\\n\\n有助于参考一些主要\\n\\n文档，所以当你发现\\n\\n自己回到教科书时\\n\\n查找某些内容，或者每当您发现\\n\\n自己要返回\\n\\n库的文档以查找\\n\\n转换变压器时，绝对当我也这样\\n\\n做时，您有一个记忆，您对\\n\\n\\n库的某些文档如何工作有一些记忆，但它要好得多\\n\\n查找它，所以这同样适用于\\n\\n\\n接下来我想简单地谈谈\\n\\n约束提示我也发现这\\n\\n非常有趣\\n\\n呃这基本上是\\n\\n\\n在 llms 的输出中强制使用某个模板的技术\\n\\n所以指导\\n\\n实际上是来自微软的一个例子 呃，\\n\\n这里我们强制\\n\\nllm 的输出将是 Json，这\\n\\n实际上保证了输出将\\n\\n采用这种形式，因为它们进入并\\n\\n扰乱了\\n\\n从 Transformer 出来的所有不同令牌的概率，\\n\\n并且如果 他们夹紧这些\\n\\n标记，然后 Transformer 仅\\n\\n填充此处的空白，然后您\\n\\n可以对\\n\\n可以进入这些空白的内容实施额外的限制，因此这\\n\\n可能非常有用，我认为这种\\n\\n约束采样也\\n\\n非常有趣，\\n\\n我也想要 就\\n\\n微调而言，您可以在\\n\\n快速工程方面取得很大进展，\\n\\n但也可以考虑\\n\\n微调您的模型，现在微调\\n\\n模型意味着您实际上将\\n\\n更改模型的权重 模型\\n\\n\\n在实践中做到这一点变得更加容易，这是因为\\n\\n\\n最近已经开发了许多技术并拥有用于呃的库，\\n\\n例如\\n\\n像 Laura 这样的参数高效微调技术，\\n\\n确保您只 趋势是，\\n\\n您只训练模型的小型 Spar 种类，\\n\\n因此大部分模型都\\n\\n被固定在基础模型上，并且\\n\\n允许更改其中的某些部分，\\n\\n这在经验上仍然工作得很好，\\n\\n并且使得仅进行调整的成本要低得多\\n\\n模型的小部分，\\n\\n呃，这也意味着，\\n\\n因为模型的大部分都是固定的，所以\\n\\n您可以使用非常低的精度推理\\n\\n来计算这些部分，因为它们\\n\\n不会通过梯度下降进行更新\\n\\n，因此这使得一切变得\\n\\n更加高效 此外，正如\\n\\n\\n我提到的，目前我们有许多基于开源高质量的模型，\\n\\n我认为 llama 非常好，\\n\\n尽管它没有商业许可，\\n\\n我相信现在需要\\n\\n记住的一些事情是，\\n\\n基本上微调是一个 涉及更多的\\n\\n技术，它需要更多\\n\\n的技术，我认为\\n\\n要做正确的事情需要技术专业知识，它需要人类数据\\n\\n承包商来处理数据集和/或\\n\\n合成数据管道，这可能\\n\\n非常复杂，这肯定会\\n\\n大大减慢您的迭代周期，\\n\\n我会 说在高水平上 sft 是\\n\\n可以实现的，因为它只是你\\n\\n持续的语言建模任务它\\n\\n相对简单，但 rlhref 我\\n\\n想说这是一个非常多的研究\\n\\n领域，甚至更难\\n\\n开始工作，所以我可能不会\\n\\n建议有人只是尝试 推出\\n\\n自己的 rlh 实现，这些\\n\\n东西非常不稳定，非常\\n\\n难以训练，呃，\\n\\n我认为现在对初学者非常友好，\\n\\n而且它也可能会\\n\\n很快发生变化，\\n\\n所以我认为这些是我的默认\\n\\n建议 现在，\\n\\n我会将您的任务分为两个\\n\\n主要部分，第一部分实现您的最佳\\n\\n性能，第二部分按此顺序优化您的\\n\\n性能第一，\\n\\n\\n目前最好的性能来自 GT4 模型，它是\\n\\n迄今为止最有能力\\n\\n使用提示的 非常详细，他们\\n\\n有很多任务内容，相关的\\n\\n相关信息和说明，就像\\n\\n\\n如果任务承包商无法给\\n\\n您回复电子邮件，您会告诉他们什么一样，但也要\\n\\n记住，任务承包商是人\\n\\n，他们有内在的想法。 独白，\\n\\n他们非常聪明等等，LLM不\\n\\n具备这些品质，所以一定要仔细\\n\\n思考LLM的心理，\\n\\n并提供提示，\\n\\n检索并在这些提示中添加任何相关的上下文\\n\\n和信息，这些提示\\n\\n基本上指的是很多提示\\n\\n工程技术其中一些\\n\\n在上面的幻灯片中突出显示，但这\\n\\n是一个非常大的空间，我\\n\\n只是建议您在网上寻找快速的\\n\\n工程技术，那里\\n\\n有很多内容要涵盖，\\n\\n用几个商店示例进行实验，\\n\\n这指的是您不知道的内容 不仅仅是想\\n\\n告诉你想尽可能地展示，\\n\\n所以给它提供\\n\\n所有的例子，帮助它真正\\n\\n理解你的意思，如果你可以使用\\n\\n工具和插件来卸载\\n\\n对 llms 来说很难的任务，\\n\\n然后考虑不仅仅是 一个\\n\\n提示和答案考虑潜在的\\n\\n链和反射，以及如何将\\n\\n它们粘合在一起以及如何\\n\\n可能制作多个样本等等\\n\\n\\n最后，如果您认为自己已经挤出了\\n\\n提示工程，我认为您\\n\\n应该坚持一段时间看看 在\\n\\n某些方面可能会根据\\n\\n您的应用程序对模型进行微调，但预计这会\\n\\n慢得多并且不断发展，然后\\n\\n\\n这里有一个专家脆弱的研究区，我会说这是 rlhf，如果可以的话，它\\n\\n目前确实比 sft 工作得更好一些\\n\\n让它工作，但\\n\\n我想说的是，这非常复杂，\\n\\n为了优化你的成本，尝试\\n\\n探索较低容量的模型或更短的\\n\\n提示等等，\\n\\n我还想就\\n\\n我认为 llms 目前的用例说几句话\\n\\n非常适合，所以\\n\\n特别要注意的是，\\n\\n今天的 llms 有很多限制，\\n\\n所以我会在\\n\\n所有应用程序模型中牢记这一点，\\n\\n顺便说一句，这可能是一个完整的\\n\\n演讲，所以我没有时间\\n\\n详细介绍模型可能有偏见 他们\\n\\n可能编造幻觉信息\\n\\n他们可能有推理错误 他们可能\\n\\n在整个类别的\\n\\n应用程序中陷入困境 他们有知识限制，\\n\\n因此他们可能不知道上述任何信息\\n\\n说 2021 年 9 月 他们很\\n\\n容易受到大范围的影响\\n\\n有点像\\n\\nTwitter 上每天都会出现的攻击，包括即时注入\\n\\n越狱攻击、数据中毒攻击\\n\\n等，所以我现在的建议\\n\\n是在低风险应用程序中使用 llms，将\\n\\n它们与人工监督结合起来，将\\n\\n它们作为\\n\\n灵感和建议的来源 并认为\\n\\n副驾驶而不是完全\\n\\n自主的代理就像在\\n\\n某个地方执行任务一样，只是\\n\\n\\n现在还不清楚模型是否在那里，\\n\\n所以我想说 gpt4\\n\\n是一个令人惊叹的神器，我非常感谢\\n\\n它的存在 它很漂亮，\\n\\n它拥有跨多个领域的大量知识，\\n\\n可以编写数学代码等等，\\n\\n此外，还有一个蓬勃发展的生态\\n\\n系统，其中包括正在构建的所有其他内容，\\n\\n并将其纳入\\n\\n生态系统中，其中一些\\n\\n东西是这些 我已经讨论过的事情，\\n\\n所有这些功能都触手可及，\\n\\n\\n所以这里是向\\n\\nGPT 询问问题以\\n\\n提示它并获得响应所需的一切代码，在这种\\n\\n情况下我说，呃，你能说一些\\n\\n启发性的话吗？  Microsoft build 2023 的观众，\\n\\n\\n我刚刚将其输入 Python 并\\n\\n逐字 gpt4 说了以下内容，呃，顺便说一句，\\n\\n我不知道\\n\\n他们在主题演讲中使用了这个技巧，所以我\\n\\n认为我很聪明，\\n\\n但它真的很擅长\\n\\n女士们、先生们，入侵者和\\n\\n开拓者，微软构建 2023\\n\\n欢迎来到杰出思想的聚会，\\n\\n你们是\\n\\n未来的建筑师，\\n\\n塑造\\n\\n人类繁荣发展的数字领域的远见者，拥抱\\n\\n技术的无限可能性，让\\n\\n您的想法飞翔 发挥你的\\n\\n想象力，让我们一起为子孙后代创造一个更加\\n\\n互联、非凡、包容的世界，\\n\\n准备好\\n\\n释放你的创造力，画布\\n\\n未知的事物，将梦想变成现实，\\n\\n你的旅程从今天开始\\n\\n好吗？\\n\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# methods of file objects\n",
    "with open('build.txt', encoding=\"utf-8\") as f:\n",
    "    f.read()\n",
    "    f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f2f924b-eba0-46d0-8df6-18bac1b8dfc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"foreign  [Music]  researcher and founding member of open  AI Andre carpathy  [Applause]  hi everyone  I'm happy to be here to tell you about  the state of gbt and more generally  about the rapidly growing ecosystem of  large language models  so I would like to partition the talk  into two parts  in the first part I would like to tell  you about how we train GPT assistance  and then in the second part we will we  are going to take a look at how we can  use these assistants effectively for  your applications  so first let's take a look at the  emerging recipe for how to train these  assistants and keep in mind that this is  all very new and still rapidly evolving  but so far the recipe looks something  like this  now this is kind of a complicated slide  so I'm going to go through it piece by  piece but roughly speaking we have four  major stages free training supervised  fine tuning reward modeling  reinforcement learning and they follow  each other serially  now in each stage we have a data set  that is that powers that stage we have  an algorithm that for our purposes will  be a objective and a and over for uh for  training the neural network and then we  have a resulting model and then there's  some notes on the bottom so the first  stage we're going to start with is the  pre-training stage now this stage is  kind of special in this diagram and this  diagram is not to scale because this  stage is where all of the computational  work basically happens this is 99 of the  training uh compute time and also flops  and so this is where we are dealing with  internet scale data sets with thousands  of gpus in the store computer and also  months of training potentially the other  three stages are fine-tuning stages that  are much more along the lines of a small  few number of gpus and hours or days  so let's take a look at the pre-training  stage to achieve a base model  first we're going to gather a large  amount of data here's an example of what  we call a data mixture that comes from  this uh paper that was released by meta  where they released this llama based  model now you can see roughly the kinds  of data sets that enter into these  collections so we have common crawl  which is just a web scrape C4 which is  also a common crawl and then some high  quality data sets as well so for example  GitHub Wikipedia books archive stack  exchange and so on these are all mixed  up together and then they are sampled  According to some given proportions and  that forms the training set for the  neural net for the GPT  now before we can actually train on this  data we need to go through one more  pre-processing step and that is  tokenization and this is basically a  translation of the raw text that we  scrape from the internet into sequences  of integers because that's the native  representation over which gpts function  now this is a lossless kind of  translation between pieces of text and  tokens and integers and there are a  number of algorithms for this stage  typically for example you could use  something like byte pair encoding which  iteratively merges little text chunks  and groups them into tokens  and so here I'm showing some example  chunks of these tokens and then this is  the raw integer sequence that will  actually feed into a Transformer  now here I'm showing two uh sort of like  examples for hyper parameters that  govern this stage so gpt4 we did not  release too much information about how  it was trained and so on so I'm using  gpt3's numbers but gpt3 is of course a  little bit old by now about three years  ago but llama is a fairly recent model  from meta so these are roughly the  orders of magnitude that we're dealing  with when we're doing pre-training  the vocabulary size is usually a couple  10 000 tokens the context length is  usually something like two thousand four  thousand or nowadays even 100 000 and  this governs the maximum number of  integers that the GPT will look at when  it's trying to predict the next integer  in a sequence  um you can see that roughly the number  of parameters is say 65 billion for  llama now even though llama has only 65b  parameters compared to gvt3's 175  billion parameters llama is a  significantly more powerful model and  intuitively that's because the model is  trained for significantly longer in this  case 1.4 trillion tokens instead of just  300 billion tokens so you shouldn't  judge the power of a model just by the  number of parameters that it contains  below I'm showing some tables of rough  number of rough hyper parameters that  typically go into specifying the  Transformer neural network so the number  of heads the dimension size number of  layers and so on and on the bottom I'm  showing some training hyper parameters  so for example to train the 65b model  meta used 2000 gpus roughly 21 days  train of training and and roughly  several million dollars and so that's  the rough orders of magnitude that you  should have in mind for the pre-training  stage  now when we're actually pre-chaining  what happens roughly speaking we are  going to take our tokens and we're going  to lay them out into Data batches so we  have these arrays that will feed into  the Transformer and these arrays are B  the batch size and these are all  independent examples stacked up in rows  and B by T T being the maximum context  length so in my picture I only have 10.  the context length so this could be two  thousand four thousand Etc so these are  extremely long rows and what we do is we  take these documents and we pack them  into rows and we delimit them with these  Special end of text tokens uh basically  it's on the Transformer where a new  document begins  and so here I have a few examples of  documents and then I've stretched them  out into uh into this input  now we're going to feed all of these  numbers into Transformer and let me let  me just focus on a single particular  cell but the same thing will happen at  every every cell in this diagram so  let's look at the green cell the green  cell is going to take a look at all of  the tokens before it so all of the  tokens in yellow and we're going to feed  that entire context into the Transformer  neural network  and the Transformer is going to try to  predict the next token in the sequence  in this case in red  now the Transformer I don't have too  much time to unfortunately go into the  full details of this neural network  architecture is just a large blob of  neural net stuff for our purposes and  it's got several 10 billion parameters  typically or something like that and of  course as they tune these parameters  you're getting slightly different uh  predicted distributions for every single  one of these cells  and so for example if our vocabulary  size is 50 257 tokens then we're going  to have that many numbers because we  need to specify a probability  distribution for what comes next so  basically we have a probability for  whatever may follow now in this specific  example for the specific cell 513 will  come next and so we can use this as a  source of supervision to update our  Transformers weights and so we're  applying this basically on every single  cell in parallel and we keep swap  swapping batches and we're trying to get  the Transformer to make the correct  predictions over what token comes next  in a sequence  so let me show you more concretely what  this looks like when you train one of  these models this is actually coming  from a New York Times and they trained a  small GPT on Shakespeare and so here's a  small snippet of Shakespeare and they  trained a GPT on it now in the beginning  at initialization the GPT starts with  completely random weights so you're just  getting completely random outputs as  well  but over time as you train uh the GPT  longer and longer you are getting more  and more coherent and consistent uh sort  of samples from the model and the way  you sample from it of course is you uh  predict what comes next you sample from  that distribution and you keep feeding  that back into the process and you can  basically uh sample large sequences and  so by the end you see that the  Transformer has learned about words and  where to put spaces and where to put  commas and so on and so we're making  more and more consistent predictions  over time  these are the kinds of plots that you're  looking at when you're doing model  pre-training effectively we're looking  at the loss function over time as you  train and low loss means that our  Transformer is predicting the correct uh  is giving a higher probability to the  correct next integer in a sequence  now what are we going to do with this  model once we've trained it after a  month well the first thing that we  noticed we the field is that these  models basically in the process of  language modeling learn very powerful  General representations and it's  possible to very efficiently fine-tune  them for any arbitrary Downstream task  you might be interested in so as an  example if you're interested in  sentiment classification the approach  used to be that you collect a bunch of  positives and negatives and then you  train some kind of an NLP model for for  that but the new approach is ignore  sentiment classification  go off and do large language model  pre-training train the large Transformer  and then you can only you may only have  a few examples and you can very  efficiently fine tune your model for  that task and so this works very well in  practice and the reason for this is that  basically the Transformer is forced to  multitask a huge amount of tasks in the  language modeling task because just just  in terms of predicting the next token  it's forced to understand a lot about  the structure of the of the text and all  the different concepts they're in  so that was GPT one now around the time  of gpt2 people noticed that actually  even better than fine tuning you can  actually prompt these models very  effectively so these are language models  and they want to complete documents so  you can actually trick them into  performing tasks just by arranging these  fake documents so in this example for uh  for example we have some passage and  then we sort of like do QA qaqa this is  called a few shot prompt and then we do  q and then as the Transformer is trying  to complete the document it's actually  answering our question and so this is an  example of prompt engineering a base  model making the belief that it's sort  of imitating a document and getting it  to perform a task  and so this kicked off I think the era  of I would say prompting over  fine-tuning and seeing that this  actually can work extremely well on a  lot of problems even without training  any neural networks fine-tuning or so on  now since then we've seen an entire  evolutionary tree of Base models that  everyone has trained not all of these  models are available for example the  gpt4 base model was never released the  gpt4 model that you might be interacting  with over API is not a base model it's  an assistant model and we're going to  cover how to get those in a bit gpt3  based model is available via the API  under the name DaVinci and gpt2 base  model is available even as weights on  our GitHub repo but currently the best  available base model probably is the  Llama series from meta although it is  not commercially licensed  now one thing to point out is base  models are not assistance they don't  want to answer to they don't want to  make answers to your questions they just  want to complete documents so if you  tell them write a poem about the brand  cheese it will just you know it will  answer questions with more questions  it's just completing what it thinks as a  document  however you can prompt them in a  specific way for base models that that  is more likely to work so as an example  here's a poem about bread and cheese and  in that case it will auto-complete  correctly  you can even trick base models into  being assistance and the way you would  do this is you would create like a  specific few shot prompt that makes it  look like there's some kind of a  document between a human and assistant  and they're exchanging sort of um  information and then at the bottom you  sort of put your query at the end and  the base model will sort of like  condition itself into being like a  helpful assistant and kind of answer but  this is not very reliable and doesn't  work super well in practice although it  can be done  so instead we have a different path to  make actual gbt assistance not just base  model document completers and so that  takes us into supervised fine-tuning so  in the supervised fine tuning stage we  are going to collect small but high  quality data sets and in this case we're  going to ask human contractors to gather  data of the format of the form prompt  and ideal response and we're going to  collect lots of these typically tens of  thousands or something like that  and then we're going to still do  language modeling on this data so  nothing change algorithmically we're  just swapping out a training set so it  used to be internet documents which is a  high quantity local  four basically QA prompt response kind  of data and that is low quantity high  quality  so we still do language modeling and  then after training we get an sfd model  and you can actually deploy these models  and they are actual assistants and they  work to some extent let me show you what  an example demonstration might look like  so here's something that a human  contractor might come up with here's  some random prompt can you write a short  introduction about the relevance of the  term monopsony or something like that  and then the contractor also writes out  an ideal response and when they write  out these responses they are following  extensive labeling documentations and  they're being asked to be helpful  truthful and harmless and those labeling  instructions here you probably can't  read it another can I but they're long  and this is just people following  instructions and trying to complete  these prompts  so that's what the data set looks like  and you can train these models and this  works to some extent  now you can actually continue the  pipeline from here on and go into rlhf  reinforcement learning from Human  feedback that consists of both reward  modeling and reinforcement learning so  let me cover that and then I'll come  back to why you may want to go through  the extra steps and how that compares to  just sft models  so in the reward modeling step what  we're going to do is we're now going to  shift our data collection to be of the  form of comparisons so here's an example  of what our data set will look like  I have the same prompt identical prompt  on the top which is asking the assistant  to write a program or function that  checks if a given string is a palindrome  and then what we do is we take the sft  model which we've already trained and we  create multiple completions so in this  case we have three completions that the  model has created  and then we ask people to rank these  completions so if you stare at this for  a while and by the way these are very  difficult things to do to compare some  of these predictions and this can take  people even hours for a single prompt  completion pairs but let's say we  decided that one of these is much better  than the others and so on so we rank  them  then we can follow that with something  that looks very much kind of like a  binary classification on all the  possible pairs between these completions  so what we do now is we lay out our  prompt in rows and the prompts is  identical across all three rows here so  it's all the same prompt but the  completion of this vary and so the  yellow tokens are coming from the sft  model then what we do is we append  another special reward uh readout token  at the end and we basically only  supervise the Transformer at this single  green token and the Transformer will  predict some reward for how good that  completion is for that prompt  and so basically it makes a guess about  the quality of each completion and then  once it makes a guess for every one of  them we also have the ground truth which  is telling us the ranking of them and so  we can actually enforce that some of  these numbers should be much higher than  others and so on WE formulate this into  a loss function and we train our model  to make reward predictions that are  consistent with the ground truth coming  from the comparisons from all these  contractors so that's how we train our  reward model and that allows us to score  how good a completion is for a prompt  once we have a reward model this is we  can't deploy this because this is not  very useful as an assistant by itself  but it's very useful for the reinforce  reinforcement learning stage that  follows now because we have a reward  model we can score the quality of any  arbitrary completion for any given  prompt so what we do during  reinforcement learning is we basically  get again a large collection of prompts  and now we do reinforcement learning  with respect to the reward model so  here's what that looks like  we we take a single prompt we lay it out  in rows and now we use the sft we use  the basically the model we'd like to  train which is initialized at sft model  to create some completions in yellow  and then we append the reward token  again and we read off the reward  according to the reward model which is  now kept fixed it doesn't change anymore  and now the reward model tells us the  quality of every single completion for  each for these prompts and so what we  can do is we can now just basically  apply the same language modeling loss  function but we're currently training on  the yellow tokens and we are weighing  the language modeling objective by the  rewards indicated by the reward model so  as an example in the first row the  reward model said that this is a fairly  high score in completion and so all the  tokens that we happen to sample on the  first row are going to get reinforced  and they're going to get higher  probabilities for the future  conversely on the second row the reward  model really did not like this  completion negative 1.2 and so therefore  every single token that we sampled in  that second row is going to get a  slightly higher probability for the  future and we do this over and over on  many prompts on many batches and  basically we get a policy which creates  yellow tokens here and it basically all  of them all of the completions here will  score high according to the reward model  that we trained in the previous stage  so that's how we train uh that's what  the rohf pipeline is  now and then at the end you get a model  that you could deploy and so and as an  example chat GPT is an rlhf model but  some other models that you might come  across like for example of the kuna 13B  and so on these are sft models so we  have base models sft models and rlh  models and that's kind of like the state  of things there  now why would you want to do rlhf so one  answer that is kind of not that exciting  is that it just works better so this  comes from the instructor gbt paper  according to these experiments a while  ago now these PPO models are rlhf and we  see that they are basically just  preferred in a lot of comparisons when  we give them to humans so humans just  prefer out basically tokens that come  from our religion models compared to sft  models compared to base model that is  prompted to be an assistant and so it  just works better but you might ask why  why does it work better and I don't  think that there's a single like amazing  answer that the community has really  like agreed on but I will just offer one  uh one reason potentially and it has to  do with the asymmetry between how easy  computationally it is to compare versus  generate so let's take an example of  generating a haiku suppose I ask a model  to write a haiku about paper clips if  you're a contractor trying to give train  data then imagine being a contractor  collecting basically data for the sft  stage how are you supposed to create a  nice haiku for a paperclip you might  just not be very good at that but if I  give you a few examples of haikus you  might be able to appreciate some of  these haikus a lot more than others and  so judging which one of these is good is  much easier task and so basically this  asymmetry makes it so that comparisons  are a better way to potentially leverage  yourself as a human and your judgment to  create a slightly better model  now our religious models are not  strictly an improvement on the base  models in some cases so in particular  we've noticed for example that they lose  some entropy so that means that they  give More PT results they can output  lower variations uh like they can output  samples with lower variation than base  model so base model has lots of entropy  and we'll go with lots of diverse  outputs  so for example one one kind of place  where I still prefer to use a base model  is in the setup where  you basically have N Things and you want  to generate more things like it and so  here is an example that I just cooked up  I want to generate cool Pokemon names I  gave it seven Pokemon names and I asked  the base model to complete the document  and it gave me a lot more Pokemon names  uh these are fictitious I tried to look  them up I don't believe they're actual  Pokemons and uh this is the kind of task  that I think base model would be good at  because it still has lots of entropy it  will give you lots of diverse cool kind  of more things that look like whatever  you give it before  so this is what uh this is number having  all said all that these are kind of like  the assistant models that are probably  available to you at this point uh that  there's a team at Berkeley that ranked a  lot of the available assistant models  and gave them basically ELO ratings So  currently some of the best models of  course are gpt4 by far I would say  followed by Claude gvt 3.5 and then a  number of models some of these might be  available as weights like the kuna koala  Etc and the first three rows here are  all they're all rohf models and all of  the other models to my knowledge are sft  models I believe  okay so that's how we train these models  on the high level now I'm going to  switch gears and let's look at how we  can best apply and the GPT assistant  model to your problems now I would like  to work  in a setting of a concrete example so  let's  let's work with a concrete example here  let's say that you are working on an  article or a blog post and you're going  to write this sentence at the end  California's population is 53 times that  of Alaska so for some reason you want to  compare the populations of these two  states  think about the rich internal monologue  and Tool use and how much work actually  goes computationally in your brain to  generate this one final sentence so  here's maybe what that could look like  in your brain okay for this next step  let me blog uh for my blog let me  compare these two populations  okay first I'm going to obviously need  to get both of these populations now I  know that I probably don't know these  populations off the top of my head so  I'm kind of like aware of what I know or  don't know of my self-knowledge right  so I go I do some tool use and I go to  Wikipedia and I look up California's  population and Alaska's population  now I know that I should divide the two  but again I know that dividing three 9.2  by 0.74 is very unlikely to succeed  that's not the kind of thing that I can  do in my head and so therefore I'm gonna  rely on the calculator so I'm going to  use a calculator punch it in and see  that the output is roughly 53.  and then maybe I do some reflection and  Sanity checks in my brain so does 53  make sense well that's quite quite a  large fraction but then California is  the most popular state so maybe that  looks okay  so then I have all the information I  might need and now I get to the sort of  creative portion of writing so I might  start to write something like California  has 53 x times greater and then I think  to myself that's actually like really  awkward phrasing so let me actually  delete that and let me try again and so  as I'm writing I have this separate  process almost inspecting what I'm  writing and judging whether it looks  good or not  and then maybe I delete and maybe I  reframe it and then maybe I'm happy with  what comes out  so basically long story short a ton  happens under the hood in terms of your  internal monologue when you create  sentences like this but what does a  sentence like this look like when we are  training a GPT on it  From gpt's perspective this is just a  sequence of tokens  so GPT when it's reading or generating  these tokens it just goes  and each chunk is roughly the same  amount of computational work for each  token and these Transformers are not  very shallow networks they have about 80  layers of reasoning but 80 is still not  like too much and so this Transformer is  going to do its best to imitate but of  course the process here looks very very  different from the process that you took  so in particular in our final artifacts  in the data sets that we create and then  eventually feed to llms all of that  internal dialogue is completely stripped  and uh and unlike you the GPT will look  at every single token and spend the same  amount of compute on every one of them  and so you can't expect it to actually  like well you can't expect it to do to  uh sort of do too much work per token  so and also in particular basically  these Transformers are just like token  simulators so they don't know what they  don't know like they just imitate the  next token they don't know what they're  good at or not good at they just tried  their best imitate the next token they  don't reflect in the loop they don't  sanity check anything they don't correct  their mistakes along the way by default  they just uh sample token sequences  they don't have separate inner monologue  streams in their head right that are  evaluating what's happening now they do  have some sort of cognitive advantages I  would say and that is that they do  actually have a very large fact-based  knowledge across a vast number of areas  because they have say several 10 billion  parameters so it's a lot of storage for  a lot of facts  but and they also I think have a  relatively large and perfect working  memory so whatever fixed into the  whatever fits into the context window is  immediately available to the Transformer  through its internal self-attention  mechanism and so it's kind of like um  Perfect Memory but it's got that finite  size but the Transformer has a very  direct access to it and so it can like  losslessly remember anything that is  inside its context window  so that's kind of how I would compare  those two and the reason I bring all of  this up is because I think to a large  extent prompting is just making up for  this sort of cognitive difference  between these two kind of architectures  um like our brains here and llm brains  you can look at it that way almost  so here's one thing that people found  for example works pretty well in  practice especially if your tasks  require reasoning you can't expect the  Transformer to make to to do too much  reasoning per token and so you have to  really spread out the reasoning across  more and more tokens so for example you  can't give a Transformer a very  complicated question and expect it to  get the answer in a single token there's  just not enough time for it these  Transformers need tokens to think quote  unquote I like to say sometimes and so  this is some of the things that work  well you may for example have a few shot  prompt that shows the Transformer that  it should like show its work when it's  answering a question when it's answering  the question and if you give a few  examples the Transformer will imitate  that template and it will just end up  working out better in terms of its  evaluation  Additionally you can elicit this kind of  behavior from the Transformer by saying  let's think step by step because this  condition is the transformer into sort  of like showing its work and because it  kind of snaps into a mode of showing its  work it's going to do less computational  work per token and so it's more likely  to succeed as a result because it's  making  um slower reasoning over time  here's another example this one is  called self-consistency we saw that we  had the ability to start writing and  then if it didn't work out I can try  again and I can try multiple times and  uh and maybe  um select the one that worked best so in  these kinds of approaches you may sample  not just once but you may sample  multiple times and then have some  process for finding the ones that aren't  good and then keeping just those samples  or doing a majority vote or something  like that so basically these  Transformers in the process as they  predict the next token just like you  they can get unlucky and they can they  could sample and not a very good token  and they can go down sort of like a  blind alley in terms of reasoning and so  unlike you they cannot recover from that  they are stuck with every single token  they sample and so they will continue  the sequence even if they even know that  this sequence is not going to work out  so give them the ability to look back  inspect or try to find uh try to  basically sample around it  here's one technique also you could it  turns out that actually llms like they  know when they've screwed up so as an  example  say you asked the model to generate a  poem that does not rhyme and it might  give you a poem but it actually Rhymes  but it turns out that especially for the  bigger models like gpt4 you can just ask  it did you meet the assignment and  actually gpt4 knows very well that it  did not meet the assignment it just kind  of got unlucky in its sampling and so it  will tell you no I didn't actually meet  the assignment here's let me try again  but without you prompting it it doesn't  even like it doesn't know it doesn't  know to revisit and uh and so on so you  have to make up for that in your prompt  you have to get it to check if you don't  ask it to check it's not going to check  by itself it's just a token simulator  I think more generally a lot of these  techniques fall into the bucket of what  I would say recreating our system too so  you might be familiar with the system  one system to thinking for humans system  one is a fast automatic process and I  think kind of corresponds to like an llm  just sampling tokens  and system two is the slower deliberate  planning sort of part of your brain  and so this is a paper actually from  just last week because this space is  pretty quickly evolving it's called tree  of thought and in tree of thought uh the  authors of this paper propose  maintaining multiple completions for any  given prompt and then they are also  scoring them along the way and keeping  the ones that are going well if that  makes sense and so a lot of people are  like really playing around with kind of  um prompt engineering to um to basically  bring back some of these abilities that  we sort of have in our brain for llms  now one thing I would like to note here  is that this is not just a prompt this  is actually prompts that are together  used with some python glue code because  you don't you actually have to maintain  multiple prompts and you also have to do  some tree search algorithm here um to  like figure out which prompt to expand  Etc so it's a symbiosis of python glue  code and individual prompts that are  called in a while loop or in a bigger  algorithm  I also think there's a really cool  parallel here to alphago alphago has a  policy for placing the next Stone when  it plays go and its policy was trained  originally by imitating humans but in  addition to this policy it also does  Monte Carlo tree search and basically it  will play out a number of possibilities  in its head and evaluate all of them and  only keep the ones that work well and so  I think this is kind of an equivalent of  alphago but for text  if that makes sense  so just like tree of thought I think  more generally people are starting to  like really Explore More General  techniques of not just the simple  question answer prompt but something  that looks a lot more like python blue  code stringing together many problems so  on the right I have an example from this  paper called react where they structure  the answer to a prompt as a sequence of  thought action observation thought  action observation and it's a full  rollout a kind of a thinking process to  answer the query and in these actions  the model is also allowed to to lose  on the left I have an example of rogpt  and now rgbt by the way became as a  project that I think got a lot of hype  recently and I think uh but I think I  still find it kind of inspirationally  interesting it's um it's a project that  allows an llm to sort of keep task list  and continue to recursively break down  tasks and I don't think this currently  works very well and I would not advise  people to use it in Practical  applications I just think it's something  to generally take inspiration from in  terms of where this is going I think  over time  so that's kind of like giving our model  system to thinking the next thing that I  find kind of interesting is  this following survey would say almost  psychological Quirk of llms is that llms  don't want to succeed they want to  imitate  you want to succeed and you should ask  for it  so what I mean by that is  when Transformers are trained they have  training sets and there can be an entire  spectrum of performance qualities in  their training data so for example there  could be some kind of a prompt for some  Physics question or something like that  and there could be a student solution  that is completely wrong but there can  also be an expert answer that is  extremely right and Transformers can't  tell the difference between like look I  mean they know they know about low  quality Solutions and high quality  Solutions but by default they want to  imitate all of it because they're just  trained on language modeling and so at  test time you actually have to ask for a  good performance so in this example in  this paper it's um they tried various  prompts and let's think step by step was  very powerful because it's sort of like  spread out the reasoning of remaining  tokens but what worked even better is  let's work this out in a step-by-step  way to be sure we have the right answer  and so it's kind of like a conditioning  on getting a right answer and this  actually makes the Transformer work  better because the Transformer doesn't  have to now hedge its probability Mass  on low quality Solutions as ridiculous  as that sounds and so basically don't be  feel free to ask for a strong solution  say something like you are a leading  expert on this topic pretend you have  iq120 Etc but don't try to ask for too  much IQ because if you ask for IQ of  like 400 you might be out of data  distribution or even worse you could be  in data distribution for some like  sci-fi stuff and it will start to like  take on some sci-fi or like role-playing  or something like that so you have to  find like the right amount of IQ I think  it's got some U-shaped curve there  next up  as we saw when we are trying to solve  problems we know we are good at and what  we're not good at and we Lean On Tools  computationally you want to do the same  potentially with your uh llms so in  particular we may want to give them  calculators code interpreters and so on  the ability to do search and uh there's  a lot of techniques for doing that one  thing to keep in mind again is that  these Transformers by default may not  know what they not don't know so you may  even want to tell the transformer in the  prompt you are not very good at mental  arithmetic whenever you need to do very  large number addition multiplication or  whatever instead use this calculator  here's how you use the calculator use  this token combination etc etc so you  have to actually like spell it out  because the model by default doesn't  know what it's good at or not good at  necessarily just like you and I you and  I might be  next up I think something that is very  interesting is we went from a world that  was retrieval only all the way the  pendulum has swung to The Other Extreme  where it's memory only in lens but  actually there's this entire space in  between of these retrieval augmented  models and this works extremely well in  practice  as I mentioned the context window of a  transformer is its working memory if you  can load the working memory with any  information that is relevant to the task  the model will work extremely well  because it can immediately access all  that memory  and so I think a lot of people are  really interested in  uh basically retrieval augmented  generation and on the bottom I have like  an example of llama index which is one  sort of data connector to lots of  different types of data and you can uh  all you can make it you can index all of  that data and you can make it accessible  to llms and the emerging recipe there is  you take relevant documents you split  them up into chunks you embed all of  them and you basically get embedding  vectors that represent that data you  store that in a vector store and then at  test time you make some kind of a query  to your vector store and you fetch  chunks that might be relevant to your  task and you stuff them into the prompt  and then you generate so this can work  quite well in practice so this is I  think similar to when you're and I solve  problems you can do everything from your  memory and Transformers have very large  and extensive memory but also it really  helps to reference some primary  documents so when you whenever you find  yourself going back to a textbook to  find something or whenever you find  yourself going back to the documentation  of a library to look something up the  transform transformers definitely when I  do that too you you have a mem you have  some memory over how some documentation  of a library works but it's much better  to look it up so the same same applies  here  next I wanted to briefly talk about  constraint prompting I also find this  very interesting  uh this is basically techniques for  um  forcing a certain template in the  outputs of llms so guidance is one  example from Microsoft actually and uh  here we are enforcing that the output  from the llm will be Json and this will  actually guarantee that the output will  take on this form because they go in and  they mess with the probabilities of all  the different tokens that come out of  the Transformer and if they clamp those  tokens and then the Transformer is only  filling in the blanks here and then you  can enforce additional restrictions on  what could go into those blanks so this  might be really helpful and I think this  kind of constraint sampling is also  extremely interesting  I also wanted to say a few words about  fine tuning it is the case that you can  get really far with prompt engineering  but it's also possible to think about  fine-tuning your models now fine-tuning  models means that you are actually going  to change the weights of the model  it is becoming a lot more accessible to  do this in practice and that's because  of a number of techniques that have been  developed and have libraries for uh very  recently so for example parameter  efficient fine-tuning techniques like  Laura make sure that you're only Trend  you're only training small Spar species  of your model so most of the model is  kept clamped at the base model and some  pieces of it are allowed to change and  this still works pretty well empirically  and makes it much cheaper to sort of  tune only small pieces of your model  uh there's also it also means that  because most of your model is clamped  you can use very low Precision inference  for computing those parts because they  are not going to be updated by gradient  descent and so that makes everything a  lot more efficient as well and in  addition we have a number of Open Source  high quality based models currently as I  mentioned I think llama is quite nice  although it is not commercially licensed  I believe right now  some things to keep in mind is that  basically fine tuning is is a lot more  technically involved it requires a lot  more I think technical expertise to do  to do right it requires human data  contractors for data sets and or  synthetic data pipelines that can be  pretty complicated this will definitely  slow down your iteration cycle by a lot  and I would say on a high level sft is  achievable because it is just your  continuing language modeling task It's  relatively straightforward but rlhref I  would say is very much research  territory and is even much harder to get  to work and so I would probably not  advise that someone just tries to roll  their own rlh of implementation these  things are pretty unstable very  difficult to train uh not something that  is I think very beginner friendly right  now and it's also potentially likely  also to to change pretty rapidly still  so I think these are my sort of default  recommendations right now  I would break up your task into two  major parts number one achieve your top  performance and number two optimize your  performance in that order  number one the best performance will  currently come from GT4 model it is the  most capable of by far  use prompts that are very detailed they  have lots of task contents relative  relevant information and instructions  think along the lines of what would you  tell a task contractor if they can't  email you back but then also keep in  mind that a task contractor is a human  and they have inner monologue and  they're very clever Etc llms do not  possess those qualities so make sure to  think through the psychology of the llm  almost and cater prompts to that  retrieve and add any relevant context  and information to these prompts  basically refer to a lot of the prompt  engineering techniques some of them are  highlighted in the slides above but also  this is a very large space and I would  just advise you to look for prompt  engineering techniques online there's a  lot to cover there  experiment with few shop examples what  this refers to is you don't just want to  tell you want to show whenever it's  possible so give it examples of  everything that helps it really  understand what you mean if you can  tools and plugins to offload a tasks  that are difficult for llms natively uh  and then think about not just a single  prompt and answer think about potential  chains and reflection and how you glue  them together and how you could  potentially make multiple samples and so  on  finally if you think you've squeezed out  prompt engineering which I think you  should stick with for a while look at  some potentially fine-tuning a model to  your application but expect this to be a  lot more slower and evolved and then  there's an expert fragile research Zone  here and I would say that is rlhf which  currently does work a bit better than  sft if you can get it to work but again  this is uh pretty involved I would say  and to optimize your costs try to  explore lower capacity models or shorter  prompts and so on  I also wanted to say a few words about  the use cases in which I think llms are  currently well suited for so in  particular note that there's a large  number of limitations to llms today and  uh so I would keep that definitely in  mind for all your applications models  and this by the way could be an entire  talk so I don't have time to cover it in  full detail models may be biased they  may fabricate hallucinate information  they may have reasoning errors they may  struggle in entire classes of  applications they have knowledge cutoffs  so they might not know any information  above say September 2021 they are  susceptible to a large range of attacks  which are sort of like coming out on  Twitter daily including prompt injection  jailbreak attacks data poisoning attacks  and so on so my recommendation right now  is use llms in low stakes applications  combine them with always with human  oversight use them as a source of  inspiration and suggestions and think  co-pilots instead of completely  autonomous agents that are just like  performing a task somewhere it's just  not clear that the models are there  right now  so I wanted to close by saying that gpt4  is an amazing artifact I'm very thankful  that it exists and it's it's beautiful  it has a ton of knowledge across so many  areas it can do math code and so on and  in addition there's this thriving  ecosystem of everything else that is  being built and incorporated into into  the ecosystem uh some of some of the  things these things I've talked about  and all of this power is accessible at  your fingertips  so here's everything that's needed in  terms of code to ask GPT for a question  to prompt it and get a response in this  case I said uh can you say something to  inspire the audience of Microsoft build  2023  and I just punched this into Python and  verbatim gpt4 said the following  uh and by the way I did not know that  they used this trick in the keynote so I  thought I was being clever but  but it is really good at this it says  ladies and gentlemen Invaders and  Trailblazers Microsoft build 2023  welcome to the Gathering of Brilliant  Minds like no other you are The  Architects of the future The Visionaries  molding the digital realm in which  Humanity thrives embrace the Limitless  possibilities of Technologies and let  your ideas soar as high as your  imagination together let's create a more  connected remarkable and inclusive world  for generations to come get ready to  unleash your creativity canvas the  unknown and turn dreams into reality  your Journey Begins today  okay\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'外国\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f.readline() reads a single line from the file\n",
    "with open('build.txt', encoding=\"utf-8\") as f:\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "    f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e30218f3-f3d9-4a35-83f9-14eb021e675d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foreign  [Music]  researcher and founding member of open  AI Andre carpathy  [Applause]  hi everyone  I'm happy to be here to tell you about  the state of gbt and more generally  about the rapidly growing ecosystem of  large language models  so I would like to partition the talk  into two parts  in the first part I would like to tell  you about how we train GPT assistance  and then in the second part we will we  are going to take a look at how we can  use these assistants effectively for  your applications  so first let's take a look at the  emerging recipe for how to train these  assistants and keep in mind that this is  all very new and still rapidly evolving  but so far the recipe looks something  like this  now this is kind of a complicated slide  so I'm going to go through it piece by  piece but roughly speaking we have four  major stages free training supervised  fine tuning reward modeling  reinforcement learning and they follow  each other serially  now in each stage we have a data set  that is that powers that stage we have  an algorithm that for our purposes will  be a objective and a and over for uh for  training the neural network and then we  have a resulting model and then there's  some notes on the bottom so the first  stage we're going to start with is the  pre-training stage now this stage is  kind of special in this diagram and this  diagram is not to scale because this  stage is where all of the computational  work basically happens this is 99 of the  training uh compute time and also flops  and so this is where we are dealing with  internet scale data sets with thousands  of gpus in the store computer and also  months of training potentially the other  three stages are fine-tuning stages that  are much more along the lines of a small  few number of gpus and hours or days  so let's take a look at the pre-training  stage to achieve a base model  first we're going to gather a large  amount of data here's an example of what  we call a data mixture that comes from  this uh paper that was released by meta  where they released this llama based  model now you can see roughly the kinds  of data sets that enter into these  collections so we have common crawl  which is just a web scrape C4 which is  also a common crawl and then some high  quality data sets as well so for example  GitHub Wikipedia books archive stack  exchange and so on these are all mixed  up together and then they are sampled  According to some given proportions and  that forms the training set for the  neural net for the GPT  now before we can actually train on this  data we need to go through one more  pre-processing step and that is  tokenization and this is basically a  translation of the raw text that we  scrape from the internet into sequences  of integers because that's the native  representation over which gpts function  now this is a lossless kind of  translation between pieces of text and  tokens and integers and there are a  number of algorithms for this stage  typically for example you could use  something like byte pair encoding which  iteratively merges little text chunks  and groups them into tokens  and so here I'm showing some example  chunks of these tokens and then this is  the raw integer sequence that will  actually feed into a Transformer  now here I'm showing two uh sort of like  examples for hyper parameters that  govern this stage so gpt4 we did not  release too much information about how  it was trained and so on so I'm using  gpt3's numbers but gpt3 is of course a  little bit old by now about three years  ago but llama is a fairly recent model  from meta so these are roughly the  orders of magnitude that we're dealing  with when we're doing pre-training  the vocabulary size is usually a couple  10 000 tokens the context length is  usually something like two thousand four  thousand or nowadays even 100 000 and  this governs the maximum number of  integers that the GPT will look at when  it's trying to predict the next integer  in a sequence  um you can see that roughly the number  of parameters is say 65 billion for  llama now even though llama has only 65b  parameters compared to gvt3's 175  billion parameters llama is a  significantly more powerful model and  intuitively that's because the model is  trained for significantly longer in this  case 1.4 trillion tokens instead of just  300 billion tokens so you shouldn't  judge the power of a model just by the  number of parameters that it contains  below I'm showing some tables of rough  number of rough hyper parameters that  typically go into specifying the  Transformer neural network so the number  of heads the dimension size number of  layers and so on and on the bottom I'm  showing some training hyper parameters  so for example to train the 65b model  meta used 2000 gpus roughly 21 days  train of training and and roughly  several million dollars and so that's  the rough orders of magnitude that you  should have in mind for the pre-training  stage  now when we're actually pre-chaining  what happens roughly speaking we are  going to take our tokens and we're going  to lay them out into Data batches so we  have these arrays that will feed into  the Transformer and these arrays are B  the batch size and these are all  independent examples stacked up in rows  and B by T T being the maximum context  length so in my picture I only have 10.  the context length so this could be two  thousand four thousand Etc so these are  extremely long rows and what we do is we  take these documents and we pack them  into rows and we delimit them with these  Special end of text tokens uh basically  it's on the Transformer where a new  document begins  and so here I have a few examples of  documents and then I've stretched them  out into uh into this input  now we're going to feed all of these  numbers into Transformer and let me let  me just focus on a single particular  cell but the same thing will happen at  every every cell in this diagram so  let's look at the green cell the green  cell is going to take a look at all of  the tokens before it so all of the  tokens in yellow and we're going to feed  that entire context into the Transformer  neural network  and the Transformer is going to try to  predict the next token in the sequence  in this case in red  now the Transformer I don't have too  much time to unfortunately go into the  full details of this neural network  architecture is just a large blob of  neural net stuff for our purposes and  it's got several 10 billion parameters  typically or something like that and of  course as they tune these parameters  you're getting slightly different uh  predicted distributions for every single  one of these cells  and so for example if our vocabulary  size is 50 257 tokens then we're going  to have that many numbers because we  need to specify a probability  distribution for what comes next so  basically we have a probability for  whatever may follow now in this specific  example for the specific cell 513 will  come next and so we can use this as a  source of supervision to update our  Transformers weights and so we're  applying this basically on every single  cell in parallel and we keep swap  swapping batches and we're trying to get  the Transformer to make the correct  predictions over what token comes next  in a sequence  so let me show you more concretely what  this looks like when you train one of  these models this is actually coming  from a New York Times and they trained a  small GPT on Shakespeare and so here's a  small snippet of Shakespeare and they  trained a GPT on it now in the beginning  at initialization the GPT starts with  completely random weights so you're just  getting completely random outputs as  well  but over time as you train uh the GPT  longer and longer you are getting more  and more coherent and consistent uh sort  of samples from the model and the way  you sample from it of course is you uh  predict what comes next you sample from  that distribution and you keep feeding  that back into the process and you can  basically uh sample large sequences and  so by the end you see that the  Transformer has learned about words and  where to put spaces and where to put  commas and so on and so we're making  more and more consistent predictions  over time  these are the kinds of plots that you're  looking at when you're doing model  pre-training effectively we're looking  at the loss function over time as you  train and low loss means that our  Transformer is predicting the correct uh  is giving a higher probability to the  correct next integer in a sequence  now what are we going to do with this  model once we've trained it after a  month well the first thing that we  noticed we the field is that these  models basically in the process of  language modeling learn very powerful  General representations and it's  possible to very efficiently fine-tune  them for any arbitrary Downstream task  you might be interested in so as an  example if you're interested in  sentiment classification the approach  used to be that you collect a bunch of  positives and negatives and then you  train some kind of an NLP model for for  that but the new approach is ignore  sentiment classification  go off and do large language model  pre-training train the large Transformer  and then you can only you may only have  a few examples and you can very  efficiently fine tune your model for  that task and so this works very well in  practice and the reason for this is that  basically the Transformer is forced to  multitask a huge amount of tasks in the  language modeling task because just just  in terms of predicting the next token  it's forced to understand a lot about  the structure of the of the text and all  the different concepts they're in  so that was GPT one now around the time  of gpt2 people noticed that actually  even better than fine tuning you can  actually prompt these models very  effectively so these are language models  and they want to complete documents so  you can actually trick them into  performing tasks just by arranging these  fake documents so in this example for uh  for example we have some passage and  then we sort of like do QA qaqa this is  called a few shot prompt and then we do  q and then as the Transformer is trying  to complete the document it's actually  answering our question and so this is an  example of prompt engineering a base  model making the belief that it's sort  of imitating a document and getting it  to perform a task  and so this kicked off I think the era  of I would say prompting over  fine-tuning and seeing that this  actually can work extremely well on a  lot of problems even without training  any neural networks fine-tuning or so on  now since then we've seen an entire  evolutionary tree of Base models that  everyone has trained not all of these  models are available for example the  gpt4 base model was never released the  gpt4 model that you might be interacting  with over API is not a base model it's  an assistant model and we're going to  cover how to get those in a bit gpt3  based model is available via the API  under the name DaVinci and gpt2 base  model is available even as weights on  our GitHub repo but currently the best  available base model probably is the  Llama series from meta although it is  not commercially licensed  now one thing to point out is base  models are not assistance they don't  want to answer to they don't want to  make answers to your questions they just  want to complete documents so if you  tell them write a poem about the brand  cheese it will just you know it will  answer questions with more questions  it's just completing what it thinks as a  document  however you can prompt them in a  specific way for base models that that  is more likely to work so as an example  here's a poem about bread and cheese and  in that case it will auto-complete  correctly  you can even trick base models into  being assistance and the way you would  do this is you would create like a  specific few shot prompt that makes it  look like there's some kind of a  document between a human and assistant  and they're exchanging sort of um  information and then at the bottom you  sort of put your query at the end and  the base model will sort of like  condition itself into being like a  helpful assistant and kind of answer but  this is not very reliable and doesn't  work super well in practice although it  can be done  so instead we have a different path to  make actual gbt assistance not just base  model document completers and so that  takes us into supervised fine-tuning so  in the supervised fine tuning stage we  are going to collect small but high  quality data sets and in this case we're  going to ask human contractors to gather  data of the format of the form prompt  and ideal response and we're going to  collect lots of these typically tens of  thousands or something like that  and then we're going to still do  language modeling on this data so  nothing change algorithmically we're  just swapping out a training set so it  used to be internet documents which is a  high quantity local  four basically QA prompt response kind  of data and that is low quantity high  quality  so we still do language modeling and  then after training we get an sfd model  and you can actually deploy these models  and they are actual assistants and they  work to some extent let me show you what  an example demonstration might look like  so here's something that a human  contractor might come up with here's  some random prompt can you write a short  introduction about the relevance of the  term monopsony or something like that  and then the contractor also writes out  an ideal response and when they write  out these responses they are following  extensive labeling documentations and  they're being asked to be helpful  truthful and harmless and those labeling  instructions here you probably can't  read it another can I but they're long  and this is just people following  instructions and trying to complete  these prompts  so that's what the data set looks like  and you can train these models and this  works to some extent  now you can actually continue the  pipeline from here on and go into rlhf  reinforcement learning from Human  feedback that consists of both reward  modeling and reinforcement learning so  let me cover that and then I'll come  back to why you may want to go through  the extra steps and how that compares to  just sft models  so in the reward modeling step what  we're going to do is we're now going to  shift our data collection to be of the  form of comparisons so here's an example  of what our data set will look like  I have the same prompt identical prompt  on the top which is asking the assistant  to write a program or function that  checks if a given string is a palindrome  and then what we do is we take the sft  model which we've already trained and we  create multiple completions so in this  case we have three completions that the  model has created  and then we ask people to rank these  completions so if you stare at this for  a while and by the way these are very  difficult things to do to compare some  of these predictions and this can take  people even hours for a single prompt  completion pairs but let's say we  decided that one of these is much better  than the others and so on so we rank  them  then we can follow that with something  that looks very much kind of like a  binary classification on all the  possible pairs between these completions  so what we do now is we lay out our  prompt in rows and the prompts is  identical across all three rows here so  it's all the same prompt but the  completion of this vary and so the  yellow tokens are coming from the sft  model then what we do is we append  another special reward uh readout token  at the end and we basically only  supervise the Transformer at this single  green token and the Transformer will  predict some reward for how good that  completion is for that prompt  and so basically it makes a guess about  the quality of each completion and then  once it makes a guess for every one of  them we also have the ground truth which  is telling us the ranking of them and so  we can actually enforce that some of  these numbers should be much higher than  others and so on WE formulate this into  a loss function and we train our model  to make reward predictions that are  consistent with the ground truth coming  from the comparisons from all these  contractors so that's how we train our  reward model and that allows us to score  how good a completion is for a prompt  once we have a reward model this is we  can't deploy this because this is not  very useful as an assistant by itself  but it's very useful for the reinforce  reinforcement learning stage that  follows now because we have a reward  model we can score the quality of any  arbitrary completion for any given  prompt so what we do during  reinforcement learning is we basically  get again a large collection of prompts  and now we do reinforcement learning  with respect to the reward model so  here's what that looks like  we we take a single prompt we lay it out  in rows and now we use the sft we use  the basically the model we'd like to  train which is initialized at sft model  to create some completions in yellow  and then we append the reward token  again and we read off the reward  according to the reward model which is  now kept fixed it doesn't change anymore  and now the reward model tells us the  quality of every single completion for  each for these prompts and so what we  can do is we can now just basically  apply the same language modeling loss  function but we're currently training on  the yellow tokens and we are weighing  the language modeling objective by the  rewards indicated by the reward model so  as an example in the first row the  reward model said that this is a fairly  high score in completion and so all the  tokens that we happen to sample on the  first row are going to get reinforced  and they're going to get higher  probabilities for the future  conversely on the second row the reward  model really did not like this  completion negative 1.2 and so therefore  every single token that we sampled in  that second row is going to get a  slightly higher probability for the  future and we do this over and over on  many prompts on many batches and  basically we get a policy which creates  yellow tokens here and it basically all  of them all of the completions here will  score high according to the reward model  that we trained in the previous stage  so that's how we train uh that's what  the rohf pipeline is  now and then at the end you get a model  that you could deploy and so and as an  example chat GPT is an rlhf model but  some other models that you might come  across like for example of the kuna 13B  and so on these are sft models so we  have base models sft models and rlh  models and that's kind of like the state  of things there  now why would you want to do rlhf so one  answer that is kind of not that exciting  is that it just works better so this  comes from the instructor gbt paper  according to these experiments a while  ago now these PPO models are rlhf and we  see that they are basically just  preferred in a lot of comparisons when  we give them to humans so humans just  prefer out basically tokens that come  from our religion models compared to sft  models compared to base model that is  prompted to be an assistant and so it  just works better but you might ask why  why does it work better and I don't  think that there's a single like amazing  answer that the community has really  like agreed on but I will just offer one  uh one reason potentially and it has to  do with the asymmetry between how easy  computationally it is to compare versus  generate so let's take an example of  generating a haiku suppose I ask a model  to write a haiku about paper clips if  you're a contractor trying to give train  data then imagine being a contractor  collecting basically data for the sft  stage how are you supposed to create a  nice haiku for a paperclip you might  just not be very good at that but if I  give you a few examples of haikus you  might be able to appreciate some of  these haikus a lot more than others and  so judging which one of these is good is  much easier task and so basically this  asymmetry makes it so that comparisons  are a better way to potentially leverage  yourself as a human and your judgment to  create a slightly better model  now our religious models are not  strictly an improvement on the base  models in some cases so in particular  we've noticed for example that they lose  some entropy so that means that they  give More PT results they can output  lower variations uh like they can output  samples with lower variation than base  model so base model has lots of entropy  and we'll go with lots of diverse  outputs  so for example one one kind of place  where I still prefer to use a base model  is in the setup where  you basically have N Things and you want  to generate more things like it and so  here is an example that I just cooked up  I want to generate cool Pokemon names I  gave it seven Pokemon names and I asked  the base model to complete the document  and it gave me a lot more Pokemon names  uh these are fictitious I tried to look  them up I don't believe they're actual  Pokemons and uh this is the kind of task  that I think base model would be good at  because it still has lots of entropy it  will give you lots of diverse cool kind  of more things that look like whatever  you give it before  so this is what uh this is number having  all said all that these are kind of like  the assistant models that are probably  available to you at this point uh that  there's a team at Berkeley that ranked a  lot of the available assistant models  and gave them basically ELO ratings So  currently some of the best models of  course are gpt4 by far I would say  followed by Claude gvt 3.5 and then a  number of models some of these might be  available as weights like the kuna koala  Etc and the first three rows here are  all they're all rohf models and all of  the other models to my knowledge are sft  models I believe  okay so that's how we train these models  on the high level now I'm going to  switch gears and let's look at how we  can best apply and the GPT assistant  model to your problems now I would like  to work  in a setting of a concrete example so  let's  let's work with a concrete example here  let's say that you are working on an  article or a blog post and you're going  to write this sentence at the end  California's population is 53 times that  of Alaska so for some reason you want to  compare the populations of these two  states  think about the rich internal monologue  and Tool use and how much work actually  goes computationally in your brain to  generate this one final sentence so  here's maybe what that could look like  in your brain okay for this next step  let me blog uh for my blog let me  compare these two populations  okay first I'm going to obviously need  to get both of these populations now I  know that I probably don't know these  populations off the top of my head so  I'm kind of like aware of what I know or  don't know of my self-knowledge right  so I go I do some tool use and I go to  Wikipedia and I look up California's  population and Alaska's population  now I know that I should divide the two  but again I know that dividing three 9.2  by 0.74 is very unlikely to succeed  that's not the kind of thing that I can  do in my head and so therefore I'm gonna  rely on the calculator so I'm going to  use a calculator punch it in and see  that the output is roughly 53.  and then maybe I do some reflection and  Sanity checks in my brain so does 53  make sense well that's quite quite a  large fraction but then California is  the most popular state so maybe that  looks okay  so then I have all the information I  might need and now I get to the sort of  creative portion of writing so I might  start to write something like California  has 53 x times greater and then I think  to myself that's actually like really  awkward phrasing so let me actually  delete that and let me try again and so  as I'm writing I have this separate  process almost inspecting what I'm  writing and judging whether it looks  good or not  and then maybe I delete and maybe I  reframe it and then maybe I'm happy with  what comes out  so basically long story short a ton  happens under the hood in terms of your  internal monologue when you create  sentences like this but what does a  sentence like this look like when we are  training a GPT on it  From gpt's perspective this is just a  sequence of tokens  so GPT when it's reading or generating  these tokens it just goes  and each chunk is roughly the same  amount of computational work for each  token and these Transformers are not  very shallow networks they have about 80  layers of reasoning but 80 is still not  like too much and so this Transformer is  going to do its best to imitate but of  course the process here looks very very  different from the process that you took  so in particular in our final artifacts  in the data sets that we create and then  eventually feed to llms all of that  internal dialogue is completely stripped  and uh and unlike you the GPT will look  at every single token and spend the same  amount of compute on every one of them  and so you can't expect it to actually  like well you can't expect it to do to  uh sort of do too much work per token  so and also in particular basically  these Transformers are just like token  simulators so they don't know what they  don't know like they just imitate the  next token they don't know what they're  good at or not good at they just tried  their best imitate the next token they  don't reflect in the loop they don't  sanity check anything they don't correct  their mistakes along the way by default  they just uh sample token sequences  they don't have separate inner monologue  streams in their head right that are  evaluating what's happening now they do  have some sort of cognitive advantages I  would say and that is that they do  actually have a very large fact-based  knowledge across a vast number of areas  because they have say several 10 billion  parameters so it's a lot of storage for  a lot of facts  but and they also I think have a  relatively large and perfect working  memory so whatever fixed into the  whatever fits into the context window is  immediately available to the Transformer  through its internal self-attention  mechanism and so it's kind of like um  Perfect Memory but it's got that finite  size but the Transformer has a very  direct access to it and so it can like  losslessly remember anything that is  inside its context window  so that's kind of how I would compare  those two and the reason I bring all of  this up is because I think to a large  extent prompting is just making up for  this sort of cognitive difference  between these two kind of architectures  um like our brains here and llm brains  you can look at it that way almost  so here's one thing that people found  for example works pretty well in  practice especially if your tasks  require reasoning you can't expect the  Transformer to make to to do too much  reasoning per token and so you have to  really spread out the reasoning across  more and more tokens so for example you  can't give a Transformer a very  complicated question and expect it to  get the answer in a single token there's  just not enough time for it these  Transformers need tokens to think quote  unquote I like to say sometimes and so  this is some of the things that work  well you may for example have a few shot  prompt that shows the Transformer that  it should like show its work when it's  answering a question when it's answering  the question and if you give a few  examples the Transformer will imitate  that template and it will just end up  working out better in terms of its  evaluation  Additionally you can elicit this kind of  behavior from the Transformer by saying  let's think step by step because this  condition is the transformer into sort  of like showing its work and because it  kind of snaps into a mode of showing its  work it's going to do less computational  work per token and so it's more likely  to succeed as a result because it's  making  um slower reasoning over time  here's another example this one is  called self-consistency we saw that we  had the ability to start writing and  then if it didn't work out I can try  again and I can try multiple times and  uh and maybe  um select the one that worked best so in  these kinds of approaches you may sample  not just once but you may sample  multiple times and then have some  process for finding the ones that aren't  good and then keeping just those samples  or doing a majority vote or something  like that so basically these  Transformers in the process as they  predict the next token just like you  they can get unlucky and they can they  could sample and not a very good token  and they can go down sort of like a  blind alley in terms of reasoning and so  unlike you they cannot recover from that  they are stuck with every single token  they sample and so they will continue  the sequence even if they even know that  this sequence is not going to work out  so give them the ability to look back  inspect or try to find uh try to  basically sample around it  here's one technique also you could it  turns out that actually llms like they  know when they've screwed up so as an  example  say you asked the model to generate a  poem that does not rhyme and it might  give you a poem but it actually Rhymes  but it turns out that especially for the  bigger models like gpt4 you can just ask  it did you meet the assignment and  actually gpt4 knows very well that it  did not meet the assignment it just kind  of got unlucky in its sampling and so it  will tell you no I didn't actually meet  the assignment here's let me try again  but without you prompting it it doesn't  even like it doesn't know it doesn't  know to revisit and uh and so on so you  have to make up for that in your prompt  you have to get it to check if you don't  ask it to check it's not going to check  by itself it's just a token simulator  I think more generally a lot of these  techniques fall into the bucket of what  I would say recreating our system too so  you might be familiar with the system  one system to thinking for humans system  one is a fast automatic process and I  think kind of corresponds to like an llm  just sampling tokens  and system two is the slower deliberate  planning sort of part of your brain  and so this is a paper actually from  just last week because this space is  pretty quickly evolving it's called tree  of thought and in tree of thought uh the  authors of this paper propose  maintaining multiple completions for any  given prompt and then they are also  scoring them along the way and keeping  the ones that are going well if that  makes sense and so a lot of people are  like really playing around with kind of  um prompt engineering to um to basically  bring back some of these abilities that  we sort of have in our brain for llms  now one thing I would like to note here  is that this is not just a prompt this  is actually prompts that are together  used with some python glue code because  you don't you actually have to maintain  multiple prompts and you also have to do  some tree search algorithm here um to  like figure out which prompt to expand  Etc so it's a symbiosis of python glue  code and individual prompts that are  called in a while loop or in a bigger  algorithm  I also think there's a really cool  parallel here to alphago alphago has a  policy for placing the next Stone when  it plays go and its policy was trained  originally by imitating humans but in  addition to this policy it also does  Monte Carlo tree search and basically it  will play out a number of possibilities  in its head and evaluate all of them and  only keep the ones that work well and so  I think this is kind of an equivalent of  alphago but for text  if that makes sense  so just like tree of thought I think  more generally people are starting to  like really Explore More General  techniques of not just the simple  question answer prompt but something  that looks a lot more like python blue  code stringing together many problems so  on the right I have an example from this  paper called react where they structure  the answer to a prompt as a sequence of  thought action observation thought  action observation and it's a full  rollout a kind of a thinking process to  answer the query and in these actions  the model is also allowed to to lose  on the left I have an example of rogpt  and now rgbt by the way became as a  project that I think got a lot of hype  recently and I think uh but I think I  still find it kind of inspirationally  interesting it's um it's a project that  allows an llm to sort of keep task list  and continue to recursively break down  tasks and I don't think this currently  works very well and I would not advise  people to use it in Practical  applications I just think it's something  to generally take inspiration from in  terms of where this is going I think  over time  so that's kind of like giving our model  system to thinking the next thing that I  find kind of interesting is  this following survey would say almost  psychological Quirk of llms is that llms  don't want to succeed they want to  imitate  you want to succeed and you should ask  for it  so what I mean by that is  when Transformers are trained they have  training sets and there can be an entire  spectrum of performance qualities in  their training data so for example there  could be some kind of a prompt for some  Physics question or something like that  and there could be a student solution  that is completely wrong but there can  also be an expert answer that is  extremely right and Transformers can't  tell the difference between like look I  mean they know they know about low  quality Solutions and high quality  Solutions but by default they want to  imitate all of it because they're just  trained on language modeling and so at  test time you actually have to ask for a  good performance so in this example in  this paper it's um they tried various  prompts and let's think step by step was  very powerful because it's sort of like  spread out the reasoning of remaining  tokens but what worked even better is  let's work this out in a step-by-step  way to be sure we have the right answer  and so it's kind of like a conditioning  on getting a right answer and this  actually makes the Transformer work  better because the Transformer doesn't  have to now hedge its probability Mass  on low quality Solutions as ridiculous  as that sounds and so basically don't be  feel free to ask for a strong solution  say something like you are a leading  expert on this topic pretend you have  iq120 Etc but don't try to ask for too  much IQ because if you ask for IQ of  like 400 you might be out of data  distribution or even worse you could be  in data distribution for some like  sci-fi stuff and it will start to like  take on some sci-fi or like role-playing  or something like that so you have to  find like the right amount of IQ I think  it's got some U-shaped curve there  next up  as we saw when we are trying to solve  problems we know we are good at and what  we're not good at and we Lean On Tools  computationally you want to do the same  potentially with your uh llms so in  particular we may want to give them  calculators code interpreters and so on  the ability to do search and uh there's  a lot of techniques for doing that one  thing to keep in mind again is that  these Transformers by default may not  know what they not don't know so you may  even want to tell the transformer in the  prompt you are not very good at mental  arithmetic whenever you need to do very  large number addition multiplication or  whatever instead use this calculator  here's how you use the calculator use  this token combination etc etc so you  have to actually like spell it out  because the model by default doesn't  know what it's good at or not good at  necessarily just like you and I you and  I might be  next up I think something that is very  interesting is we went from a world that  was retrieval only all the way the  pendulum has swung to The Other Extreme  where it's memory only in lens but  actually there's this entire space in  between of these retrieval augmented  models and this works extremely well in  practice  as I mentioned the context window of a  transformer is its working memory if you  can load the working memory with any  information that is relevant to the task  the model will work extremely well  because it can immediately access all  that memory  and so I think a lot of people are  really interested in  uh basically retrieval augmented  generation and on the bottom I have like  an example of llama index which is one  sort of data connector to lots of  different types of data and you can uh  all you can make it you can index all of  that data and you can make it accessible  to llms and the emerging recipe there is  you take relevant documents you split  them up into chunks you embed all of  them and you basically get embedding  vectors that represent that data you  store that in a vector store and then at  test time you make some kind of a query  to your vector store and you fetch  chunks that might be relevant to your  task and you stuff them into the prompt  and then you generate so this can work  quite well in practice so this is I  think similar to when you're and I solve  problems you can do everything from your  memory and Transformers have very large  and extensive memory but also it really  helps to reference some primary  documents so when you whenever you find  yourself going back to a textbook to  find something or whenever you find  yourself going back to the documentation  of a library to look something up the  transform transformers definitely when I  do that too you you have a mem you have  some memory over how some documentation  of a library works but it's much better  to look it up so the same same applies  here  next I wanted to briefly talk about  constraint prompting I also find this  very interesting  uh this is basically techniques for  um  forcing a certain template in the  outputs of llms so guidance is one  example from Microsoft actually and uh  here we are enforcing that the output  from the llm will be Json and this will  actually guarantee that the output will  take on this form because they go in and  they mess with the probabilities of all  the different tokens that come out of  the Transformer and if they clamp those  tokens and then the Transformer is only  filling in the blanks here and then you  can enforce additional restrictions on  what could go into those blanks so this  might be really helpful and I think this  kind of constraint sampling is also  extremely interesting  I also wanted to say a few words about  fine tuning it is the case that you can  get really far with prompt engineering  but it's also possible to think about  fine-tuning your models now fine-tuning  models means that you are actually going  to change the weights of the model  it is becoming a lot more accessible to  do this in practice and that's because  of a number of techniques that have been  developed and have libraries for uh very  recently so for example parameter  efficient fine-tuning techniques like  Laura make sure that you're only Trend  you're only training small Spar species  of your model so most of the model is  kept clamped at the base model and some  pieces of it are allowed to change and  this still works pretty well empirically  and makes it much cheaper to sort of  tune only small pieces of your model  uh there's also it also means that  because most of your model is clamped  you can use very low Precision inference  for computing those parts because they  are not going to be updated by gradient  descent and so that makes everything a  lot more efficient as well and in  addition we have a number of Open Source  high quality based models currently as I  mentioned I think llama is quite nice  although it is not commercially licensed  I believe right now  some things to keep in mind is that  basically fine tuning is is a lot more  technically involved it requires a lot  more I think technical expertise to do  to do right it requires human data  contractors for data sets and or  synthetic data pipelines that can be  pretty complicated this will definitely  slow down your iteration cycle by a lot  and I would say on a high level sft is  achievable because it is just your  continuing language modeling task It's  relatively straightforward but rlhref I  would say is very much research  territory and is even much harder to get  to work and so I would probably not  advise that someone just tries to roll  their own rlh of implementation these  things are pretty unstable very  difficult to train uh not something that  is I think very beginner friendly right  now and it's also potentially likely  also to to change pretty rapidly still  so I think these are my sort of default  recommendations right now  I would break up your task into two  major parts number one achieve your top  performance and number two optimize your  performance in that order  number one the best performance will  currently come from GT4 model it is the  most capable of by far  use prompts that are very detailed they  have lots of task contents relative  relevant information and instructions  think along the lines of what would you  tell a task contractor if they can't  email you back but then also keep in  mind that a task contractor is a human  and they have inner monologue and  they're very clever Etc llms do not  possess those qualities so make sure to  think through the psychology of the llm  almost and cater prompts to that  retrieve and add any relevant context  and information to these prompts  basically refer to a lot of the prompt  engineering techniques some of them are  highlighted in the slides above but also  this is a very large space and I would  just advise you to look for prompt  engineering techniques online there's a  lot to cover there  experiment with few shop examples what  this refers to is you don't just want to  tell you want to show whenever it's  possible so give it examples of  everything that helps it really  understand what you mean if you can  tools and plugins to offload a tasks  that are difficult for llms natively uh  and then think about not just a single  prompt and answer think about potential  chains and reflection and how you glue  them together and how you could  potentially make multiple samples and so  on  finally if you think you've squeezed out  prompt engineering which I think you  should stick with for a while look at  some potentially fine-tuning a model to  your application but expect this to be a  lot more slower and evolved and then  there's an expert fragile research Zone  here and I would say that is rlhf which  currently does work a bit better than  sft if you can get it to work but again  this is uh pretty involved I would say  and to optimize your costs try to  explore lower capacity models or shorter  prompts and so on  I also wanted to say a few words about  the use cases in which I think llms are  currently well suited for so in  particular note that there's a large  number of limitations to llms today and  uh so I would keep that definitely in  mind for all your applications models  and this by the way could be an entire  talk so I don't have time to cover it in  full detail models may be biased they  may fabricate hallucinate information  they may have reasoning errors they may  struggle in entire classes of  applications they have knowledge cutoffs  so they might not know any information  above say September 2021 they are  susceptible to a large range of attacks  which are sort of like coming out on  Twitter daily including prompt injection  jailbreak attacks data poisoning attacks  and so on so my recommendation right now  is use llms in low stakes applications  combine them with always with human  oversight use them as a source of  inspiration and suggestions and think  co-pilots instead of completely  autonomous agents that are just like  performing a task somewhere it's just  not clear that the models are there  right now  so I wanted to close by saying that gpt4  is an amazing artifact I'm very thankful  that it exists and it's it's beautiful  it has a ton of knowledge across so many  areas it can do math code and so on and  in addition there's this thriving  ecosystem of everything else that is  being built and incorporated into into  the ecosystem uh some of some of the  things these things I've talked about  and all of this power is accessible at  your fingertips  so here's everything that's needed in  terms of code to ask GPT for a question  to prompt it and get a response in this  case I said uh can you say something to  inspire the audience of Microsoft build  2023  and I just punched this into Python and  verbatim gpt4 said the following  uh and by the way I did not know that  they used this trick in the keynote so I  thought I was being clever but  but it is really good at this it says  ladies and gentlemen Invaders and  Trailblazers Microsoft build 2023  welcome to the Gathering of Brilliant  Minds like no other you are The  Architects of the future The Visionaries  molding the digital realm in which  Humanity thrives embrace the Limitless  possibilities of Technologies and let  your ideas soar as high as your  imagination together let's create a more  connected remarkable and inclusive world  for generations to come get ready to  unleash your creativity canvas the  unknown and turn dreams into reality  your Journey Begins today  okay\n",
      "外国\n",
      "\n",
      "[音乐]\n",
      "\n",
      "研究员和开放人工智能创始成员\n",
      "\n",
      "安德烈·卡帕西\n",
      "\n",
      "[鼓掌]\n",
      "\n",
      "大家好，\n",
      "\n",
      "我很高兴能在这里向大家介绍\n",
      "\n",
      "GBT 的现状，以及更广泛的\n",
      "\n",
      "关于快速发展的大型语言模型生态系统的信息，\n",
      "\n",
      "\n",
      "所以我想 将演讲\n",
      "\n",
      "分为两个部分，\n",
      "\n",
      "在第一部分中，我想告诉\n",
      "\n",
      "您我们如何训练 GPT 辅助，\n",
      "\n",
      "然后在第二部分中，我们将\n",
      "\n",
      "看看如何在\n",
      "\n",
      "\n",
      "您的应用程序中有效地使用这些助手，\n",
      "\n",
      "所以首先 让我们看一下\n",
      "\n",
      "如何训练这些助手的新兴配方，\n",
      "\n",
      "请记住，这都是\n",
      "\n",
      "非常新的，并且仍在快速发展，\n",
      "\n",
      "但到目前为止，配方看起来\n",
      "\n",
      "像这样，\n",
      "\n",
      "现在这是一张复杂的幻灯片，\n",
      "\n",
      "所以我要 一点一点地进行，\n",
      "\n",
      "但粗略地说，我们有四个\n",
      "\n",
      "主要阶段：免费训练、监督微调、奖励\n",
      "\n",
      "建模、\n",
      "\n",
      "强化学习，它们按顺序相互跟随，\n",
      "\n",
      "\n",
      "现在在每个阶段，我们都有一个数据集，\n",
      "\n",
      "为该阶段提供动力，我们有\n",
      "\n",
      "一个算法， 就我们的目的而言，这将\n",
      "\n",
      "是\n",
      "\n",
      "训练神经网络的目标，然后我们将\n",
      "\n",
      "得到一个结果模型，然后\n",
      "\n",
      "在底部有一些注释，因此\n",
      "\n",
      "我们要开始的第一个阶段是\n",
      "\n",
      "预训练阶段 现在这个阶段\n",
      "\n",
      "在这个图中有点特殊，这个\n",
      "\n",
      "图不是按比例的，因为这个\n",
      "\n",
      "阶段是所有计算\n",
      "\n",
      "工作基本上发生的地方，这是\n",
      "\n",
      "训练呃计算时间的 99，还有失败\n",
      "\n",
      "，所以这就是我们正在处理的地方\n",
      "\n",
      "互联网规模的数据集，\n",
      "\n",
      "商店计算机中包含数千个 GPU，并且\n",
      "\n",
      "可能需要数月的训练，其他\n",
      "\n",
      "三个阶段是微调阶段，\n",
      "\n",
      "更多的是沿着\n",
      "\n",
      "少量 GPU 和数小时或数天的路线，\n",
      "\n",
      "所以让我们看一下 在预训练\n",
      "\n",
      "阶段，为了首先实现基本模型，\n",
      "\n",
      "我们将收集\n",
      "\n",
      "大量数据，这是\n",
      "\n",
      "我们所谓的数据混合的示例，该示例来自\n",
      "\n",
      "Meta 发布的这篇呃论文，\n",
      "\n",
      "他们在该论文中发布了基于 llama 的模型\n",
      "\n",
      "现在，您可以粗略地看到\n",
      "\n",
      "进入这些集合的数据集类型，\n",
      "\n",
      "因此我们有常见的爬行，\n",
      "\n",
      "这只是一个网络抓取 C4，这也是\n",
      "\n",
      "一个常见的爬行，然后还有一些高质量\n",
      "\n",
      "的数据集，例如\n",
      "\n",
      "GitHub 维基百科图书档案 堆栈\n",
      "\n",
      "交换等等这些都\n",
      "\n",
      "混合在一起，然后\n",
      "\n",
      "根据给定的比例对它们进行采样，\n",
      "\n",
      "形成\n",
      "\n",
      "GPT 神经网络的训练集，然后\n",
      "\n",
      "我们才能真正训练这些\n",
      "\n",
      "数据，我们需要再经历一次\n",
      "\n",
      "预处理步骤，即\n",
      "\n",
      "标记化，这基本上是\n",
      "\n",
      "我们\n",
      "\n",
      "从互联网上抓取的\n",
      "\n",
      "原始文本到整数序列的翻译，因为这是\n",
      "\n",
      "gpts 现在运行的本机表示，\n",
      "\n",
      "这是文本片段\n",
      "\n",
      "和文本片段之间的无损翻译。\n",
      "\n",
      "令牌和整数，\n",
      "\n",
      "此阶段有许多算法，\n",
      "\n",
      "例如，您通常可以使用\n",
      "\n",
      "字节对编码之类的算法，它\n",
      "\n",
      "迭代地合并小文本块\n",
      "\n",
      "并将它们分组为令牌，\n",
      "\n",
      "因此在这里我展示了\n",
      "\n",
      "这些令牌的一些示例块，然后 这是\n",
      "\n",
      "\n",
      "实际输入到 Transformer 的原始整数序列，\n",
      "\n",
      "现在在这里我展示了两个类似的\n",
      "\n",
      "示例，用于\n",
      "\n",
      "控制这个阶段的超参数，所以 gpt4 我们没有\n",
      "\n",
      "发布太多关于\n",
      "\n",
      "它是如何训练的信息等等 我使用的是\n",
      "\n",
      "gpt3 的数字，但 gpt3 当然\n",
      "\n",
      "在大约三年前有点旧了，\n",
      "\n",
      "但 llama 是来自 meta 的一个相当新的模型，\n",
      "\n",
      "所以这些大致是\n",
      "\n",
      "我们在\n",
      "\n",
      "做 pre 时处理的数量级。  - 训练\n",
      "\n",
      "词汇量通常是几个\n",
      "\n",
      "10 000 个标记，上下文长度通常是\n",
      "\n",
      "\n",
      "204000 个，现在甚至是 100000 个，\n",
      "\n",
      "这决定了\n",
      "\n",
      "GPT 在\n",
      "\n",
      "尝试预测下一个整数时将查看的最大整数数量\n",
      "\n",
      "在序列中，\n",
      "\n",
      "您可以看到 llama 现在的参数数量大致为\n",
      "\n",
      "650 亿个，\n",
      "\n",
      "尽管 llama\n",
      "\n",
      "与 gvt3 的 1750\n",
      "\n",
      "亿个参数相比只有 65b 个参数，但 llama 是一个\n",
      "\n",
      "更强大的模型，\n",
      "\n",
      "直观上这是因为该模型\n",
      "\n",
      "经过了显着的训练 在这种\n",
      "\n",
      "情况下，更长的时间是 1.4 万亿个令牌，而不是\n",
      "\n",
      "3000 亿个令牌，因此您不应该\n",
      "\n",
      "仅通过模型包含的参数数量来判断模型的能力，\n",
      "\n",
      "\n",
      "下面我将显示一些通常会出现的\n",
      "\n",
      "粗略超参数的粗略数量表\n",
      "\n",
      "指定\n",
      "\n",
      "Transformer 神经网络，\n",
      "\n",
      "头数、维度大小、\n",
      "\n",
      "层数等，在底部，我\n",
      "\n",
      "显示了一些训练超参数，\n",
      "\n",
      "例如训练 65b 模型元，\n",
      "\n",
      "使用 2000 GPU 大约 21 天\n",
      "\n",
      "的训练 大约\n",
      "\n",
      "几百万美元，所以这是\n",
      "\n",
      "你现在在\n",
      "\n",
      "预训练阶段应该记住的粗略数量级，\n",
      "\n",
      "\n",
      "当我们实际上预链接发生的\n",
      "\n",
      "事情时，粗略地说，我们\n",
      "\n",
      "将拿走我们的代币，我们\n",
      "\n",
      "将它们放入数据批次中，这样我们\n",
      "\n",
      "就有这些数组将输入到\n",
      "\n",
      "Transformer 中，这些数组是 B\n",
      "\n",
      "批次大小，这些都是按\n",
      "\n",
      "行堆叠的独立示例，\n",
      "\n",
      "B 乘 T T 是最大上下文\n",
      "\n",
      "长度，所以在我的图片中 我只有 10.\n",
      "\n",
      "上下文长度，所以这可能是\n",
      "\n",
      "两千四千等等，所以这些是\n",
      "\n",
      "非常长的行，我们所做的就是\n",
      "\n",
      "获取这些文档，将它们打包\n",
      "\n",
      "成行，然后用这些\n",
      "\n",
      "特殊的文本结尾标记来分隔它们 基本上\n",
      "\n",
      "它是在 Transformer 上\n",
      "\n",
      "开始新文档的，\n",
      "\n",
      "所以这里我有一些\n",
      "\n",
      "文档示例，然后我将它们\n",
      "\n",
      "延伸到这个输入中，\n",
      "\n",
      "现在我们将把所有这些\n",
      "\n",
      "数字输入 Transformer，让我让\n",
      "\n",
      "我只关注一个特定的\n",
      "\n",
      "单元格，但\n",
      "\n",
      "该图中的每个单元格都会发生同样的事情，所以\n",
      "\n",
      "让我们看一下绿色单元格，绿色\n",
      "\n",
      "单元格将查看\n",
      "\n",
      "它之前的所有标记，因此其中的所有\n",
      "\n",
      "标记 黄色，我们将把\n",
      "\n",
      "整个上下文输入 Transformer\n",
      "\n",
      "神经网络，\n",
      "\n",
      "Transformer 将尝试\n",
      "\n",
      "预测序列中的下一个标记，\n",
      "\n",
      "在这种情况下为红色，\n",
      "\n",
      "现在 Transformer 不幸的是我没有\n",
      "\n",
      "太多时间去 深入了解\n",
      "\n",
      "这个神经网络架构的全部细节\n",
      "\n",
      "只是\n",
      "\n",
      "为了我们的目的而设计的一大块神经网络东西，\n",
      "\n",
      "它通常有几百亿个参数\n",
      "\n",
      "或类似的东西，\n",
      "\n",
      "当然，当他们调整这些参数时，\n",
      "\n",
      "你会得到稍微不同的呃\n",
      "\n",
      "预测分布 对于\n",
      "\n",
      "这些单元中的每一个，\n",
      "\n",
      "例如，如果我们的词汇量\n",
      "\n",
      "大小是 50 257 个标记，那么我们将\n",
      "\n",
      "拥有那么多数字，因为我们\n",
      "\n",
      "需要指定\n",
      "\n",
      "接下来发生的情况的概率分布，所以\n",
      "\n",
      "基本上我们有一个概率\n",
      "\n",
      "现在在这个具体\n",
      "\n",
      "示例中，接下来将出现特定单元格 513，\n",
      "\n",
      "因此我们可以使用它作为\n",
      "\n",
      "监督源来更新我们的\n",
      "\n",
      "Transformers 权重，因此我们\n",
      "\n",
      "基本上并行地将其应用于每个\n",
      "\n",
      "单元格，并且我们不断交换\n",
      "\n",
      "交换批次 我们正在尝试让\n",
      "\n",
      "Transformer 对序列中\n",
      "\n",
      "接下来出现的标记做出正确的预测，\n",
      "\n",
      "\n",
      "所以让我更具体地向您展示\n",
      "\n",
      "当您训练其中一个\n",
      "\n",
      "模型时这实际上是\n",
      "\n",
      "来自《纽约时报》的模型，并且 他们\n",
      "\n",
      "在莎士比亚上训练了一个小 GPT，所以这是\n",
      "\n",
      "莎士比亚的一小段，他们\n",
      "\n",
      "现在在初始化开始时训练了一个 GPT，\n",
      "\n",
      "GPT 以\n",
      "\n",
      "完全随机的权重开始，所以你也\n",
      "\n",
      "得到完全随机的输出，\n",
      "\n",
      "\n",
      "但随着时间的推移 你训练呃 GPT 的时间\n",
      "\n",
      "越来越长，你会从模型中得到\n",
      "\n",
      "越来越连贯和一致的\n",
      "\n",
      "样本，\n",
      "\n",
      "你从中采样的方式当然是你呃\n",
      "\n",
      "预测接下来会发生什么，你从\n",
      "\n",
      "这个分布中采样，然后你继续提供\n",
      "\n",
      "它 回到这个过程，你\n",
      "\n",
      "基本上可以对大序列进行采样，\n",
      "\n",
      "所以到最后你会看到\n",
      "\n",
      "Transformer 已经学会了单词、在\n",
      "\n",
      "哪里放置空格、在哪里放置\n",
      "\n",
      "逗号等等，所以我们正在做出\n",
      "\n",
      "越来越一致的预测\n",
      "\n",
      "随着时间的推移，\n",
      "\n",
      "这些是您在\n",
      "\n",
      "\n",
      "有效地进行模型预训练时所看到的图表类型，我们在训练\n",
      "\n",
      "时会随着时间的推移查看损失函数，\n",
      "\n",
      "低损失意味着我们的\n",
      "\n",
      "Transformer 正在预测正确的\n",
      "\n",
      "呃 为序列中正确的下一个整数提供更高的概率\n",
      "\n",
      "\n",
      "现在，\n",
      "\n",
      "一旦我们在一个月后训练好该模型，我们将如何处理该模型，\n",
      "\n",
      "我们\n",
      "\n",
      "注意到我们领域的第一件事是这些\n",
      "\n",
      "模型基本上处于以下过程：\n",
      "\n",
      "语言建模学习非常强大的\n",
      "\n",
      "通用表示，并且\n",
      "\n",
      "可以非常有效地\n",
      "\n",
      "针对您可能感兴趣的任何下游任务对它们进行微调，\n",
      "\n",
      "\n",
      "例如，如果您对\n",
      "\n",
      "情感分类感兴趣，\n",
      "\n",
      "过去的方法是收集一堆\n",
      "\n",
      "正面和负面，然后你\n",
      "\n",
      "为此训练某种 NLP 模型，\n",
      "\n",
      "但新方法是忽略\n",
      "\n",
      "情感分类\n",
      "\n",
      "并进行大型语言模型\n",
      "\n",
      "预训练，训练大型 Transformer\n",
      "\n",
      "，然后你只能有\n",
      "\n",
      "几个例子 你可以非常\n",
      "\n",
      "有效地微调\n",
      "\n",
      "该任务的模型，因此这在实践中非常有效，其\n",
      "\n",
      "原因是\n",
      "\n",
      "基本上 Transformer 被迫\n",
      "\n",
      "在语言建模任务中执行大量任务，\n",
      "\n",
      "因为仅就以下方面而言：\n",
      "\n",
      "预测下一个标记，\n",
      "\n",
      "它被迫了解很多关于\n",
      "\n",
      "文本的结构以及\n",
      "\n",
      "它们所包含的所有不同概念，\n",
      "\n",
      "所以这是 GPT 一个现在大约在\n",
      "\n",
      "gpt2 的时间人们注意到，实际上\n",
      "\n",
      "甚至比微调更好\n",
      "\n",
      "非常有效地提示这些模型，\n",
      "\n",
      "所以这些是语言模型\n",
      "\n",
      "，它们想要完成文档，这样\n",
      "\n",
      "你实际上可以\n",
      "\n",
      "通过排列这些\n",
      "\n",
      "假文档来欺骗它们执行任务，所以在这个例子中，\n",
      "\n",
      "例如，我们有一些段落，\n",
      "\n",
      "然后我们有点像这样做 QA qaqa 这被\n",
      "\n",
      "称为“一些镜头提示”，然后我们执行\n",
      "\n",
      "q，然后当 Transformer 尝试\n",
      "\n",
      "完成文档时，它实际上是在\n",
      "\n",
      "回答我们的问题，所以这是一个\n",
      "\n",
      "提示工程基础模型的示例，人们\n",
      "\n",
      "相信它是在\n",
      "\n",
      "模仿 一个文档并让它\n",
      "\n",
      "执行一项任务，\n",
      "\n",
      "所以这拉开了我想说的时代，我想说的是，\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "即使没有训练\n",
      "\n",
      "任何神经网络微调，这实际上也可以很好地解决很多问题 等等，\n",
      "\n",
      "从那时起，我们已经看到了每个人都训练过的\n",
      "\n",
      "基本模型的完整进化树，但\n",
      "\n",
      "并非所有这些\n",
      "\n",
      "模型都可用，例如，\n",
      "\n",
      "gpt4 基本模型从未发布过，\n",
      "\n",
      "您可能通过 API 与之交互的 gpt4 模型\n",
      "\n",
      "则不可用 一个基本模型，它是\n",
      "\n",
      "一个辅助模型，我们将\n",
      "\n",
      "介绍如何获得这些\n",
      "\n",
      "基于 gpt3 的模型可以\n",
      "\n",
      "通过名为 DaVinci 的 API 获得，而 gpt2 基本\n",
      "\n",
      "模型甚至可以作为\n",
      "\n",
      "我们 GitHub 存储库上的权重使用，但目前是最好的\n",
      "\n",
      "可用的基本模型可能是\n",
      "\n",
      "来自meta的Llama系列，尽管它\n",
      "\n",
      "没有商业许可\n",
      "\n",
      "现在需要指出的是基本\n",
      "\n",
      "模型并不能提供帮助，他们\n",
      "\n",
      "不想回答他们不想\n",
      "\n",
      "回答你的问题，他们只是\n",
      "\n",
      "想要 完成文件，所以如果你\n",
      "\n",
      "告诉他们写一首关于该品牌\n",
      "\n",
      "奶酪的诗，你就会知道它会\n",
      "\n",
      "回答更多问题，它\n",
      "\n",
      "只是完成它认为作为\n",
      "\n",
      "文件的内容，\n",
      "\n",
      "但是你可以以\n",
      "\n",
      "特定的方式提示他们基本模型\n",
      "\n",
      "更有可能起作用，因此作为一个例子，\n",
      "\n",
      "这里有一首关于面包和奶酪的诗，\n",
      "\n",
      "在这种情况下，它将正确自动完成，\n",
      "\n",
      "\n",
      "您甚至可以欺骗基础模型来\n",
      "\n",
      "提供帮助，而您\n",
      "\n",
      "这样做的方式是您将创建像\n",
      "\n",
      "特定的几个 镜头提示\n",
      "\n",
      "看起来就像\n",
      "\n",
      "人类和助手之间有某种文档\n",
      "\n",
      "，他们正在交换某种嗯\n",
      "\n",
      "信息，然后在底部你将\n",
      "\n",
      "查询放在最后，\n",
      "\n",
      "基本模型将有点像\n",
      "\n",
      "条件 它本身就像一个\n",
      "\n",
      "有用的助手和某种答案，但这\n",
      "\n",
      "不是很可靠，\n",
      "\n",
      "并且在实践中效果不佳，尽管\n",
      "\n",
      "可以这样做，因此\n",
      "\n",
      "我们有不同的路径来\n",
      "\n",
      "使实际的 GBT 帮助不仅仅是基本\n",
      "\n",
      "模型文档完成器和 因此，这\n",
      "\n",
      "使我们进入监督微调阶段，因此\n",
      "\n",
      "在监督微调阶段，我们\n",
      "\n",
      "将收集少量但高质量\n",
      "\n",
      "的数据集，在这种情况下，我们\n",
      "\n",
      "将要求人工承包商收集表单提示\n",
      "\n",
      "格式的数据，\n",
      "\n",
      "并 理想的响应，我们将\n",
      "\n",
      "收集大量这样的数据，通常是\n",
      "\n",
      "数万个或类似的东西，\n",
      "\n",
      "然后我们仍然会对\n",
      "\n",
      "这些数据进行语言建模，因此\n",
      "\n",
      "算法上不会发生任何变化，我们\n",
      "\n",
      "只是交换训练集，以便它\n",
      "\n",
      "使用 作为互联网文档，这是一种\n",
      "\n",
      "大量的本地\n",
      "\n",
      "四基本上 QA 提示响应类型的\n",
      "\n",
      "数据，并且是低数量\n",
      "\n",
      "高质量的，\n",
      "\n",
      "所以我们仍然进行语言建模，\n",
      "\n",
      "然后在训练之后我们得到一个 sfd 模型\n",
      "\n",
      "，你可以实际部署这些模型，\n",
      "\n",
      "它们是 实际的助理，他们\n",
      "\n",
      "在某种程度上工作让我向您\n",
      "\n",
      "展示示例演示可能是什么样子\n",
      "\n",
      "所以这是人类\n",
      "\n",
      "承包商可能想出的东西这是\n",
      "\n",
      "一些随机提示您可以写一个关于术语垄断或类似术语\n",
      "\n",
      "的相关性的简短介绍吗\n",
      "\n",
      "\n",
      "然后承包商也会写出\n",
      "\n",
      "一个理想的回复，当他们\n",
      "\n",
      "写出这些回复时，他们会遵循\n",
      "\n",
      "大量的标签文件，并被\n",
      "\n",
      "要求提供帮助、\n",
      "\n",
      "真实且无害，而\n",
      "\n",
      "这里的标签说明您可能无法\n",
      "\n",
      "阅读其他可以 我但是它们很长，\n",
      "\n",
      "这只是人们遵循\n",
      "\n",
      "指示并尝试完成\n",
      "\n",
      "这些提示，\n",
      "\n",
      "这就是数据集的样子，\n",
      "\n",
      "您可以训练这些模型，这\n",
      "\n",
      "在某种程度上有效，\n",
      "\n",
      "现在您实际上可以\n",
      "\n",
      "从这里继续管道， 进入来自\n",
      "\n",
      "人类反馈的 rlhf 强化学习，\n",
      "\n",
      "其中包括奖励\n",
      "\n",
      "建模和强化学习，所以\n",
      "\n",
      "让我介绍这一点，然后我会\n",
      "\n",
      "回到为什么你可能想要执行\n",
      "\n",
      "额外的步骤，以及它与 sft 模型相比如何，\n",
      "\n",
      "\n",
      "所以 奖励建模步骤\n",
      "\n",
      "我们要做的就是将\n",
      "\n",
      "我们的数据收集转变为\n",
      "\n",
      "比较的形式，所以这里是\n",
      "\n",
      "我们的数据集的一个例子\n",
      "\n",
      "我有相同的提示相同的提示\n",
      "\n",
      "top 要求助手\n",
      "\n",
      "编写一个程序或函数来\n",
      "\n",
      "检查给定的字符串是否是回文\n",
      "\n",
      "，然后我们要做的是采用\n",
      "\n",
      "我们已经训练过的 sft 模型并\n",
      "\n",
      "创建多个完成，因此在本\n",
      "\n",
      "例中我们有三个\n",
      "\n",
      "模型创建的完成结果，\n",
      "\n",
      "然后我们要求人们对这些\n",
      "\n",
      "完成情况进行排名，因此，如果您盯着它\n",
      "\n",
      "一段时间，顺便说一下，\n",
      "\n",
      "比较其中一些\n",
      "\n",
      "预测是非常困难的，这可能会花费\n",
      "\n",
      "人们甚至几个小时的时间 单个提示\n",
      "\n",
      "完成对，但假设我们\n",
      "\n",
      "认为其中一个比\n",
      "\n",
      "其他更好，依此类推，因此我们对它们进行排名，\n",
      "\n",
      "\n",
      "然后我们可以用\n",
      "\n",
      "看起来非常像对这些之间\n",
      "\n",
      "所有可能的对进行二元分类的东西来跟踪\n",
      "\n",
      "\n",
      "所以我们现在要做的就是将\n",
      "\n",
      "提示按行布局，并且\n",
      "\n",
      "这里所有三行的提示都是相同的，所以提示\n",
      "\n",
      "都是相同的，但是\n",
      "\n",
      "完成的情况有所不同，所以\n",
      "\n",
      "黄色标记来自 sft\n",
      "\n",
      "模型，那么我们的 要做的是我们在最后附加\n",
      "\n",
      "另一个特殊奖励呃读出令牌\n",
      "\n",
      "，我们基本上只\n",
      "\n",
      "在这个绿色令牌上监督 Transformer，\n",
      "\n",
      "并且 Transformer 会\n",
      "\n",
      "预测一些奖励，以判断\n",
      "\n",
      "该提示的完成程度\n",
      "\n",
      "，因此基本上它会猜测\n",
      "\n",
      "每个完成的质量，然后\n",
      "\n",
      "一旦它对每个完成进行猜测，\n",
      "\n",
      "我们也有基本事实，它\n",
      "\n",
      "告诉我们它们的排名，因此\n",
      "\n",
      "我们实际上可以强制其中一些\n",
      "\n",
      "数字应该比其他数字高得多，\n",
      "\n",
      "依此类推 我们将其公式化为\n",
      "\n",
      "损失函数，并训练我们的模型\n",
      "\n",
      "做出奖励预测，该预测\n",
      "\n",
      "\n",
      "与所有这些承包商的比较得出的基本事实一致，\n",
      "\n",
      "这就是我们训练\n",
      "\n",
      "奖励模型的方式，并让我们能够对\n",
      "\n",
      "完成情况进行评分 对于提示，\n",
      "\n",
      "一旦我们有了奖励模型，我们就\n",
      "\n",
      "无法部署它，因为这\n",
      "\n",
      "本身作为助手不是很有用，\n",
      "\n",
      "但对于现在接下来的强化\n",
      "\n",
      "强化学习阶段非常有用，\n",
      "\n",
      "因为我们有一个\n",
      "\n",
      "可以评分的奖励模型 任何\n",
      "\n",
      "给定\n",
      "\n",
      "提示的任意完成的质量，所以我们在\n",
      "\n",
      "强化学习期间所做的就是我们基本上\n",
      "\n",
      "再次得到大量提示，\n",
      "\n",
      "现在我们\n",
      "\n",
      "针对奖励模型进行强化学习，所以\n",
      "\n",
      "这就是\n",
      "\n",
      "我们采取的单一提示 提示我们将其排成行，\n",
      "\n",
      "现在我们使用 sft 我们\n",
      "\n",
      "基本上使用我们想要\n",
      "\n",
      "训练的模型，该模型在 sft 模型中初始化\n",
      "\n",
      "以创建一些黄色的完成\n",
      "\n",
      "，然后我们再次附加奖励令牌\n",
      "\n",
      "，我们读出\n",
      "\n",
      "根据\n",
      "\n",
      "现在保持固定的奖励模型进行奖励，它不再改变\n",
      "\n",
      "，现在奖励模型告诉我们\n",
      "\n",
      "每个提示的每个完成的质量\n",
      "\n",
      "，所以我们\n",
      "\n",
      "可以做的是我们现在可以基本上\n",
      "\n",
      "应用相同的 语言建模损失\n",
      "\n",
      "函数，但我们目前正在对\n",
      "\n",
      "黄色标记进行训练，并且我们正在通过\n",
      "\n",
      "\n",
      "奖励模型指示的奖励来权衡语言建模目标，因此\n",
      "\n",
      "作为第一行的示例，\n",
      "\n",
      "奖励模型表示这是一个相当\n",
      "\n",
      "高的分数 因此，\n",
      "\n",
      "我们在第一行中碰巧采样的所有令牌都\n",
      "\n",
      "将得到强化，\n",
      "\n",
      "并且它们将在未来获得更高的\n",
      "\n",
      "概率，\n",
      "\n",
      "相反，在第二行中，奖励\n",
      "\n",
      "模型确实不喜欢这种\n",
      "\n",
      "完成负数 1.2，所以 因此，\n",
      "\n",
      "我们在第二行中采样的每个令牌\n",
      "\n",
      "\n",
      "未来的概率都会稍高一些，\n",
      "\n",
      "我们在许多批次的许多提示中一遍又一遍地执行此操作，\n",
      "\n",
      "\n",
      "基本上我们得到了一个\n",
      "\n",
      "在这里创建黄色令牌的策略，它基本上全部\n",
      "\n",
      "根据我们在前一阶段训练的奖励模型，这里所有的完成都会\n",
      "\n",
      "得分很高，这\n",
      "\n",
      "\n",
      "就是我们训练的方式，呃，这就是\n",
      "\n",
      "\n",
      "现在的 rohf 管道，最后你会得到一个\n",
      "\n",
      "可以部署的模型，所以 举个\n",
      "\n",
      "例子，聊天 GPT 是 rlhf 模型，但\n",
      "\n",
      "您可能会遇到的其他一些模型，\n",
      "\n",
      "例如 kuna 13B 等，\n",
      "\n",
      "这些都是 sft 模型，因此我们\n",
      "\n",
      "有基本模型 sft 模型和 rlh\n",
      "\n",
      "模型，这有点像状态\n",
      "\n",
      "\n",
      "现在为什么你想做 rlhf 所以一个\n",
      "\n",
      "不太令人兴奋的答案是\n",
      "\n",
      "它效果更好所以这\n",
      "\n",
      "来自导师 GBT 论文\n",
      "\n",
      "根据不久前的这些实验\n",
      "\n",
      "现在这些 PPO 模型是 rlhf 并且我们\n",
      "\n",
      "看到当我们把它们给人类时，它们基本上只是\n",
      "\n",
      "在很多比较中被首选，\n",
      "\n",
      "所以人类只是更\n",
      "\n",
      "喜欢\n",
      "\n",
      "来自我们宗教模型的基本令牌，与 sft\n",
      "\n",
      "模型相比，与被\n",
      "\n",
      "提示成为助手的基本模型相比，所以它\n",
      "\n",
      "只是 效果更好，但你可能会问为什么\n",
      "\n",
      "它效果更好，我不\n",
      "\n",
      "认为有一个像\n",
      "\n",
      "社区真正\n",
      "\n",
      "同意的令人惊奇的答案，但我只会提供\n",
      "\n",
      "一个可能的原因，它与 比较\n",
      "\n",
      "与生成在计算上的容易程度之间存在不对称性，\n",
      "\n",
      "\n",
      "所以让我们以\n",
      "\n",
      "生成俳句为例假设我要求一个模型\n",
      "\n",
      "写一篇关于回形针的俳句，如果\n",
      "\n",
      "你是一个试图提供火车\n",
      "\n",
      "数据的承包商，那么想象一下你是一个\n",
      "\n",
      "基本上收集的承包商 sft 阶段的数据\n",
      "\n",
      "你应该如何为回形针创建一个\n",
      "\n",
      "漂亮的俳句 你可能\n",
      "\n",
      "不太擅长，但如果我\n",
      "\n",
      "给你一些俳句的例子，你\n",
      "\n",
      "可能会比其中一些\n",
      "\n",
      "俳句更欣赏\n",
      "\n",
      "因此，判断其中哪一个是好的是\n",
      "\n",
      "更容易的任务，因此基本上这种\n",
      "\n",
      "不对称使得比较成为\n",
      "\n",
      "一种更好的方式，可以潜在地利用\n",
      "\n",
      "自己作为一个人的能力，并且你的判断力可以\n",
      "\n",
      "创建一个稍微更好的模型，\n",
      "\n",
      "现在我们的宗教模型不是\n",
      "\n",
      "\n",
      "在某些情况下，严格地对基本模型进行了改进，因此\n",
      "\n",
      "我们特别注意到，例如它们丢失了\n",
      "\n",
      "一些熵，这意味着它们\n",
      "\n",
      "给出了更多 PT 结果，它们可以输出\n",
      "\n",
      "更低的变化，就像它们可以输出\n",
      "\n",
      "比基本模型具有更低变化的样本一样\n",
      "\n",
      "所以基础模型有很多熵\n",
      "\n",
      "，我们会使用很多不同的\n",
      "\n",
      "输出，\n",
      "\n",
      "例如，\n",
      "\n",
      "我仍然更喜欢使用基础模型的一种地方\n",
      "\n",
      "是在设置中，\n",
      "\n",
      "你基本上有 N 个东西，并且你想要\n",
      "\n",
      "生成更多 类似的东西，所以\n",
      "\n",
      "这是我刚刚编写的一个例子，\n",
      "\n",
      "我想生成很酷的神奇宝贝名称，我\n",
      "\n",
      "给了它七个神奇宝贝名称，我要求\n",
      "\n",
      "基本模型完成文档，\n",
      "\n",
      "它给了我更多神奇宝贝名称，\n",
      "\n",
      "呃这些都是虚构的 我试着\n",
      "\n",
      "查找它们，我不相信它们是真正的\n",
      "\n",
      "口袋妖怪，呃，这是\n",
      "\n",
      "我认为基本模型擅长的任务，\n",
      "\n",
      "因为它仍然有很多熵，它\n",
      "\n",
      "会给你很多不同的很酷的东西\n",
      "\n",
      "更多的东西看起来像\n",
      "\n",
      "你之前给的任何东西，\n",
      "\n",
      "所以这就是呃这是数字，\n",
      "\n",
      "所有这些都说这些有点像\n",
      "\n",
      "\n",
      "你现在可能可以使用的助理模型呃，\n",
      "\n",
      "伯克利有一个团队排名\n",
      "\n",
      "很多可用的辅助模型，\n",
      "\n",
      "并给了它们基本上 ELO 评级 所以\n",
      "\n",
      "目前一些最好的模型当然\n",
      "\n",
      "是 gpt4 我想说其次\n",
      "\n",
      "是 Claude gvt 3.5 然后是\n",
      "\n",
      "许多模型 其中一些可能可以作为\n",
      "\n",
      "权重使用，例如 kuna 考拉\n",
      "\n",
      "等，这里的前三行都是\n",
      "\n",
      "rohf 模型，据\n",
      "\n",
      "我所知，所有其他模型都是 sft\n",
      "\n",
      "模型，我相信\n",
      "\n",
      "没问题，所以这就是我们在高水平上训练这些模型的方式，\n",
      "\n",
      "现在我要\n",
      "\n",
      "切换齿轮 让我们看看如何\n",
      "\n",
      "最好地应用 GPT 助手\n",
      "\n",
      "模型来解决您的问题现在我想\n",
      "\n",
      "\n",
      "在一个具体示例的设置中工作所以让我们\n",
      "\n",
      "\n",
      "在这里使用一个具体示例\n",
      "\n",
      "假设您正在写一篇\n",
      "\n",
      "文章或一个 博客文章，你要\n",
      "\n",
      "在最后写下这句话\n",
      "\n",
      "加利福尼亚州的人口是\n",
      "\n",
      "阿拉斯加的 53 倍，所以出于某种原因你想\n",
      "\n",
      "比较这两个州的人口，\n",
      "\n",
      "\n",
      "想想丰富的内心独白\n",
      "\n",
      "和工具的使用以及实际工作量\n",
      "\n",
      "在你的大脑中进行计算以\n",
      "\n",
      "生成最后一个句子，所以\n",
      "\n",
      "这可能是\n",
      "\n",
      "你大脑中的样子好吧，下一步\n",
      "\n",
      "让我写博客呃我的博客让我\n",
      "\n",
      "比较这两个群体\n",
      "\n",
      "好吧首先我显然需要\n",
      "\n",
      "现在我\n",
      "\n",
      "知道我可能不知道这些\n",
      "\n",
      "人群，所以\n",
      "\n",
      "我有点像意识到我知道什么或\n",
      "\n",
      "不知道我的自我认知，\n",
      "\n",
      "所以我去我 使用一些工具，我访问\n",
      "\n",
      "维基百科，查找加利福尼亚州的\n",
      "\n",
      "人口和阿拉斯加的人口，\n",
      "\n",
      "现在我知道我应该将两者相除，\n",
      "\n",
      "但我再次知道，将 3 9.2\n",
      "\n",
      "除以 0.74 不太可能成功，\n",
      "\n",
      "这不是我想要的那种事情 我可以\n",
      "\n",
      "在脑海中完成，因此我将\n",
      "\n",
      "依赖计算器，因此我将\n",
      "\n",
      "使用计算器输入它并查看\n",
      "\n",
      "输出大约为 53。\n",
      "\n",
      "然后也许我会\n",
      "\n",
      "在我的大脑中进行一些反思和健全性检查 所以 53\n",
      "\n",
      "很有意义吗？这是一个相当\n",
      "\n",
      "大的分数，但加利福尼亚是\n",
      "\n",
      "最受欢迎的州，所以也许\n",
      "\n",
      "看起来不错，所以\n",
      "\n",
      "然后我有了我\n",
      "\n",
      "可能需要的所有信息，现在我开始\n",
      "\n",
      "写作的创造性部分，所以我可能\n",
      "\n",
      "开始写一些像加州\n",
      "\n",
      "有 53 倍大的东西，然后我认为这\n",
      "\n",
      "实际上是非常\n",
      "\n",
      "尴尬的措辞，所以让我实际上\n",
      "\n",
      "删除它并让我再试一次，所以\n",
      "\n",
      "当我写的时候，我有一个单独的\n",
      "\n",
      "过程几乎检查我的内容 我正在\n",
      "\n",
      "写作并判断它是否看起来\n",
      "\n",
      "不错，\n",
      "\n",
      "然后也许我会删除它，也许我会\n",
      "\n",
      "重新构建它，然后也许我对结果感到满意\n",
      "\n",
      "\n",
      "所以基本上是长话短说，在\n",
      "\n",
      "你的内心独白方面，在幕后发生了很多事情\n",
      "\n",
      "你创建\n",
      "\n",
      "这样的句子，但是\n",
      "\n",
      "当我们在\n",
      "\n",
      "其上训练 GPT 时，这样的句子是什么样的？\n",
      "\n",
      "从 gpt 的角度来看，这只是一个\n",
      "\n",
      "标记序列，\n",
      "\n",
      "所以 GPT 当它读取或生成\n",
      "\n",
      "这些标记时，它就会运行，\n",
      "\n",
      "每个块大致是\n",
      "\n",
      "每个 token 的计算工作量相同，\n",
      "\n",
      "这些 Transformer 并不是\n",
      "\n",
      "很浅的网络，它们有大约 80\n",
      "\n",
      "层推理，但 80 层仍然\n",
      "\n",
      "不算太多，所以这个 Transformer\n",
      "\n",
      "将尽力模仿，但\n",
      "\n",
      "当然这里的过程看起来\n",
      "\n",
      "与您采取的过程非常不同，\n",
      "\n",
      "特别是在\n",
      "\n",
      "我们创建的数据集中的最终工件中，然后\n",
      "\n",
      "最终将其提供给 llms，所有\n",
      "\n",
      "内部对话都被完全剥离\n",
      "\n",
      "，呃，与您不同的是，GPT 会查看\n",
      "\n",
      "每一个标记 并且\n",
      "\n",
      "在每一个上花费相同的计算量，\n",
      "\n",
      "所以你不能指望它实际上会\n",
      "\n",
      "很好，你不能指望它对\n",
      "\n",
      "每个令牌做太多的工作，\n",
      "\n",
      "所以特别是基本上\n",
      "\n",
      "这些 Transformer 就像令牌\n",
      "\n",
      "模拟器一样，所以他们不知道自己\n",
      "\n",
      "不知道什么，就像他们只是模仿\n",
      "\n",
      "下一个令牌一样，他们不知道自己\n",
      "\n",
      "擅长或不擅长什么，他们只是\n",
      "\n",
      "尽力模仿他们不擅长的下一个令牌\n",
      "\n",
      "不会在循环中反映出来，他们不会进行\n",
      "\n",
      "理智检查，默认情况下他们不会纠正\n",
      "\n",
      "错误，\n",
      "\n",
      "他们只是呃样本标记序列，\n",
      "\n",
      "\n",
      "他们的头脑中没有单独的内部独白流来\n",
      "\n",
      "评估现在正在发生的事情\n",
      "\n",
      "我想说的是，他们确实有某种认知优势，那\n",
      "\n",
      "就是他们\n",
      "\n",
      "实际上拥有\n",
      "\n",
      "跨大量领域的大量基于事实的知识，\n",
      "\n",
      "因为他们有数百亿个\n",
      "\n",
      "参数，因此需要大量存储空间来存储\n",
      "\n",
      "大量数据。 事实，\n",
      "\n",
      "但我认为它们也有一个\n",
      "\n",
      "相对较大和完美的工作\n",
      "\n",
      "记忆，所以无论固定\n",
      "\n",
      "在上下文窗口中的什么内容，都可以\n",
      "\n",
      "\n",
      "通过其内部的自我注意\n",
      "\n",
      "机制立即供 Transformer 使用，所以它有点像\n",
      "\n",
      "完美记忆，但它是 得到了有限的\n",
      "\n",
      "大小，但是 Transformer 可以非常\n",
      "\n",
      "直接地访问它，因此它可以\n",
      "\n",
      "无损地记住\n",
      "\n",
      "其上下文窗口内的任何内容，\n",
      "\n",
      "这就是我比较\n",
      "\n",
      "这两者的方式，我提出所有\n",
      "\n",
      "这些的原因是因为我 我认为在\n",
      "\n",
      "很大程度上，提示只是弥补\n",
      "\n",
      "这两种架构之间的认知差异，\n",
      "\n",
      "\n",
      "就像我们的大脑和 llm 的大脑，\n",
      "\n",
      "你几乎可以这样看，\n",
      "\n",
      "所以这是人们发现的一件事，\n",
      "\n",
      "例如，效果很好 在\n",
      "\n",
      "实践中，特别是如果您的任务\n",
      "\n",
      "需要推理，您不能期望\n",
      "\n",
      "Transformer 对\n",
      "\n",
      "每个令牌进行太多推理，因此您必须\n",
      "\n",
      "真正将推理分散到\n",
      "\n",
      "越来越多的令牌上，例如您\n",
      "\n",
      "不能给 Transformer 一个非常\n",
      "\n",
      "复杂的问题，并期望它能\n",
      "\n",
      "在一个令牌中得到答案，\n",
      "\n",
      "只是没有足够的时间，这些\n",
      "\n",
      "变形金刚需要令牌来思考\n",
      "\n",
      "引用不引用我有时喜欢说，所以\n",
      "\n",
      "这是一些效果\n",
      "\n",
      "很好的东西，例如你可以 有一些镜头\n",
      "\n",
      "提示，向 Transformer 表明\n",
      "\n",
      "它应该在回答问题时展示其工作，\n",
      "\n",
      "\n",
      "如果您给出一些\n",
      "\n",
      "示例，Transformer 将模仿\n",
      "\n",
      "该模板，并且最终会\n",
      "\n",
      "表现得更好\n",
      "\n",
      "\n",
      "此外，您可以\n",
      "\n",
      "通过说让我们\n",
      "\n",
      "一步一步思考来从 Transformer 中引出这种行为，因为这种\n",
      "\n",
      "条件是 Transformer\n",
      "\n",
      "类似于显示其工作，并且因为它会\n",
      "\n",
      "突然进入显示其工作的模式，因此它会\n",
      "\n",
      "\n",
      "每个令牌做的计算工作更少，因此它更有可能\n",
      "\n",
      "成功，因为\n",
      "\n",
      "\n",
      "随着时间的推移，它会使推理速度变慢，这是\n",
      "\n",
      "另一个例子，这被\n",
      "\n",
      "称为自我一致性，我们看到我们\n",
      "\n",
      "有能力开始编写，\n",
      "\n",
      "然后如果没有的话' 不解决我可以再试\n",
      "\n",
      "一次，我可以尝试多次，\n",
      "\n",
      "呃，也许\n",
      "\n",
      "嗯，选择最有效的一个，所以在\n",
      "\n",
      "这些方法中，你可能\n",
      "\n",
      "不仅仅采样一次，但你可能采样\n",
      "\n",
      "多次，然后有一些\n",
      "\n",
      "过程来找到 那些\n",
      "\n",
      "不好的，然后只保留这些样本\n",
      "\n",
      "或进行多数投票或\n",
      "\n",
      "类似的事情，所以基本上这些\n",
      "\n",
      "变形金刚在这个过程中，因为他们\n",
      "\n",
      "像你一样预测下一个令牌，\n",
      "\n",
      "他们可能会运气不好，他们\n",
      "\n",
      "可以采样而不是一个 非常好的令牌\n",
      "\n",
      "，他们在推理方面可能会像死胡同一样走下去，\n",
      "\n",
      "所以\n",
      "\n",
      "与你不同的是，他们无法从中恢复过来，他们\n",
      "\n",
      "被困在他们采样的每一个令牌上\n",
      "\n",
      "，所以他们会继续这个\n",
      "\n",
      "序列，即使他们甚至知道\n",
      "\n",
      "这个 序列不会成功，\n",
      "\n",
      "所以让他们能够回顾\n",
      "\n",
      "检查或尝试找到呃尝试\n",
      "\n",
      "基本上围绕它进行采样这是\n",
      "\n",
      "一种技术，你也可以事实\n",
      "\n",
      "证明，实际上llms就像他们\n",
      "\n",
      "知道什么时候搞砸了一样 举个\n",
      "\n",
      "例子，\n",
      "\n",
      "假设你要求模型生成一首\n",
      "\n",
      "不押韵的诗，它可能\n",
      "\n",
      "会给你一首诗，但它实际上押韵，\n",
      "\n",
      "但事实证明，特别是对于\n",
      "\n",
      "像 gpt4 这样的更大的模型，你可以只问\n",
      "\n",
      "它你是否满足了作业并且\n",
      "\n",
      "实际上 gpt4 非常清楚它\n",
      "\n",
      "没有满足任务要求，它\n",
      "\n",
      "只是在采样中有点不幸，所以它\n",
      "\n",
      "会告诉你不，我实际上没有满足\n",
      "\n",
      "任务要求，让我再试一次，\n",
      "\n",
      "但如果没有你提示它它\n",
      "\n",
      "甚至不会 就像它不知道它不\n",
      "\n",
      "知道重新访问等等所以你\n",
      "\n",
      "必须在提示中弥补这一点\n",
      "\n",
      "你必须让它检查如果你不\n",
      "\n",
      "要求它检查它就不会检查\n",
      "\n",
      "就其本身而言，它只是一个令牌模拟器，\n",
      "\n",
      "我认为更一般地说，这些技术中的很多都属于\n",
      "\n",
      "\n",
      "我所说的重新创建我们的系统的范畴，因此\n",
      "\n",
      "您可能会熟悉系统\n",
      "\n",
      "一系统，以人类的角度思考系统\n",
      "\n",
      "一是一个快速自动过程 我\n",
      "\n",
      "认为有点像法学硕士，\n",
      "\n",
      "只是对令牌进行采样，\n",
      "\n",
      "而系统二是你大脑中较慢的深思熟虑的\n",
      "\n",
      "计划，\n",
      "\n",
      "所以这是一篇实际上是\n",
      "\n",
      "上周的论文，因为这个空间正在\n",
      "\n",
      "快速发展，它被称为\n",
      "\n",
      "思想树 在思想树上，呃，\n",
      "\n",
      "这篇论文的作者建议\n",
      "\n",
      "为任何\n",
      "\n",
      "给定的提示维护多个完成，然后他们还会\n",
      "\n",
      "一路上对它们进行评分，并保留\n",
      "\n",
      "那些进展顺利的（如果\n",
      "\n",
      "有意义的话），所以很多人\n",
      "\n",
      "真的很喜欢\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "现在，我想在这里指出的一件事\n",
      "\n",
      "是，这不仅仅是一个提示，这\n",
      "\n",
      "实际上是提示\n",
      "\n",
      "与一些 python 胶水代码一起使用，因为\n",
      "\n",
      "你实际上不需要维护\n",
      "\n",
      "多个提示，而且你还必须\n",
      "\n",
      "在这里做一些树搜索算法，\n",
      "\n",
      "比如找出要扩展的提示\n",
      "\n",
      "等等，所以它是 python 胶水代码的共生\n",
      "\n",
      "和\n",
      "\n",
      "在 while 循环或更大的算法中调用的单独提示\n",
      "\n",
      "\n",
      "我还认为这里与 alphago 有一个非常酷的\n",
      "\n",
      "相似之处 alphago 有一个\n",
      "\n",
      "在下围棋时放置下一颗棋子的策略，\n",
      "\n",
      "它的策略\n",
      "\n",
      "最初是通过模仿人类来训练的，但\n",
      "\n",
      "除此之外 对于这个策略，它也进行\n",
      "\n",
      "蒙特卡罗树搜索，基本上它\n",
      "\n",
      "会在头脑中发挥出多种可能性\n",
      "\n",
      "并评估所有这些可能性，\n",
      "\n",
      "只保留那些工作良好的，所以\n",
      "\n",
      "我认为这有点类似于\n",
      "\n",
      "alphago，但对于 文本，\n",
      "\n",
      "如果这有意义的话，\n",
      "\n",
      "就像思想树一样，我认为\n",
      "\n",
      "更普遍的是，人们开始真正\n",
      "\n",
      "喜欢探索更通用的\n",
      "\n",
      "技术，不仅仅是简单的\n",
      "\n",
      "问题答案提示，而且\n",
      "\n",
      "看起来更像是Python蓝色\n",
      "\n",
      "代码，将许多问题串在一起，\n",
      "\n",
      "等等 右边我有一个\n",
      "\n",
      "名为“反应”的论文的例子，其中他们将\n",
      "\n",
      "提示的答案构建为一系列\n",
      "\n",
      "思想行动观察思想\n",
      "\n",
      "行动观察，这是一个全面的\n",
      "\n",
      "推出，一种回答查询的思维过程，\n",
      "\n",
      "在这些行动中\n",
      "\n",
      "模型 也允许\n",
      "\n",
      "在左边输掉我有一个 rogpt 的例子，顺便说一句，\n",
      "\n",
      "现在 rgbt 变成了一个\n",
      "\n",
      "项目，我认为最近得到了很多炒作，\n",
      "\n",
      "我想呃，但我认为我\n",
      "\n",
      "仍然发现它有点鼓舞人心，很\n",
      "\n",
      "有趣 嗯，这是一个\n",
      "\n",
      "允许 llm 保留任务列表\n",
      "\n",
      "并继续递归地分解\n",
      "\n",
      "任务的项目，我认为这目前\n",
      "\n",
      "效果不是很好，我不会建议\n",
      "\n",
      "人们在实际应用中使用它，\n",
      "\n",
      "我只是认为它是\n",
      "\n",
      "通常会从事情的发展方向中获得灵感，\n",
      "\n",
      "我认为随着时间的推移，这\n",
      "\n",
      "\n",
      "有点像让我们的模型\n",
      "\n",
      "系统思考下一件我\n",
      "\n",
      "觉得有趣的事情是\n",
      "\n",
      "下面的调查会说几乎\n",
      "\n",
      "心理上的 llms 的怪癖是 llms\n",
      "\n",
      "不 不想成功，他们想\n",
      "\n",
      "模仿\n",
      "\n",
      "你，想要成功，你应该提出\n",
      "\n",
      "要求，\n",
      "\n",
      "所以我的意思是，\n",
      "\n",
      "当变形金刚接受训练时，他们有\n",
      "\n",
      "训练集，并且他们的训练数据中可能有完整的\n",
      "\n",
      "性能质量范围，\n",
      "\n",
      "因此对于 例如，\n",
      "\n",
      "可能会出现某种\n",
      "\n",
      "物理问题或类似问题的提示，\n",
      "\n",
      "并且可能存在完全错误的学生解决方案，\n",
      "\n",
      "但\n",
      "\n",
      "也可能存在\n",
      "\n",
      "非常正确的专家答案，而变形金刚无法区分\n",
      "\n",
      "之间的区别 看起来我的\n",
      "\n",
      "意思是他们知道他们知道低\n",
      "\n",
      "质量解决方案和高质量\n",
      "\n",
      "解决方案，但默认情况下他们想要\n",
      "\n",
      "模仿所有这些，因为他们只是接受\n",
      "\n",
      "过语言建模培训，所以在\n",
      "\n",
      "测试时你实际上必须要求\n",
      "\n",
      "良好的性能，所以在 本文中的这个例子\n",
      "\n",
      "是，嗯，他们尝试了各种\n",
      "\n",
      "提示，让我们认为一步一步\n",
      "\n",
      "非常强大，因为它有点像\n",
      "\n",
      "展开剩余标记的推理，\n",
      "\n",
      "但更好的是\n",
      "\n",
      "让我们以一步一步的方式解决这个问题\n",
      "\n",
      "确保我们有正确的答案，\n",
      "\n",
      "所以这有点像\n",
      "\n",
      "获得正确答案的条件，这\n",
      "\n",
      "实际上使 Transformer 工作得\n",
      "\n",
      "更好，因为 Transformer\n",
      "\n",
      "现在不必\n",
      "\n",
      "在低质量解决方案上对冲其概率质量，\n",
      "\n",
      "就像那样荒谬 听起来，所以基本上不要\n",
      "\n",
      "随意要求一个强有力的解决方案，\n",
      "\n",
      "比如你是\n",
      "\n",
      "这个话题的领先专家，假装你有智商\n",
      "\n",
      "120等，但不要试图要求\n",
      "\n",
      "太多的智商，因为如果你要求的智商\n",
      "\n",
      "比如 400，你可能会脱离数据\n",
      "\n",
      "分发，或者更糟糕的是，你可能会\n",
      "\n",
      "参与一些科幻小说之类的数据分发，\n",
      "\n",
      "它会开始喜欢\n",
      "\n",
      "接受一些科幻小说或角色扮演\n",
      "\n",
      "或类似的东西，所以你必须\n",
      "\n",
      "找到合适的智商量 我认为\n",
      "\n",
      "\n",
      "接下来会有一些 U 形曲线，\n",
      "\n",
      "正如我们在尝试解决\n",
      "\n",
      "我们知道自己擅长和\n",
      "\n",
      "不擅长的问题时所看到的那样，并且我们依靠计算工具\n",
      "\n",
      "希望对\n",
      "\n",
      "您的 LLMS 做同样的事情，所以\n",
      "\n",
      "特别是我们可能希望为他们提供\n",
      "\n",
      "计算器、代码解释器等\n",
      "\n",
      "进行搜索的能力，并且有\n",
      "\n",
      "很多技术可以做到这一点，\n",
      "\n",
      "需要再次记住的是，\n",
      "\n",
      "这些 默认情况下，变压器可能不\n",
      "\n",
      "知道他们不知道什么，因此您\n",
      "\n",
      "甚至可能想在提示中告诉变压器\n",
      "\n",
      "您不太擅长\n",
      "\n",
      "心算，每当您需要进行非常\n",
      "\n",
      "大的数字加法乘法或\n",
      "\n",
      "其他任何操作时，请使用这里的计算器\n",
      "\n",
      "你如何使用计算器使用\n",
      "\n",
      "这个标记组合等等所以你必须\n",
      "\n",
      "真正喜欢拼写它\n",
      "\n",
      "因为默认情况下的模型不\n",
      "\n",
      "知道它擅长或不擅长什么，\n",
      "\n",
      "就像你和我一样，你和\n",
      "\n",
      "我可能是\n",
      "\n",
      "下一个 我认为非常\n",
      "\n",
      "有趣的是，我们从一个只进行检索的世界一路\n",
      "\n",
      "\n",
      "摆动到了另一个极端，\n",
      "\n",
      "在那里它的记忆仅存在于镜头中，但\n",
      "\n",
      "实际上在\n",
      "\n",
      "这些检索增强\n",
      "\n",
      "模型之间存在着整个空间，并且这个作品 在实践中非常好，\n",
      "\n",
      "\n",
      "正如我提到的，变压器的上下文窗口\n",
      "\n",
      "是它的工作内存，如果您\n",
      "\n",
      "可以使用与任务相关的任何信息加载工作内存，则\n",
      "\n",
      "\n",
      "模型将工作得非常好，\n",
      "\n",
      "因为它可以立即访问所有\n",
      "\n",
      "内存\n",
      "\n",
      "，所以我 我认为很多人\n",
      "\n",
      "真的对\n",
      "\n",
      "呃基本上检索增强\n",
      "\n",
      "生成感兴趣，在底部我有\n",
      "\n",
      "一个 llama 索引的例子，它是一种\n",
      "\n",
      "连接到许多\n",
      "\n",
      "不同类型数据的数据连接器，你可以呃\n",
      "\n",
      "你可以做的一切 可以索引所有\n",
      "\n",
      "这些数据，并且可以使其可供\n",
      "\n",
      "llms 和新兴配方访问，您可以\n",
      "\n",
      "获取相关文档，将\n",
      "\n",
      "它们分成多个块，然后嵌入所有这些数据\n",
      "\n",
      "，基本上得到了\n",
      "\n",
      "表示数据的嵌入向量，您将\n",
      "\n",
      "其存储在 矢量存储，然后在\n",
      "\n",
      "测试时，您\n",
      "\n",
      "对矢量存储进行某种查询，并获取\n",
      "\n",
      "可能与您的任务相关的块，并将\n",
      "\n",
      "它们填充到提示中，\n",
      "\n",
      "然后生成，因此这\n",
      "\n",
      "在实践中可以很好地工作，所以这 我\n",
      "\n",
      "认为类似于当你和我解决\n",
      "\n",
      "问题时，你可以凭记忆做所有事情，\n",
      "\n",
      "变形金刚有非常大\n",
      "\n",
      "和广泛的记忆，但它也确实\n",
      "\n",
      "有助于参考一些主要\n",
      "\n",
      "文档，所以当你发现\n",
      "\n",
      "自己回到教科书时\n",
      "\n",
      "查找某些内容，或者每当您发现\n",
      "\n",
      "自己要返回\n",
      "\n",
      "库的文档以查找\n",
      "\n",
      "转换变压器时，绝对当我也这样\n",
      "\n",
      "做时，您有一个记忆，您对\n",
      "\n",
      "\n",
      "库的某些文档如何工作有一些记忆，但它要好得多\n",
      "\n",
      "查找它，所以这同样适用于\n",
      "\n",
      "\n",
      "接下来我想简单地谈谈\n",
      "\n",
      "约束提示我也发现这\n",
      "\n",
      "非常有趣\n",
      "\n",
      "呃这基本上是\n",
      "\n",
      "\n",
      "在 llms 的输出中强制使用某个模板的技术\n",
      "\n",
      "所以指导\n",
      "\n",
      "实际上是来自微软的一个例子 呃，\n",
      "\n",
      "这里我们强制\n",
      "\n",
      "llm 的输出将是 Json，这\n",
      "\n",
      "实际上保证了输出将\n",
      "\n",
      "采用这种形式，因为它们进入并\n",
      "\n",
      "扰乱了\n",
      "\n",
      "从 Transformer 出来的所有不同令牌的概率，\n",
      "\n",
      "并且如果 他们夹紧这些\n",
      "\n",
      "标记，然后 Transformer 仅\n",
      "\n",
      "填充此处的空白，然后您\n",
      "\n",
      "可以对\n",
      "\n",
      "可以进入这些空白的内容实施额外的限制，因此这\n",
      "\n",
      "可能非常有用，我认为这种\n",
      "\n",
      "约束采样也\n",
      "\n",
      "非常有趣，\n",
      "\n",
      "我也想要 就\n",
      "\n",
      "微调而言，您可以在\n",
      "\n",
      "快速工程方面取得很大进展，\n",
      "\n",
      "但也可以考虑\n",
      "\n",
      "微调您的模型，现在微调\n",
      "\n",
      "模型意味着您实际上将\n",
      "\n",
      "更改模型的权重 模型\n",
      "\n",
      "\n",
      "在实践中做到这一点变得更加容易，这是因为\n",
      "\n",
      "\n",
      "最近已经开发了许多技术并拥有用于呃的库，\n",
      "\n",
      "例如\n",
      "\n",
      "像 Laura 这样的参数高效微调技术，\n",
      "\n",
      "确保您只 趋势是，\n",
      "\n",
      "您只训练模型的小型 Spar 种类，\n",
      "\n",
      "因此大部分模型都\n",
      "\n",
      "被固定在基础模型上，并且\n",
      "\n",
      "允许更改其中的某些部分，\n",
      "\n",
      "这在经验上仍然工作得很好，\n",
      "\n",
      "并且使得仅进行调整的成本要低得多\n",
      "\n",
      "模型的小部分，\n",
      "\n",
      "呃，这也意味着，\n",
      "\n",
      "因为模型的大部分都是固定的，所以\n",
      "\n",
      "您可以使用非常低的精度推理\n",
      "\n",
      "来计算这些部分，因为它们\n",
      "\n",
      "不会通过梯度下降进行更新\n",
      "\n",
      "，因此这使得一切变得\n",
      "\n",
      "更加高效 此外，正如\n",
      "\n",
      "\n",
      "我提到的，目前我们有许多基于开源高质量的模型，\n",
      "\n",
      "我认为 llama 非常好，\n",
      "\n",
      "尽管它没有商业许可，\n",
      "\n",
      "我相信现在需要\n",
      "\n",
      "记住的一些事情是，\n",
      "\n",
      "基本上微调是一个 涉及更多的\n",
      "\n",
      "技术，它需要更多\n",
      "\n",
      "的技术，我认为\n",
      "\n",
      "要做正确的事情需要技术专业知识，它需要人类数据\n",
      "\n",
      "承包商来处理数据集和/或\n",
      "\n",
      "合成数据管道，这可能\n",
      "\n",
      "非常复杂，这肯定会\n",
      "\n",
      "大大减慢您的迭代周期，\n",
      "\n",
      "我会 说在高水平上 sft 是\n",
      "\n",
      "可以实现的，因为它只是你\n",
      "\n",
      "持续的语言建模任务它\n",
      "\n",
      "相对简单，但 rlhref 我\n",
      "\n",
      "想说这是一个非常多的研究\n",
      "\n",
      "领域，甚至更难\n",
      "\n",
      "开始工作，所以我可能不会\n",
      "\n",
      "建议有人只是尝试 推出\n",
      "\n",
      "自己的 rlh 实现，这些\n",
      "\n",
      "东西非常不稳定，非常\n",
      "\n",
      "难以训练，呃，\n",
      "\n",
      "我认为现在对初学者非常友好，\n",
      "\n",
      "而且它也可能会\n",
      "\n",
      "很快发生变化，\n",
      "\n",
      "所以我认为这些是我的默认\n",
      "\n",
      "建议 现在，\n",
      "\n",
      "我会将您的任务分为两个\n",
      "\n",
      "主要部分，第一部分实现您的最佳\n",
      "\n",
      "性能，第二部分按此顺序优化您的\n",
      "\n",
      "性能第一，\n",
      "\n",
      "\n",
      "目前最好的性能来自 GT4 模型，它是\n",
      "\n",
      "迄今为止最有能力\n",
      "\n",
      "使用提示的 非常详细，他们\n",
      "\n",
      "有很多任务内容，相关的\n",
      "\n",
      "相关信息和说明，就像\n",
      "\n",
      "\n",
      "如果任务承包商无法给\n",
      "\n",
      "您回复电子邮件，您会告诉他们什么一样，但也要\n",
      "\n",
      "记住，任务承包商是人\n",
      "\n",
      "，他们有内在的想法。 独白，\n",
      "\n",
      "他们非常聪明等等，LLM不\n",
      "\n",
      "具备这些品质，所以一定要仔细\n",
      "\n",
      "思考LLM的心理，\n",
      "\n",
      "并提供提示，\n",
      "\n",
      "检索并在这些提示中添加任何相关的上下文\n",
      "\n",
      "和信息，这些提示\n",
      "\n",
      "基本上指的是很多提示\n",
      "\n",
      "工程技术其中一些\n",
      "\n",
      "在上面的幻灯片中突出显示，但这\n",
      "\n",
      "是一个非常大的空间，我\n",
      "\n",
      "只是建议您在网上寻找快速的\n",
      "\n",
      "工程技术，那里\n",
      "\n",
      "有很多内容要涵盖，\n",
      "\n",
      "用几个商店示例进行实验，\n",
      "\n",
      "这指的是您不知道的内容 不仅仅是想\n",
      "\n",
      "告诉你想尽可能地展示，\n",
      "\n",
      "所以给它提供\n",
      "\n",
      "所有的例子，帮助它真正\n",
      "\n",
      "理解你的意思，如果你可以使用\n",
      "\n",
      "工具和插件来卸载\n",
      "\n",
      "对 llms 来说很难的任务，\n",
      "\n",
      "然后考虑不仅仅是 一个\n",
      "\n",
      "提示和答案考虑潜在的\n",
      "\n",
      "链和反射，以及如何将\n",
      "\n",
      "它们粘合在一起以及如何\n",
      "\n",
      "可能制作多个样本等等\n",
      "\n",
      "\n",
      "最后，如果您认为自己已经挤出了\n",
      "\n",
      "提示工程，我认为您\n",
      "\n",
      "应该坚持一段时间看看 在\n",
      "\n",
      "某些方面可能会根据\n",
      "\n",
      "您的应用程序对模型进行微调，但预计这会\n",
      "\n",
      "慢得多并且不断发展，然后\n",
      "\n",
      "\n",
      "这里有一个专家脆弱的研究区，我会说这是 rlhf，如果可以的话，它\n",
      "\n",
      "目前确实比 sft 工作得更好一些\n",
      "\n",
      "让它工作，但\n",
      "\n",
      "我想说的是，这非常复杂，\n",
      "\n",
      "为了优化你的成本，尝试\n",
      "\n",
      "探索较低容量的模型或更短的\n",
      "\n",
      "提示等等，\n",
      "\n",
      "我还想就\n",
      "\n",
      "我认为 llms 目前的用例说几句话\n",
      "\n",
      "非常适合，所以\n",
      "\n",
      "特别要注意的是，\n",
      "\n",
      "今天的 llms 有很多限制，\n",
      "\n",
      "所以我会在\n",
      "\n",
      "所有应用程序模型中牢记这一点，\n",
      "\n",
      "顺便说一句，这可能是一个完整的\n",
      "\n",
      "演讲，所以我没有时间\n",
      "\n",
      "详细介绍模型可能有偏见 他们\n",
      "\n",
      "可能编造幻觉信息\n",
      "\n",
      "他们可能有推理错误 他们可能\n",
      "\n",
      "在整个类别的\n",
      "\n",
      "应用程序中陷入困境 他们有知识限制，\n",
      "\n",
      "因此他们可能不知道上述任何信息\n",
      "\n",
      "说 2021 年 9 月 他们很\n",
      "\n",
      "容易受到大范围的影响\n",
      "\n",
      "有点像\n",
      "\n",
      "Twitter 上每天都会出现的攻击，包括即时注入\n",
      "\n",
      "越狱攻击、数据中毒攻击\n",
      "\n",
      "等，所以我现在的建议\n",
      "\n",
      "是在低风险应用程序中使用 llms，将\n",
      "\n",
      "它们与人工监督结合起来，将\n",
      "\n",
      "它们作为\n",
      "\n",
      "灵感和建议的来源 并认为\n",
      "\n",
      "副驾驶而不是完全\n",
      "\n",
      "自主的代理就像在\n",
      "\n",
      "某个地方执行任务一样，只是\n",
      "\n",
      "\n",
      "现在还不清楚模型是否在那里，\n",
      "\n",
      "所以我想说 gpt4\n",
      "\n",
      "是一个令人惊叹的神器，我非常感谢\n",
      "\n",
      "它的存在 它很漂亮，\n",
      "\n",
      "它拥有跨多个领域的大量知识，\n",
      "\n",
      "可以编写数学代码等等，\n",
      "\n",
      "此外，还有一个蓬勃发展的生态\n",
      "\n",
      "系统，其中包括正在构建的所有其他内容，\n",
      "\n",
      "并将其纳入\n",
      "\n",
      "生态系统中，其中一些\n",
      "\n",
      "东西是这些 我已经讨论过的事情，\n",
      "\n",
      "所有这些功能都触手可及，\n",
      "\n",
      "\n",
      "所以这里是向\n",
      "\n",
      "GPT 询问问题以\n",
      "\n",
      "提示它并获得响应所需的一切代码，在这种\n",
      "\n",
      "情况下我说，呃，你能说一些\n",
      "\n",
      "启发性的话吗？  Microsoft build 2023 的观众，\n",
      "\n",
      "\n",
      "我刚刚将其输入 Python 并\n",
      "\n",
      "逐字 gpt4 说了以下内容，呃，顺便说一句，\n",
      "\n",
      "我不知道\n",
      "\n",
      "他们在主题演讲中使用了这个技巧，所以我\n",
      "\n",
      "认为我很聪明，\n",
      "\n",
      "但它真的很擅长\n",
      "\n",
      "女士们、先生们，入侵者和\n",
      "\n",
      "开拓者，微软构建 2023\n",
      "\n",
      "欢迎来到杰出思想的聚会，\n",
      "\n",
      "你们是\n",
      "\n",
      "未来的建筑师，\n",
      "\n",
      "塑造\n",
      "\n",
      "人类繁荣发展的数字领域的远见者，拥抱\n",
      "\n",
      "技术的无限可能性，让\n",
      "\n",
      "您的想法飞翔 发挥你的\n",
      "\n",
      "想象力，让我们一起为子孙后代创造一个更加\n",
      "\n",
      "互联、非凡、包容的世界，\n",
      "\n",
      "准备好\n",
      "\n",
      "释放你的创造力，画布\n",
      "\n",
      "未知的事物，将梦想变成现实，\n",
      "\n",
      "你的旅程从今天开始\n",
      "\n",
      "好吗？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for reading lines from a file\n",
    "with open('build.txt', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        print(line, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76720dfc-afd8-4ffc-b50d-c7281266ad84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"foreign  [Music]  researcher and founding member of open  AI Andre carpathy  [Applause]  hi everyone  I'm happy to be here to tell you about  the state of gbt and more generally  about the rapidly growing ecosystem of  large language models  so I would like to partition the talk  into two parts  in the first part I would like to tell  you about how we train GPT assistance  and then in the second part we will we  are going to take a look at how we can  use these assistants effectively for  your applications  so first let's take a look at the  emerging recipe for how to train these  assistants and keep in mind that this is  all very new and still rapidly evolving  but so far the recipe looks something  like this  now this is kind of a complicated slide  so I'm going to go through it piece by  piece but roughly speaking we have four  major stages free training supervised  fine tuning reward modeling  reinforcement learning and they follow  each other serially  now in each stage we have a data set  that is that powers that stage we have  an algorithm that for our purposes will  be a objective and a and over for uh for  training the neural network and then we  have a resulting model and then there's  some notes on the bottom so the first  stage we're going to start with is the  pre-training stage now this stage is  kind of special in this diagram and this  diagram is not to scale because this  stage is where all of the computational  work basically happens this is 99 of the  training uh compute time and also flops  and so this is where we are dealing with  internet scale data sets with thousands  of gpus in the store computer and also  months of training potentially the other  three stages are fine-tuning stages that  are much more along the lines of a small  few number of gpus and hours or days  so let's take a look at the pre-training  stage to achieve a base model  first we're going to gather a large  amount of data here's an example of what  we call a data mixture that comes from  this uh paper that was released by meta  where they released this llama based  model now you can see roughly the kinds  of data sets that enter into these  collections so we have common crawl  which is just a web scrape C4 which is  also a common crawl and then some high  quality data sets as well so for example  GitHub Wikipedia books archive stack  exchange and so on these are all mixed  up together and then they are sampled  According to some given proportions and  that forms the training set for the  neural net for the GPT  now before we can actually train on this  data we need to go through one more  pre-processing step and that is  tokenization and this is basically a  translation of the raw text that we  scrape from the internet into sequences  of integers because that's the native  representation over which gpts function  now this is a lossless kind of  translation between pieces of text and  tokens and integers and there are a  number of algorithms for this stage  typically for example you could use  something like byte pair encoding which  iteratively merges little text chunks  and groups them into tokens  and so here I'm showing some example  chunks of these tokens and then this is  the raw integer sequence that will  actually feed into a Transformer  now here I'm showing two uh sort of like  examples for hyper parameters that  govern this stage so gpt4 we did not  release too much information about how  it was trained and so on so I'm using  gpt3's numbers but gpt3 is of course a  little bit old by now about three years  ago but llama is a fairly recent model  from meta so these are roughly the  orders of magnitude that we're dealing  with when we're doing pre-training  the vocabulary size is usually a couple  10 000 tokens the context length is  usually something like two thousand four  thousand or nowadays even 100 000 and  this governs the maximum number of  integers that the GPT will look at when  it's trying to predict the next integer  in a sequence  um you can see that roughly the number  of parameters is say 65 billion for  llama now even though llama has only 65b  parameters compared to gvt3's 175  billion parameters llama is a  significantly more powerful model and  intuitively that's because the model is  trained for significantly longer in this  case 1.4 trillion tokens instead of just  300 billion tokens so you shouldn't  judge the power of a model just by the  number of parameters that it contains  below I'm showing some tables of rough  number of rough hyper parameters that  typically go into specifying the  Transformer neural network so the number  of heads the dimension size number of  layers and so on and on the bottom I'm  showing some training hyper parameters  so for example to train the 65b model  meta used 2000 gpus roughly 21 days  train of training and and roughly  several million dollars and so that's  the rough orders of magnitude that you  should have in mind for the pre-training  stage  now when we're actually pre-chaining  what happens roughly speaking we are  going to take our tokens and we're going  to lay them out into Data batches so we  have these arrays that will feed into  the Transformer and these arrays are B  the batch size and these are all  independent examples stacked up in rows  and B by T T being the maximum context  length so in my picture I only have 10.  the context length so this could be two  thousand four thousand Etc so these are  extremely long rows and what we do is we  take these documents and we pack them  into rows and we delimit them with these  Special end of text tokens uh basically  it's on the Transformer where a new  document begins  and so here I have a few examples of  documents and then I've stretched them  out into uh into this input  now we're going to feed all of these  numbers into Transformer and let me let  me just focus on a single particular  cell but the same thing will happen at  every every cell in this diagram so  let's look at the green cell the green  cell is going to take a look at all of  the tokens before it so all of the  tokens in yellow and we're going to feed  that entire context into the Transformer  neural network  and the Transformer is going to try to  predict the next token in the sequence  in this case in red  now the Transformer I don't have too  much time to unfortunately go into the  full details of this neural network  architecture is just a large blob of  neural net stuff for our purposes and  it's got several 10 billion parameters  typically or something like that and of  course as they tune these parameters  you're getting slightly different uh  predicted distributions for every single  one of these cells  and so for example if our vocabulary  size is 50 257 tokens then we're going  to have that many numbers because we  need to specify a probability  distribution for what comes next so  basically we have a probability for  whatever may follow now in this specific  example for the specific cell 513 will  come next and so we can use this as a  source of supervision to update our  Transformers weights and so we're  applying this basically on every single  cell in parallel and we keep swap  swapping batches and we're trying to get  the Transformer to make the correct  predictions over what token comes next  in a sequence  so let me show you more concretely what  this looks like when you train one of  these models this is actually coming  from a New York Times and they trained a  small GPT on Shakespeare and so here's a  small snippet of Shakespeare and they  trained a GPT on it now in the beginning  at initialization the GPT starts with  completely random weights so you're just  getting completely random outputs as  well  but over time as you train uh the GPT  longer and longer you are getting more  and more coherent and consistent uh sort  of samples from the model and the way  you sample from it of course is you uh  predict what comes next you sample from  that distribution and you keep feeding  that back into the process and you can  basically uh sample large sequences and  so by the end you see that the  Transformer has learned about words and  where to put spaces and where to put  commas and so on and so we're making  more and more consistent predictions  over time  these are the kinds of plots that you're  looking at when you're doing model  pre-training effectively we're looking  at the loss function over time as you  train and low loss means that our  Transformer is predicting the correct uh  is giving a higher probability to the  correct next integer in a sequence  now what are we going to do with this  model once we've trained it after a  month well the first thing that we  noticed we the field is that these  models basically in the process of  language modeling learn very powerful  General representations and it's  possible to very efficiently fine-tune  them for any arbitrary Downstream task  you might be interested in so as an  example if you're interested in  sentiment classification the approach  used to be that you collect a bunch of  positives and negatives and then you  train some kind of an NLP model for for  that but the new approach is ignore  sentiment classification  go off and do large language model  pre-training train the large Transformer  and then you can only you may only have  a few examples and you can very  efficiently fine tune your model for  that task and so this works very well in  practice and the reason for this is that  basically the Transformer is forced to  multitask a huge amount of tasks in the  language modeling task because just just  in terms of predicting the next token  it's forced to understand a lot about  the structure of the of the text and all  the different concepts they're in  so that was GPT one now around the time  of gpt2 people noticed that actually  even better than fine tuning you can  actually prompt these models very  effectively so these are language models  and they want to complete documents so  you can actually trick them into  performing tasks just by arranging these  fake documents so in this example for uh  for example we have some passage and  then we sort of like do QA qaqa this is  called a few shot prompt and then we do  q and then as the Transformer is trying  to complete the document it's actually  answering our question and so this is an  example of prompt engineering a base  model making the belief that it's sort  of imitating a document and getting it  to perform a task  and so this kicked off I think the era  of I would say prompting over  fine-tuning and seeing that this  actually can work extremely well on a  lot of problems even without training  any neural networks fine-tuning or so on  now since then we've seen an entire  evolutionary tree of Base models that  everyone has trained not all of these  models are available for example the  gpt4 base model was never released the  gpt4 model that you might be interacting  with over API is not a base model it's  an assistant model and we're going to  cover how to get those in a bit gpt3  based model is available via the API  under the name DaVinci and gpt2 base  model is available even as weights on  our GitHub repo but currently the best  available base model probably is the  Llama series from meta although it is  not commercially licensed  now one thing to point out is base  models are not assistance they don't  want to answer to they don't want to  make answers to your questions they just  want to complete documents so if you  tell them write a poem about the brand  cheese it will just you know it will  answer questions with more questions  it's just completing what it thinks as a  document  however you can prompt them in a  specific way for base models that that  is more likely to work so as an example  here's a poem about bread and cheese and  in that case it will auto-complete  correctly  you can even trick base models into  being assistance and the way you would  do this is you would create like a  specific few shot prompt that makes it  look like there's some kind of a  document between a human and assistant  and they're exchanging sort of um  information and then at the bottom you  sort of put your query at the end and  the base model will sort of like  condition itself into being like a  helpful assistant and kind of answer but  this is not very reliable and doesn't  work super well in practice although it  can be done  so instead we have a different path to  make actual gbt assistance not just base  model document completers and so that  takes us into supervised fine-tuning so  in the supervised fine tuning stage we  are going to collect small but high  quality data sets and in this case we're  going to ask human contractors to gather  data of the format of the form prompt  and ideal response and we're going to  collect lots of these typically tens of  thousands or something like that  and then we're going to still do  language modeling on this data so  nothing change algorithmically we're  just swapping out a training set so it  used to be internet documents which is a  high quantity local  four basically QA prompt response kind  of data and that is low quantity high  quality  so we still do language modeling and  then after training we get an sfd model  and you can actually deploy these models  and they are actual assistants and they  work to some extent let me show you what  an example demonstration might look like  so here's something that a human  contractor might come up with here's  some random prompt can you write a short  introduction about the relevance of the  term monopsony or something like that  and then the contractor also writes out  an ideal response and when they write  out these responses they are following  extensive labeling documentations and  they're being asked to be helpful  truthful and harmless and those labeling  instructions here you probably can't  read it another can I but they're long  and this is just people following  instructions and trying to complete  these prompts  so that's what the data set looks like  and you can train these models and this  works to some extent  now you can actually continue the  pipeline from here on and go into rlhf  reinforcement learning from Human  feedback that consists of both reward  modeling and reinforcement learning so  let me cover that and then I'll come  back to why you may want to go through  the extra steps and how that compares to  just sft models  so in the reward modeling step what  we're going to do is we're now going to  shift our data collection to be of the  form of comparisons so here's an example  of what our data set will look like  I have the same prompt identical prompt  on the top which is asking the assistant  to write a program or function that  checks if a given string is a palindrome  and then what we do is we take the sft  model which we've already trained and we  create multiple completions so in this  case we have three completions that the  model has created  and then we ask people to rank these  completions so if you stare at this for  a while and by the way these are very  difficult things to do to compare some  of these predictions and this can take  people even hours for a single prompt  completion pairs but let's say we  decided that one of these is much better  than the others and so on so we rank  them  then we can follow that with something  that looks very much kind of like a  binary classification on all the  possible pairs between these completions  so what we do now is we lay out our  prompt in rows and the prompts is  identical across all three rows here so  it's all the same prompt but the  completion of this vary and so the  yellow tokens are coming from the sft  model then what we do is we append  another special reward uh readout token  at the end and we basically only  supervise the Transformer at this single  green token and the Transformer will  predict some reward for how good that  completion is for that prompt  and so basically it makes a guess about  the quality of each completion and then  once it makes a guess for every one of  them we also have the ground truth which  is telling us the ranking of them and so  we can actually enforce that some of  these numbers should be much higher than  others and so on WE formulate this into  a loss function and we train our model  to make reward predictions that are  consistent with the ground truth coming  from the comparisons from all these  contractors so that's how we train our  reward model and that allows us to score  how good a completion is for a prompt  once we have a reward model this is we  can't deploy this because this is not  very useful as an assistant by itself  but it's very useful for the reinforce  reinforcement learning stage that  follows now because we have a reward  model we can score the quality of any  arbitrary completion for any given  prompt so what we do during  reinforcement learning is we basically  get again a large collection of prompts  and now we do reinforcement learning  with respect to the reward model so  here's what that looks like  we we take a single prompt we lay it out  in rows and now we use the sft we use  the basically the model we'd like to  train which is initialized at sft model  to create some completions in yellow  and then we append the reward token  again and we read off the reward  according to the reward model which is  now kept fixed it doesn't change anymore  and now the reward model tells us the  quality of every single completion for  each for these prompts and so what we  can do is we can now just basically  apply the same language modeling loss  function but we're currently training on  the yellow tokens and we are weighing  the language modeling objective by the  rewards indicated by the reward model so  as an example in the first row the  reward model said that this is a fairly  high score in completion and so all the  tokens that we happen to sample on the  first row are going to get reinforced  and they're going to get higher  probabilities for the future  conversely on the second row the reward  model really did not like this  completion negative 1.2 and so therefore  every single token that we sampled in  that second row is going to get a  slightly higher probability for the  future and we do this over and over on  many prompts on many batches and  basically we get a policy which creates  yellow tokens here and it basically all  of them all of the completions here will  score high according to the reward model  that we trained in the previous stage  so that's how we train uh that's what  the rohf pipeline is  now and then at the end you get a model  that you could deploy and so and as an  example chat GPT is an rlhf model but  some other models that you might come  across like for example of the kuna 13B  and so on these are sft models so we  have base models sft models and rlh  models and that's kind of like the state  of things there  now why would you want to do rlhf so one  answer that is kind of not that exciting  is that it just works better so this  comes from the instructor gbt paper  according to these experiments a while  ago now these PPO models are rlhf and we  see that they are basically just  preferred in a lot of comparisons when  we give them to humans so humans just  prefer out basically tokens that come  from our religion models compared to sft  models compared to base model that is  prompted to be an assistant and so it  just works better but you might ask why  why does it work better and I don't  think that there's a single like amazing  answer that the community has really  like agreed on but I will just offer one  uh one reason potentially and it has to  do with the asymmetry between how easy  computationally it is to compare versus  generate so let's take an example of  generating a haiku suppose I ask a model  to write a haiku about paper clips if  you're a contractor trying to give train  data then imagine being a contractor  collecting basically data for the sft  stage how are you supposed to create a  nice haiku for a paperclip you might  just not be very good at that but if I  give you a few examples of haikus you  might be able to appreciate some of  these haikus a lot more than others and  so judging which one of these is good is  much easier task and so basically this  asymmetry makes it so that comparisons  are a better way to potentially leverage  yourself as a human and your judgment to  create a slightly better model  now our religious models are not  strictly an improvement on the base  models in some cases so in particular  we've noticed for example that they lose  some entropy so that means that they  give More PT results they can output  lower variations uh like they can output  samples with lower variation than base  model so base model has lots of entropy  and we'll go with lots of diverse  outputs  so for example one one kind of place  where I still prefer to use a base model  is in the setup where  you basically have N Things and you want  to generate more things like it and so  here is an example that I just cooked up  I want to generate cool Pokemon names I  gave it seven Pokemon names and I asked  the base model to complete the document  and it gave me a lot more Pokemon names  uh these are fictitious I tried to look  them up I don't believe they're actual  Pokemons and uh this is the kind of task  that I think base model would be good at  because it still has lots of entropy it  will give you lots of diverse cool kind  of more things that look like whatever  you give it before  so this is what uh this is number having  all said all that these are kind of like  the assistant models that are probably  available to you at this point uh that  there's a team at Berkeley that ranked a  lot of the available assistant models  and gave them basically ELO ratings So  currently some of the best models of  course are gpt4 by far I would say  followed by Claude gvt 3.5 and then a  number of models some of these might be  available as weights like the kuna koala  Etc and the first three rows here are  all they're all rohf models and all of  the other models to my knowledge are sft  models I believe  okay so that's how we train these models  on the high level now I'm going to  switch gears and let's look at how we  can best apply and the GPT assistant  model to your problems now I would like  to work  in a setting of a concrete example so  let's  let's work with a concrete example here  let's say that you are working on an  article or a blog post and you're going  to write this sentence at the end  California's population is 53 times that  of Alaska so for some reason you want to  compare the populations of these two  states  think about the rich internal monologue  and Tool use and how much work actually  goes computationally in your brain to  generate this one final sentence so  here's maybe what that could look like  in your brain okay for this next step  let me blog uh for my blog let me  compare these two populations  okay first I'm going to obviously need  to get both of these populations now I  know that I probably don't know these  populations off the top of my head so  I'm kind of like aware of what I know or  don't know of my self-knowledge right  so I go I do some tool use and I go to  Wikipedia and I look up California's  population and Alaska's population  now I know that I should divide the two  but again I know that dividing three 9.2  by 0.74 is very unlikely to succeed  that's not the kind of thing that I can  do in my head and so therefore I'm gonna  rely on the calculator so I'm going to  use a calculator punch it in and see  that the output is roughly 53.  and then maybe I do some reflection and  Sanity checks in my brain so does 53  make sense well that's quite quite a  large fraction but then California is  the most popular state so maybe that  looks okay  so then I have all the information I  might need and now I get to the sort of  creative portion of writing so I might  start to write something like California  has 53 x times greater and then I think  to myself that's actually like really  awkward phrasing so let me actually  delete that and let me try again and so  as I'm writing I have this separate  process almost inspecting what I'm  writing and judging whether it looks  good or not  and then maybe I delete and maybe I  reframe it and then maybe I'm happy with  what comes out  so basically long story short a ton  happens under the hood in terms of your  internal monologue when you create  sentences like this but what does a  sentence like this look like when we are  training a GPT on it  From gpt's perspective this is just a  sequence of tokens  so GPT when it's reading or generating  these tokens it just goes  and each chunk is roughly the same  amount of computational work for each  token and these Transformers are not  very shallow networks they have about 80  layers of reasoning but 80 is still not  like too much and so this Transformer is  going to do its best to imitate but of  course the process here looks very very  different from the process that you took  so in particular in our final artifacts  in the data sets that we create and then  eventually feed to llms all of that  internal dialogue is completely stripped  and uh and unlike you the GPT will look  at every single token and spend the same  amount of compute on every one of them  and so you can't expect it to actually  like well you can't expect it to do to  uh sort of do too much work per token  so and also in particular basically  these Transformers are just like token  simulators so they don't know what they  don't know like they just imitate the  next token they don't know what they're  good at or not good at they just tried  their best imitate the next token they  don't reflect in the loop they don't  sanity check anything they don't correct  their mistakes along the way by default  they just uh sample token sequences  they don't have separate inner monologue  streams in their head right that are  evaluating what's happening now they do  have some sort of cognitive advantages I  would say and that is that they do  actually have a very large fact-based  knowledge across a vast number of areas  because they have say several 10 billion  parameters so it's a lot of storage for  a lot of facts  but and they also I think have a  relatively large and perfect working  memory so whatever fixed into the  whatever fits into the context window is  immediately available to the Transformer  through its internal self-attention  mechanism and so it's kind of like um  Perfect Memory but it's got that finite  size but the Transformer has a very  direct access to it and so it can like  losslessly remember anything that is  inside its context window  so that's kind of how I would compare  those two and the reason I bring all of  this up is because I think to a large  extent prompting is just making up for  this sort of cognitive difference  between these two kind of architectures  um like our brains here and llm brains  you can look at it that way almost  so here's one thing that people found  for example works pretty well in  practice especially if your tasks  require reasoning you can't expect the  Transformer to make to to do too much  reasoning per token and so you have to  really spread out the reasoning across  more and more tokens so for example you  can't give a Transformer a very  complicated question and expect it to  get the answer in a single token there's  just not enough time for it these  Transformers need tokens to think quote  unquote I like to say sometimes and so  this is some of the things that work  well you may for example have a few shot  prompt that shows the Transformer that  it should like show its work when it's  answering a question when it's answering  the question and if you give a few  examples the Transformer will imitate  that template and it will just end up  working out better in terms of its  evaluation  Additionally you can elicit this kind of  behavior from the Transformer by saying  let's think step by step because this  condition is the transformer into sort  of like showing its work and because it  kind of snaps into a mode of showing its  work it's going to do less computational  work per token and so it's more likely  to succeed as a result because it's  making  um slower reasoning over time  here's another example this one is  called self-consistency we saw that we  had the ability to start writing and  then if it didn't work out I can try  again and I can try multiple times and  uh and maybe  um select the one that worked best so in  these kinds of approaches you may sample  not just once but you may sample  multiple times and then have some  process for finding the ones that aren't  good and then keeping just those samples  or doing a majority vote or something  like that so basically these  Transformers in the process as they  predict the next token just like you  they can get unlucky and they can they  could sample and not a very good token  and they can go down sort of like a  blind alley in terms of reasoning and so  unlike you they cannot recover from that  they are stuck with every single token  they sample and so they will continue  the sequence even if they even know that  this sequence is not going to work out  so give them the ability to look back  inspect or try to find uh try to  basically sample around it  here's one technique also you could it  turns out that actually llms like they  know when they've screwed up so as an  example  say you asked the model to generate a  poem that does not rhyme and it might  give you a poem but it actually Rhymes  but it turns out that especially for the  bigger models like gpt4 you can just ask  it did you meet the assignment and  actually gpt4 knows very well that it  did not meet the assignment it just kind  of got unlucky in its sampling and so it  will tell you no I didn't actually meet  the assignment here's let me try again  but without you prompting it it doesn't  even like it doesn't know it doesn't  know to revisit and uh and so on so you  have to make up for that in your prompt  you have to get it to check if you don't  ask it to check it's not going to check  by itself it's just a token simulator  I think more generally a lot of these  techniques fall into the bucket of what  I would say recreating our system too so  you might be familiar with the system  one system to thinking for humans system  one is a fast automatic process and I  think kind of corresponds to like an llm  just sampling tokens  and system two is the slower deliberate  planning sort of part of your brain  and so this is a paper actually from  just last week because this space is  pretty quickly evolving it's called tree  of thought and in tree of thought uh the  authors of this paper propose  maintaining multiple completions for any  given prompt and then they are also  scoring them along the way and keeping  the ones that are going well if that  makes sense and so a lot of people are  like really playing around with kind of  um prompt engineering to um to basically  bring back some of these abilities that  we sort of have in our brain for llms  now one thing I would like to note here  is that this is not just a prompt this  is actually prompts that are together  used with some python glue code because  you don't you actually have to maintain  multiple prompts and you also have to do  some tree search algorithm here um to  like figure out which prompt to expand  Etc so it's a symbiosis of python glue  code and individual prompts that are  called in a while loop or in a bigger  algorithm  I also think there's a really cool  parallel here to alphago alphago has a  policy for placing the next Stone when  it plays go and its policy was trained  originally by imitating humans but in  addition to this policy it also does  Monte Carlo tree search and basically it  will play out a number of possibilities  in its head and evaluate all of them and  only keep the ones that work well and so  I think this is kind of an equivalent of  alphago but for text  if that makes sense  so just like tree of thought I think  more generally people are starting to  like really Explore More General  techniques of not just the simple  question answer prompt but something  that looks a lot more like python blue  code stringing together many problems so  on the right I have an example from this  paper called react where they structure  the answer to a prompt as a sequence of  thought action observation thought  action observation and it's a full  rollout a kind of a thinking process to  answer the query and in these actions  the model is also allowed to to lose  on the left I have an example of rogpt  and now rgbt by the way became as a  project that I think got a lot of hype  recently and I think uh but I think I  still find it kind of inspirationally  interesting it's um it's a project that  allows an llm to sort of keep task list  and continue to recursively break down  tasks and I don't think this currently  works very well and I would not advise  people to use it in Practical  applications I just think it's something  to generally take inspiration from in  terms of where this is going I think  over time  so that's kind of like giving our model  system to thinking the next thing that I  find kind of interesting is  this following survey would say almost  psychological Quirk of llms is that llms  don't want to succeed they want to  imitate  you want to succeed and you should ask  for it  so what I mean by that is  when Transformers are trained they have  training sets and there can be an entire  spectrum of performance qualities in  their training data so for example there  could be some kind of a prompt for some  Physics question or something like that  and there could be a student solution  that is completely wrong but there can  also be an expert answer that is  extremely right and Transformers can't  tell the difference between like look I  mean they know they know about low  quality Solutions and high quality  Solutions but by default they want to  imitate all of it because they're just  trained on language modeling and so at  test time you actually have to ask for a  good performance so in this example in  this paper it's um they tried various  prompts and let's think step by step was  very powerful because it's sort of like  spread out the reasoning of remaining  tokens but what worked even better is  let's work this out in a step-by-step  way to be sure we have the right answer  and so it's kind of like a conditioning  on getting a right answer and this  actually makes the Transformer work  better because the Transformer doesn't  have to now hedge its probability Mass  on low quality Solutions as ridiculous  as that sounds and so basically don't be  feel free to ask for a strong solution  say something like you are a leading  expert on this topic pretend you have  iq120 Etc but don't try to ask for too  much IQ because if you ask for IQ of  like 400 you might be out of data  distribution or even worse you could be  in data distribution for some like  sci-fi stuff and it will start to like  take on some sci-fi or like role-playing  or something like that so you have to  find like the right amount of IQ I think  it's got some U-shaped curve there  next up  as we saw when we are trying to solve  problems we know we are good at and what  we're not good at and we Lean On Tools  computationally you want to do the same  potentially with your uh llms so in  particular we may want to give them  calculators code interpreters and so on  the ability to do search and uh there's  a lot of techniques for doing that one  thing to keep in mind again is that  these Transformers by default may not  know what they not don't know so you may  even want to tell the transformer in the  prompt you are not very good at mental  arithmetic whenever you need to do very  large number addition multiplication or  whatever instead use this calculator  here's how you use the calculator use  this token combination etc etc so you  have to actually like spell it out  because the model by default doesn't  know what it's good at or not good at  necessarily just like you and I you and  I might be  next up I think something that is very  interesting is we went from a world that  was retrieval only all the way the  pendulum has swung to The Other Extreme  where it's memory only in lens but  actually there's this entire space in  between of these retrieval augmented  models and this works extremely well in  practice  as I mentioned the context window of a  transformer is its working memory if you  can load the working memory with any  information that is relevant to the task  the model will work extremely well  because it can immediately access all  that memory  and so I think a lot of people are  really interested in  uh basically retrieval augmented  generation and on the bottom I have like  an example of llama index which is one  sort of data connector to lots of  different types of data and you can uh  all you can make it you can index all of  that data and you can make it accessible  to llms and the emerging recipe there is  you take relevant documents you split  them up into chunks you embed all of  them and you basically get embedding  vectors that represent that data you  store that in a vector store and then at  test time you make some kind of a query  to your vector store and you fetch  chunks that might be relevant to your  task and you stuff them into the prompt  and then you generate so this can work  quite well in practice so this is I  think similar to when you're and I solve  problems you can do everything from your  memory and Transformers have very large  and extensive memory but also it really  helps to reference some primary  documents so when you whenever you find  yourself going back to a textbook to  find something or whenever you find  yourself going back to the documentation  of a library to look something up the  transform transformers definitely when I  do that too you you have a mem you have  some memory over how some documentation  of a library works but it's much better  to look it up so the same same applies  here  next I wanted to briefly talk about  constraint prompting I also find this  very interesting  uh this is basically techniques for  um  forcing a certain template in the  outputs of llms so guidance is one  example from Microsoft actually and uh  here we are enforcing that the output  from the llm will be Json and this will  actually guarantee that the output will  take on this form because they go in and  they mess with the probabilities of all  the different tokens that come out of  the Transformer and if they clamp those  tokens and then the Transformer is only  filling in the blanks here and then you  can enforce additional restrictions on  what could go into those blanks so this  might be really helpful and I think this  kind of constraint sampling is also  extremely interesting  I also wanted to say a few words about  fine tuning it is the case that you can  get really far with prompt engineering  but it's also possible to think about  fine-tuning your models now fine-tuning  models means that you are actually going  to change the weights of the model  it is becoming a lot more accessible to  do this in practice and that's because  of a number of techniques that have been  developed and have libraries for uh very  recently so for example parameter  efficient fine-tuning techniques like  Laura make sure that you're only Trend  you're only training small Spar species  of your model so most of the model is  kept clamped at the base model and some  pieces of it are allowed to change and  this still works pretty well empirically  and makes it much cheaper to sort of  tune only small pieces of your model  uh there's also it also means that  because most of your model is clamped  you can use very low Precision inference  for computing those parts because they  are not going to be updated by gradient  descent and so that makes everything a  lot more efficient as well and in  addition we have a number of Open Source  high quality based models currently as I  mentioned I think llama is quite nice  although it is not commercially licensed  I believe right now  some things to keep in mind is that  basically fine tuning is is a lot more  technically involved it requires a lot  more I think technical expertise to do  to do right it requires human data  contractors for data sets and or  synthetic data pipelines that can be  pretty complicated this will definitely  slow down your iteration cycle by a lot  and I would say on a high level sft is  achievable because it is just your  continuing language modeling task It's  relatively straightforward but rlhref I  would say is very much research  territory and is even much harder to get  to work and so I would probably not  advise that someone just tries to roll  their own rlh of implementation these  things are pretty unstable very  difficult to train uh not something that  is I think very beginner friendly right  now and it's also potentially likely  also to to change pretty rapidly still  so I think these are my sort of default  recommendations right now  I would break up your task into two  major parts number one achieve your top  performance and number two optimize your  performance in that order  number one the best performance will  currently come from GT4 model it is the  most capable of by far  use prompts that are very detailed they  have lots of task contents relative  relevant information and instructions  think along the lines of what would you  tell a task contractor if they can't  email you back but then also keep in  mind that a task contractor is a human  and they have inner monologue and  they're very clever Etc llms do not  possess those qualities so make sure to  think through the psychology of the llm  almost and cater prompts to that  retrieve and add any relevant context  and information to these prompts  basically refer to a lot of the prompt  engineering techniques some of them are  highlighted in the slides above but also  this is a very large space and I would  just advise you to look for prompt  engineering techniques online there's a  lot to cover there  experiment with few shop examples what  this refers to is you don't just want to  tell you want to show whenever it's  possible so give it examples of  everything that helps it really  understand what you mean if you can  tools and plugins to offload a tasks  that are difficult for llms natively uh  and then think about not just a single  prompt and answer think about potential  chains and reflection and how you glue  them together and how you could  potentially make multiple samples and so  on  finally if you think you've squeezed out  prompt engineering which I think you  should stick with for a while look at  some potentially fine-tuning a model to  your application but expect this to be a  lot more slower and evolved and then  there's an expert fragile research Zone  here and I would say that is rlhf which  currently does work a bit better than  sft if you can get it to work but again  this is uh pretty involved I would say  and to optimize your costs try to  explore lower capacity models or shorter  prompts and so on  I also wanted to say a few words about  the use cases in which I think llms are  currently well suited for so in  particular note that there's a large  number of limitations to llms today and  uh so I would keep that definitely in  mind for all your applications models  and this by the way could be an entire  talk so I don't have time to cover it in  full detail models may be biased they  may fabricate hallucinate information  they may have reasoning errors they may  struggle in entire classes of  applications they have knowledge cutoffs  so they might not know any information  above say September 2021 they are  susceptible to a large range of attacks  which are sort of like coming out on  Twitter daily including prompt injection  jailbreak attacks data poisoning attacks  and so on so my recommendation right now  is use llms in low stakes applications  combine them with always with human  oversight use them as a source of  inspiration and suggestions and think  co-pilots instead of completely  autonomous agents that are just like  performing a task somewhere it's just  not clear that the models are there  right now  so I wanted to close by saying that gpt4  is an amazing artifact I'm very thankful  that it exists and it's it's beautiful  it has a ton of knowledge across so many  areas it can do math code and so on and  in addition there's this thriving  ecosystem of everything else that is  being built and incorporated into into  the ecosystem uh some of some of the  things these things I've talked about  and all of this power is accessible at  your fingertips  so here's everything that's needed in  terms of code to ask GPT for a question  to prompt it and get a response in this  case I said uh can you say something to  inspire the audience of Microsoft build  2023  and I just punched this into Python and  verbatim gpt4 said the following  uh and by the way I did not know that  they used this trick in the keynote so I  thought I was being clever but  but it is really good at this it says  ladies and gentlemen Invaders and  Trailblazers Microsoft build 2023  welcome to the Gathering of Brilliant  Minds like no other you are The  Architects of the future The Visionaries  molding the digital realm in which  Humanity thrives embrace the Limitless  possibilities of Technologies and let  your ideas soar as high as your  imagination together let's create a more  connected remarkable and inclusive world  for generations to come get ready to  unleash your creativity canvas the  unknown and turn dreams into reality  your Journey Begins today  okay\\n\",\n",
       " '外国\\n',\n",
       " '\\n',\n",
       " '[音乐]\\n',\n",
       " '\\n',\n",
       " '研究员和开放人工智能创始成员\\n',\n",
       " '\\n',\n",
       " '安德烈·卡帕西\\n',\n",
       " '\\n',\n",
       " '[鼓掌]\\n',\n",
       " '\\n',\n",
       " '大家好，\\n',\n",
       " '\\n',\n",
       " '我很高兴能在这里向大家介绍\\n',\n",
       " '\\n',\n",
       " 'GBT 的现状，以及更广泛的\\n',\n",
       " '\\n',\n",
       " '关于快速发展的大型语言模型生态系统的信息，\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '所以我想 将演讲\\n',\n",
       " '\\n',\n",
       " '分为两个部分，\\n',\n",
       " '\\n',\n",
       " '在第一部分中，我想告诉\\n',\n",
       " '\\n',\n",
       " '您我们如何训练 GPT 辅助，\\n',\n",
       " '\\n',\n",
       " '然后在第二部分中，我们将\\n',\n",
       " '\\n',\n",
       " '看看如何在\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '您的应用程序中有效地使用这些助手，\\n',\n",
       " '\\n',\n",
       " '所以首先 让我们看一下\\n',\n",
       " '\\n',\n",
       " '如何训练这些助手的新兴配方，\\n',\n",
       " '\\n',\n",
       " '请记住，这都是\\n',\n",
       " '\\n',\n",
       " '非常新的，并且仍在快速发展，\\n',\n",
       " '\\n',\n",
       " '但到目前为止，配方看起来\\n',\n",
       " '\\n',\n",
       " '像这样，\\n',\n",
       " '\\n',\n",
       " '现在这是一张复杂的幻灯片，\\n',\n",
       " '\\n',\n",
       " '所以我要 一点一点地进行，\\n',\n",
       " '\\n',\n",
       " '但粗略地说，我们有四个\\n',\n",
       " '\\n',\n",
       " '主要阶段：免费训练、监督微调、奖励\\n',\n",
       " '\\n',\n",
       " '建模、\\n',\n",
       " '\\n',\n",
       " '强化学习，它们按顺序相互跟随，\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '现在在每个阶段，我们都有一个数据集，\\n',\n",
       " '\\n',\n",
       " '为该阶段提供动力，我们有\\n',\n",
       " '\\n',\n",
       " '一个算法， 就我们的目的而言，这将\\n',\n",
       " '\\n',\n",
       " '是\\n',\n",
       " '\\n',\n",
       " '训练神经网络的目标，然后我们将\\n',\n",
       " '\\n',\n",
       " '得到一个结果模型，然后\\n',\n",
       " '\\n',\n",
       " '在底部有一些注释，因此\\n',\n",
       " '\\n',\n",
       " '我们要开始的第一个阶段是\\n',\n",
       " '\\n',\n",
       " '预训练阶段 现在这个阶段\\n',\n",
       " '\\n',\n",
       " '在这个图中有点特殊，这个\\n',\n",
       " '\\n',\n",
       " '图不是按比例的，因为这个\\n',\n",
       " '\\n',\n",
       " '阶段是所有计算\\n',\n",
       " '\\n',\n",
       " '工作基本上发生的地方，这是\\n',\n",
       " '\\n',\n",
       " '训练呃计算时间的 99，还有失败\\n',\n",
       " '\\n',\n",
       " '，所以这就是我们正在处理的地方\\n',\n",
       " '\\n',\n",
       " '互联网规模的数据集，\\n',\n",
       " '\\n',\n",
       " '商店计算机中包含数千个 GPU，并且\\n',\n",
       " '\\n',\n",
       " '可能需要数月的训练，其他\\n',\n",
       " '\\n',\n",
       " '三个阶段是微调阶段，\\n',\n",
       " '\\n',\n",
       " '更多的是沿着\\n',\n",
       " '\\n',\n",
       " '少量 GPU 和数小时或数天的路线，\\n',\n",
       " '\\n',\n",
       " '所以让我们看一下 在预训练\\n',\n",
       " '\\n',\n",
       " '阶段，为了首先实现基本模型，\\n',\n",
       " '\\n',\n",
       " '我们将收集\\n',\n",
       " '\\n',\n",
       " '大量数据，这是\\n',\n",
       " '\\n',\n",
       " '我们所谓的数据混合的示例，该示例来自\\n',\n",
       " '\\n',\n",
       " 'Meta 发布的这篇呃论文，\\n',\n",
       " '\\n',\n",
       " '他们在该论文中发布了基于 llama 的模型\\n',\n",
       " '\\n',\n",
       " '现在，您可以粗略地看到\\n',\n",
       " '\\n',\n",
       " '进入这些集合的数据集类型，\\n',\n",
       " '\\n',\n",
       " '因此我们有常见的爬行，\\n',\n",
       " '\\n',\n",
       " '这只是一个网络抓取 C4，这也是\\n',\n",
       " '\\n',\n",
       " '一个常见的爬行，然后还有一些高质量\\n',\n",
       " '\\n',\n",
       " '的数据集，例如\\n',\n",
       " '\\n',\n",
       " 'GitHub 维基百科图书档案 堆栈\\n',\n",
       " '\\n',\n",
       " '交换等等这些都\\n',\n",
       " '\\n',\n",
       " '混合在一起，然后\\n',\n",
       " '\\n',\n",
       " '根据给定的比例对它们进行采样，\\n',\n",
       " '\\n',\n",
       " '形成\\n',\n",
       " '\\n',\n",
       " 'GPT 神经网络的训练集，然后\\n',\n",
       " '\\n',\n",
       " '我们才能真正训练这些\\n',\n",
       " '\\n',\n",
       " '数据，我们需要再经历一次\\n',\n",
       " '\\n',\n",
       " '预处理步骤，即\\n',\n",
       " '\\n',\n",
       " '标记化，这基本上是\\n',\n",
       " '\\n',\n",
       " '我们\\n',\n",
       " '\\n',\n",
       " '从互联网上抓取的\\n',\n",
       " '\\n',\n",
       " '原始文本到整数序列的翻译，因为这是\\n',\n",
       " '\\n',\n",
       " 'gpts 现在运行的本机表示，\\n',\n",
       " '\\n',\n",
       " '这是文本片段\\n',\n",
       " '\\n',\n",
       " '和文本片段之间的无损翻译。\\n',\n",
       " '\\n',\n",
       " '令牌和整数，\\n',\n",
       " '\\n',\n",
       " '此阶段有许多算法，\\n',\n",
       " '\\n',\n",
       " '例如，您通常可以使用\\n',\n",
       " '\\n',\n",
       " '字节对编码之类的算法，它\\n',\n",
       " '\\n',\n",
       " '迭代地合并小文本块\\n',\n",
       " '\\n',\n",
       " '并将它们分组为令牌，\\n',\n",
       " '\\n',\n",
       " '因此在这里我展示了\\n',\n",
       " '\\n',\n",
       " '这些令牌的一些示例块，然后 这是\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '实际输入到 Transformer 的原始整数序列，\\n',\n",
       " '\\n',\n",
       " '现在在这里我展示了两个类似的\\n',\n",
       " '\\n',\n",
       " '示例，用于\\n',\n",
       " '\\n',\n",
       " '控制这个阶段的超参数，所以 gpt4 我们没有\\n',\n",
       " '\\n',\n",
       " '发布太多关于\\n',\n",
       " '\\n',\n",
       " '它是如何训练的信息等等 我使用的是\\n',\n",
       " '\\n',\n",
       " 'gpt3 的数字，但 gpt3 当然\\n',\n",
       " '\\n',\n",
       " '在大约三年前有点旧了，\\n',\n",
       " '\\n',\n",
       " '但 llama 是来自 meta 的一个相当新的模型，\\n',\n",
       " '\\n',\n",
       " '所以这些大致是\\n',\n",
       " '\\n',\n",
       " '我们在\\n',\n",
       " '\\n',\n",
       " '做 pre 时处理的数量级。  - 训练\\n',\n",
       " '\\n',\n",
       " '词汇量通常是几个\\n',\n",
       " '\\n',\n",
       " '10 000 个标记，上下文长度通常是\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '204000 个，现在甚至是 100000 个，\\n',\n",
       " '\\n',\n",
       " '这决定了\\n',\n",
       " '\\n',\n",
       " 'GPT 在\\n',\n",
       " '\\n',\n",
       " '尝试预测下一个整数时将查看的最大整数数量\\n',\n",
       " '\\n',\n",
       " '在序列中，\\n',\n",
       " '\\n',\n",
       " '您可以看到 llama 现在的参数数量大致为\\n',\n",
       " '\\n',\n",
       " '650 亿个，\\n',\n",
       " '\\n',\n",
       " '尽管 llama\\n',\n",
       " '\\n',\n",
       " '与 gvt3 的 1750\\n',\n",
       " '\\n',\n",
       " '亿个参数相比只有 65b 个参数，但 llama 是一个\\n',\n",
       " '\\n',\n",
       " '更强大的模型，\\n',\n",
       " '\\n',\n",
       " '直观上这是因为该模型\\n',\n",
       " '\\n',\n",
       " '经过了显着的训练 在这种\\n',\n",
       " '\\n',\n",
       " '情况下，更长的时间是 1.4 万亿个令牌，而不是\\n',\n",
       " '\\n',\n",
       " '3000 亿个令牌，因此您不应该\\n',\n",
       " '\\n',\n",
       " '仅通过模型包含的参数数量来判断模型的能力，\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '下面我将显示一些通常会出现的\\n',\n",
       " '\\n',\n",
       " '粗略超参数的粗略数量表\\n',\n",
       " '\\n',\n",
       " '指定\\n',\n",
       " '\\n',\n",
       " 'Transformer 神经网络，\\n',\n",
       " '\\n',\n",
       " '头数、维度大小、\\n',\n",
       " '\\n',\n",
       " '层数等，在底部，我\\n',\n",
       " '\\n',\n",
       " '显示了一些训练超参数，\\n',\n",
       " '\\n',\n",
       " '例如训练 65b 模型元，\\n',\n",
       " '\\n',\n",
       " '使用 2000 GPU 大约 21 天\\n',\n",
       " '\\n',\n",
       " '的训练 大约\\n',\n",
       " '\\n',\n",
       " '几百万美元，所以这是\\n',\n",
       " '\\n',\n",
       " '你现在在\\n',\n",
       " '\\n',\n",
       " '预训练阶段应该记住的粗略数量级，\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '当我们实际上预链接发生的\\n',\n",
       " '\\n',\n",
       " '事情时，粗略地说，我们\\n',\n",
       " '\\n',\n",
       " '将拿走我们的代币，我们\\n',\n",
       " '\\n',\n",
       " '将它们放入数据批次中，这样我们\\n',\n",
       " '\\n',\n",
       " '就有这些数组将输入到\\n',\n",
       " '\\n',\n",
       " 'Transformer 中，这些数组是 B\\n',\n",
       " '\\n',\n",
       " '批次大小，这些都是按\\n',\n",
       " '\\n',\n",
       " '行堆叠的独立示例，\\n',\n",
       " '\\n',\n",
       " 'B 乘 T T 是最大上下文\\n',\n",
       " '\\n',\n",
       " '长度，所以在我的图片中 我只有 10.\\n',\n",
       " '\\n',\n",
       " '上下文长度，所以这可能是\\n',\n",
       " '\\n',\n",
       " '两千四千等等，所以这些是\\n',\n",
       " '\\n',\n",
       " '非常长的行，我们所做的就是\\n',\n",
       " '\\n',\n",
       " '获取这些文档，将它们打包\\n',\n",
       " '\\n',\n",
       " '成行，然后用这些\\n',\n",
       " '\\n',\n",
       " '特殊的文本结尾标记来分隔它们 基本上\\n',\n",
       " '\\n',\n",
       " '它是在 Transformer 上\\n',\n",
       " '\\n',\n",
       " '开始新文档的，\\n',\n",
       " '\\n',\n",
       " '所以这里我有一些\\n',\n",
       " '\\n',\n",
       " '文档示例，然后我将它们\\n',\n",
       " '\\n',\n",
       " '延伸到这个输入中，\\n',\n",
       " '\\n',\n",
       " '现在我们将把所有这些\\n',\n",
       " '\\n',\n",
       " '数字输入 Transformer，让我让\\n',\n",
       " '\\n',\n",
       " '我只关注一个特定的\\n',\n",
       " '\\n',\n",
       " '单元格，但\\n',\n",
       " '\\n',\n",
       " '该图中的每个单元格都会发生同样的事情，所以\\n',\n",
       " '\\n',\n",
       " '让我们看一下绿色单元格，绿色\\n',\n",
       " '\\n',\n",
       " '单元格将查看\\n',\n",
       " '\\n',\n",
       " '它之前的所有标记，因此其中的所有\\n',\n",
       " '\\n',\n",
       " '标记 黄色，我们将把\\n',\n",
       " '\\n',\n",
       " '整个上下文输入 Transformer\\n',\n",
       " '\\n',\n",
       " '神经网络，\\n',\n",
       " '\\n',\n",
       " 'Transformer 将尝试\\n',\n",
       " '\\n',\n",
       " '预测序列中的下一个标记，\\n',\n",
       " '\\n',\n",
       " '在这种情况下为红色，\\n',\n",
       " '\\n',\n",
       " '现在 Transformer 不幸的是我没有\\n',\n",
       " '\\n',\n",
       " '太多时间去 深入了解\\n',\n",
       " '\\n',\n",
       " '这个神经网络架构的全部细节\\n',\n",
       " '\\n',\n",
       " '只是\\n',\n",
       " '\\n',\n",
       " '为了我们的目的而设计的一大块神经网络东西，\\n',\n",
       " '\\n',\n",
       " '它通常有几百亿个参数\\n',\n",
       " '\\n',\n",
       " '或类似的东西，\\n',\n",
       " '\\n',\n",
       " '当然，当他们调整这些参数时，\\n',\n",
       " '\\n',\n",
       " '你会得到稍微不同的呃\\n',\n",
       " '\\n',\n",
       " '预测分布 对于\\n',\n",
       " '\\n',\n",
       " '这些单元中的每一个，\\n',\n",
       " '\\n',\n",
       " '例如，如果我们的词汇量\\n',\n",
       " '\\n',\n",
       " '大小是 50 257 个标记，那么我们将\\n',\n",
       " '\\n',\n",
       " '拥有那么多数字，因为我们\\n',\n",
       " '\\n',\n",
       " '需要指定\\n',\n",
       " '\\n',\n",
       " '接下来发生的情况的概率分布，所以\\n',\n",
       " '\\n',\n",
       " '基本上我们有一个概率\\n',\n",
       " '\\n',\n",
       " '现在在这个具体\\n',\n",
       " '\\n',\n",
       " '示例中，接下来将出现特定单元格 513，\\n',\n",
       " '\\n',\n",
       " '因此我们可以使用它作为\\n',\n",
       " '\\n',\n",
       " '监督源来更新我们的\\n',\n",
       " '\\n',\n",
       " 'Transformers 权重，因此我们\\n',\n",
       " '\\n',\n",
       " '基本上并行地将其应用于每个\\n',\n",
       " '\\n',\n",
       " '单元格，并且我们不断交换\\n',\n",
       " '\\n',\n",
       " '交换批次 我们正在尝试让\\n',\n",
       " '\\n',\n",
       " 'Transformer 对序列中\\n',\n",
       " '\\n',\n",
       " '接下来出现的标记做出正确的预测，\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '所以让我更具体地向您展示\\n',\n",
       " '\\n',\n",
       " '当您训练其中一个\\n',\n",
       " '\\n',\n",
       " '模型时这实际上是\\n',\n",
       " '\\n',\n",
       " '来自《纽约时报》的模型，并且 他们\\n',\n",
       " '\\n',\n",
       " '在莎士比亚上训练了一个小 GPT，所以这是\\n',\n",
       " '\\n',\n",
       " '莎士比亚的一小段，他们\\n',\n",
       " '\\n',\n",
       " '现在在初始化开始时训练了一个 GPT，\\n',\n",
       " '\\n',\n",
       " 'GPT 以\\n',\n",
       " '\\n',\n",
       " '完全随机的权重开始，所以你也\\n',\n",
       " '\\n',\n",
       " '得到完全随机的输出，\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '但随着时间的推移 你训练呃 GPT 的时间\\n',\n",
       " '\\n',\n",
       " '越来越长，你会从模型中得到\\n',\n",
       " '\\n',\n",
       " '越来越连贯和一致的\\n',\n",
       " '\\n',\n",
       " '样本，\\n',\n",
       " '\\n',\n",
       " '你从中采样的方式当然是你呃\\n',\n",
       " '\\n',\n",
       " '预测接下来会发生什么，你从\\n',\n",
       " '\\n',\n",
       " '这个分布中采样，然后你继续提供\\n',\n",
       " '\\n',\n",
       " '它 回到这个过程，你\\n',\n",
       " '\\n',\n",
       " '基本上可以对大序列进行采样，\\n',\n",
       " '\\n',\n",
       " '所以到最后你会看到\\n',\n",
       " '\\n',\n",
       " 'Transformer 已经学会了单词、在\\n',\n",
       " '\\n',\n",
       " '哪里放置空格、在哪里放置\\n',\n",
       " '\\n',\n",
       " '逗号等等，所以我们正在做出\\n',\n",
       " '\\n',\n",
       " '越来越一致的预测\\n',\n",
       " '\\n',\n",
       " '随着时间的推移，\\n',\n",
       " '\\n',\n",
       " '这些是您在\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '有效地进行模型预训练时所看到的图表类型，我们在训练\\n',\n",
       " '\\n',\n",
       " '时会随着时间的推移查看损失函数，\\n',\n",
       " '\\n',\n",
       " '低损失意味着我们的\\n',\n",
       " '\\n',\n",
       " 'Transformer 正在预测正确的\\n',\n",
       " '\\n',\n",
       " '呃 为序列中正确的下一个整数提供更高的概率\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '现在，\\n',\n",
       " '\\n',\n",
       " '一旦我们在一个月后训练好该模型，我们将如何处理该模型，\\n',\n",
       " '\\n',\n",
       " '我们\\n',\n",
       " '\\n',\n",
       " '注意到我们领域的第一件事是这些\\n',\n",
       " '\\n',\n",
       " '模型基本上处于以下过程：\\n',\n",
       " '\\n',\n",
       " '语言建模学习非常强大的\\n',\n",
       " '\\n',\n",
       " '通用表示，并且\\n',\n",
       " '\\n',\n",
       " '可以非常有效地\\n',\n",
       " '\\n',\n",
       " '针对您可能感兴趣的任何下游任务对它们进行微调，\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '例如，如果您对\\n',\n",
       " '\\n',\n",
       " '情感分类感兴趣，\\n',\n",
       " '\\n',\n",
       " '过去的方法是收集一堆\\n',\n",
       " '\\n',\n",
       " '正面和负面，然后你\\n',\n",
       " '\\n',\n",
       " '为此训练某种 NLP 模型，\\n',\n",
       " '\\n',\n",
       " '但新方法是忽略\\n',\n",
       " '\\n',\n",
       " '情感分类\\n',\n",
       " '\\n',\n",
       " '并进行大型语言模型\\n',\n",
       " '\\n',\n",
       " '预训练，训练大型 Transformer\\n',\n",
       " '\\n',\n",
       " '，然后你只能有\\n',\n",
       " '\\n',\n",
       " '几个例子 你可以非常\\n',\n",
       " '\\n',\n",
       " '有效地微调\\n',\n",
       " '\\n',\n",
       " '该任务的模型，因此这在实践中非常有效，其\\n',\n",
       " '\\n',\n",
       " '原因是\\n',\n",
       " '\\n',\n",
       " '基本上 Transformer 被迫\\n',\n",
       " '\\n',\n",
       " '在语言建模任务中执行大量任务，\\n',\n",
       " '\\n',\n",
       " '因为仅就以下方面而言：\\n',\n",
       " '\\n',\n",
       " '预测下一个标记，\\n',\n",
       " '\\n',\n",
       " '它被迫了解很多关于\\n',\n",
       " '\\n',\n",
       " '文本的结构以及\\n',\n",
       " '\\n',\n",
       " '它们所包含的所有不同概念，\\n',\n",
       " '\\n',\n",
       " '所以这是 GPT 一个现在大约在\\n',\n",
       " '\\n',\n",
       " 'gpt2 的时间人们注意到，实际上\\n',\n",
       " '\\n',\n",
       " '甚至比微调更好\\n',\n",
       " '\\n',\n",
       " '非常有效地提示这些模型，\\n',\n",
       " '\\n',\n",
       " '所以这些是语言模型\\n',\n",
       " '\\n',\n",
       " '，它们想要完成文档，这样\\n',\n",
       " '\\n',\n",
       " '你实际上可以\\n',\n",
       " '\\n',\n",
       " '通过排列这些\\n',\n",
       " '\\n',\n",
       " '假文档来欺骗它们执行任务，所以在这个例子中，\\n',\n",
       " '\\n',\n",
       " '例如，我们有一些段落，\\n',\n",
       " '\\n',\n",
       " '然后我们有点像这样做 QA qaqa 这被\\n',\n",
       " '\\n',\n",
       " '称为“一些镜头提示”，然后我们执行\\n',\n",
       " '\\n',\n",
       " 'q，然后当 Transformer 尝试\\n',\n",
       " '\\n',\n",
       " '完成文档时，它实际上是在\\n',\n",
       " '\\n',\n",
       " '回答我们的问题，所以这是一个\\n',\n",
       " '\\n',\n",
       " '提示工程基础模型的示例，人们\\n',\n",
       " '\\n',\n",
       " '相信它是在\\n',\n",
       " '\\n',\n",
       " '模仿 一个文档并让它\\n',\n",
       " '\\n',\n",
       " '执行一项任务，\\n',\n",
       " '\\n',\n",
       " '所以这拉开了我想说的时代，我想说的是，\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '即使没有训练\\n',\n",
       " '\\n',\n",
       " '任何神经网络微调，这实际上也可以很好地解决很多问题 等等，\\n',\n",
       " '\\n',\n",
       " '从那时起，我们已经看到了每个人都训练过的\\n',\n",
       " '\\n',\n",
       " '基本模型的完整进化树，但\\n',\n",
       " '\\n',\n",
       " '并非所有这些\\n',\n",
       " '\\n',\n",
       " '模型都可用，例如，\\n',\n",
       " '\\n',\n",
       " 'gpt4 基本模型从未发布过，\\n',\n",
       " '\\n',\n",
       " '您可能通过 API 与之交互的 gpt4 模型\\n',\n",
       " '\\n',\n",
       " '则不可用 一个基本模型，它是\\n',\n",
       " '\\n',\n",
       " '一个辅助模型，我们将\\n',\n",
       " '\\n',\n",
       " '介绍如何获得这些\\n',\n",
       " '\\n',\n",
       " '基于 gpt3 的模型可以\\n',\n",
       " '\\n',\n",
       " '通过名为 DaVinci 的 API 获得，而 gpt2 基本\\n',\n",
       " '\\n',\n",
       " '模型甚至可以作为\\n',\n",
       " '\\n',\n",
       " '我们 GitHub 存储库上的权重使用，但目前是最好的\\n',\n",
       " '\\n',\n",
       " '可用的基本模型可能是\\n',\n",
       " '\\n',\n",
       " '来自meta的Llama系列，尽管它\\n',\n",
       " '\\n',\n",
       " '没有商业许可\\n',\n",
       " '\\n',\n",
       " '现在需要指出的是基本\\n',\n",
       " '\\n',\n",
       " '模型并不能提供帮助，他们\\n',\n",
       " '\\n',\n",
       " '不想回答他们不想\\n',\n",
       " '\\n',\n",
       " '回答你的问题，他们只是\\n',\n",
       " '\\n',\n",
       " '想要 完成文件，所以如果你\\n',\n",
       " '\\n',\n",
       " '告诉他们写一首关于该品牌\\n',\n",
       " '\\n',\n",
       " '奶酪的诗，你就会知道它会\\n',\n",
       " '\\n',\n",
       " '回答更多问题，它\\n',\n",
       " '\\n',\n",
       " '只是完成它认为作为\\n',\n",
       " '\\n',\n",
       " '文件的内容，\\n',\n",
       " '\\n',\n",
       " '但是你可以以\\n',\n",
       " '\\n',\n",
       " '特定的方式提示他们基本模型\\n',\n",
       " '\\n',\n",
       " '更有可能起作用，因此作为一个例子，\\n',\n",
       " '\\n',\n",
       " '这里有一首关于面包和奶酪的诗，\\n',\n",
       " '\\n',\n",
       " '在这种情况下，它将正确自动完成，\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '您甚至可以欺骗基础模型来\\n',\n",
       " '\\n',\n",
       " '提供帮助，而您\\n',\n",
       " '\\n',\n",
       " '这样做的方式是您将创建像\\n',\n",
       " '\\n',\n",
       " '特定的几个 镜头提示\\n',\n",
       " '\\n',\n",
       " '看起来就像\\n',\n",
       " '\\n',\n",
       " '人类和助手之间有某种文档\\n',\n",
       " '\\n',\n",
       " '，他们正在交换某种嗯\\n',\n",
       " '\\n',\n",
       " '信息，然后在底部你将\\n',\n",
       " '\\n',\n",
       " '查询放在最后，\\n',\n",
       " '\\n',\n",
       " '基本模型将有点像\\n',\n",
       " '\\n',\n",
       " '条件 它本身就像一个\\n',\n",
       " '\\n',\n",
       " '有用的助手和某种答案，但这\\n',\n",
       " '\\n',\n",
       " '不是很可靠，\\n',\n",
       " '\\n',\n",
       " '并且在实践中效果不佳，尽管\\n',\n",
       " '\\n',\n",
       " '可以这样做，因此\\n',\n",
       " '\\n',\n",
       " '我们有不同的路径来\\n',\n",
       " '\\n',\n",
       " '使实际的 GBT 帮助不仅仅是基本\\n',\n",
       " '\\n',\n",
       " '模型文档完成器和 因此，这\\n',\n",
       " '\\n',\n",
       " '使我们进入监督微调阶段，因此\\n',\n",
       " '\\n',\n",
       " '在监督微调阶段，我们\\n',\n",
       " '\\n',\n",
       " '将收集少量但高质量\\n',\n",
       " '\\n',\n",
       " '的数据集，在这种情况下，我们\\n',\n",
       " '\\n',\n",
       " '将要求人工承包商收集表单提示\\n',\n",
       " '\\n',\n",
       " '格式的数据，\\n',\n",
       " '\\n',\n",
       " '并 理想的响应，我们将\\n',\n",
       " '\\n',\n",
       " '收集大量这样的数据，通常是\\n',\n",
       " '\\n',\n",
       " '数万个或类似的东西，\\n',\n",
       " '\\n',\n",
       " '然后我们仍然会对\\n',\n",
       " '\\n',\n",
       " '这些数据进行语言建模，因此\\n',\n",
       " '\\n',\n",
       " '算法上不会发生任何变化，我们\\n',\n",
       " '\\n',\n",
       " '只是交换训练集，以便它\\n',\n",
       " '\\n',\n",
       " '使用 作为互联网文档，这是一种\\n',\n",
       " '\\n',\n",
       " '大量的本地\\n',\n",
       " '\\n',\n",
       " '四基本上 QA 提示响应类型的\\n',\n",
       " '\\n',\n",
       " '数据，并且是低数量\\n',\n",
       " '\\n',\n",
       " '高质量的，\\n',\n",
       " '\\n',\n",
       " '所以我们仍然进行语言建模，\\n',\n",
       " '\\n',\n",
       " '然后在训练之后我们得到一个 sfd 模型\\n',\n",
       " '\\n',\n",
       " '，你可以实际部署这些模型，\\n',\n",
       " '\\n',\n",
       " '它们是 实际的助理，他们\\n',\n",
       " '\\n',\n",
       " '在某种程度上工作让我向您\\n',\n",
       " '\\n',\n",
       " '展示示例演示可能是什么样子\\n',\n",
       " '\\n',\n",
       " '所以这是人类\\n',\n",
       " '\\n',\n",
       " '承包商可能想出的东西这是\\n',\n",
       " '\\n',\n",
       " '一些随机提示您可以写一个关于术语垄断或类似术语\\n',\n",
       " '\\n',\n",
       " '的相关性的简短介绍吗\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '然后承包商也会写出\\n',\n",
       " '\\n',\n",
       " '一个理想的回复，当他们\\n',\n",
       " '\\n',\n",
       " '写出这些回复时，他们会遵循\\n',\n",
       " '\\n',\n",
       " '大量的标签文件，并被\\n',\n",
       " '\\n',\n",
       " '要求提供帮助、\\n',\n",
       " '\\n',\n",
       " '真实且无害，而\\n',\n",
       " '\\n',\n",
       " '这里的标签说明您可能无法\\n',\n",
       " '\\n',\n",
       " '阅读其他可以 我但是它们很长，\\n',\n",
       " '\\n',\n",
       " '这只是人们遵循\\n',\n",
       " '\\n',\n",
       " '指示并尝试完成\\n',\n",
       " '\\n',\n",
       " '这些提示，\\n',\n",
       " '\\n',\n",
       " '这就是数据集的样子，\\n',\n",
       " '\\n',\n",
       " '您可以训练这些模型，这\\n',\n",
       " '\\n',\n",
       " '在某种程度上有效，\\n',\n",
       " '\\n',\n",
       " '现在您实际上可以\\n',\n",
       " '\\n',\n",
       " '从这里继续管道， 进入来自\\n',\n",
       " '\\n',\n",
       " '人类反馈的 rlhf 强化学习，\\n',\n",
       " '\\n',\n",
       " '其中包括奖励\\n',\n",
       " '\\n',\n",
       " '建模和强化学习，所以\\n',\n",
       " '\\n',\n",
       " '让我介绍这一点，然后我会\\n',\n",
       " '\\n',\n",
       " '回到为什么你可能想要执行\\n',\n",
       " '\\n',\n",
       " '额外的步骤，以及它与 sft 模型相比如何，\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '所以 奖励建模步骤\\n',\n",
       " '\\n',\n",
       " '我们要做的就是将\\n',\n",
       " '\\n',\n",
       " '我们的数据收集转变为\\n',\n",
       " '\\n',\n",
       " '比较的形式，所以这里是\\n',\n",
       " '\\n',\n",
       " '我们的数据集的一个例子\\n',\n",
       " '\\n',\n",
       " '我有相同的提示相同的提示\\n',\n",
       " '\\n',\n",
       " 'top 要求助手\\n',\n",
       " '\\n',\n",
       " '编写一个程序或函数来\\n',\n",
       " '\\n',\n",
       " '检查给定的字符串是否是回文\\n',\n",
       " '\\n',\n",
       " '，然后我们要做的是采用\\n',\n",
       " '\\n',\n",
       " '我们已经训练过的 sft 模型并\\n',\n",
       " '\\n',\n",
       " '创建多个完成，因此在本\\n',\n",
       " '\\n',\n",
       " '例中我们有三个\\n',\n",
       " '\\n',\n",
       " '模型创建的完成结果，\\n',\n",
       " '\\n',\n",
       " '然后我们要求人们对这些\\n',\n",
       " '\\n',\n",
       " '完成情况进行排名，因此，如果您盯着它\\n',\n",
       " '\\n',\n",
       " '一段时间，顺便说一下，\\n',\n",
       " '\\n',\n",
       " '比较其中一些\\n',\n",
       " '\\n',\n",
       " '预测是非常困难的，这可能会花费\\n',\n",
       " '\\n',\n",
       " '人们甚至几个小时的时间 单个提示\\n',\n",
       " '\\n',\n",
       " '完成对，但假设我们\\n',\n",
       " '\\n',\n",
       " '认为其中一个比\\n',\n",
       " '\\n',\n",
       " '其他更好，依此类推，因此我们对它们进行排名，\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '然后我们可以用\\n',\n",
       " '\\n',\n",
       " '看起来非常像对这些之间\\n',\n",
       " '\\n',\n",
       " '所有可能的对进行二元分类的东西来跟踪\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '所以我们现在要做的就是将\\n',\n",
       " '\\n',\n",
       " '提示按行布局，并且\\n',\n",
       " '\\n',\n",
       " '这里所有三行的提示都是相同的，所以提示\\n',\n",
       " '\\n',\n",
       " '都是相同的，但是\\n',\n",
       " '\\n',\n",
       " '完成的情况有所不同，所以\\n',\n",
       " '\\n',\n",
       " '黄色标记来自 sft\\n',\n",
       " '\\n',\n",
       " '模型，那么我们的 要做的是我们在最后附加\\n',\n",
       " '\\n',\n",
       " '另一个特殊奖励呃读出令牌\\n',\n",
       " '\\n',\n",
       " '，我们基本上只\\n',\n",
       " '\\n',\n",
       " '在这个绿色令牌上监督 Transformer，\\n',\n",
       " '\\n',\n",
       " '并且 Transformer 会\\n',\n",
       " '\\n',\n",
       " '预测一些奖励，以判断\\n',\n",
       " '\\n',\n",
       " '该提示的完成程度\\n',\n",
       " '\\n',\n",
       " '，因此基本上它会猜测\\n',\n",
       " '\\n',\n",
       " '每个完成的质量，然后\\n',\n",
       " '\\n',\n",
       " '一旦它对每个完成进行猜测，\\n',\n",
       " '\\n',\n",
       " '我们也有基本事实，它\\n',\n",
       " '\\n',\n",
       " '告诉我们它们的排名，因此\\n',\n",
       " '\\n',\n",
       " '我们实际上可以强制其中一些\\n',\n",
       " '\\n',\n",
       " '数字应该比其他数字高得多，\\n',\n",
       " '\\n',\n",
       " '依此类推 我们将其公式化为\\n',\n",
       " '\\n',\n",
       " '损失函数，并训练我们的模型\\n',\n",
       " '\\n',\n",
       " '做出奖励预测，该预测\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '与所有这些承包商的比较得出的基本事实一致，\\n',\n",
       " '\\n',\n",
       " '这就是我们训练\\n',\n",
       " '\\n',\n",
       " '奖励模型的方式，并让我们能够对\\n',\n",
       " '\\n',\n",
       " '完成情况进行评分 对于提示，\\n',\n",
       " '\\n',\n",
       " '一旦我们有了奖励模型，我们就\\n',\n",
       " '\\n',\n",
       " '无法部署它，因为这\\n',\n",
       " '\\n',\n",
       " '本身作为助手不是很有用，\\n',\n",
       " '\\n',\n",
       " '但对于现在接下来的强化\\n',\n",
       " '\\n',\n",
       " '强化学习阶段非常有用，\\n',\n",
       " '\\n',\n",
       " '因为我们有一个\\n',\n",
       " '\\n',\n",
       " '可以评分的奖励模型 任何\\n',\n",
       " '\\n',\n",
       " '给定\\n',\n",
       " '\\n',\n",
       " '提示的任意完成的质量，所以我们在\\n',\n",
       " '\\n',\n",
       " '强化学习期间所做的就是我们基本上\\n',\n",
       " '\\n',\n",
       " '再次得到大量提示，\\n',\n",
       " '\\n',\n",
       " '现在我们\\n',\n",
       " '\\n',\n",
       " '针对奖励模型进行强化学习，所以\\n',\n",
       " '\\n',\n",
       " '这就是\\n',\n",
       " '\\n',\n",
       " '我们采取的单一提示 提示我们将其排成行，\\n',\n",
       " '\\n',\n",
       " '现在我们使用 sft 我们\\n',\n",
       " '\\n',\n",
       " '基本上使用我们想要\\n',\n",
       " '\\n',\n",
       " '训练的模型，该模型在 sft 模型中初始化\\n',\n",
       " '\\n',\n",
       " '以创建一些黄色的完成\\n',\n",
       " '\\n',\n",
       " '，然后我们再次附加奖励令牌\\n',\n",
       " '\\n',\n",
       " '，我们读出\\n',\n",
       " '\\n',\n",
       " '根据\\n',\n",
       " '\\n',\n",
       " '现在保持固定的奖励模型进行奖励，它不再改变\\n',\n",
       " '\\n',\n",
       " '，现在奖励模型告诉我们\\n',\n",
       " '\\n',\n",
       " '每个提示的每个完成的质量\\n',\n",
       " '\\n',\n",
       " '，所以我们\\n',\n",
       " '\\n',\n",
       " '可以做的是我们现在可以基本上\\n',\n",
       " '\\n',\n",
       " '应用相同的 语言建模损失\\n',\n",
       " '\\n',\n",
       " '函数，但我们目前正在对\\n',\n",
       " '\\n',\n",
       " '黄色标记进行训练，并且我们正在通过\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '奖励模型指示的奖励来权衡语言建模目标，因此\\n',\n",
       " '\\n',\n",
       " '作为第一行的示例，\\n',\n",
       " '\\n',\n",
       " '奖励模型表示这是一个相当\\n',\n",
       " '\\n',\n",
       " '高的分数 因此，\\n',\n",
       " '\\n',\n",
       " '我们在第一行中碰巧采样的所有令牌都\\n',\n",
       " '\\n',\n",
       " '将得到强化，\\n',\n",
       " '\\n',\n",
       " '并且它们将在未来获得更高的\\n',\n",
       " '\\n',\n",
       " '概率，\\n',\n",
       " '\\n',\n",
       " '相反，在第二行中，奖励\\n',\n",
       " '\\n',\n",
       " '模型确实不喜欢这种\\n',\n",
       " '\\n',\n",
       " '完成负数 1.2，所以 因此，\\n',\n",
       " '\\n',\n",
       " '我们在第二行中采样的每个令牌\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '未来的概率都会稍高一些，\\n',\n",
       " '\\n',\n",
       " '我们在许多批次的许多提示中一遍又一遍地执行此操作，\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '基本上我们得到了一个\\n',\n",
       " '\\n',\n",
       " '在这里创建黄色令牌的策略，它基本上全部\\n',\n",
       " '\\n',\n",
       " '根据我们在前一阶段训练的奖励模型，这里所有的完成都会\\n',\n",
       " '\\n',\n",
       " '得分很高，这\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '就是我们训练的方式，呃，这就是\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '现在的 rohf 管道，最后你会得到一个\\n',\n",
       " '\\n',\n",
       " '可以部署的模型，所以 举个\\n',\n",
       " '\\n',\n",
       " '例子，聊天 GPT 是 rlhf 模型，但\\n',\n",
       " '\\n',\n",
       " '您可能会遇到的其他一些模型，\\n',\n",
       " '\\n',\n",
       " '例如 kuna 13B 等，\\n',\n",
       " '\\n',\n",
       " '这些都是 sft 模型，因此我们\\n',\n",
       " '\\n',\n",
       " '有基本模型 sft 模型和 rlh\\n',\n",
       " '\\n',\n",
       " '模型，这有点像状态\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '现在为什么你想做 rlhf 所以一个\\n',\n",
       " '\\n',\n",
       " '不太令人兴奋的答案是\\n',\n",
       " '\\n',\n",
       " '它效果更好所以这\\n',\n",
       " '\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all the lines of a file in a list, use list(f) or f.readlines()\n",
    "with open('build.txt', encoding=\"utf-8\") as f:\n",
    "    f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2f7b1ae-07fd-4962-a7be-3178b7c7a51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f.write(string) writes the contents of string to the file, returning the number of characters written\n",
    "with open('build.txt', 'r+', encoding=\"utf-8\") as f:\n",
    "    f.write('This is a test\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf9c3cba-f8bc-41b7-ace5-d90e2820a917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"('the answer', 42)\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Other types of objects need to be converted\n",
    "value = ('the answer', 42)\n",
    "s = str(value)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aeffb784-9413-488d-a3c8-b908b1ea9575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f.tell()?\n",
    "with open('build.txt', encoding=\"utf-8\") as f:\n",
    "    f.tell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a1b11c0-76bb-40fa-92c8-31c0c6533d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1, \"simple\", \"list\"]'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving structured data with json (JavaSript Object Notation)\n",
    "import json\n",
    "x = [1, 'simple', 'list']\n",
    "json.dumps(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8658d8b4-7148-4bbb-8385-707ba48083f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump() simply serializes the object to a text file\n",
    "with open('build.txt', 'r+', encoding=\"utf-8\") as f:\n",
    "    json.dump(x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e91e99-e470-48af-9d01-e6a80dc46543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
