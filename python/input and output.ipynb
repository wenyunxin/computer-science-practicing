{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75c7c30-7c89-47b3-8b87-57d29135d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f8c2ba-b990-4204-bf7f-df356114a1e3",
   "metadata": {},
   "source": [
    "## Fancier Output Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f619fa7-a201-4725-9fcd-fa9d952f0541",
   "metadata": {},
   "source": [
    "three ways of writing values:\n",
    "- expression statements\n",
    "- print() function\n",
    "- using the write() method of file objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e24154-fa4d-4f24-b31b-240bcd24e124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x103d0df60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The standard output file can be referenced as sys.stdout\n",
    "import sys\n",
    "sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886adafe-b6d0-464a-a946-f40de29ef536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Results of the 2016 Referendum'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# several ways to format output, first is to use formatted string literals\n",
    "year = 2016\n",
    "event = 'Referendum'\n",
    "f'Results of the {year} {event}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd50671f-850c-4ffb-94bc-a0dbecb96546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'42572654 YES votes 49.67%'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second is to use str.format() method of strings\n",
    "yes_votes = 42_572_654\n",
    "no_votes = 43_132_495\n",
    "percentage = yes_votes / (yes_votes + no_votes)\n",
    "'{:-8} YES votes {:2.2%}'.format(yes_votes, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7c3d0a9-87e0-473f-8c3d-bd93e6e08bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"'Hello world.'\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Hello world.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'0.14285714285714285'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of x is 32.5, and y is 40000...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello, world\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, world\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"'hello, world\\\\n'\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello, world\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"(32.5, 40000, ('spam', 'eggs'))\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conver any value to a string with the repr() or str() functions\n",
    "# str() function is meant to return representations of values that are fairly human-readable\n",
    "# repr() is meant to generate representations that can be read by the interpreter\n",
    "# For objects which don't have a particular representation for human consumption, str() function will return the same value as repr()\n",
    "# Many values, such as numbers or structures like lists and dictionaries, have the same representation using either function\n",
    "# string, in particular, has two distinct representations\n",
    "s = 'Hello world.'\n",
    "\n",
    "str(s)\n",
    "repr(s)\n",
    "eval(repr(s))\n",
    "\n",
    "str(1/7)\n",
    "\n",
    "x = 10 * 3.25\n",
    "y = 200 * 200\n",
    "s = 'The value of x is ' + repr(x) + ', and y is ' + repr(y) + '...'\n",
    "print(s)\n",
    "\n",
    "hello = 'hello, world\\n'\n",
    "# don't have a particular representation for human consumption?\n",
    "hello\n",
    "print(hello)\n",
    "hellos = repr(hello)\n",
    "hellos\n",
    "print(hellos)\n",
    "\n",
    "repr((x, y, ('spam', 'eggs')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42fda7a8-e399-40f8-9650-cdaa26fb1d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of pi is approximately 3.142.\n",
      "Sjoerd     ==>        4127\n",
      "Jack       ==>        4098\n",
      "Dcab       ==>        7678\n",
      "My hovercraft is full of eels\n",
      "My hovercraft is full of 'eels'\n",
      "Debugging bugs='roaches' 13 living room\n"
     ]
    }
   ],
   "source": [
    "# formatted string literals (f-strings) include the value of Python expressions inside a string\n",
    "import math\n",
    "print(f'The value of pi is approximately {math.pi:.3f}.')\n",
    "\n",
    "# Passing an integer after the : will cause the field to be a minimum number of characters wide\n",
    "# This is useful for making columns line up\n",
    "table = {'Sjoerd': 4127, 'Jack': 4098, 'Dcab': 7678}\n",
    "for name, phone in table.items():\n",
    "    print(f'{name:10} ==>  {phone:10d}')\n",
    "\n",
    "# Other modifiers can be used to convert the value before it is formatted\n",
    "# !a applies ascii(), !s applies str(), !r applies repr()\n",
    "animals = 'eels'\n",
    "print(f'My hovercraft is full of {animals}')\n",
    "print(f'My hovercraft is full of {animals!r}')\n",
    "\n",
    "# The = specifier can be used to expand an expression to the text of the expression\n",
    "bugs = 'roaches'\n",
    "count = 13\n",
    "area = 'living room'\n",
    "print(f'Debugging {bugs=} {count} {area}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc2cf616-0d34-4787-8472-3b4374cb9d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are the knights who say \"Ni!\"\n",
      "spam and eggs\n",
      "eggs and spam\n",
      "This spam is absolutely horrible.\n",
      "The story of Bill, Manfred, and Georg.\n",
      "Jack: 4098; Sjoerd: 4127; Dcab: 8637678\n",
      "Jack: 4098; Sjoerd: 4127; Dcab: 8637678\n",
      "Help on built-in function vars in module builtins:\n",
      "\n",
      "vars(...)\n",
      "    vars([object]) -> dictionary\n",
      "    \n",
      "    Without arguments, equivalent to locals().\n",
      "    With an argument, equivalent to object.__dict__.\n",
      "\n",
      " 1   1    1\n",
      " 2   4    8\n",
      " 3   9   27\n",
      " 4  16   64\n",
      " 5  25  125\n",
      " 6  36  216\n",
      " 7  49  343\n",
      " 8  64  512\n",
      " 9  81  729\n",
      "10 100 1000\n"
     ]
    }
   ],
   "source": [
    "# basic usage of str.format()\n",
    "print('We are the {} who say \"{}!\"'.format('knights', 'Ni'))\n",
    "\n",
    "# A number in the braces can be used to refer to the position of the object passed into the str.format()\n",
    "print('{0} and {1}'.format('spam', 'eggs'))\n",
    "print('{1} and {0}'.format('spam', 'eggs'))\n",
    "\n",
    "# keyword arguments used in str.format() method\n",
    "print('This {food} is {adjective}.'.format(\n",
    "    food='spam', adjective='absolutely horrible'))\n",
    "\n",
    "# positional and keyword arguments can be combined\n",
    "print('The story of {0}, {1}, and {other}.'.format('Bill', 'Manfred',\n",
    "                                                   other='Georg'))\n",
    "\n",
    "# passing the dict and using square brackets to access the keys\n",
    "table = {'Sjoerd': 4127, 'Jack': 4098, 'Dcab': 8637678}\n",
    "print('Jack: {0[Jack]:d}; Sjoerd: {0[Sjoerd]:d}; '\n",
    "      'Dcab: {0[Dcab]:d}'.format(table))\n",
    "\n",
    "# passing the dict as keyword arguments with the ** notation\n",
    "table = {'Sjoerd': 4127, 'Jack': 4098, 'Dcab': 8637678}\n",
    "print('Jack: {Jack:d}; Sjoerd: {Sjoerd:d}; Dcab: {Dcab:d}'.format(**table))\n",
    "\n",
    "help(vars)\n",
    "\n",
    "for x in range(1, 11):\n",
    "    print('{0:2d} {1:3d} {2:4d}'.format(x, x**2, x**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6402dbc4-6bda-48c4-8ca0-706d55283e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1   1    1\n",
      " 2   4    8\n",
      " 3   9   27\n",
      " 4  16   64\n",
      " 5  25  125\n",
      " 6  36  216\n",
      " 7  49  343\n",
      " 8  64  512\n",
      " 9  81  729\n",
      "10 100 1000\n"
     ]
    }
   ],
   "source": [
    "# formatted manually\n",
    "for x in range(1, 11):\n",
    "    print(repr(x).rjust(2), repr(x**2).rjust(3), repr(x**3).rjust(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bd6a690-36b4-4e17-a9a2-1fef6157725c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00012'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'-003.14'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'3.14159265359'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# str.zfill() pads a numeric string on the left with zeros\n",
    "'12'.zfill(5)\n",
    "'-3.14'.zfill(7)\n",
    "'3.14159265359'.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2de707-c115-43c3-a4e5-b7dba014e13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of pi is approximately 3.142.\n"
     ]
    }
   ],
   "source": [
    "# the % operator can be used for string formatting, known as string interpolation\n",
    "print('The value of pi is approximately %5.3f.' % math.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0581417d-48b3-4719-bccb-98e31a15bf43",
   "metadata": {},
   "source": [
    "More information can be found in the [printf-style String Formatting](https://docs.python.org/3/library/stdtypes.html#old-string-formatting) section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba449fc0-8d6d-474c-b3d0-acf93aac69d8",
   "metadata": {},
   "source": [
    "## Reading and Writing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83c8ff6a-790c-4455-a4b1-d17440f85e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='build.txt' mode='r' encoding='utf-8'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open() returns a file object, and is most commonly used with two positional arguments and one keyword argument\n",
    "# open(filename, mode, encoding=None)\n",
    "f = open('build.txt', 'r', encoding=\"utf-8\")\n",
    "f\n",
    "\n",
    "# if not using the with keyword, call f.close() to close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43650250-f079-4d60-a011-33d3287a3ddd",
   "metadata": {},
   "source": [
    "Normally, files are opened in *text mode*. The mode argument describes the way in which the file will be used\n",
    "- `r` the file will only be read\n",
    "- `w` for only writing (an existing file with the same name will be erased)\n",
    "- `a` opens the file for appending, any data written to the file is automatically added to the end\n",
    "- `r+` opens the file for both reading and writing\n",
    "- the mode argument is optional, `r` will be assumed if it's omitted\n",
    "\n",
    "Appending a `b` to the mode opens the file in *binary mode*. Binary mode data is read and written as [bytes](https://docs.python.org/3/library/stdtypes.html#bytes) objects. You can not specify *encoding* when opening file in binary mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b5b6348-d9c4-4758-a929-b7e6a35b177b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"foreign  [Music]  researcher and founding member of open  AI Andre carpathy  [Applause]  hi everyone  I'm happy to be here to tell you about  the state of gbt and more generally  about the rapidly growing ecosystem of  large language models  so I would like to partition the talk  into two parts  in the first part I would like to tell  you about how we train GPT assistance  and then in the second part we will we  are going to take a look at how we can  use these assistants effectively for  your applications  so first let's take a look at the  emerging recipe for how to train these  assistants and keep in mind that this is  all very new and still rapidly evolving  but so far the recipe looks something  like this  now this is kind of a complicated slide  so I'm going to go through it piece by  piece but roughly speaking we have four  major stages free training supervised  fine tuning reward modeling  reinforcement learning and they follow  each other serially  now in each stage we have a data set  that is that powers that stage we have  an algorithm that for our purposes will  be a objective and a and over for uh for  training the neural network and then we  have a resulting model and then there's  some notes on the bottom so the first  stage we're going to start with is the  pre-training stage now this stage is  kind of special in this diagram and this  diagram is not to scale because this  stage is where all of the computational  work basically happens this is 99 of the  training uh compute time and also flops  and so this is where we are dealing with  internet scale data sets with thousands  of gpus in the store computer and also  months of training potentially the other  three stages are fine-tuning stages that  are much more along the lines of a small  few number of gpus and hours or days  so let's take a look at the pre-training  stage to achieve a base model  first we're going to gather a large  amount of data here's an example of what  we call a data mixture that comes from  this uh paper that was released by meta  where they released this llama based  model now you can see roughly the kinds  of data sets that enter into these  collections so we have common crawl  which is just a web scrape C4 which is  also a common crawl and then some high  quality data sets as well so for example  GitHub Wikipedia books archive stack  exchange and so on these are all mixed  up together and then they are sampled  According to some given proportions and  that forms the training set for the  neural net for the GPT  now before we can actually train on this  data we need to go through one more  pre-processing step and that is  tokenization and this is basically a  translation of the raw text that we  scrape from the internet into sequences  of integers because that's the native  representation over which gpts function  now this is a lossless kind of  translation between pieces of text and  tokens and integers and there are a  number of algorithms for this stage  typically for example you could use  something like byte pair encoding which  iteratively merges little text chunks  and groups them into tokens  and so here I'm showing some example  chunks of these tokens and then this is  the raw integer sequence that will  actually feed into a Transformer  now here I'm showing two uh sort of like  examples for hyper parameters that  govern this stage so gpt4 we did not  release too much information about how  it was trained and so on so I'm using  gpt3's numbers but gpt3 is of course a  little bit old by now about three years  ago but llama is a fairly recent model  from meta so these are roughly the  orders of magnitude that we're dealing  with when we're doing pre-training  the vocabulary size is usually a couple  10 000 tokens the context length is  usually something like two thousand four  thousand or nowadays even 100 000 and  this governs the maximum number of  integers that the GPT will look at when  it's trying to predict the next integer  in a sequence  um you can see that roughly the number  of parameters is say 65 billion for  llama now even though llama has only 65b  parameters compared to gvt3's 175  billion parameters llama is a  significantly more powerful model and  intuitively that's because the model is  trained for significantly longer in this  case 1.4 trillion tokens instead of just  300 billion tokens so you shouldn't  judge the power of a model just by the  number of parameters that it contains  below I'm showing some tables of rough  number of rough hyper parameters that  typically go into specifying the  Transformer neural network so the number  of heads the dimension size number of  layers and so on and on the bottom I'm  showing some training hyper parameters  so for example to train the 65b model  meta used 2000 gpus roughly 21 days  train of training and and roughly  several million dollars and so that's  the rough orders of magnitude that you  should have in mind for the pre-training  stage  now when we're actually pre-chaining  what happens roughly speaking we are  going to take our tokens and we're going  to lay them out into Data batches so we  have these arrays that will feed into  the Transformer and these arrays are B  the batch size and these are all  independent examples stacked up in rows  and B by T T being the maximum context  length so in my picture I only have 10.  the context length so this could be two  thousand four thousand Etc so these are  extremely long rows and what we do is we  take these documents and we pack them  into rows and we delimit them with these  Special end of text tokens uh basically  it's on the Transformer where a new  document begins  and so here I have a few examples of  documents and then I've stretched them  out into uh into this input  now we're going to feed all of these  numbers into Transformer and let me let  me just focus on a single particular  cell but the same thing will happen at  every every cell in this diagram so  let's look at the green cell the green  cell is going to take a look at all of  the tokens before it so all of the  tokens in yellow and we're going to feed  that entire context into the Transformer  neural network  and the Transformer is going to try to  predict the next token in the sequence  in this case in red  now the Transformer I don't have too  much time to unfortunately go into the  full details of this neural network  architecture is just a large blob of  neural net stuff for our purposes and  it's got several 10 billion parameters  typically or something like that and of  course as they tune these parameters  you're getting slightly different uh  predicted distributions for every single  one of these cells  and so for example if our vocabulary  size is 50 257 tokens then we're going  to have that many numbers because we  need to specify a probability  distribution for what comes next so  basically we have a probability for  whatever may follow now in this specific  example for the specific cell 513 will  come next and so we can use this as a  source of supervision to update our  Transformers weights and so we're  applying this basically on every single  cell in parallel and we keep swap  swapping batches and we're trying to get  the Transformer to make the correct  predictions over what token comes next  in a sequence  so let me show you more concretely what  this looks like when you train one of  these models this is actually coming  from a New York Times and they trained a  small GPT on Shakespeare and so here's a  small snippet of Shakespeare and they  trained a GPT on it now in the beginning  at initialization the GPT starts with  completely random weights so you're just  getting completely random outputs as  well  but over time as you train uh the GPT  longer and longer you are getting more  and more coherent and consistent uh sort  of samples from the model and the way  you sample from it of course is you uh  predict what comes next you sample from  that distribution and you keep feeding  that back into the process and you can  basically uh sample large sequences and  so by the end you see that the  Transformer has learned about words and  where to put spaces and where to put  commas and so on and so we're making  more and more consistent predictions  over time  these are the kinds of plots that you're  looking at when you're doing model  pre-training effectively we're looking  at the loss function over time as you  train and low loss means that our  Transformer is predicting the correct uh  is giving a higher probability to the  correct next integer in a sequence  now what are we going to do with this  model once we've trained it after a  month well the first thing that we  noticed we the field is that these  models basically in the process of  language modeling learn very powerful  General representations and it's  possible to very efficiently fine-tune  them for any arbitrary Downstream task  you might be interested in so as an  example if you're interested in  sentiment classification the approach  used to be that you collect a bunch of  positives and negatives and then you  train some kind of an NLP model for for  that but the new approach is ignore  sentiment classification  go off and do large language model  pre-training train the large Transformer  and then you can only you may only have  a few examples and you can very  efficiently fine tune your model for  that task and so this works very well in  practice and the reason for this is that  basically the Transformer is forced to  multitask a huge amount of tasks in the  language modeling task because just just  in terms of predicting the next token  it's forced to understand a lot about  the structure of the of the text and all  the different concepts they're in  so that was GPT one now around the time  of gpt2 people noticed that actually  even better than fine tuning you can  actually prompt these models very  effectively so these are language models  and they want to complete documents so  you can actually trick them into  performing tasks just by arranging these  fake documents so in this example for uh  for example we have some passage and  then we sort of like do QA qaqa this is  called a few shot prompt and then we do  q and then as the Transformer is trying  to complete the document it's actually  answering our question and so this is an  example of prompt engineering a base  model making the belief that it's sort  of imitating a document and getting it  to perform a task  and so this kicked off I think the era  of I would say prompting over  fine-tuning and seeing that this  actually can work extremely well on a  lot of problems even without training  any neural networks fine-tuning or so on  now since then we've seen an entire  evolutionary tree of Base models that  everyone has trained not all of these  models are available for example the  gpt4 base model was never released the  gpt4 model that you might be interacting  with over API is not a base model it's  an assistant model and we're going to  cover how to get those in a bit gpt3  based model is available via the API  under the name DaVinci and gpt2 base  model is available even as weights on  our GitHub repo but currently the best  available base model probably is the  Llama series from meta although it is  not commercially licensed  now one thing to point out is base  models are not assistance they don't  want to answer to they don't want to  make answers to your questions they just  want to complete documents so if you  tell them write a poem about the brand  cheese it will just you know it will  answer questions with more questions  it's just completing what it thinks as a  document  however you can prompt them in a  specific way for base models that that  is more likely to work so as an example  here's a poem about bread and cheese and  in that case it will auto-complete  correctly  you can even trick base models into  being assistance and the way you would  do this is you would create like a  specific few shot prompt that makes it  look like there's some kind of a  document between a human and assistant  and they're exchanging sort of um  information and then at the bottom you  sort of put your query at the end and  the base model will sort of like  condition itself into being like a  helpful assistant and kind of answer but  this is not very reliable and doesn't  work super well in practice although it  can be done  so instead we have a different path to  make actual gbt assistance not just base  model document completers and so that  takes us into supervised fine-tuning so  in the supervised fine tuning stage we  are going to collect small but high  quality data sets and in this case we're  going to ask human contractors to gather  data of the format of the form prompt  and ideal response and we're going to  collect lots of these typically tens of  thousands or something like that  and then we're going to still do  language modeling on this data so  nothing change algorithmically we're  just swapping out a training set so it  used to be internet documents which is a  high quantity local  four basically QA prompt response kind  of data and that is low quantity high  quality  so we still do language modeling and  then after training we get an sfd model  and you can actually deploy these models  and they are actual assistants and they  work to some extent let me show you what  an example demonstration might look like  so here's something that a human  contractor might come up with here's  some random prompt can you write a short  introduction about the relevance of the  term monopsony or something like that  and then the contractor also writes out  an ideal response and when they write  out these responses they are following  extensive labeling documentations and  they're being asked to be helpful  truthful and harmless and those labeling  instructions here you probably can't  read it another can I but they're long  and this is just people following  instructions and trying to complete  these prompts  so that's what the data set looks like  and you can train these models and this  works to some extent  now you can actually continue the  pipeline from here on and go into rlhf  reinforcement learning from Human  feedback that consists of both reward  modeling and reinforcement learning so  let me cover that and then I'll come  back to why you may want to go through  the extra steps and how that compares to  just sft models  so in the reward modeling step what  we're going to do is we're now going to  shift our data collection to be of the  form of comparisons so here's an example  of what our data set will look like  I have the same prompt identical prompt  on the top which is asking the assistant  to write a program or function that  checks if a given string is a palindrome  and then what we do is we take the sft  model which we've already trained and we  create multiple completions so in this  case we have three completions that the  model has created  and then we ask people to rank these  completions so if you stare at this for  a while and by the way these are very  difficult things to do to compare some  of these predictions and this can take  people even hours for a single prompt  completion pairs but let's say we  decided that one of these is much better  than the others and so on so we rank  them  then we can follow that with something  that looks very much kind of like a  binary classification on all the  possible pairs between these completions  so what we do now is we lay out our  prompt in rows and the prompts is  identical across all three rows here so  it's all the same prompt but the  completion of this vary and so the  yellow tokens are coming from the sft  model then what we do is we append  another special reward uh readout token  at the end and we basically only  supervise the Transformer at this single  green token and the Transformer will  predict some reward for how good that  completion is for that prompt  and so basically it makes a guess about  the quality of each completion and then  once it makes a guess for every one of  them we also have the ground truth which  is telling us the ranking of them and so  we can actually enforce that some of  these numbers should be much higher than  others and so on WE formulate this into  a loss function and we train our model  to make reward predictions that are  consistent with the ground truth coming  from the comparisons from all these  contractors so that's how we train our  reward model and that allows us to score  how good a completion is for a prompt  once we have a reward model this is we  can't deploy this because this is not  very useful as an assistant by itself  but it's very useful for the reinforce  reinforcement learning stage that  follows now because we have a reward  model we can score the quality of any  arbitrary completion for any given  prompt so what we do during  reinforcement learning is we basically  get again a large collection of prompts  and now we do reinforcement learning  with respect to the reward model so  here's what that looks like  we we take a single prompt we lay it out  in rows and now we use the sft we use  the basically the model we'd like to  train which is initialized at sft model  to create some completions in yellow  and then we append the reward token  again and we read off the reward  according to the reward model which is  now kept fixed it doesn't change anymore  and now the reward model tells us the  quality of every single completion for  each for these prompts and so what we  can do is we can now just basically  apply the same language modeling loss  function but we're currently training on  the yellow tokens and we are weighing  the language modeling objective by the  rewards indicated by the reward model so  as an example in the first row the  reward model said that this is a fairly  high score in completion and so all the  tokens that we happen to sample on the  first row are going to get reinforced  and they're going to get higher  probabilities for the future  conversely on the second row the reward  model really did not like this  completion negative 1.2 and so therefore  every single token that we sampled in  that second row is going to get a  slightly higher probability for the  future and we do this over and over on  many prompts on many batches and  basically we get a policy which creates  yellow tokens here and it basically all  of them all of the completions here will  score high according to the reward model  that we trained in the previous stage  so that's how we train uh that's what  the rohf pipeline is  now and then at the end you get a model  that you could deploy and so and as an  example chat GPT is an rlhf model but  some other models that you might come  across like for example of the kuna 13B  and so on these are sft models so we  have base models sft models and rlh  models and that's kind of like the state  of things there  now why would you want to do rlhf so one  answer that is kind of not that exciting  is that it just works better so this  comes from the instructor gbt paper  according to these experiments a while  ago now these PPO models are rlhf and we  see that they are basically just  preferred in a lot of comparisons when  we give them to humans so humans just  prefer out basically tokens that come  from our religion models compared to sft  models compared to base model that is  prompted to be an assistant and so it  just works better but you might ask why  why does it work better and I don't  think that there's a single like amazing  answer that the community has really  like agreed on but I will just offer one  uh one reason potentially and it has to  do with the asymmetry between how easy  computationally it is to compare versus  generate so let's take an example of  generating a haiku suppose I ask a model  to write a haiku about paper clips if  you're a contractor trying to give train  data then imagine being a contractor  collecting basically data for the sft  stage how are you supposed to create a  nice haiku for a paperclip you might  just not be very good at that but if I  give you a few examples of haikus you  might be able to appreciate some of  these haikus a lot more than others and  so judging which one of these is good is  much easier task and so basically this  asymmetry makes it so that comparisons  are a better way to potentially leverage  yourself as a human and your judgment to  create a slightly better model  now our religious models are not  strictly an improvement on the base  models in some cases so in particular  we've noticed for example that they lose  some entropy so that means that they  give More PT results they can output  lower variations uh like they can output  samples with lower variation than base  model so base model has lots of entropy  and we'll go with lots of diverse  outputs  so for example one one kind of place  where I still prefer to use a base model  is in the setup where  you basically have N Things and you want  to generate more things like it and so  here is an example that I just cooked up  I want to generate cool Pokemon names I  gave it seven Pokemon names and I asked  the base model to complete the document  and it gave me a lot more Pokemon names  uh these are fictitious I tried to look  them up I don't believe they're actual  Pokemons and uh this is the kind of task  that I think base model would be good at  because it still has lots of entropy it  will give you lots of diverse cool kind  of more things that look like whatever  you give it before  so this is what uh this is number having  all said all that these are kind of like  the assistant models that are probably  available to you at this point uh that  there's a team at Berkeley that ranked a  lot of the available assistant models  and gave them basically ELO ratings So  currently some of the best models of  course are gpt4 by far I would say  followed by Claude gvt 3.5 and then a  number of models some of these might be  available as weights like the kuna koala  Etc and the first three rows here are  all they're all rohf models and all of  the other models to my knowledge are sft  models I believe  okay so that's how we train these models  on the high level now I'm going to  switch gears and let's look at how we  can best apply and the GPT assistant  model to your problems now I would like  to work  in a setting of a concrete example so  let's  let's work with a concrete example here  let's say that you are working on an  article or a blog post and you're going  to write this sentence at the end  California's population is 53 times that  of Alaska so for some reason you want to  compare the populations of these two  states  think about the rich internal monologue  and Tool use and how much work actually  goes computationally in your brain to  generate this one final sentence so  here's maybe what that could look like  in your brain okay for this next step  let me blog uh for my blog let me  compare these two populations  okay first I'm going to obviously need  to get both of these populations now I  know that I probably don't know these  populations off the top of my head so  I'm kind of like aware of what I know or  don't know of my self-knowledge right  so I go I do some tool use and I go to  Wikipedia and I look up California's  population and Alaska's population  now I know that I should divide the two  but again I know that dividing three 9.2  by 0.74 is very unlikely to succeed  that's not the kind of thing that I can  do in my head and so therefore I'm gonna  rely on the calculator so I'm going to  use a calculator punch it in and see  that the output is roughly 53.  and then maybe I do some reflection and  Sanity checks in my brain so does 53  make sense well that's quite quite a  large fraction but then California is  the most popular state so maybe that  looks okay  so then I have all the information I  might need and now I get to the sort of  creative portion of writing so I might  start to write something like California  has 53 x times greater and then I think  to myself that's actually like really  awkward phrasing so let me actually  delete that and let me try again and so  as I'm writing I have this separate  process almost inspecting what I'm  writing and judging whether it looks  good or not  and then maybe I delete and maybe I  reframe it and then maybe I'm happy with  what comes out  so basically long story short a ton  happens under the hood in terms of your  internal monologue when you create  sentences like this but what does a  sentence like this look like when we are  training a GPT on it  From gpt's perspective this is just a  sequence of tokens  so GPT when it's reading or generating  these tokens it just goes  and each chunk is roughly the same  amount of computational work for each  token and these Transformers are not  very shallow networks they have about 80  layers of reasoning but 80 is still not  like too much and so this Transformer is  going to do its best to imitate but of  course the process here looks very very  different from the process that you took  so in particular in our final artifacts  in the data sets that we create and then  eventually feed to llms all of that  internal dialogue is completely stripped  and uh and unlike you the GPT will look  at every single token and spend the same  amount of compute on every one of them  and so you can't expect it to actually  like well you can't expect it to do to  uh sort of do too much work per token  so and also in particular basically  these Transformers are just like token  simulators so they don't know what they  don't know like they just imitate the  next token they don't know what they're  good at or not good at they just tried  their best imitate the next token they  don't reflect in the loop they don't  sanity check anything they don't correct  their mistakes along the way by default  they just uh sample token sequences  they don't have separate inner monologue  streams in their head right that are  evaluating what's happening now they do  have some sort of cognitive advantages I  would say and that is that they do  actually have a very large fact-based  knowledge across a vast number of areas  because they have say several 10 billion  parameters so it's a lot of storage for  a lot of facts  but and they also I think have a  relatively large and perfect working  memory so whatever fixed into the  whatever fits into the context window is  immediately available to the Transformer  through its internal self-attention  mechanism and so it's kind of like um  Perfect Memory but it's got that finite  size but the Transformer has a very  direct access to it and so it can like  losslessly remember anything that is  inside its context window  so that's kind of how I would compare  those two and the reason I bring all of  this up is because I think to a large  extent prompting is just making up for  this sort of cognitive difference  between these two kind of architectures  um like our brains here and llm brains  you can look at it that way almost  so here's one thing that people found  for example works pretty well in  practice especially if your tasks  require reasoning you can't expect the  Transformer to make to to do too much  reasoning per token and so you have to  really spread out the reasoning across  more and more tokens so for example you  can't give a Transformer a very  complicated question and expect it to  get the answer in a single token there's  just not enough time for it these  Transformers need tokens to think quote  unquote I like to say sometimes and so  this is some of the things that work  well you may for example have a few shot  prompt that shows the Transformer that  it should like show its work when it's  answering a question when it's answering  the question and if you give a few  examples the Transformer will imitate  that template and it will just end up  working out better in terms of its  evaluation  Additionally you can elicit this kind of  behavior from the Transformer by saying  let's think step by step because this  condition is the transformer into sort  of like showing its work and because it  kind of snaps into a mode of showing its  work it's going to do less computational  work per token and so it's more likely  to succeed as a result because it's  making  um slower reasoning over time  here's another example this one is  called self-consistency we saw that we  had the ability to start writing and  then if it didn't work out I can try  again and I can try multiple times and  uh and maybe  um select the one that worked best so in  these kinds of approaches you may sample  not just once but you may sample  multiple times and then have some  process for finding the ones that aren't  good and then keeping just those samples  or doing a majority vote or something  like that so basically these  Transformers in the process as they  predict the next token just like you  they can get unlucky and they can they  could sample and not a very good token  and they can go down sort of like a  blind alley in terms of reasoning and so  unlike you they cannot recover from that  they are stuck with every single token  they sample and so they will continue  the sequence even if they even know that  this sequence is not going to work out  so give them the ability to look back  inspect or try to find uh try to  basically sample around it  here's one technique also you could it  turns out that actually llms like they  know when they've screwed up so as an  example  say you asked the model to generate a  poem that does not rhyme and it might  give you a poem but it actually Rhymes  but it turns out that especially for the  bigger models like gpt4 you can just ask  it did you meet the assignment and  actually gpt4 knows very well that it  did not meet the assignment it just kind  of got unlucky in its sampling and so it  will tell you no I didn't actually meet  the assignment here's let me try again  but without you prompting it it doesn't  even like it doesn't know it doesn't  know to revisit and uh and so on so you  have to make up for that in your prompt  you have to get it to check if you don't  ask it to check it's not going to check  by itself it's just a token simulator  I think more generally a lot of these  techniques fall into the bucket of what  I would say recreating our system too so  you might be familiar with the system  one system to thinking for humans system  one is a fast automatic process and I  think kind of corresponds to like an llm  just sampling tokens  and system two is the slower deliberate  planning sort of part of your brain  and so this is a paper actually from  just last week because this space is  pretty quickly evolving it's called tree  of thought and in tree of thought uh the  authors of this paper propose  maintaining multiple completions for any  given prompt and then they are also  scoring them along the way and keeping  the ones that are going well if that  makes sense and so a lot of people are  like really playing around with kind of  um prompt engineering to um to basically  bring back some of these abilities that  we sort of have in our brain for llms  now one thing I would like to note here  is that this is not just a prompt this  is actually prompts that are together  used with some python glue code because  you don't you actually have to maintain  multiple prompts and you also have to do  some tree search algorithm here um to  like figure out which prompt to expand  Etc so it's a symbiosis of python glue  code and individual prompts that are  called in a while loop or in a bigger  algorithm  I also think there's a really cool  parallel here to alphago alphago has a  policy for placing the next Stone when  it plays go and its policy was trained  originally by imitating humans but in  addition to this policy it also does  Monte Carlo tree search and basically it  will play out a number of possibilities  in its head and evaluate all of them and  only keep the ones that work well and so  I think this is kind of an equivalent of  alphago but for text  if that makes sense  so just like tree of thought I think  more generally people are starting to  like really Explore More General  techniques of not just the simple  question answer prompt but something  that looks a lot more like python blue  code stringing together many problems so  on the right I have an example from this  paper called react where they structure  the answer to a prompt as a sequence of  thought action observation thought  action observation and it's a full  rollout a kind of a thinking process to  answer the query and in these actions  the model is also allowed to to lose  on the left I have an example of rogpt  and now rgbt by the way became as a  project that I think got a lot of hype  recently and I think uh but I think I  still find it kind of inspirationally  interesting it's um it's a project that  allows an llm to sort of keep task list  and continue to recursively break down  tasks and I don't think this currently  works very well and I would not advise  people to use it in Practical  applications I just think it's something  to generally take inspiration from in  terms of where this is going I think  over time  so that's kind of like giving our model  system to thinking the next thing that I  find kind of interesting is  this following survey would say almost  psychological Quirk of llms is that llms  don't want to succeed they want to  imitate  you want to succeed and you should ask  for it  so what I mean by that is  when Transformers are trained they have  training sets and there can be an entire  spectrum of performance qualities in  their training data so for example there  could be some kind of a prompt for some  Physics question or something like that  and there could be a student solution  that is completely wrong but there can  also be an expert answer that is  extremely right and Transformers can't  tell the difference between like look I  mean they know they know about low  quality Solutions and high quality  Solutions but by default they want to  imitate all of it because they're just  trained on language modeling and so at  test time you actually have to ask for a  good performance so in this example in  this paper it's um they tried various  prompts and let's think step by step was  very powerful because it's sort of like  spread out the reasoning of remaining  tokens but what worked even better is  let's work this out in a step-by-step  way to be sure we have the right answer  and so it's kind of like a conditioning  on getting a right answer and this  actually makes the Transformer work  better because the Transformer doesn't  have to now hedge its probability Mass  on low quality Solutions as ridiculous  as that sounds and so basically don't be  feel free to ask for a strong solution  say something like you are a leading  expert on this topic pretend you have  iq120 Etc but don't try to ask for too  much IQ because if you ask for IQ of  like 400 you might be out of data  distribution or even worse you could be  in data distribution for some like  sci-fi stuff and it will start to like  take on some sci-fi or like role-playing  or something like that so you have to  find like the right amount of IQ I think  it's got some U-shaped curve there  next up  as we saw when we are trying to solve  problems we know we are good at and what  we're not good at and we Lean On Tools  computationally you want to do the same  potentially with your uh llms so in  particular we may want to give them  calculators code interpreters and so on  the ability to do search and uh there's  a lot of techniques for doing that one  thing to keep in mind again is that  these Transformers by default may not  know what they not don't know so you may  even want to tell the transformer in the  prompt you are not very good at mental  arithmetic whenever you need to do very  large number addition multiplication or  whatever instead use this calculator  here's how you use the calculator use  this token combination etc etc so you  have to actually like spell it out  because the model by default doesn't  know what it's good at or not good at  necessarily just like you and I you and  I might be  next up I think something that is very  interesting is we went from a world that  was retrieval only all the way the  pendulum has swung to The Other Extreme  where it's memory only in lens but  actually there's this entire space in  between of these retrieval augmented  models and this works extremely well in  practice  as I mentioned the context window of a  transformer is its working memory if you  can load the working memory with any  information that is relevant to the task  the model will work extremely well  because it can immediately access all  that memory  and so I think a lot of people are  really interested in  uh basically retrieval augmented  generation and on the bottom I have like  an example of llama index which is one  sort of data connector to lots of  different types of data and you can uh  all you can make it you can index all of  that data and you can make it accessible  to llms and the emerging recipe there is  you take relevant documents you split  them up into chunks you embed all of  them and you basically get embedding  vectors that represent that data you  store that in a vector store and then at  test time you make some kind of a query  to your vector store and you fetch  chunks that might be relevant to your  task and you stuff them into the prompt  and then you generate so this can work  quite well in practice so this is I  think similar to when you're and I solve  problems you can do everything from your  memory and Transformers have very large  and extensive memory but also it really  helps to reference some primary  documents so when you whenever you find  yourself going back to a textbook to  find something or whenever you find  yourself going back to the documentation  of a library to look something up the  transform transformers definitely when I  do that too you you have a mem you have  some memory over how some documentation  of a library works but it's much better  to look it up so the same same applies  here  next I wanted to briefly talk about  constraint prompting I also find this  very interesting  uh this is basically techniques for  um  forcing a certain template in the  outputs of llms so guidance is one  example from Microsoft actually and uh  here we are enforcing that the output  from the llm will be Json and this will  actually guarantee that the output will  take on this form because they go in and  they mess with the probabilities of all  the different tokens that come out of  the Transformer and if they clamp those  tokens and then the Transformer is only  filling in the blanks here and then you  can enforce additional restrictions on  what could go into those blanks so this  might be really helpful and I think this  kind of constraint sampling is also  extremely interesting  I also wanted to say a few words about  fine tuning it is the case that you can  get really far with prompt engineering  but it's also possible to think about  fine-tuning your models now fine-tuning  models means that you are actually going  to change the weights of the model  it is becoming a lot more accessible to  do this in practice and that's because  of a number of techniques that have been  developed and have libraries for uh very  recently so for example parameter  efficient fine-tuning techniques like  Laura make sure that you're only Trend  you're only training small Spar species  of your model so most of the model is  kept clamped at the base model and some  pieces of it are allowed to change and  this still works pretty well empirically  and makes it much cheaper to sort of  tune only small pieces of your model  uh there's also it also means that  because most of your model is clamped  you can use very low Precision inference  for computing those parts because they  are not going to be updated by gradient  descent and so that makes everything a  lot more efficient as well and in  addition we have a number of Open Source  high quality based models currently as I  mentioned I think llama is quite nice  although it is not commercially licensed  I believe right now  some things to keep in mind is that  basically fine tuning is is a lot more  technically involved it requires a lot  more I think technical expertise to do  to do right it requires human data  contractors for data sets and or  synthetic data pipelines that can be  pretty complicated this will definitely  slow down your iteration cycle by a lot  and I would say on a high level sft is  achievable because it is just your  continuing language modeling task It's  relatively straightforward but rlhref I  would say is very much research  territory and is even much harder to get  to work and so I would probably not  advise that someone just tries to roll  their own rlh of implementation these  things are pretty unstable very  difficult to train uh not something that  is I think very beginner friendly right  now and it's also potentially likely  also to to change pretty rapidly still  so I think these are my sort of default  recommendations right now  I would break up your task into two  major parts number one achieve your top  performance and number two optimize your  performance in that order  number one the best performance will  currently come from GT4 model it is the  most capable of by far  use prompts that are very detailed they  have lots of task contents relative  relevant information and instructions  think along the lines of what would you  tell a task contractor if they can't  email you back but then also keep in  mind that a task contractor is a human  and they have inner monologue and  they're very clever Etc llms do not  possess those qualities so make sure to  think through the psychology of the llm  almost and cater prompts to that  retrieve and add any relevant context  and information to these prompts  basically refer to a lot of the prompt  engineering techniques some of them are  highlighted in the slides above but also  this is a very large space and I would  just advise you to look for prompt  engineering techniques online there's a  lot to cover there  experiment with few shop examples what  this refers to is you don't just want to  tell you want to show whenever it's  possible so give it examples of  everything that helps it really  understand what you mean if you can  tools and plugins to offload a tasks  that are difficult for llms natively uh  and then think about not just a single  prompt and answer think about potential  chains and reflection and how you glue  them together and how you could  potentially make multiple samples and so  on  finally if you think you've squeezed out  prompt engineering which I think you  should stick with for a while look at  some potentially fine-tuning a model to  your application but expect this to be a  lot more slower and evolved and then  there's an expert fragile research Zone  here and I would say that is rlhf which  currently does work a bit better than  sft if you can get it to work but again  this is uh pretty involved I would say  and to optimize your costs try to  explore lower capacity models or shorter  prompts and so on  I also wanted to say a few words about  the use cases in which I think llms are  currently well suited for so in  particular note that there's a large  number of limitations to llms today and  uh so I would keep that definitely in  mind for all your applications models  and this by the way could be an entire  talk so I don't have time to cover it in  full detail models may be biased they  may fabricate hallucinate information  they may have reasoning errors they may  struggle in entire classes of  applications they have knowledge cutoffs  so they might not know any information  above say September 2021 they are  susceptible to a large range of attacks  which are sort of like coming out on  Twitter daily including prompt injection  jailbreak attacks data poisoning attacks  and so on so my recommendation right now  is use llms in low stakes applications  combine them with always with human  oversight use them as a source of  inspiration and suggestions and think  co-pilots instead of completely  autonomous agents that are just like  performing a task somewhere it's just  not clear that the models are there  right now  so I wanted to close by saying that gpt4  is an amazing artifact I'm very thankful  that it exists and it's it's beautiful  it has a ton of knowledge across so many  areas it can do math code and so on and  in addition there's this thriving  ecosystem of everything else that is  being built and incorporated into into  the ecosystem uh some of some of the  things these things I've talked about  and all of this power is accessible at  your fingertips  so here's everything that's needed in  terms of code to ask GPT for a question  to prompt it and get a response in this  case I said uh can you say something to  inspire the audience of Microsoft build  2023  and I just punched this into Python and  verbatim gpt4 said the following  uh and by the way I did not know that  they used this trick in the keynote so I  thought I was being clever but  but it is really good at this it says  ladies and gentlemen Invaders and  Trailblazers Microsoft build 2023  welcome to the Gathering of Brilliant  Minds like no other you are The  Architects of the future The Visionaries  molding the digital realm in which  Humanity thrives embrace the Limitless  possibilities of Technologies and let  your ideas soar as high as your  imagination together let's create a more  connected remarkable and inclusive world  for generations to come get ready to  unleash your creativity canvas the  unknown and turn dreams into reality  your Journey Begins today  okay\\n\\n\\n[]\\n\\n\\n\\n\\n\\n[]\\n\\n\\n\\n\\n\\nGBT \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n GPT \\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n 99\\n\\n\\n\\n\\n\\n GPU\\n\\n\\n\\n\\n\\n\\n\\n GPU \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMeta \\n\\n llama \\n\\n\\n\\n\\n\\n\\n\\n C4\\n\\n\\n\\n\\n\\nGitHub  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGPT \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ngpts \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n Transformer \\n\\n\\n\\n\\n\\n gpt4 \\n\\n\\n\\n \\n\\ngpt3  gpt3 \\n\\n\\n\\n llama  meta \\n\\n\\n\\n\\n\\n pre   - \\n\\n\\n\\n10 000 \\n\\n\\n204000  100000 \\n\\n\\n\\nGPT \\n\\n\\n\\n\\n\\n llama \\n\\n650 \\n\\n llama\\n\\n gvt3  1750\\n\\n 65b  llama \\n\\n\\n\\n\\n\\n \\n\\n 1.4 \\n\\n3000 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformer \\n\\n\\n\\n\\n\\n\\n\\n 65b \\n\\n 2000 GPU  21 \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformer  B\\n\\n\\n\\n\\n\\nB  T T \\n\\n  10.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Transformer\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n Transformer\\n\\n\\n\\nTransformer \\n\\n\\n\\n\\n\\n Transformer \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n 50 257 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n 513\\n\\n\\n\\n\\n\\nTransformers \\n\\n\\n\\n\\n\\n \\n\\nTransformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n GPT\\n\\n\\n\\n GPT\\n\\nGPT \\n\\n\\n\\n\\n\\n\\n  GPT \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nTransformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformer \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n NLP \\n\\n\\n\\n\\n\\n\\n\\n Transformer\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n GPT \\n\\ngpt2 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n QA qaqa \\n\\n\\n\\nq Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\ngpt4 \\n\\n API  gpt4 \\n\\n \\n\\n\\n\\n\\n\\n gpt3 \\n\\n DaVinci  API  gpt2 \\n\\n\\n\\n GitHub \\n\\n\\n\\nmetaLlama\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n GBT \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n QA \\n\\n\\n\\n\\n\\n\\n\\n sfd \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n rlhf \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n sft \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ntop \\n\\n\\n\\n\\n\\n\\n\\n sft \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n sft\\n\\n \\n\\n\\n\\n\\n\\n Transformer\\n\\n Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n sft \\n\\n\\n\\n sft \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n 1.2 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n rohf \\n\\n \\n\\n GPT  rlhf \\n\\n\\n\\n kuna 13B \\n\\n sft \\n\\n sft  rlh\\n\\n\\n\\n\\n rlhf \\n\\n\\n\\n\\n\\n GBT \\n\\n\\n\\n PPO  rlhf \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n sft\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n sft \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n PT \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n N \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n ELO  \\n\\n\\n\\n gpt4 \\n\\n Claude gvt 3.5 \\n\\n \\n\\n kuna \\n\\n\\n\\nrohf \\n\\n sft\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n GPT \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n 53 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n 3 9.2\\n\\n 0.74 \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n 53\\n\\n\\n\\n  53\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n 53 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n GPT \\n\\n gpt \\n\\n\\n\\n GPT \\n\\n\\n\\n\\n\\n token \\n\\n Transformer \\n\\n 80\\n\\n 80 \\n\\n Transformer\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n llms\\n\\n\\n\\nGPT \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Transformer \\n\\n \\n\\n Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n llm \\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nTransformer \\n\\n\\n\\n\\n\\n\\n\\n Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n Transformer \\n\\n\\n\\n\\n\\n\\nTransformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Transformer \\n\\n Transformer\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n' \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nllms\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n gpt4 \\n\\n\\n\\n gpt4 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n python \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n python \\n\\n\\n\\n while \\n\\n\\n alphago \\n\\n alphago \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nalphago \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPython\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n rogpt \\n\\n rgbt \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n llm \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n llms  llms\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Transformer \\n\\n Transformer\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n120\\n\\n\\n\\n 400\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n U \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n LLMS \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n llama \\n\\n\\n\\n\\n\\n \\n\\n\\n\\nllms \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n llms \\n\\n\\n\\n \\n\\n\\n\\nllm  Json\\n\\n\\n\\n\\n\\n\\n\\n Transformer \\n\\n \\n\\n Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Laura \\n\\n \\n\\n Spar \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n llama \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n/\\n\\n\\n\\n\\n\\n\\n\\n  sft \\n\\n\\n\\n\\n\\n rlhref \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n rlh \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n GT4 \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\nLLM\\n\\n\\n\\nLLM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n llms \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n rlhf\\n\\n sft \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n llms \\n\\n\\n\\n\\n\\n llms \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n 2021  9  \\n\\n\\n\\n\\n\\nTwitter \\n\\n\\n\\n\\n\\n llms\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n gpt4\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nGPT \\n\\n\\n\\n\\n\\n  Microsoft build 2023 \\n\\n\\n Python \\n\\n gpt4 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it is good practice to use the with keyword when dealing with file objects\n",
    "with open('build.txt', encoding=\"utf-8\") as f:\n",
    "    read_data = f.read()\n",
    "\n",
    "read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5d9e989-b30b-43d7-b012-9f936ca831f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"foreign  [Music]  researcher and founding member of open  AI Andre carpathy  [Applause]  hi everyone  I'm happy to be here to tell you about  the state of gbt and more generally  about the rapidly growing ecosystem of  large language models  so I would like to partition the talk  into two parts  in the first part I would like to tell  you about how we train GPT assistance  and then in the second part we will we  are going to take a look at how we can  use these assistants effectively for  your applications  so first let's take a look at the  emerging recipe for how to train these  assistants and keep in mind that this is  all very new and still rapidly evolving  but so far the recipe looks something  like this  now this is kind of a complicated slide  so I'm going to go through it piece by  piece but roughly speaking we have four  major stages free training supervised  fine tuning reward modeling  reinforcement learning and they follow  each other serially  now in each stage we have a data set  that is that powers that stage we have  an algorithm that for our purposes will  be a objective and a and over for uh for  training the neural network and then we  have a resulting model and then there's  some notes on the bottom so the first  stage we're going to start with is the  pre-training stage now this stage is  kind of special in this diagram and this  diagram is not to scale because this  stage is where all of the computational  work basically happens this is 99 of the  training uh compute time and also flops  and so this is where we are dealing with  internet scale data sets with thousands  of gpus in the store computer and also  months of training potentially the other  three stages are fine-tuning stages that  are much more along the lines of a small  few number of gpus and hours or days  so let's take a look at the pre-training  stage to achieve a base model  first we're going to gather a large  amount of data here's an example of what  we call a data mixture that comes from  this uh paper that was released by meta  where they released this llama based  model now you can see roughly the kinds  of data sets that enter into these  collections so we have common crawl  which is just a web scrape C4 which is  also a common crawl and then some high  quality data sets as well so for example  GitHub Wikipedia books archive stack  exchange and so on these are all mixed  up together and then they are sampled  According to some given proportions and  that forms the training set for the  neural net for the GPT  now before we can actually train on this  data we need to go through one more  pre-processing step and that is  tokenization and this is basically a  translation of the raw text that we  scrape from the internet into sequences  of integers because that's the native  representation over which gpts function  now this is a lossless kind of  translation between pieces of text and  tokens and integers and there are a  number of algorithms for this stage  typically for example you could use  something like byte pair encoding which  iteratively merges little text chunks  and groups them into tokens  and so here I'm showing some example  chunks of these tokens and then this is  the raw integer sequence that will  actually feed into a Transformer  now here I'm showing two uh sort of like  examples for hyper parameters that  govern this stage so gpt4 we did not  release too much information about how  it was trained and so on so I'm using  gpt3's numbers but gpt3 is of course a  little bit old by now about three years  ago but llama is a fairly recent model  from meta so these are roughly the  orders of magnitude that we're dealing  with when we're doing pre-training  the vocabulary size is usually a couple  10 000 tokens the context length is  usually something like two thousand four  thousand or nowadays even 100 000 and  this governs the maximum number of  integers that the GPT will look at when  it's trying to predict the next integer  in a sequence  um you can see that roughly the number  of parameters is say 65 billion for  llama now even though llama has only 65b  parameters compared to gvt3's 175  billion parameters llama is a  significantly more powerful model and  intuitively that's because the model is  trained for significantly longer in this  case 1.4 trillion tokens instead of just  300 billion tokens so you shouldn't  judge the power of a model just by the  number of parameters that it contains  below I'm showing some tables of rough  number of rough hyper parameters that  typically go into specifying the  Transformer neural network so the number  of heads the dimension size number of  layers and so on and on the bottom I'm  showing some training hyper parameters  so for example to train the 65b model  meta used 2000 gpus roughly 21 days  train of training and and roughly  several million dollars and so that's  the rough orders of magnitude that you  should have in mind for the pre-training  stage  now when we're actually pre-chaining  what happens roughly speaking we are  going to take our tokens and we're going  to lay them out into Data batches so we  have these arrays that will feed into  the Transformer and these arrays are B  the batch size and these are all  independent examples stacked up in rows  and B by T T being the maximum context  length so in my picture I only have 10.  the context length so this could be two  thousand four thousand Etc so these are  extremely long rows and what we do is we  take these documents and we pack them  into rows and we delimit them with these  Special end of text tokens uh basically  it's on the Transformer where a new  document begins  and so here I have a few examples of  documents and then I've stretched them  out into uh into this input  now we're going to feed all of these  numbers into Transformer and let me let  me just focus on a single particular  cell but the same thing will happen at  every every cell in this diagram so  let's look at the green cell the green  cell is going to take a look at all of  the tokens before it so all of the  tokens in yellow and we're going to feed  that entire context into the Transformer  neural network  and the Transformer is going to try to  predict the next token in the sequence  in this case in red  now the Transformer I don't have too  much time to unfortunately go into the  full details of this neural network  architecture is just a large blob of  neural net stuff for our purposes and  it's got several 10 billion parameters  typically or something like that and of  course as they tune these parameters  you're getting slightly different uh  predicted distributions for every single  one of these cells  and so for example if our vocabulary  size is 50 257 tokens then we're going  to have that many numbers because we  need to specify a probability  distribution for what comes next so  basically we have a probability for  whatever may follow now in this specific  example for the specific cell 513 will  come next and so we can use this as a  source of supervision to update our  Transformers weights and so we're  applying this basically on every single  cell in parallel and we keep swap  swapping batches and we're trying to get  the Transformer to make the correct  predictions over what token comes next  in a sequence  so let me show you more concretely what  this looks like when you train one of  these models this is actually coming  from a New York Times and they trained a  small GPT on Shakespeare and so here's a  small snippet of Shakespeare and they  trained a GPT on it now in the beginning  at initialization the GPT starts with  completely random weights so you're just  getting completely random outputs as  well  but over time as you train uh the GPT  longer and longer you are getting more  and more coherent and consistent uh sort  of samples from the model and the way  you sample from it of course is you uh  predict what comes next you sample from  that distribution and you keep feeding  that back into the process and you can  basically uh sample large sequences and  so by the end you see that the  Transformer has learned about words and  where to put spaces and where to put  commas and so on and so we're making  more and more consistent predictions  over time  these are the kinds of plots that you're  looking at when you're doing model  pre-training effectively we're looking  at the loss function over time as you  train and low loss means that our  Transformer is predicting the correct uh  is giving a higher probability to the  correct next integer in a sequence  now what are we going to do with this  model once we've trained it after a  month well the first thing that we  noticed we the field is that these  models basically in the process of  language modeling learn very powerful  General representations and it's  possible to very efficiently fine-tune  them for any arbitrary Downstream task  you might be interested in so as an  example if you're interested in  sentiment classification the approach  used to be that you collect a bunch of  positives and negatives and then you  train some kind of an NLP model for for  that but the new approach is ignore  sentiment classification  go off and do large language model  pre-training train the large Transformer  and then you can only you may only have  a few examples and you can very  efficiently fine tune your model for  that task and so this works very well in  practice and the reason for this is that  basically the Transformer is forced to  multitask a huge amount of tasks in the  language modeling task because just just  in terms of predicting the next token  it's forced to understand a lot about  the structure of the of the text and all  the different concepts they're in  so that was GPT one now around the time  of gpt2 people noticed that actually  even better than fine tuning you can  actually prompt these models very  effectively so these are language models  and they want to complete documents so  you can actually trick them into  performing tasks just by arranging these  fake documents so in this example for uh  for example we have some passage and  then we sort of like do QA qaqa this is  called a few shot prompt and then we do  q and then as the Transformer is trying  to complete the document it's actually  answering our question and so this is an  example of prompt engineering a base  model making the belief that it's sort  of imitating a document and getting it  to perform a task  and so this kicked off I think the era  of I would say prompting over  fine-tuning and seeing that this  actually can work extremely well on a  lot of problems even without training  any neural networks fine-tuning or so on  now since then we've seen an entire  evolutionary tree of Base models that  everyone has trained not all of these  models are available for example the  gpt4 base model was never released the  gpt4 model that you might be interacting  with over API is not a base model it's  an assistant model and we're going to  cover how to get those in a bit gpt3  based model is available via the API  under the name DaVinci and gpt2 base  model is available even as weights on  our GitHub repo but currently the best  available base model probably is the  Llama series from meta although it is  not commercially licensed  now one thing to point out is base  models are not assistance they don't  want to answer to they don't want to  make answers to your questions they just  want to complete documents so if you  tell them write a poem about the brand  cheese it will just you know it will  answer questions with more questions  it's just completing what it thinks as a  document  however you can prompt them in a  specific way for base models that that  is more likely to work so as an example  here's a poem about bread and cheese and  in that case it will auto-complete  correctly  you can even trick base models into  being assistance and the way you would  do this is you would create like a  specific few shot prompt that makes it  look like there's some kind of a  document between a human and assistant  and they're exchanging sort of um  information and then at the bottom you  sort of put your query at the end and  the base model will sort of like  condition itself into being like a  helpful assistant and kind of answer but  this is not very reliable and doesn't  work super well in practice although it  can be done  so instead we have a different path to  make actual gbt assistance not just base  model document completers and so that  takes us into supervised fine-tuning so  in the supervised fine tuning stage we  are going to collect small but high  quality data sets and in this case we're  going to ask human contractors to gather  data of the format of the form prompt  and ideal response and we're going to  collect lots of these typically tens of  thousands or something like that  and then we're going to still do  language modeling on this data so  nothing change algorithmically we're  just swapping out a training set so it  used to be internet documents which is a  high quantity local  four basically QA prompt response kind  of data and that is low quantity high  quality  so we still do language modeling and  then after training we get an sfd model  and you can actually deploy these models  and they are actual assistants and they  work to some extent let me show you what  an example demonstration might look like  so here's something that a human  contractor might come up with here's  some random prompt can you write a short  introduction about the relevance of the  term monopsony or something like that  and then the contractor also writes out  an ideal response and when they write  out these responses they are following  extensive labeling documentations and  they're being asked to be helpful  truthful and harmless and those labeling  instructions here you probably can't  read it another can I but they're long  and this is just people following  instructions and trying to complete  these prompts  so that's what the data set looks like  and you can train these models and this  works to some extent  now you can actually continue the  pipeline from here on and go into rlhf  reinforcement learning from Human  feedback that consists of both reward  modeling and reinforcement learning so  let me cover that and then I'll come  back to why you may want to go through  the extra steps and how that compares to  just sft models  so in the reward modeling step what  we're going to do is we're now going to  shift our data collection to be of the  form of comparisons so here's an example  of what our data set will look like  I have the same prompt identical prompt  on the top which is asking the assistant  to write a program or function that  checks if a given string is a palindrome  and then what we do is we take the sft  model which we've already trained and we  create multiple completions so in this  case we have three completions that the  model has created  and then we ask people to rank these  completions so if you stare at this for  a while and by the way these are very  difficult things to do to compare some  of these predictions and this can take  people even hours for a single prompt  completion pairs but let's say we  decided that one of these is much better  than the others and so on so we rank  them  then we can follow that with something  that looks very much kind of like a  binary classification on all the  possible pairs between these completions  so what we do now is we lay out our  prompt in rows and the prompts is  identical across all three rows here so  it's all the same prompt but the  completion of this vary and so the  yellow tokens are coming from the sft  model then what we do is we append  another special reward uh readout token  at the end and we basically only  supervise the Transformer at this single  green token and the Transformer will  predict some reward for how good that  completion is for that prompt  and so basically it makes a guess about  the quality of each completion and then  once it makes a guess for every one of  them we also have the ground truth which  is telling us the ranking of them and so  we can actually enforce that some of  these numbers should be much higher than  others and so on WE formulate this into  a loss function and we train our model  to make reward predictions that are  consistent with the ground truth coming  from the comparisons from all these  contractors so that's how we train our  reward model and that allows us to score  how good a completion is for a prompt  once we have a reward model this is we  can't deploy this because this is not  very useful as an assistant by itself  but it's very useful for the reinforce  reinforcement learning stage that  follows now because we have a reward  model we can score the quality of any  arbitrary completion for any given  prompt so what we do during  reinforcement learning is we basically  get again a large collection of prompts  and now we do reinforcement learning  with respect to the reward model so  here's what that looks like  we we take a single prompt we lay it out  in rows and now we use the sft we use  the basically the model we'd like to  train which is initialized at sft model  to create some completions in yellow  and then we append the reward token  again and we read off the reward  according to the reward model which is  now kept fixed it doesn't change anymore  and now the reward model tells us the  quality of every single completion for  each for these prompts and so what we  can do is we can now just basically  apply the same language modeling loss  function but we're currently training on  the yellow tokens and we are weighing  the language modeling objective by the  rewards indicated by the reward model so  as an example in the first row the  reward model said that this is a fairly  high score in completion and so all the  tokens that we happen to sample on the  first row are going to get reinforced  and they're going to get higher  probabilities for the future  conversely on the second row the reward  model really did not like this  completion negative 1.2 and so therefore  every single token that we sampled in  that second row is going to get a  slightly higher probability for the  future and we do this over and over on  many prompts on many batches and  basically we get a policy which creates  yellow tokens here and it basically all  of them all of the completions here will  score high according to the reward model  that we trained in the previous stage  so that's how we train uh that's what  the rohf pipeline is  now and then at the end you get a model  that you could deploy and so and as an  example chat GPT is an rlhf model but  some other models that you might come  across like for example of the kuna 13B  and so on these are sft models so we  have base models sft models and rlh  models and that's kind of like the state  of things there  now why would you want to do rlhf so one  answer that is kind of not that exciting  is that it just works better so this  comes from the instructor gbt paper  according to these experiments a while  ago now these PPO models are rlhf and we  see that they are basically just  preferred in a lot of comparisons when  we give them to humans so humans just  prefer out basically tokens that come  from our religion models compared to sft  models compared to base model that is  prompted to be an assistant and so it  just works better but you might ask why  why does it work better and I don't  think that there's a single like amazing  answer that the community has really  like agreed on but I will just offer one  uh one reason potentially and it has to  do with the asymmetry between how easy  computationally it is to compare versus  generate so let's take an example of  generating a haiku suppose I ask a model  to write a haiku about paper clips if  you're a contractor trying to give train  data then imagine being a contractor  collecting basically data for the sft  stage how are you supposed to create a  nice haiku for a paperclip you might  just not be very good at that but if I  give you a few examples of haikus you  might be able to appreciate some of  these haikus a lot more than others and  so judging which one of these is good is  much easier task and so basically this  asymmetry makes it so that comparisons  are a better way to potentially leverage  yourself as a human and your judgment to  create a slightly better model  now our religious models are not  strictly an improvement on the base  models in some cases so in particular  we've noticed for example that they lose  some entropy so that means that they  give More PT results they can output  lower variations uh like they can output  samples with lower variation than base  model so base model has lots of entropy  and we'll go with lots of diverse  outputs  so for example one one kind of place  where I still prefer to use a base model  is in the setup where  you basically have N Things and you want  to generate more things like it and so  here is an example that I just cooked up  I want to generate cool Pokemon names I  gave it seven Pokemon names and I asked  the base model to complete the document  and it gave me a lot more Pokemon names  uh these are fictitious I tried to look  them up I don't believe they're actual  Pokemons and uh this is the kind of task  that I think base model would be good at  because it still has lots of entropy it  will give you lots of diverse cool kind  of more things that look like whatever  you give it before  so this is what uh this is number having  all said all that these are kind of like  the assistant models that are probably  available to you at this point uh that  there's a team at Berkeley that ranked a  lot of the available assistant models  and gave them basically ELO ratings So  currently some of the best models of  course are gpt4 by far I would say  followed by Claude gvt 3.5 and then a  number of models some of these might be  available as weights like the kuna koala  Etc and the first three rows here are  all they're all rohf models and all of  the other models to my knowledge are sft  models I believe  okay so that's how we train these models  on the high level now I'm going to  switch gears and let's look at how we  can best apply and the GPT assistant  model to your problems now I would like  to work  in a setting of a concrete example so  let's  let's work with a concrete example here  let's say that you are working on an  article or a blog post and you're going  to write this sentence at the end  California's population is 53 times that  of Alaska so for some reason you want to  compare the populations of these two  states  think about the rich internal monologue  and Tool use and how much work actually  goes computationally in your brain to  generate this one final sentence so  here's maybe what that could look like  in your brain okay for this next step  let me blog uh for my blog let me  compare these two populations  okay first I'm going to obviously need  to get both of these populations now I  know that I probably don't know these  populations off the top of my head so  I'm kind of like aware of what I know or  don't know of my self-knowledge right  so I go I do some tool use and I go to  Wikipedia and I look up California's  population and Alaska's population  now I know that I should divide the two  but again I know that dividing three 9.2  by 0.74 is very unlikely to succeed  that's not the kind of thing that I can  do in my head and so therefore I'm gonna  rely on the calculator so I'm going to  use a calculator punch it in and see  that the output is roughly 53.  and then maybe I do some reflection and  Sanity checks in my brain so does 53  make sense well that's quite quite a  large fraction but then California is  the most popular state so maybe that  looks okay  so then I have all the information I  might need and now I get to the sort of  creative portion of writing so I might  start to write something like California  has 53 x times greater and then I think  to myself that's actually like really  awkward phrasing so let me actually  delete that and let me try again and so  as I'm writing I have this separate  process almost inspecting what I'm  writing and judging whether it looks  good or not  and then maybe I delete and maybe I  reframe it and then maybe I'm happy with  what comes out  so basically long story short a ton  happens under the hood in terms of your  internal monologue when you create  sentences like this but what does a  sentence like this look like when we are  training a GPT on it  From gpt's perspective this is just a  sequence of tokens  so GPT when it's reading or generating  these tokens it just goes  and each chunk is roughly the same  amount of computational work for each  token and these Transformers are not  very shallow networks they have about 80  layers of reasoning but 80 is still not  like too much and so this Transformer is  going to do its best to imitate but of  course the process here looks very very  different from the process that you took  so in particular in our final artifacts  in the data sets that we create and then  eventually feed to llms all of that  internal dialogue is completely stripped  and uh and unlike you the GPT will look  at every single token and spend the same  amount of compute on every one of them  and so you can't expect it to actually  like well you can't expect it to do to  uh sort of do too much work per token  so and also in particular basically  these Transformers are just like token  simulators so they don't know what they  don't know like they just imitate the  next token they don't know what they're  good at or not good at they just tried  their best imitate the next token they  don't reflect in the loop they don't  sanity check anything they don't correct  their mistakes along the way by default  they just uh sample token sequences  they don't have separate inner monologue  streams in their head right that are  evaluating what's happening now they do  have some sort of cognitive advantages I  would say and that is that they do  actually have a very large fact-based  knowledge across a vast number of areas  because they have say several 10 billion  parameters so it's a lot of storage for  a lot of facts  but and they also I think have a  relatively large and perfect working  memory so whatever fixed into the  whatever fits into the context window is  immediately available to the Transformer  through its internal self-attention  mechanism and so it's kind of like um  Perfect Memory but it's got that finite  size but the Transformer has a very  direct access to it and so it can like  losslessly remember anything that is  inside its context window  so that's kind of how I would compare  those two and the reason I bring all of  this up is because I think to a large  extent prompting is just making up for  this sort of cognitive difference  between these two kind of architectures  um like our brains here and llm brains  you can look at it that way almost  so here's one thing that people found  for example works pretty well in  practice especially if your tasks  require reasoning you can't expect the  Transformer to make to to do too much  reasoning per token and so you have to  really spread out the reasoning across  more and more tokens so for example you  can't give a Transformer a very  complicated question and expect it to  get the answer in a single token there's  just not enough time for it these  Transformers need tokens to think quote  unquote I like to say sometimes and so  this is some of the things that work  well you may for example have a few shot  prompt that shows the Transformer that  it should like show its work when it's  answering a question when it's answering  the question and if you give a few  examples the Transformer will imitate  that template and it will just end up  working out better in terms of its  evaluation  Additionally you can elicit this kind of  behavior from the Transformer by saying  let's think step by step because this  condition is the transformer into sort  of like showing its work and because it  kind of snaps into a mode of showing its  work it's going to do less computational  work per token and so it's more likely  to succeed as a result because it's  making  um slower reasoning over time  here's another example this one is  called self-consistency we saw that we  had the ability to start writing and  then if it didn't work out I can try  again and I can try multiple times and  uh and maybe  um select the one that worked best so in  these kinds of approaches you may sample  not just once but you may sample  multiple times and then have some  process for finding the ones that aren't  good and then keeping just those samples  or doing a majority vote or something  like that so basically these  Transformers in the process as they  predict the next token just like you  they can get unlucky and they can they  could sample and not a very good token  and they can go down sort of like a  blind alley in terms of reasoning and so  unlike you they cannot recover from that  they are stuck with every single token  they sample and so they will continue  the sequence even if they even know that  this sequence is not going to work out  so give them the ability to look back  inspect or try to find uh try to  basically sample around it  here's one technique also you could it  turns out that actually llms like they  know when they've screwed up so as an  example  say you asked the model to generate a  poem that does not rhyme and it might  give you a poem but it actually Rhymes  but it turns out that especially for the  bigger models like gpt4 you can just ask  it did you meet the assignment and  actually gpt4 knows very well that it  did not meet the assignment it just kind  of got unlucky in its sampling and so it  will tell you no I didn't actually meet  the assignment here's let me try again  but without you prompting it it doesn't  even like it doesn't know it doesn't  know to revisit and uh and so on so you  have to make up for that in your prompt  you have to get it to check if you don't  ask it to check it's not going to check  by itself it's just a token simulator  I think more generally a lot of these  techniques fall into the bucket of what  I would say recreating our system too so  you might be familiar with the system  one system to thinking for humans system  one is a fast automatic process and I  think kind of corresponds to like an llm  just sampling tokens  and system two is the slower deliberate  planning sort of part of your brain  and so this is a paper actually from  just last week because this space is  pretty quickly evolving it's called tree  of thought and in tree of thought uh the  authors of this paper propose  maintaining multiple completions for any  given prompt and then they are also  scoring them along the way and keeping  the ones that are going well if that  makes sense and so a lot of people are  like really playing around with kind of  um prompt engineering to um to basically  bring back some of these abilities that  we sort of have in our brain for llms  now one thing I would like to note here  is that this is not just a prompt this  is actually prompts that are together  used with some python glue code because  you don't you actually have to maintain  multiple prompts and you also have to do  some tree search algorithm here um to  like figure out which prompt to expand  Etc so it's a symbiosis of python glue  code and individual prompts that are  called in a while loop or in a bigger  algorithm  I also think there's a really cool  parallel here to alphago alphago has a  policy for placing the next Stone when  it plays go and its policy was trained  originally by imitating humans but in  addition to this policy it also does  Monte Carlo tree search and basically it  will play out a number of possibilities  in its head and evaluate all of them and  only keep the ones that work well and so  I think this is kind of an equivalent of  alphago but for text  if that makes sense  so just like tree of thought I think  more generally people are starting to  like really Explore More General  techniques of not just the simple  question answer prompt but something  that looks a lot more like python blue  code stringing together many problems so  on the right I have an example from this  paper called react where they structure  the answer to a prompt as a sequence of  thought action observation thought  action observation and it's a full  rollout a kind of a thinking process to  answer the query and in these actions  the model is also allowed to to lose  on the left I have an example of rogpt  and now rgbt by the way became as a  project that I think got a lot of hype  recently and I think uh but I think I  still find it kind of inspirationally  interesting it's um it's a project that  allows an llm to sort of keep task list  and continue to recursively break down  tasks and I don't think this currently  works very well and I would not advise  people to use it in Practical  applications I just think it's something  to generally take inspiration from in  terms of where this is going I think  over time  so that's kind of like giving our model  system to thinking the next thing that I  find kind of interesting is  this following survey would say almost  psychological Quirk of llms is that llms  don't want to succeed they want to  imitate  you want to succeed and you should ask  for it  so what I mean by that is  when Transformers are trained they have  training sets and there can be an entire  spectrum of performance qualities in  their training data so for example there  could be some kind of a prompt for some  Physics question or something like that  and there could be a student solution  that is completely wrong but there can  also be an expert answer that is  extremely right and Transformers can't  tell the difference between like look I  mean they know they know about low  quality Solutions and high quality  Solutions but by default they want to  imitate all of it because they're just  trained on language modeling and so at  test time you actually have to ask for a  good performance so in this example in  this paper it's um they tried various  prompts and let's think step by step was  very powerful because it's sort of like  spread out the reasoning of remaining  tokens but what worked even better is  let's work this out in a step-by-step  way to be sure we have the right answer  and so it's kind of like a conditioning  on getting a right answer and this  actually makes the Transformer work  better because the Transformer doesn't  have to now hedge its probability Mass  on low quality Solutions as ridiculous  as that sounds and so basically don't be  feel free to ask for a strong solution  say something like you are a leading  expert on this topic pretend you have  iq120 Etc but don't try to ask for too  much IQ because if you ask for IQ of  like 400 you might be out of data  distribution or even worse you could be  in data distribution for some like  sci-fi stuff and it will start to like  take on some sci-fi or like role-playing  or something like that so you have to  find like the right amount of IQ I think  it's got some U-shaped curve there  next up  as we saw when we are trying to solve  problems we know we are good at and what  we're not good at and we Lean On Tools  computationally you want to do the same  potentially with your uh llms so in  particular we may want to give them  calculators code interpreters and so on  the ability to do search and uh there's  a lot of techniques for doing that one  thing to keep in mind again is that  these Transformers by default may not  know what they not don't know so you may  even want to tell the transformer in the  prompt you are not very good at mental  arithmetic whenever you need to do very  large number addition multiplication or  whatever instead use this calculator  here's how you use the calculator use  this token combination etc etc so you  have to actually like spell it out  because the model by default doesn't  know what it's good at or not good at  necessarily just like you and I you and  I might be  next up I think something that is very  interesting is we went from a world that  was retrieval only all the way the  pendulum has swung to The Other Extreme  where it's memory only in lens but  actually there's this entire space in  between of these retrieval augmented  models and this works extremely well in  practice  as I mentioned the context window of a  transformer is its working memory if you  can load the working memory with any  information that is relevant to the task  the model will work extremely well  because it can immediately access all  that memory  and so I think a lot of people are  really interested in  uh basically retrieval augmented  generation and on the bottom I have like  an example of llama index which is one  sort of data connector to lots of  different types of data and you can uh  all you can make it you can index all of  that data and you can make it accessible  to llms and the emerging recipe there is  you take relevant documents you split  them up into chunks you embed all of  them and you basically get embedding  vectors that represent that data you  store that in a vector store and then at  test time you make some kind of a query  to your vector store and you fetch  chunks that might be relevant to your  task and you stuff them into the prompt  and then you generate so this can work  quite well in practice so this is I  think similar to when you're and I solve  problems you can do everything from your  memory and Transformers have very large  and extensive memory but also it really  helps to reference some primary  documents so when you whenever you find  yourself going back to a textbook to  find something or whenever you find  yourself going back to the documentation  of a library to look something up the  transform transformers definitely when I  do that too you you have a mem you have  some memory over how some documentation  of a library works but it's much better  to look it up so the same same applies  here  next I wanted to briefly talk about  constraint prompting I also find this  very interesting  uh this is basically techniques for  um  forcing a certain template in the  outputs of llms so guidance is one  example from Microsoft actually and uh  here we are enforcing that the output  from the llm will be Json and this will  actually guarantee that the output will  take on this form because they go in and  they mess with the probabilities of all  the different tokens that come out of  the Transformer and if they clamp those  tokens and then the Transformer is only  filling in the blanks here and then you  can enforce additional restrictions on  what could go into those blanks so this  might be really helpful and I think this  kind of constraint sampling is also  extremely interesting  I also wanted to say a few words about  fine tuning it is the case that you can  get really far with prompt engineering  but it's also possible to think about  fine-tuning your models now fine-tuning  models means that you are actually going  to change the weights of the model  it is becoming a lot more accessible to  do this in practice and that's because  of a number of techniques that have been  developed and have libraries for uh very  recently so for example parameter  efficient fine-tuning techniques like  Laura make sure that you're only Trend  you're only training small Spar species  of your model so most of the model is  kept clamped at the base model and some  pieces of it are allowed to change and  this still works pretty well empirically  and makes it much cheaper to sort of  tune only small pieces of your model  uh there's also it also means that  because most of your model is clamped  you can use very low Precision inference  for computing those parts because they  are not going to be updated by gradient  descent and so that makes everything a  lot more efficient as well and in  addition we have a number of Open Source  high quality based models currently as I  mentioned I think llama is quite nice  although it is not commercially licensed  I believe right now  some things to keep in mind is that  basically fine tuning is is a lot more  technically involved it requires a lot  more I think technical expertise to do  to do right it requires human data  contractors for data sets and or  synthetic data pipelines that can be  pretty complicated this will definitely  slow down your iteration cycle by a lot  and I would say on a high level sft is  achievable because it is just your  continuing language modeling task It's  relatively straightforward but rlhref I  would say is very much research  territory and is even much harder to get  to work and so I would probably not  advise that someone just tries to roll  their own rlh of implementation these  things are pretty unstable very  difficult to train uh not something that  is I think very beginner friendly right  now and it's also potentially likely  also to to change pretty rapidly still  so I think these are my sort of default  recommendations right now  I would break up your task into two  major parts number one achieve your top  performance and number two optimize your  performance in that order  number one the best performance will  currently come from GT4 model it is the  most capable of by far  use prompts that are very detailed they  have lots of task contents relative  relevant information and instructions  think along the lines of what would you  tell a task contractor if they can't  email you back but then also keep in  mind that a task contractor is a human  and they have inner monologue and  they're very clever Etc llms do not  possess those qualities so make sure to  think through the psychology of the llm  almost and cater prompts to that  retrieve and add any relevant context  and information to these prompts  basically refer to a lot of the prompt  engineering techniques some of them are  highlighted in the slides above but also  this is a very large space and I would  just advise you to look for prompt  engineering techniques online there's a  lot to cover there  experiment with few shop examples what  this refers to is you don't just want to  tell you want to show whenever it's  possible so give it examples of  everything that helps it really  understand what you mean if you can  tools and plugins to offload a tasks  that are difficult for llms natively uh  and then think about not just a single  prompt and answer think about potential  chains and reflection and how you glue  them together and how you could  potentially make multiple samples and so  on  finally if you think you've squeezed out  prompt engineering which I think you  should stick with for a while look at  some potentially fine-tuning a model to  your application but expect this to be a  lot more slower and evolved and then  there's an expert fragile research Zone  here and I would say that is rlhf which  currently does work a bit better than  sft if you can get it to work but again  this is uh pretty involved I would say  and to optimize your costs try to  explore lower capacity models or shorter  prompts and so on  I also wanted to say a few words about  the use cases in which I think llms are  currently well suited for so in  particular note that there's a large  number of limitations to llms today and  uh so I would keep that definitely in  mind for all your applications models  and this by the way could be an entire  talk so I don't have time to cover it in  full detail models may be biased they  may fabricate hallucinate information  they may have reasoning errors they may  struggle in entire classes of  applications they have knowledge cutoffs  so they might not know any information  above say September 2021 they are  susceptible to a large range of attacks  which are sort of like coming out on  Twitter daily including prompt injection  jailbreak attacks data poisoning attacks  and so on so my recommendation right now  is use llms in low stakes applications  combine them with always with human  oversight use them as a source of  inspiration and suggestions and think  co-pilots instead of completely  autonomous agents that are just like  performing a task somewhere it's just  not clear that the models are there  right now  so I wanted to close by saying that gpt4  is an amazing artifact I'm very thankful  that it exists and it's it's beautiful  it has a ton of knowledge across so many  areas it can do math code and so on and  in addition there's this thriving  ecosystem of everything else that is  being built and incorporated into into  the ecosystem uh some of some of the  things these things I've talked about  and all of this power is accessible at  your fingertips  so here's everything that's needed in  terms of code to ask GPT for a question  to prompt it and get a response in this  case I said uh can you say something to  inspire the audience of Microsoft build  2023  and I just punched this into Python and  verbatim gpt4 said the following  uh and by the way I did not know that  they used this trick in the keynote so I  thought I was being clever but  but it is really good at this it says  ladies and gentlemen Invaders and  Trailblazers Microsoft build 2023  welcome to the Gathering of Brilliant  Minds like no other you are The  Architects of the future The Visionaries  molding the digital realm in which  Humanity thrives embrace the Limitless  possibilities of Technologies and let  your ideas soar as high as your  imagination together let's create a more  connected remarkable and inclusive world  for generations to come get ready to  unleash your creativity canvas the  unknown and turn dreams into reality  your Journey Begins today  okay\\n\\n\\n[]\\n\\n\\n\\n\\n\\n[]\\n\\n\\n\\n\\n\\nGBT \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n GPT \\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n 99\\n\\n\\n\\n\\n\\n GPU\\n\\n\\n\\n\\n\\n\\n\\n GPU \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMeta \\n\\n llama \\n\\n\\n\\n\\n\\n\\n\\n C4\\n\\n\\n\\n\\n\\nGitHub  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGPT \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ngpts \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n Transformer \\n\\n\\n\\n\\n\\n gpt4 \\n\\n\\n\\n \\n\\ngpt3  gpt3 \\n\\n\\n\\n llama  meta \\n\\n\\n\\n\\n\\n pre   - \\n\\n\\n\\n10 000 \\n\\n\\n204000  100000 \\n\\n\\n\\nGPT \\n\\n\\n\\n\\n\\n llama \\n\\n650 \\n\\n llama\\n\\n gvt3  1750\\n\\n 65b  llama \\n\\n\\n\\n\\n\\n \\n\\n 1.4 \\n\\n3000 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformer \\n\\n\\n\\n\\n\\n\\n\\n 65b \\n\\n 2000 GPU  21 \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformer  B\\n\\n\\n\\n\\n\\nB  T T \\n\\n  10.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Transformer\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n Transformer\\n\\n\\n\\nTransformer \\n\\n\\n\\n\\n\\n Transformer \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n 50 257 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n 513\\n\\n\\n\\n\\n\\nTransformers \\n\\n\\n\\n\\n\\n \\n\\nTransformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n GPT\\n\\n\\n\\n GPT\\n\\nGPT \\n\\n\\n\\n\\n\\n\\n  GPT \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nTransformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTransformer \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n NLP \\n\\n\\n\\n\\n\\n\\n\\n Transformer\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n GPT \\n\\ngpt2 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n QA qaqa \\n\\n\\n\\nq Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\ngpt4 \\n\\n API  gpt4 \\n\\n \\n\\n\\n\\n\\n\\n gpt3 \\n\\n DaVinci  API  gpt2 \\n\\n\\n\\n GitHub \\n\\n\\n\\nmetaLlama\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n GBT \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n QA \\n\\n\\n\\n\\n\\n\\n\\n sfd \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n rlhf \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n sft \\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ntop \\n\\n\\n\\n\\n\\n\\n\\n sft \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n sft\\n\\n \\n\\n\\n\\n\\n\\n Transformer\\n\\n Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n sft \\n\\n\\n\\n sft \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n 1.2 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n rohf \\n\\n \\n\\n GPT  rlhf \\n\\n\\n\\n kuna 13B \\n\\n sft \\n\\n sft  rlh\\n\\n\\n\\n\\n rlhf \\n\\n\\n\\n\\n\\n GBT \\n\\n\\n\\n PPO  rlhf \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n sft\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n sft \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n PT \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n N \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n ELO  \\n\\n\\n\\n gpt4 \\n\\n Claude gvt 3.5 \\n\\n \\n\\n kuna \\n\\n\\n\\nrohf \\n\\n sft\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n GPT \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n 53 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n 3 9.2\\n\\n 0.74 \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n 53\\n\\n\\n\\n  53\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n 53 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n GPT \\n\\n gpt \\n\\n\\n\\n GPT \\n\\n\\n\\n\\n\\n token \\n\\n Transformer \\n\\n 80\\n\\n 80 \\n\\n Transformer\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n llms\\n\\n\\n\\nGPT \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Transformer \\n\\n \\n\\n Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n llm \\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nTransformer \\n\\n\\n\\n\\n\\n\\n\\n Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n Transformer \\n\\n\\n\\n\\n\\n\\nTransformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Transformer \\n\\n Transformer\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n' \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nllms\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n gpt4 \\n\\n\\n\\n gpt4 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n python \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n python \\n\\n\\n\\n while \\n\\n\\n alphago \\n\\n alphago \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nalphago \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPython\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n rogpt \\n\\n rgbt \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n llm \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n llms  llms\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Transformer \\n\\n Transformer\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n120\\n\\n\\n\\n 400\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n U \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n LLMS \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n llama \\n\\n\\n\\n\\n\\n \\n\\n\\n\\nllms \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n llms \\n\\n\\n\\n \\n\\n\\n\\nllm  Json\\n\\n\\n\\n\\n\\n\\n\\n Transformer \\n\\n \\n\\n Transformer \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Laura \\n\\n \\n\\n Spar \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n llama \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n/\\n\\n\\n\\n\\n\\n\\n\\n  sft \\n\\n\\n\\n\\n\\n rlhref \\n\\n\\n\\n\\n\\n\\n\\n \\n\\n rlh \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n GT4 \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\nLLM\\n\\n\\n\\nLLM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n llms \\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n rlhf\\n\\n sft \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n llms \\n\\n\\n\\n\\n\\n llms \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n 2021  9  \\n\\n\\n\\n\\n\\nTwitter \\n\\n\\n\\n\\n\\n llms\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n gpt4\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nGPT \\n\\n\\n\\n\\n\\n  Microsoft build 2023 \\n\\n\\n Python \\n\\n gpt4 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# methods of file objects\n",
    "with open('build.txt', encoding=\"utf-8\") as f:\n",
    "    f.read()\n",
    "    f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f2f924b-eba0-46d0-8df6-18bac1b8dfc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"foreign  [Music]  researcher and founding member of open  AI Andre carpathy  [Applause]  hi everyone  I'm happy to be here to tell you about  the state of gbt and more generally  about the rapidly growing ecosystem of  large language models  so I would like to partition the talk  into two parts  in the first part I would like to tell  you about how we train GPT assistance  and then in the second part we will we  are going to take a look at how we can  use these assistants effectively for  your applications  so first let's take a look at the  emerging recipe for how to train these  assistants and keep in mind that this is  all very new and still rapidly evolving  but so far the recipe looks something  like this  now this is kind of a complicated slide  so I'm going to go through it piece by  piece but roughly speaking we have four  major stages free training supervised  fine tuning reward modeling  reinforcement learning and they follow  each other serially  now in each stage we have a data set  that is that powers that stage we have  an algorithm that for our purposes will  be a objective and a and over for uh for  training the neural network and then we  have a resulting model and then there's  some notes on the bottom so the first  stage we're going to start with is the  pre-training stage now this stage is  kind of special in this diagram and this  diagram is not to scale because this  stage is where all of the computational  work basically happens this is 99 of the  training uh compute time and also flops  and so this is where we are dealing with  internet scale data sets with thousands  of gpus in the store computer and also  months of training potentially the other  three stages are fine-tuning stages that  are much more along the lines of a small  few number of gpus and hours or days  so let's take a look at the pre-training  stage to achieve a base model  first we're going to gather a large  amount of data here's an example of what  we call a data mixture that comes from  this uh paper that was released by meta  where they released this llama based  model now you can see roughly the kinds  of data sets that enter into these  collections so we have common crawl  which is just a web scrape C4 which is  also a common crawl and then some high  quality data sets as well so for example  GitHub Wikipedia books archive stack  exchange and so on these are all mixed  up together and then they are sampled  According to some given proportions and  that forms the training set for the  neural net for the GPT  now before we can actually train on this  data we need to go through one more  pre-processing step and that is  tokenization and this is basically a  translation of the raw text that we  scrape from the internet into sequences  of integers because that's the native  representation over which gpts function  now this is a lossless kind of  translation between pieces of text and  tokens and integers and there are a  number of algorithms for this stage  typically for example you could use  something like byte pair encoding which  iteratively merges little text chunks  and groups them into tokens  and so here I'm showing some example  chunks of these tokens and then this is  the raw integer sequence that will  actually feed into a Transformer  now here I'm showing two uh sort of like  examples for hyper parameters that  govern this stage so gpt4 we did not  release too much information about how  it was trained and so on so I'm using  gpt3's numbers but gpt3 is of course a  little bit old by now about three years  ago but llama is a fairly recent model  from meta so these are roughly the  orders of magnitude that we're dealing  with when we're doing pre-training  the vocabulary size is usually a couple  10 000 tokens the context length is  usually something like two thousand four  thousand or nowadays even 100 000 and  this governs the maximum number of  integers that the GPT will look at when  it's trying to predict the next integer  in a sequence  um you can see that roughly the number  of parameters is say 65 billion for  llama now even though llama has only 65b  parameters compared to gvt3's 175  billion parameters llama is a  significantly more powerful model and  intuitively that's because the model is  trained for significantly longer in this  case 1.4 trillion tokens instead of just  300 billion tokens so you shouldn't  judge the power of a model just by the  number of parameters that it contains  below I'm showing some tables of rough  number of rough hyper parameters that  typically go into specifying the  Transformer neural network so the number  of heads the dimension size number of  layers and so on and on the bottom I'm  showing some training hyper parameters  so for example to train the 65b model  meta used 2000 gpus roughly 21 days  train of training and and roughly  several million dollars and so that's  the rough orders of magnitude that you  should have in mind for the pre-training  stage  now when we're actually pre-chaining  what happens roughly speaking we are  going to take our tokens and we're going  to lay them out into Data batches so we  have these arrays that will feed into  the Transformer and these arrays are B  the batch size and these are all  independent examples stacked up in rows  and B by T T being the maximum context  length so in my picture I only have 10.  the context length so this could be two  thousand four thousand Etc so these are  extremely long rows and what we do is we  take these documents and we pack them  into rows and we delimit them with these  Special end of text tokens uh basically  it's on the Transformer where a new  document begins  and so here I have a few examples of  documents and then I've stretched them  out into uh into this input  now we're going to feed all of these  numbers into Transformer and let me let  me just focus on a single particular  cell but the same thing will happen at  every every cell in this diagram so  let's look at the green cell the green  cell is going to take a look at all of  the tokens before it so all of the  tokens in yellow and we're going to feed  that entire context into the Transformer  neural network  and the Transformer is going to try to  predict the next token in the sequence  in this case in red  now the Transformer I don't have too  much time to unfortunately go into the  full details of this neural network  architecture is just a large blob of  neural net stuff for our purposes and  it's got several 10 billion parameters  typically or something like that and of  course as they tune these parameters  you're getting slightly different uh  predicted distributions for every single  one of these cells  and so for example if our vocabulary  size is 50 257 tokens then we're going  to have that many numbers because we  need to specify a probability  distribution for what comes next so  basically we have a probability for  whatever may follow now in this specific  example for the specific cell 513 will  come next and so we can use this as a  source of supervision to update our  Transformers weights and so we're  applying this basically on every single  cell in parallel and we keep swap  swapping batches and we're trying to get  the Transformer to make the correct  predictions over what token comes next  in a sequence  so let me show you more concretely what  this looks like when you train one of  these models this is actually coming  from a New York Times and they trained a  small GPT on Shakespeare and so here's a  small snippet of Shakespeare and they  trained a GPT on it now in the beginning  at initialization the GPT starts with  completely random weights so you're just  getting completely random outputs as  well  but over time as you train uh the GPT  longer and longer you are getting more  and more coherent and consistent uh sort  of samples from the model and the way  you sample from it of course is you uh  predict what comes next you sample from  that distribution and you keep feeding  that back into the process and you can  basically uh sample large sequences and  so by the end you see that the  Transformer has learned about words and  where to put spaces and where to put  commas and so on and so we're making  more and more consistent predictions  over time  these are the kinds of plots that you're  looking at when you're doing model  pre-training effectively we're looking  at the loss function over time as you  train and low loss means that our  Transformer is predicting the correct uh  is giving a higher probability to the  correct next integer in a sequence  now what are we going to do with this  model once we've trained it after a  month well the first thing that we  noticed we the field is that these  models basically in the process of  language modeling learn very powerful  General representations and it's  possible to very efficiently fine-tune  them for any arbitrary Downstream task  you might be interested in so as an  example if you're interested in  sentiment classification the approach  used to be that you collect a bunch of  positives and negatives and then you  train some kind of an NLP model for for  that but the new approach is ignore  sentiment classification  go off and do large language model  pre-training train the large Transformer  and then you can only you may only have  a few examples and you can very  efficiently fine tune your model for  that task and so this works very well in  practice and the reason for this is that  basically the Transformer is forced to  multitask a huge amount of tasks in the  language modeling task because just just  in terms of predicting the next token  it's forced to understand a lot about  the structure of the of the text and all  the different concepts they're in  so that was GPT one now around the time  of gpt2 people noticed that actually  even better than fine tuning you can  actually prompt these models very  effectively so these are language models  and they want to complete documents so  you can actually trick them into  performing tasks just by arranging these  fake documents so in this example for uh  for example we have some passage and  then we sort of like do QA qaqa this is  called a few shot prompt and then we do  q and then as the Transformer is trying  to complete the document it's actually  answering our question and so this is an  example of prompt engineering a base  model making the belief that it's sort  of imitating a document and getting it  to perform a task  and so this kicked off I think the era  of I would say prompting over  fine-tuning and seeing that this  actually can work extremely well on a  lot of problems even without training  any neural networks fine-tuning or so on  now since then we've seen an entire  evolutionary tree of Base models that  everyone has trained not all of these  models are available for example the  gpt4 base model was never released the  gpt4 model that you might be interacting  with over API is not a base model it's  an assistant model and we're going to  cover how to get those in a bit gpt3  based model is available via the API  under the name DaVinci and gpt2 base  model is available even as weights on  our GitHub repo but currently the best  available base model probably is the  Llama series from meta although it is  not commercially licensed  now one thing to point out is base  models are not assistance they don't  want to answer to they don't want to  make answers to your questions they just  want to complete documents so if you  tell them write a poem about the brand  cheese it will just you know it will  answer questions with more questions  it's just completing what it thinks as a  document  however you can prompt them in a  specific way for base models that that  is more likely to work so as an example  here's a poem about bread and cheese and  in that case it will auto-complete  correctly  you can even trick base models into  being assistance and the way you would  do this is you would create like a  specific few shot prompt that makes it  look like there's some kind of a  document between a human and assistant  and they're exchanging sort of um  information and then at the bottom you  sort of put your query at the end and  the base model will sort of like  condition itself into being like a  helpful assistant and kind of answer but  this is not very reliable and doesn't  work super well in practice although it  can be done  so instead we have a different path to  make actual gbt assistance not just base  model document completers and so that  takes us into supervised fine-tuning so  in the supervised fine tuning stage we  are going to collect small but high  quality data sets and in this case we're  going to ask human contractors to gather  data of the format of the form prompt  and ideal response and we're going to  collect lots of these typically tens of  thousands or something like that  and then we're going to still do  language modeling on this data so  nothing change algorithmically we're  just swapping out a training set so it  used to be internet documents which is a  high quantity local  four basically QA prompt response kind  of data and that is low quantity high  quality  so we still do language modeling and  then after training we get an sfd model  and you can actually deploy these models  and they are actual assistants and they  work to some extent let me show you what  an example demonstration might look like  so here's something that a human  contractor might come up with here's  some random prompt can you write a short  introduction about the relevance of the  term monopsony or something like that  and then the contractor also writes out  an ideal response and when they write  out these responses they are following  extensive labeling documentations and  they're being asked to be helpful  truthful and harmless and those labeling  instructions here you probably can't  read it another can I but they're long  and this is just people following  instructions and trying to complete  these prompts  so that's what the data set looks like  and you can train these models and this  works to some extent  now you can actually continue the  pipeline from here on and go into rlhf  reinforcement learning from Human  feedback that consists of both reward  modeling and reinforcement learning so  let me cover that and then I'll come  back to why you may want to go through  the extra steps and how that compares to  just sft models  so in the reward modeling step what  we're going to do is we're now going to  shift our data collection to be of the  form of comparisons so here's an example  of what our data set will look like  I have the same prompt identical prompt  on the top which is asking the assistant  to write a program or function that  checks if a given string is a palindrome  and then what we do is we take the sft  model which we've already trained and we  create multiple completions so in this  case we have three completions that the  model has created  and then we ask people to rank these  completions so if you stare at this for  a while and by the way these are very  difficult things to do to compare some  of these predictions and this can take  people even hours for a single prompt  completion pairs but let's say we  decided that one of these is much better  than the others and so on so we rank  them  then we can follow that with something  that looks very much kind of like a  binary classification on all the  possible pairs between these completions  so what we do now is we lay out our  prompt in rows and the prompts is  identical across all three rows here so  it's all the same prompt but the  completion of this vary and so the  yellow tokens are coming from the sft  model then what we do is we append  another special reward uh readout token  at the end and we basically only  supervise the Transformer at this single  green token and the Transformer will  predict some reward for how good that  completion is for that prompt  and so basically it makes a guess about  the quality of each completion and then  once it makes a guess for every one of  them we also have the ground truth which  is telling us the ranking of them and so  we can actually enforce that some of  these numbers should be much higher than  others and so on WE formulate this into  a loss function and we train our model  to make reward predictions that are  consistent with the ground truth coming  from the comparisons from all these  contractors so that's how we train our  reward model and that allows us to score  how good a completion is for a prompt  once we have a reward model this is we  can't deploy this because this is not  very useful as an assistant by itself  but it's very useful for the reinforce  reinforcement learning stage that  follows now because we have a reward  model we can score the quality of any  arbitrary completion for any given  prompt so what we do during  reinforcement learning is we basically  get again a large collection of prompts  and now we do reinforcement learning  with respect to the reward model so  here's what that looks like  we we take a single prompt we lay it out  in rows and now we use the sft we use  the basically the model we'd like to  train which is initialized at sft model  to create some completions in yellow  and then we append the reward token  again and we read off the reward  according to the reward model which is  now kept fixed it doesn't change anymore  and now the reward model tells us the  quality of every single completion for  each for these prompts and so what we  can do is we can now just basically  apply the same language modeling loss  function but we're currently training on  the yellow tokens and we are weighing  the language modeling objective by the  rewards indicated by the reward model so  as an example in the first row the  reward model said that this is a fairly  high score in completion and so all the  tokens that we happen to sample on the  first row are going to get reinforced  and they're going to get higher  probabilities for the future  conversely on the second row the reward  model really did not like this  completion negative 1.2 and so therefore  every single token that we sampled in  that second row is going to get a  slightly higher probability for the  future and we do this over and over on  many prompts on many batches and  basically we get a policy which creates  yellow tokens here and it basically all  of them all of the completions here will  score high according to the reward model  that we trained in the previous stage  so that's how we train uh that's what  the rohf pipeline is  now and then at the end you get a model  that you could deploy and so and as an  example chat GPT is an rlhf model but  some other models that you might come  across like for example of the kuna 13B  and so on these are sft models so we  have base models sft models and rlh  models and that's kind of like the state  of things there  now why would you want to do rlhf so one  answer that is kind of not that exciting  is that it just works better so this  comes from the instructor gbt paper  according to these experiments a while  ago now these PPO models are rlhf and we  see that they are basically just  preferred in a lot of comparisons when  we give them to humans so humans just  prefer out basically tokens that come  from our religion models compared to sft  models compared to base model that is  prompted to be an assistant and so it  just works better but you might ask why  why does it work better and I don't  think that there's a single like amazing  answer that the community has really  like agreed on but I will just offer one  uh one reason potentially and it has to  do with the asymmetry between how easy  computationally it is to compare versus  generate so let's take an example of  generating a haiku suppose I ask a model  to write a haiku about paper clips if  you're a contractor trying to give train  data then imagine being a contractor  collecting basically data for the sft  stage how are you supposed to create a  nice haiku for a paperclip you might  just not be very good at that but if I  give you a few examples of haikus you  might be able to appreciate some of  these haikus a lot more than others and  so judging which one of these is good is  much easier task and so basically this  asymmetry makes it so that comparisons  are a better way to potentially leverage  yourself as a human and your judgment to  create a slightly better model  now our religious models are not  strictly an improvement on the base  models in some cases so in particular  we've noticed for example that they lose  some entropy so that means that they  give More PT results they can output  lower variations uh like they can output  samples with lower variation than base  model so base model has lots of entropy  and we'll go with lots of diverse  outputs  so for example one one kind of place  where I still prefer to use a base model  is in the setup where  you basically have N Things and you want  to generate more things like it and so  here is an example that I just cooked up  I want to generate cool Pokemon names I  gave it seven Pokemon names and I asked  the base model to complete the document  and it gave me a lot more Pokemon names  uh these are fictitious I tried to look  them up I don't believe they're actual  Pokemons and uh this is the kind of task  that I think base model would be good at  because it still has lots of entropy it  will give you lots of diverse cool kind  of more things that look like whatever  you give it before  so this is what uh this is number having  all said all that these are kind of like  the assistant models that are probably  available to you at this point uh that  there's a team at Berkeley that ranked a  lot of the available assistant models  and gave them basically ELO ratings So  currently some of the best models of  course are gpt4 by far I would say  followed by Claude gvt 3.5 and then a  number of models some of these might be  available as weights like the kuna koala  Etc and the first three rows here are  all they're all rohf models and all of  the other models to my knowledge are sft  models I believe  okay so that's how we train these models  on the high level now I'm going to  switch gears and let's look at how we  can best apply and the GPT assistant  model to your problems now I would like  to work  in a setting of a concrete example so  let's  let's work with a concrete example here  let's say that you are working on an  article or a blog post and you're going  to write this sentence at the end  California's population is 53 times that  of Alaska so for some reason you want to  compare the populations of these two  states  think about the rich internal monologue  and Tool use and how much work actually  goes computationally in your brain to  generate this one final sentence so  here's maybe what that could look like  in your brain okay for this next step  let me blog uh for my blog let me  compare these two populations  okay first I'm going to obviously need  to get both of these populations now I  know that I probably don't know these  populations off the top of my head so  I'm kind of like aware of what I know or  don't know of my self-knowledge right  so I go I do some tool use and I go to  Wikipedia and I look up California's  population and Alaska's population  now I know that I should divide the two  but again I know that dividing three 9.2  by 0.74 is very unlikely to succeed  that's not the kind of thing that I can  do in my head and so therefore I'm gonna  rely on the calculator so I'm going to  use a calculator punch it in and see  that the output is roughly 53.  and then maybe I do some reflection and  Sanity checks in my brain so does 53  make sense well that's quite quite a  large fraction but then California is  the most popular state so maybe that  looks okay  so then I have all the information I  might need and now I get to the sort of  creative portion of writing so I might  start to write something like California  has 53 x times greater and then I think  to myself that's actually like really  awkward phrasing so let me actually  delete that and let me try again and so  as I'm writing I have this separate  process almost inspecting what I'm  writing and judging whether it looks  good or not  and then maybe I delete and maybe I  reframe it and then maybe I'm happy with  what comes out  so basically long story short a ton  happens under the hood in terms of your  internal monologue when you create  sentences like this but what does a  sentence like this look like when we are  training a GPT on it  From gpt's perspective this is just a  sequence of tokens  so GPT when it's reading or generating  these tokens it just goes  and each chunk is roughly the same  amount of computational work for each  token and these Transformers are not  very shallow networks they have about 80  layers of reasoning but 80 is still not  like too much and so this Transformer is  going to do its best to imitate but of  course the process here looks very very  different from the process that you took  so in particular in our final artifacts  in the data sets that we create and then  eventually feed to llms all of that  internal dialogue is completely stripped  and uh and unlike you the GPT will look  at every single token and spend the same  amount of compute on every one of them  and so you can't expect it to actually  like well you can't expect it to do to  uh sort of do too much work per token  so and also in particular basically  these Transformers are just like token  simulators so they don't know what they  don't know like they just imitate the  next token they don't know what they're  good at or not good at they just tried  their best imitate the next token they  don't reflect in the loop they don't  sanity check anything they don't correct  their mistakes along the way by default  they just uh sample token sequences  they don't have separate inner monologue  streams in their head right that are  evaluating what's happening now they do  have some sort of cognitive advantages I  would say and that is that they do  actually have a very large fact-based  knowledge across a vast number of areas  because they have say several 10 billion  parameters so it's a lot of storage for  a lot of facts  but and they also I think have a  relatively large and perfect working  memory so whatever fixed into the  whatever fits into the context window is  immediately available to the Transformer  through its internal self-attention  mechanism and so it's kind of like um  Perfect Memory but it's got that finite  size but the Transformer has a very  direct access to it and so it can like  losslessly remember anything that is  inside its context window  so that's kind of how I would compare  those two and the reason I bring all of  this up is because I think to a large  extent prompting is just making up for  this sort of cognitive difference  between these two kind of architectures  um like our brains here and llm brains  you can look at it that way almost  so here's one thing that people found  for example works pretty well in  practice especially if your tasks  require reasoning you can't expect the  Transformer to make to to do too much  reasoning per token and so you have to  really spread out the reasoning across  more and more tokens so for example you  can't give a Transformer a very  complicated question and expect it to  get the answer in a single token there's  just not enough time for it these  Transformers need tokens to think quote  unquote I like to say sometimes and so  this is some of the things that work  well you may for example have a few shot  prompt that shows the Transformer that  it should like show its work when it's  answering a question when it's answering  the question and if you give a few  examples the Transformer will imitate  that template and it will just end up  working out better in terms of its  evaluation  Additionally you can elicit this kind of  behavior from the Transformer by saying  let's think step by step because this  condition is the transformer into sort  of like showing its work and because it  kind of snaps into a mode of showing its  work it's going to do less computational  work per token and so it's more likely  to succeed as a result because it's  making  um slower reasoning over time  here's another example this one is  called self-consistency we saw that we  had the ability to start writing and  then if it didn't work out I can try  again and I can try multiple times and  uh and maybe  um select the one that worked best so in  these kinds of approaches you may sample  not just once but you may sample  multiple times and then have some  process for finding the ones that aren't  good and then keeping just those samples  or doing a majority vote or something  like that so basically these  Transformers in the process as they  predict the next token just like you  they can get unlucky and they can they  could sample and not a very good token  and they can go down sort of like a  blind alley in terms of reasoning and so  unlike you they cannot recover from that  they are stuck with every single token  they sample and so they will continue  the sequence even if they even know that  this sequence is not going to work out  so give them the ability to look back  inspect or try to find uh try to  basically sample around it  here's one technique also you could it  turns out that actually llms like they  know when they've screwed up so as an  example  say you asked the model to generate a  poem that does not rhyme and it might  give you a poem but it actually Rhymes  but it turns out that especially for the  bigger models like gpt4 you can just ask  it did you meet the assignment and  actually gpt4 knows very well that it  did not meet the assignment it just kind  of got unlucky in its sampling and so it  will tell you no I didn't actually meet  the assignment here's let me try again  but without you prompting it it doesn't  even like it doesn't know it doesn't  know to revisit and uh and so on so you  have to make up for that in your prompt  you have to get it to check if you don't  ask it to check it's not going to check  by itself it's just a token simulator  I think more generally a lot of these  techniques fall into the bucket of what  I would say recreating our system too so  you might be familiar with the system  one system to thinking for humans system  one is a fast automatic process and I  think kind of corresponds to like an llm  just sampling tokens  and system two is the slower deliberate  planning sort of part of your brain  and so this is a paper actually from  just last week because this space is  pretty quickly evolving it's called tree  of thought and in tree of thought uh the  authors of this paper propose  maintaining multiple completions for any  given prompt and then they are also  scoring them along the way and keeping  the ones that are going well if that  makes sense and so a lot of people are  like really playing around with kind of  um prompt engineering to um to basically  bring back some of these abilities that  we sort of have in our brain for llms  now one thing I would like to note here  is that this is not just a prompt this  is actually prompts that are together  used with some python glue code because  you don't you actually have to maintain  multiple prompts and you also have to do  some tree search algorithm here um to  like figure out which prompt to expand  Etc so it's a symbiosis of python glue  code and individual prompts that are  called in a while loop or in a bigger  algorithm  I also think there's a really cool  parallel here to alphago alphago has a  policy for placing the next Stone when  it plays go and its policy was trained  originally by imitating humans but in  addition to this policy it also does  Monte Carlo tree search and basically it  will play out a number of possibilities  in its head and evaluate all of them and  only keep the ones that work well and so  I think this is kind of an equivalent of  alphago but for text  if that makes sense  so just like tree of thought I think  more generally people are starting to  like really Explore More General  techniques of not just the simple  question answer prompt but something  that looks a lot more like python blue  code stringing together many problems so  on the right I have an example from this  paper called react where they structure  the answer to a prompt as a sequence of  thought action observation thought  action observation and it's a full  rollout a kind of a thinking process to  answer the query and in these actions  the model is also allowed to to lose  on the left I have an example of rogpt  and now rgbt by the way became as a  project that I think got a lot of hype  recently and I think uh but I think I  still find it kind of inspirationally  interesting it's um it's a project that  allows an llm to sort of keep task list  and continue to recursively break down  tasks and I don't think this currently  works very well and I would not advise  people to use it in Practical  applications I just think it's something  to generally take inspiration from in  terms of where this is going I think  over time  so that's kind of like giving our model  system to thinking the next thing that I  find kind of interesting is  this following survey would say almost  psychological Quirk of llms is that llms  don't want to succeed they want to  imitate  you want to succeed and you should ask  for it  so what I mean by that is  when Transformers are trained they have  training sets and there can be an entire  spectrum of performance qualities in  their training data so for example there  could be some kind of a prompt for some  Physics question or something like that  and there could be a student solution  that is completely wrong but there can  also be an expert answer that is  extremely right and Transformers can't  tell the difference between like look I  mean they know they know about low  quality Solutions and high quality  Solutions but by default they want to  imitate all of it because they're just  trained on language modeling and so at  test time you actually have to ask for a  good performance so in this example in  this paper it's um they tried various  prompts and let's think step by step was  very powerful because it's sort of like  spread out the reasoning of remaining  tokens but what worked even better is  let's work this out in a step-by-step  way to be sure we have the right answer  and so it's kind of like a conditioning  on getting a right answer and this  actually makes the Transformer work  better because the Transformer doesn't  have to now hedge its probability Mass  on low quality Solutions as ridiculous  as that sounds and so basically don't be  feel free to ask for a strong solution  say something like you are a leading  expert on this topic pretend you have  iq120 Etc but don't try to ask for too  much IQ because if you ask for IQ of  like 400 you might be out of data  distribution or even worse you could be  in data distribution for some like  sci-fi stuff and it will start to like  take on some sci-fi or like role-playing  or something like that so you have to  find like the right amount of IQ I think  it's got some U-shaped curve there  next up  as we saw when we are trying to solve  problems we know we are good at and what  we're not good at and we Lean On Tools  computationally you want to do the same  potentially with your uh llms so in  particular we may want to give them  calculators code interpreters and so on  the ability to do search and uh there's  a lot of techniques for doing that one  thing to keep in mind again is that  these Transformers by default may not  know what they not don't know so you may  even want to tell the transformer in the  prompt you are not very good at mental  arithmetic whenever you need to do very  large number addition multiplication or  whatever instead use this calculator  here's how you use the calculator use  this token combination etc etc so you  have to actually like spell it out  because the model by default doesn't  know what it's good at or not good at  necessarily just like you and I you and  I might be  next up I think something that is very  interesting is we went from a world that  was retrieval only all the way the  pendulum has swung to The Other Extreme  where it's memory only in lens but  actually there's this entire space in  between of these retrieval augmented  models and this works extremely well in  practice  as I mentioned the context window of a  transformer is its working memory if you  can load the working memory with any  information that is relevant to the task  the model will work extremely well  because it can immediately access all  that memory  and so I think a lot of people are  really interested in  uh basically retrieval augmented  generation and on the bottom I have like  an example of llama index which is one  sort of data connector to lots of  different types of data and you can uh  all you can make it you can index all of  that data and you can make it accessible  to llms and the emerging recipe there is  you take relevant documents you split  them up into chunks you embed all of  them and you basically get embedding  vectors that represent that data you  store that in a vector store and then at  test time you make some kind of a query  to your vector store and you fetch  chunks that might be relevant to your  task and you stuff them into the prompt  and then you generate so this can work  quite well in practice so this is I  think similar to when you're and I solve  problems you can do everything from your  memory and Transformers have very large  and extensive memory but also it really  helps to reference some primary  documents so when you whenever you find  yourself going back to a textbook to  find something or whenever you find  yourself going back to the documentation  of a library to look something up the  transform transformers definitely when I  do that too you you have a mem you have  some memory over how some documentation  of a library works but it's much better  to look it up so the same same applies  here  next I wanted to briefly talk about  constraint prompting I also find this  very interesting  uh this is basically techniques for  um  forcing a certain template in the  outputs of llms so guidance is one  example from Microsoft actually and uh  here we are enforcing that the output  from the llm will be Json and this will  actually guarantee that the output will  take on this form because they go in and  they mess with the probabilities of all  the different tokens that come out of  the Transformer and if they clamp those  tokens and then the Transformer is only  filling in the blanks here and then you  can enforce additional restrictions on  what could go into those blanks so this  might be really helpful and I think this  kind of constraint sampling is also  extremely interesting  I also wanted to say a few words about  fine tuning it is the case that you can  get really far with prompt engineering  but it's also possible to think about  fine-tuning your models now fine-tuning  models means that you are actually going  to change the weights of the model  it is becoming a lot more accessible to  do this in practice and that's because  of a number of techniques that have been  developed and have libraries for uh very  recently so for example parameter  efficient fine-tuning techniques like  Laura make sure that you're only Trend  you're only training small Spar species  of your model so most of the model is  kept clamped at the base model and some  pieces of it are allowed to change and  this still works pretty well empirically  and makes it much cheaper to sort of  tune only small pieces of your model  uh there's also it also means that  because most of your model is clamped  you can use very low Precision inference  for computing those parts because they  are not going to be updated by gradient  descent and so that makes everything a  lot more efficient as well and in  addition we have a number of Open Source  high quality based models currently as I  mentioned I think llama is quite nice  although it is not commercially licensed  I believe right now  some things to keep in mind is that  basically fine tuning is is a lot more  technically involved it requires a lot  more I think technical expertise to do  to do right it requires human data  contractors for data sets and or  synthetic data pipelines that can be  pretty complicated this will definitely  slow down your iteration cycle by a lot  and I would say on a high level sft is  achievable because it is just your  continuing language modeling task It's  relatively straightforward but rlhref I  would say is very much research  territory and is even much harder to get  to work and so I would probably not  advise that someone just tries to roll  their own rlh of implementation these  things are pretty unstable very  difficult to train uh not something that  is I think very beginner friendly right  now and it's also potentially likely  also to to change pretty rapidly still  so I think these are my sort of default  recommendations right now  I would break up your task into two  major parts number one achieve your top  performance and number two optimize your  performance in that order  number one the best performance will  currently come from GT4 model it is the  most capable of by far  use prompts that are very detailed they  have lots of task contents relative  relevant information and instructions  think along the lines of what would you  tell a task contractor if they can't  email you back but then also keep in  mind that a task contractor is a human  and they have inner monologue and  they're very clever Etc llms do not  possess those qualities so make sure to  think through the psychology of the llm  almost and cater prompts to that  retrieve and add any relevant context  and information to these prompts  basically refer to a lot of the prompt  engineering techniques some of them are  highlighted in the slides above but also  this is a very large space and I would  just advise you to look for prompt  engineering techniques online there's a  lot to cover there  experiment with few shop examples what  this refers to is you don't just want to  tell you want to show whenever it's  possible so give it examples of  everything that helps it really  understand what you mean if you can  tools and plugins to offload a tasks  that are difficult for llms natively uh  and then think about not just a single  prompt and answer think about potential  chains and reflection and how you glue  them together and how you could  potentially make multiple samples and so  on  finally if you think you've squeezed out  prompt engineering which I think you  should stick with for a while look at  some potentially fine-tuning a model to  your application but expect this to be a  lot more slower and evolved and then  there's an expert fragile research Zone  here and I would say that is rlhf which  currently does work a bit better than  sft if you can get it to work but again  this is uh pretty involved I would say  and to optimize your costs try to  explore lower capacity models or shorter  prompts and so on  I also wanted to say a few words about  the use cases in which I think llms are  currently well suited for so in  particular note that there's a large  number of limitations to llms today and  uh so I would keep that definitely in  mind for all your applications models  and this by the way could be an entire  talk so I don't have time to cover it in  full detail models may be biased they  may fabricate hallucinate information  they may have reasoning errors they may  struggle in entire classes of  applications they have knowledge cutoffs  so they might not know any information  above say September 2021 they are  susceptible to a large range of attacks  which are sort of like coming out on  Twitter daily including prompt injection  jailbreak attacks data poisoning attacks  and so on so my recommendation right now  is use llms in low stakes applications  combine them with always with human  oversight use them as a source of  inspiration and suggestions and think  co-pilots instead of completely  autonomous agents that are just like  performing a task somewhere it's just  not clear that the models are there  right now  so I wanted to close by saying that gpt4  is an amazing artifact I'm very thankful  that it exists and it's it's beautiful  it has a ton of knowledge across so many  areas it can do math code and so on and  in addition there's this thriving  ecosystem of everything else that is  being built and incorporated into into  the ecosystem uh some of some of the  things these things I've talked about  and all of this power is accessible at  your fingertips  so here's everything that's needed in  terms of code to ask GPT for a question  to prompt it and get a response in this  case I said uh can you say something to  inspire the audience of Microsoft build  2023  and I just punched this into Python and  verbatim gpt4 said the following  uh and by the way I did not know that  they used this trick in the keynote so I  thought I was being clever but  but it is really good at this it says  ladies and gentlemen Invaders and  Trailblazers Microsoft build 2023  welcome to the Gathering of Brilliant  Minds like no other you are The  Architects of the future The Visionaries  molding the digital realm in which  Humanity thrives embrace the Limitless  possibilities of Technologies and let  your ideas soar as high as your  imagination together let's create a more  connected remarkable and inclusive world  for generations to come get ready to  unleash your creativity canvas the  unknown and turn dreams into reality  your Journey Begins today  okay\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f.readline() reads a single line from the file\n",
    "with open('build.txt', encoding=\"utf-8\") as f:\n",
    "    f.readline()\n",
    "    f.readline()\n",
    "    f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e30218f3-f3d9-4a35-83f9-14eb021e675d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foreign  [Music]  researcher and founding member of open  AI Andre carpathy  [Applause]  hi everyone  I'm happy to be here to tell you about  the state of gbt and more generally  about the rapidly growing ecosystem of  large language models  so I would like to partition the talk  into two parts  in the first part I would like to tell  you about how we train GPT assistance  and then in the second part we will we  are going to take a look at how we can  use these assistants effectively for  your applications  so first let's take a look at the  emerging recipe for how to train these  assistants and keep in mind that this is  all very new and still rapidly evolving  but so far the recipe looks something  like this  now this is kind of a complicated slide  so I'm going to go through it piece by  piece but roughly speaking we have four  major stages free training supervised  fine tuning reward modeling  reinforcement learning and they follow  each other serially  now in each stage we have a data set  that is that powers that stage we have  an algorithm that for our purposes will  be a objective and a and over for uh for  training the neural network and then we  have a resulting model and then there's  some notes on the bottom so the first  stage we're going to start with is the  pre-training stage now this stage is  kind of special in this diagram and this  diagram is not to scale because this  stage is where all of the computational  work basically happens this is 99 of the  training uh compute time and also flops  and so this is where we are dealing with  internet scale data sets with thousands  of gpus in the store computer and also  months of training potentially the other  three stages are fine-tuning stages that  are much more along the lines of a small  few number of gpus and hours or days  so let's take a look at the pre-training  stage to achieve a base model  first we're going to gather a large  amount of data here's an example of what  we call a data mixture that comes from  this uh paper that was released by meta  where they released this llama based  model now you can see roughly the kinds  of data sets that enter into these  collections so we have common crawl  which is just a web scrape C4 which is  also a common crawl and then some high  quality data sets as well so for example  GitHub Wikipedia books archive stack  exchange and so on these are all mixed  up together and then they are sampled  According to some given proportions and  that forms the training set for the  neural net for the GPT  now before we can actually train on this  data we need to go through one more  pre-processing step and that is  tokenization and this is basically a  translation of the raw text that we  scrape from the internet into sequences  of integers because that's the native  representation over which gpts function  now this is a lossless kind of  translation between pieces of text and  tokens and integers and there are a  number of algorithms for this stage  typically for example you could use  something like byte pair encoding which  iteratively merges little text chunks  and groups them into tokens  and so here I'm showing some example  chunks of these tokens and then this is  the raw integer sequence that will  actually feed into a Transformer  now here I'm showing two uh sort of like  examples for hyper parameters that  govern this stage so gpt4 we did not  release too much information about how  it was trained and so on so I'm using  gpt3's numbers but gpt3 is of course a  little bit old by now about three years  ago but llama is a fairly recent model  from meta so these are roughly the  orders of magnitude that we're dealing  with when we're doing pre-training  the vocabulary size is usually a couple  10 000 tokens the context length is  usually something like two thousand four  thousand or nowadays even 100 000 and  this governs the maximum number of  integers that the GPT will look at when  it's trying to predict the next integer  in a sequence  um you can see that roughly the number  of parameters is say 65 billion for  llama now even though llama has only 65b  parameters compared to gvt3's 175  billion parameters llama is a  significantly more powerful model and  intuitively that's because the model is  trained for significantly longer in this  case 1.4 trillion tokens instead of just  300 billion tokens so you shouldn't  judge the power of a model just by the  number of parameters that it contains  below I'm showing some tables of rough  number of rough hyper parameters that  typically go into specifying the  Transformer neural network so the number  of heads the dimension size number of  layers and so on and on the bottom I'm  showing some training hyper parameters  so for example to train the 65b model  meta used 2000 gpus roughly 21 days  train of training and and roughly  several million dollars and so that's  the rough orders of magnitude that you  should have in mind for the pre-training  stage  now when we're actually pre-chaining  what happens roughly speaking we are  going to take our tokens and we're going  to lay them out into Data batches so we  have these arrays that will feed into  the Transformer and these arrays are B  the batch size and these are all  independent examples stacked up in rows  and B by T T being the maximum context  length so in my picture I only have 10.  the context length so this could be two  thousand four thousand Etc so these are  extremely long rows and what we do is we  take these documents and we pack them  into rows and we delimit them with these  Special end of text tokens uh basically  it's on the Transformer where a new  document begins  and so here I have a few examples of  documents and then I've stretched them  out into uh into this input  now we're going to feed all of these  numbers into Transformer and let me let  me just focus on a single particular  cell but the same thing will happen at  every every cell in this diagram so  let's look at the green cell the green  cell is going to take a look at all of  the tokens before it so all of the  tokens in yellow and we're going to feed  that entire context into the Transformer  neural network  and the Transformer is going to try to  predict the next token in the sequence  in this case in red  now the Transformer I don't have too  much time to unfortunately go into the  full details of this neural network  architecture is just a large blob of  neural net stuff for our purposes and  it's got several 10 billion parameters  typically or something like that and of  course as they tune these parameters  you're getting slightly different uh  predicted distributions for every single  one of these cells  and so for example if our vocabulary  size is 50 257 tokens then we're going  to have that many numbers because we  need to specify a probability  distribution for what comes next so  basically we have a probability for  whatever may follow now in this specific  example for the specific cell 513 will  come next and so we can use this as a  source of supervision to update our  Transformers weights and so we're  applying this basically on every single  cell in parallel and we keep swap  swapping batches and we're trying to get  the Transformer to make the correct  predictions over what token comes next  in a sequence  so let me show you more concretely what  this looks like when you train one of  these models this is actually coming  from a New York Times and they trained a  small GPT on Shakespeare and so here's a  small snippet of Shakespeare and they  trained a GPT on it now in the beginning  at initialization the GPT starts with  completely random weights so you're just  getting completely random outputs as  well  but over time as you train uh the GPT  longer and longer you are getting more  and more coherent and consistent uh sort  of samples from the model and the way  you sample from it of course is you uh  predict what comes next you sample from  that distribution and you keep feeding  that back into the process and you can  basically uh sample large sequences and  so by the end you see that the  Transformer has learned about words and  where to put spaces and where to put  commas and so on and so we're making  more and more consistent predictions  over time  these are the kinds of plots that you're  looking at when you're doing model  pre-training effectively we're looking  at the loss function over time as you  train and low loss means that our  Transformer is predicting the correct uh  is giving a higher probability to the  correct next integer in a sequence  now what are we going to do with this  model once we've trained it after a  month well the first thing that we  noticed we the field is that these  models basically in the process of  language modeling learn very powerful  General representations and it's  possible to very efficiently fine-tune  them for any arbitrary Downstream task  you might be interested in so as an  example if you're interested in  sentiment classification the approach  used to be that you collect a bunch of  positives and negatives and then you  train some kind of an NLP model for for  that but the new approach is ignore  sentiment classification  go off and do large language model  pre-training train the large Transformer  and then you can only you may only have  a few examples and you can very  efficiently fine tune your model for  that task and so this works very well in  practice and the reason for this is that  basically the Transformer is forced to  multitask a huge amount of tasks in the  language modeling task because just just  in terms of predicting the next token  it's forced to understand a lot about  the structure of the of the text and all  the different concepts they're in  so that was GPT one now around the time  of gpt2 people noticed that actually  even better than fine tuning you can  actually prompt these models very  effectively so these are language models  and they want to complete documents so  you can actually trick them into  performing tasks just by arranging these  fake documents so in this example for uh  for example we have some passage and  then we sort of like do QA qaqa this is  called a few shot prompt and then we do  q and then as the Transformer is trying  to complete the document it's actually  answering our question and so this is an  example of prompt engineering a base  model making the belief that it's sort  of imitating a document and getting it  to perform a task  and so this kicked off I think the era  of I would say prompting over  fine-tuning and seeing that this  actually can work extremely well on a  lot of problems even without training  any neural networks fine-tuning or so on  now since then we've seen an entire  evolutionary tree of Base models that  everyone has trained not all of these  models are available for example the  gpt4 base model was never released the  gpt4 model that you might be interacting  with over API is not a base model it's  an assistant model and we're going to  cover how to get those in a bit gpt3  based model is available via the API  under the name DaVinci and gpt2 base  model is available even as weights on  our GitHub repo but currently the best  available base model probably is the  Llama series from meta although it is  not commercially licensed  now one thing to point out is base  models are not assistance they don't  want to answer to they don't want to  make answers to your questions they just  want to complete documents so if you  tell them write a poem about the brand  cheese it will just you know it will  answer questions with more questions  it's just completing what it thinks as a  document  however you can prompt them in a  specific way for base models that that  is more likely to work so as an example  here's a poem about bread and cheese and  in that case it will auto-complete  correctly  you can even trick base models into  being assistance and the way you would  do this is you would create like a  specific few shot prompt that makes it  look like there's some kind of a  document between a human and assistant  and they're exchanging sort of um  information and then at the bottom you  sort of put your query at the end and  the base model will sort of like  condition itself into being like a  helpful assistant and kind of answer but  this is not very reliable and doesn't  work super well in practice although it  can be done  so instead we have a different path to  make actual gbt assistance not just base  model document completers and so that  takes us into supervised fine-tuning so  in the supervised fine tuning stage we  are going to collect small but high  quality data sets and in this case we're  going to ask human contractors to gather  data of the format of the form prompt  and ideal response and we're going to  collect lots of these typically tens of  thousands or something like that  and then we're going to still do  language modeling on this data so  nothing change algorithmically we're  just swapping out a training set so it  used to be internet documents which is a  high quantity local  four basically QA prompt response kind  of data and that is low quantity high  quality  so we still do language modeling and  then after training we get an sfd model  and you can actually deploy these models  and they are actual assistants and they  work to some extent let me show you what  an example demonstration might look like  so here's something that a human  contractor might come up with here's  some random prompt can you write a short  introduction about the relevance of the  term monopsony or something like that  and then the contractor also writes out  an ideal response and when they write  out these responses they are following  extensive labeling documentations and  they're being asked to be helpful  truthful and harmless and those labeling  instructions here you probably can't  read it another can I but they're long  and this is just people following  instructions and trying to complete  these prompts  so that's what the data set looks like  and you can train these models and this  works to some extent  now you can actually continue the  pipeline from here on and go into rlhf  reinforcement learning from Human  feedback that consists of both reward  modeling and reinforcement learning so  let me cover that and then I'll come  back to why you may want to go through  the extra steps and how that compares to  just sft models  so in the reward modeling step what  we're going to do is we're now going to  shift our data collection to be of the  form of comparisons so here's an example  of what our data set will look like  I have the same prompt identical prompt  on the top which is asking the assistant  to write a program or function that  checks if a given string is a palindrome  and then what we do is we take the sft  model which we've already trained and we  create multiple completions so in this  case we have three completions that the  model has created  and then we ask people to rank these  completions so if you stare at this for  a while and by the way these are very  difficult things to do to compare some  of these predictions and this can take  people even hours for a single prompt  completion pairs but let's say we  decided that one of these is much better  than the others and so on so we rank  them  then we can follow that with something  that looks very much kind of like a  binary classification on all the  possible pairs between these completions  so what we do now is we lay out our  prompt in rows and the prompts is  identical across all three rows here so  it's all the same prompt but the  completion of this vary and so the  yellow tokens are coming from the sft  model then what we do is we append  another special reward uh readout token  at the end and we basically only  supervise the Transformer at this single  green token and the Transformer will  predict some reward for how good that  completion is for that prompt  and so basically it makes a guess about  the quality of each completion and then  once it makes a guess for every one of  them we also have the ground truth which  is telling us the ranking of them and so  we can actually enforce that some of  these numbers should be much higher than  others and so on WE formulate this into  a loss function and we train our model  to make reward predictions that are  consistent with the ground truth coming  from the comparisons from all these  contractors so that's how we train our  reward model and that allows us to score  how good a completion is for a prompt  once we have a reward model this is we  can't deploy this because this is not  very useful as an assistant by itself  but it's very useful for the reinforce  reinforcement learning stage that  follows now because we have a reward  model we can score the quality of any  arbitrary completion for any given  prompt so what we do during  reinforcement learning is we basically  get again a large collection of prompts  and now we do reinforcement learning  with respect to the reward model so  here's what that looks like  we we take a single prompt we lay it out  in rows and now we use the sft we use  the basically the model we'd like to  train which is initialized at sft model  to create some completions in yellow  and then we append the reward token  again and we read off the reward  according to the reward model which is  now kept fixed it doesn't change anymore  and now the reward model tells us the  quality of every single completion for  each for these prompts and so what we  can do is we can now just basically  apply the same language modeling loss  function but we're currently training on  the yellow tokens and we are weighing  the language modeling objective by the  rewards indicated by the reward model so  as an example in the first row the  reward model said that this is a fairly  high score in completion and so all the  tokens that we happen to sample on the  first row are going to get reinforced  and they're going to get higher  probabilities for the future  conversely on the second row the reward  model really did not like this  completion negative 1.2 and so therefore  every single token that we sampled in  that second row is going to get a  slightly higher probability for the  future and we do this over and over on  many prompts on many batches and  basically we get a policy which creates  yellow tokens here and it basically all  of them all of the completions here will  score high according to the reward model  that we trained in the previous stage  so that's how we train uh that's what  the rohf pipeline is  now and then at the end you get a model  that you could deploy and so and as an  example chat GPT is an rlhf model but  some other models that you might come  across like for example of the kuna 13B  and so on these are sft models so we  have base models sft models and rlh  models and that's kind of like the state  of things there  now why would you want to do rlhf so one  answer that is kind of not that exciting  is that it just works better so this  comes from the instructor gbt paper  according to these experiments a while  ago now these PPO models are rlhf and we  see that they are basically just  preferred in a lot of comparisons when  we give them to humans so humans just  prefer out basically tokens that come  from our religion models compared to sft  models compared to base model that is  prompted to be an assistant and so it  just works better but you might ask why  why does it work better and I don't  think that there's a single like amazing  answer that the community has really  like agreed on but I will just offer one  uh one reason potentially and it has to  do with the asymmetry between how easy  computationally it is to compare versus  generate so let's take an example of  generating a haiku suppose I ask a model  to write a haiku about paper clips if  you're a contractor trying to give train  data then imagine being a contractor  collecting basically data for the sft  stage how are you supposed to create a  nice haiku for a paperclip you might  just not be very good at that but if I  give you a few examples of haikus you  might be able to appreciate some of  these haikus a lot more than others and  so judging which one of these is good is  much easier task and so basically this  asymmetry makes it so that comparisons  are a better way to potentially leverage  yourself as a human and your judgment to  create a slightly better model  now our religious models are not  strictly an improvement on the base  models in some cases so in particular  we've noticed for example that they lose  some entropy so that means that they  give More PT results they can output  lower variations uh like they can output  samples with lower variation than base  model so base model has lots of entropy  and we'll go with lots of diverse  outputs  so for example one one kind of place  where I still prefer to use a base model  is in the setup where  you basically have N Things and you want  to generate more things like it and so  here is an example that I just cooked up  I want to generate cool Pokemon names I  gave it seven Pokemon names and I asked  the base model to complete the document  and it gave me a lot more Pokemon names  uh these are fictitious I tried to look  them up I don't believe they're actual  Pokemons and uh this is the kind of task  that I think base model would be good at  because it still has lots of entropy it  will give you lots of diverse cool kind  of more things that look like whatever  you give it before  so this is what uh this is number having  all said all that these are kind of like  the assistant models that are probably  available to you at this point uh that  there's a team at Berkeley that ranked a  lot of the available assistant models  and gave them basically ELO ratings So  currently some of the best models of  course are gpt4 by far I would say  followed by Claude gvt 3.5 and then a  number of models some of these might be  available as weights like the kuna koala  Etc and the first three rows here are  all they're all rohf models and all of  the other models to my knowledge are sft  models I believe  okay so that's how we train these models  on the high level now I'm going to  switch gears and let's look at how we  can best apply and the GPT assistant  model to your problems now I would like  to work  in a setting of a concrete example so  let's  let's work with a concrete example here  let's say that you are working on an  article or a blog post and you're going  to write this sentence at the end  California's population is 53 times that  of Alaska so for some reason you want to  compare the populations of these two  states  think about the rich internal monologue  and Tool use and how much work actually  goes computationally in your brain to  generate this one final sentence so  here's maybe what that could look like  in your brain okay for this next step  let me blog uh for my blog let me  compare these two populations  okay first I'm going to obviously need  to get both of these populations now I  know that I probably don't know these  populations off the top of my head so  I'm kind of like aware of what I know or  don't know of my self-knowledge right  so I go I do some tool use and I go to  Wikipedia and I look up California's  population and Alaska's population  now I know that I should divide the two  but again I know that dividing three 9.2  by 0.74 is very unlikely to succeed  that's not the kind of thing that I can  do in my head and so therefore I'm gonna  rely on the calculator so I'm going to  use a calculator punch it in and see  that the output is roughly 53.  and then maybe I do some reflection and  Sanity checks in my brain so does 53  make sense well that's quite quite a  large fraction but then California is  the most popular state so maybe that  looks okay  so then I have all the information I  might need and now I get to the sort of  creative portion of writing so I might  start to write something like California  has 53 x times greater and then I think  to myself that's actually like really  awkward phrasing so let me actually  delete that and let me try again and so  as I'm writing I have this separate  process almost inspecting what I'm  writing and judging whether it looks  good or not  and then maybe I delete and maybe I  reframe it and then maybe I'm happy with  what comes out  so basically long story short a ton  happens under the hood in terms of your  internal monologue when you create  sentences like this but what does a  sentence like this look like when we are  training a GPT on it  From gpt's perspective this is just a  sequence of tokens  so GPT when it's reading or generating  these tokens it just goes  and each chunk is roughly the same  amount of computational work for each  token and these Transformers are not  very shallow networks they have about 80  layers of reasoning but 80 is still not  like too much and so this Transformer is  going to do its best to imitate but of  course the process here looks very very  different from the process that you took  so in particular in our final artifacts  in the data sets that we create and then  eventually feed to llms all of that  internal dialogue is completely stripped  and uh and unlike you the GPT will look  at every single token and spend the same  amount of compute on every one of them  and so you can't expect it to actually  like well you can't expect it to do to  uh sort of do too much work per token  so and also in particular basically  these Transformers are just like token  simulators so they don't know what they  don't know like they just imitate the  next token they don't know what they're  good at or not good at they just tried  their best imitate the next token they  don't reflect in the loop they don't  sanity check anything they don't correct  their mistakes along the way by default  they just uh sample token sequences  they don't have separate inner monologue  streams in their head right that are  evaluating what's happening now they do  have some sort of cognitive advantages I  would say and that is that they do  actually have a very large fact-based  knowledge across a vast number of areas  because they have say several 10 billion  parameters so it's a lot of storage for  a lot of facts  but and they also I think have a  relatively large and perfect working  memory so whatever fixed into the  whatever fits into the context window is  immediately available to the Transformer  through its internal self-attention  mechanism and so it's kind of like um  Perfect Memory but it's got that finite  size but the Transformer has a very  direct access to it and so it can like  losslessly remember anything that is  inside its context window  so that's kind of how I would compare  those two and the reason I bring all of  this up is because I think to a large  extent prompting is just making up for  this sort of cognitive difference  between these two kind of architectures  um like our brains here and llm brains  you can look at it that way almost  so here's one thing that people found  for example works pretty well in  practice especially if your tasks  require reasoning you can't expect the  Transformer to make to to do too much  reasoning per token and so you have to  really spread out the reasoning across  more and more tokens so for example you  can't give a Transformer a very  complicated question and expect it to  get the answer in a single token there's  just not enough time for it these  Transformers need tokens to think quote  unquote I like to say sometimes and so  this is some of the things that work  well you may for example have a few shot  prompt that shows the Transformer that  it should like show its work when it's  answering a question when it's answering  the question and if you give a few  examples the Transformer will imitate  that template and it will just end up  working out better in terms of its  evaluation  Additionally you can elicit this kind of  behavior from the Transformer by saying  let's think step by step because this  condition is the transformer into sort  of like showing its work and because it  kind of snaps into a mode of showing its  work it's going to do less computational  work per token and so it's more likely  to succeed as a result because it's  making  um slower reasoning over time  here's another example this one is  called self-consistency we saw that we  had the ability to start writing and  then if it didn't work out I can try  again and I can try multiple times and  uh and maybe  um select the one that worked best so in  these kinds of approaches you may sample  not just once but you may sample  multiple times and then have some  process for finding the ones that aren't  good and then keeping just those samples  or doing a majority vote or something  like that so basically these  Transformers in the process as they  predict the next token just like you  they can get unlucky and they can they  could sample and not a very good token  and they can go down sort of like a  blind alley in terms of reasoning and so  unlike you they cannot recover from that  they are stuck with every single token  they sample and so they will continue  the sequence even if they even know that  this sequence is not going to work out  so give them the ability to look back  inspect or try to find uh try to  basically sample around it  here's one technique also you could it  turns out that actually llms like they  know when they've screwed up so as an  example  say you asked the model to generate a  poem that does not rhyme and it might  give you a poem but it actually Rhymes  but it turns out that especially for the  bigger models like gpt4 you can just ask  it did you meet the assignment and  actually gpt4 knows very well that it  did not meet the assignment it just kind  of got unlucky in its sampling and so it  will tell you no I didn't actually meet  the assignment here's let me try again  but without you prompting it it doesn't  even like it doesn't know it doesn't  know to revisit and uh and so on so you  have to make up for that in your prompt  you have to get it to check if you don't  ask it to check it's not going to check  by itself it's just a token simulator  I think more generally a lot of these  techniques fall into the bucket of what  I would say recreating our system too so  you might be familiar with the system  one system to thinking for humans system  one is a fast automatic process and I  think kind of corresponds to like an llm  just sampling tokens  and system two is the slower deliberate  planning sort of part of your brain  and so this is a paper actually from  just last week because this space is  pretty quickly evolving it's called tree  of thought and in tree of thought uh the  authors of this paper propose  maintaining multiple completions for any  given prompt and then they are also  scoring them along the way and keeping  the ones that are going well if that  makes sense and so a lot of people are  like really playing around with kind of  um prompt engineering to um to basically  bring back some of these abilities that  we sort of have in our brain for llms  now one thing I would like to note here  is that this is not just a prompt this  is actually prompts that are together  used with some python glue code because  you don't you actually have to maintain  multiple prompts and you also have to do  some tree search algorithm here um to  like figure out which prompt to expand  Etc so it's a symbiosis of python glue  code and individual prompts that are  called in a while loop or in a bigger  algorithm  I also think there's a really cool  parallel here to alphago alphago has a  policy for placing the next Stone when  it plays go and its policy was trained  originally by imitating humans but in  addition to this policy it also does  Monte Carlo tree search and basically it  will play out a number of possibilities  in its head and evaluate all of them and  only keep the ones that work well and so  I think this is kind of an equivalent of  alphago but for text  if that makes sense  so just like tree of thought I think  more generally people are starting to  like really Explore More General  techniques of not just the simple  question answer prompt but something  that looks a lot more like python blue  code stringing together many problems so  on the right I have an example from this  paper called react where they structure  the answer to a prompt as a sequence of  thought action observation thought  action observation and it's a full  rollout a kind of a thinking process to  answer the query and in these actions  the model is also allowed to to lose  on the left I have an example of rogpt  and now rgbt by the way became as a  project that I think got a lot of hype  recently and I think uh but I think I  still find it kind of inspirationally  interesting it's um it's a project that  allows an llm to sort of keep task list  and continue to recursively break down  tasks and I don't think this currently  works very well and I would not advise  people to use it in Practical  applications I just think it's something  to generally take inspiration from in  terms of where this is going I think  over time  so that's kind of like giving our model  system to thinking the next thing that I  find kind of interesting is  this following survey would say almost  psychological Quirk of llms is that llms  don't want to succeed they want to  imitate  you want to succeed and you should ask  for it  so what I mean by that is  when Transformers are trained they have  training sets and there can be an entire  spectrum of performance qualities in  their training data so for example there  could be some kind of a prompt for some  Physics question or something like that  and there could be a student solution  that is completely wrong but there can  also be an expert answer that is  extremely right and Transformers can't  tell the difference between like look I  mean they know they know about low  quality Solutions and high quality  Solutions but by default they want to  imitate all of it because they're just  trained on language modeling and so at  test time you actually have to ask for a  good performance so in this example in  this paper it's um they tried various  prompts and let's think step by step was  very powerful because it's sort of like  spread out the reasoning of remaining  tokens but what worked even better is  let's work this out in a step-by-step  way to be sure we have the right answer  and so it's kind of like a conditioning  on getting a right answer and this  actually makes the Transformer work  better because the Transformer doesn't  have to now hedge its probability Mass  on low quality Solutions as ridiculous  as that sounds and so basically don't be  feel free to ask for a strong solution  say something like you are a leading  expert on this topic pretend you have  iq120 Etc but don't try to ask for too  much IQ because if you ask for IQ of  like 400 you might be out of data  distribution or even worse you could be  in data distribution for some like  sci-fi stuff and it will start to like  take on some sci-fi or like role-playing  or something like that so you have to  find like the right amount of IQ I think  it's got some U-shaped curve there  next up  as we saw when we are trying to solve  problems we know we are good at and what  we're not good at and we Lean On Tools  computationally you want to do the same  potentially with your uh llms so in  particular we may want to give them  calculators code interpreters and so on  the ability to do search and uh there's  a lot of techniques for doing that one  thing to keep in mind again is that  these Transformers by default may not  know what they not don't know so you may  even want to tell the transformer in the  prompt you are not very good at mental  arithmetic whenever you need to do very  large number addition multiplication or  whatever instead use this calculator  here's how you use the calculator use  this token combination etc etc so you  have to actually like spell it out  because the model by default doesn't  know what it's good at or not good at  necessarily just like you and I you and  I might be  next up I think something that is very  interesting is we went from a world that  was retrieval only all the way the  pendulum has swung to The Other Extreme  where it's memory only in lens but  actually there's this entire space in  between of these retrieval augmented  models and this works extremely well in  practice  as I mentioned the context window of a  transformer is its working memory if you  can load the working memory with any  information that is relevant to the task  the model will work extremely well  because it can immediately access all  that memory  and so I think a lot of people are  really interested in  uh basically retrieval augmented  generation and on the bottom I have like  an example of llama index which is one  sort of data connector to lots of  different types of data and you can uh  all you can make it you can index all of  that data and you can make it accessible  to llms and the emerging recipe there is  you take relevant documents you split  them up into chunks you embed all of  them and you basically get embedding  vectors that represent that data you  store that in a vector store and then at  test time you make some kind of a query  to your vector store and you fetch  chunks that might be relevant to your  task and you stuff them into the prompt  and then you generate so this can work  quite well in practice so this is I  think similar to when you're and I solve  problems you can do everything from your  memory and Transformers have very large  and extensive memory but also it really  helps to reference some primary  documents so when you whenever you find  yourself going back to a textbook to  find something or whenever you find  yourself going back to the documentation  of a library to look something up the  transform transformers definitely when I  do that too you you have a mem you have  some memory over how some documentation  of a library works but it's much better  to look it up so the same same applies  here  next I wanted to briefly talk about  constraint prompting I also find this  very interesting  uh this is basically techniques for  um  forcing a certain template in the  outputs of llms so guidance is one  example from Microsoft actually and uh  here we are enforcing that the output  from the llm will be Json and this will  actually guarantee that the output will  take on this form because they go in and  they mess with the probabilities of all  the different tokens that come out of  the Transformer and if they clamp those  tokens and then the Transformer is only  filling in the blanks here and then you  can enforce additional restrictions on  what could go into those blanks so this  might be really helpful and I think this  kind of constraint sampling is also  extremely interesting  I also wanted to say a few words about  fine tuning it is the case that you can  get really far with prompt engineering  but it's also possible to think about  fine-tuning your models now fine-tuning  models means that you are actually going  to change the weights of the model  it is becoming a lot more accessible to  do this in practice and that's because  of a number of techniques that have been  developed and have libraries for uh very  recently so for example parameter  efficient fine-tuning techniques like  Laura make sure that you're only Trend  you're only training small Spar species  of your model so most of the model is  kept clamped at the base model and some  pieces of it are allowed to change and  this still works pretty well empirically  and makes it much cheaper to sort of  tune only small pieces of your model  uh there's also it also means that  because most of your model is clamped  you can use very low Precision inference  for computing those parts because they  are not going to be updated by gradient  descent and so that makes everything a  lot more efficient as well and in  addition we have a number of Open Source  high quality based models currently as I  mentioned I think llama is quite nice  although it is not commercially licensed  I believe right now  some things to keep in mind is that  basically fine tuning is is a lot more  technically involved it requires a lot  more I think technical expertise to do  to do right it requires human data  contractors for data sets and or  synthetic data pipelines that can be  pretty complicated this will definitely  slow down your iteration cycle by a lot  and I would say on a high level sft is  achievable because it is just your  continuing language modeling task It's  relatively straightforward but rlhref I  would say is very much research  territory and is even much harder to get  to work and so I would probably not  advise that someone just tries to roll  their own rlh of implementation these  things are pretty unstable very  difficult to train uh not something that  is I think very beginner friendly right  now and it's also potentially likely  also to to change pretty rapidly still  so I think these are my sort of default  recommendations right now  I would break up your task into two  major parts number one achieve your top  performance and number two optimize your  performance in that order  number one the best performance will  currently come from GT4 model it is the  most capable of by far  use prompts that are very detailed they  have lots of task contents relative  relevant information and instructions  think along the lines of what would you  tell a task contractor if they can't  email you back but then also keep in  mind that a task contractor is a human  and they have inner monologue and  they're very clever Etc llms do not  possess those qualities so make sure to  think through the psychology of the llm  almost and cater prompts to that  retrieve and add any relevant context  and information to these prompts  basically refer to a lot of the prompt  engineering techniques some of them are  highlighted in the slides above but also  this is a very large space and I would  just advise you to look for prompt  engineering techniques online there's a  lot to cover there  experiment with few shop examples what  this refers to is you don't just want to  tell you want to show whenever it's  possible so give it examples of  everything that helps it really  understand what you mean if you can  tools and plugins to offload a tasks  that are difficult for llms natively uh  and then think about not just a single  prompt and answer think about potential  chains and reflection and how you glue  them together and how you could  potentially make multiple samples and so  on  finally if you think you've squeezed out  prompt engineering which I think you  should stick with for a while look at  some potentially fine-tuning a model to  your application but expect this to be a  lot more slower and evolved and then  there's an expert fragile research Zone  here and I would say that is rlhf which  currently does work a bit better than  sft if you can get it to work but again  this is uh pretty involved I would say  and to optimize your costs try to  explore lower capacity models or shorter  prompts and so on  I also wanted to say a few words about  the use cases in which I think llms are  currently well suited for so in  particular note that there's a large  number of limitations to llms today and  uh so I would keep that definitely in  mind for all your applications models  and this by the way could be an entire  talk so I don't have time to cover it in  full detail models may be biased they  may fabricate hallucinate information  they may have reasoning errors they may  struggle in entire classes of  applications they have knowledge cutoffs  so they might not know any information  above say September 2021 they are  susceptible to a large range of attacks  which are sort of like coming out on  Twitter daily including prompt injection  jailbreak attacks data poisoning attacks  and so on so my recommendation right now  is use llms in low stakes applications  combine them with always with human  oversight use them as a source of  inspiration and suggestions and think  co-pilots instead of completely  autonomous agents that are just like  performing a task somewhere it's just  not clear that the models are there  right now  so I wanted to close by saying that gpt4  is an amazing artifact I'm very thankful  that it exists and it's it's beautiful  it has a ton of knowledge across so many  areas it can do math code and so on and  in addition there's this thriving  ecosystem of everything else that is  being built and incorporated into into  the ecosystem uh some of some of the  things these things I've talked about  and all of this power is accessible at  your fingertips  so here's everything that's needed in  terms of code to ask GPT for a question  to prompt it and get a response in this  case I said uh can you say something to  inspire the audience of Microsoft build  2023  and I just punched this into Python and  verbatim gpt4 said the following  uh and by the way I did not know that  they used this trick in the keynote so I  thought I was being clever but  but it is really good at this it says  ladies and gentlemen Invaders and  Trailblazers Microsoft build 2023  welcome to the Gathering of Brilliant  Minds like no other you are The  Architects of the future The Visionaries  molding the digital realm in which  Humanity thrives embrace the Limitless  possibilities of Technologies and let  your ideas soar as high as your  imagination together let's create a more  connected remarkable and inclusive world  for generations to come get ready to  unleash your creativity canvas the  unknown and turn dreams into reality  your Journey Begins today  okay\n",
      "\n",
      "\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GBT \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " GPT \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " GPU\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " GPU \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Meta \n",
      "\n",
      " llama \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " C4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GitHub  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GPT \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "gpts \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " gpt4 \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "gpt3  gpt3 \n",
      "\n",
      "\n",
      "\n",
      " llama  meta \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " pre   - \n",
      "\n",
      "\n",
      "\n",
      "10 000 \n",
      "\n",
      "\n",
      "204000  100000 \n",
      "\n",
      "\n",
      "\n",
      "GPT \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " llama \n",
      "\n",
      "650 \n",
      "\n",
      " llama\n",
      "\n",
      " gvt3  1750\n",
      "\n",
      " 65b  llama \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " 1.4 \n",
      "\n",
      "3000 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65b \n",
      "\n",
      " 2000 GPU  21 \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Transformer  B\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "B  T T \n",
      "\n",
      "  10.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Transformer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " Transformer\n",
      "\n",
      "\n",
      "\n",
      "Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Transformer \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50 257 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 513\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Transformers \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " GPT\n",
      "\n",
      "\n",
      "\n",
      " GPT\n",
      "\n",
      "GPT \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  GPT \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Transformer \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " NLP \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Transformer\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " GPT \n",
      "\n",
      "gpt2 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " QA qaqa \n",
      "\n",
      "\n",
      "\n",
      "q Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "gpt4 \n",
      "\n",
      " API  gpt4 \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " gpt3 \n",
      "\n",
      " DaVinci  API  gpt2 \n",
      "\n",
      "\n",
      "\n",
      " GitHub \n",
      "\n",
      "\n",
      "\n",
      "metaLlama\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " GBT \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " QA \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " sfd \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " rlhf \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " sft \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " sft \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " sft\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Transformer\n",
      "\n",
      " Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " sft \n",
      "\n",
      "\n",
      "\n",
      " sft \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 1.2 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " rohf \n",
      "\n",
      " \n",
      "\n",
      " GPT  rlhf \n",
      "\n",
      "\n",
      "\n",
      " kuna 13B \n",
      "\n",
      " sft \n",
      "\n",
      " sft  rlh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " rlhf \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " GBT \n",
      "\n",
      "\n",
      "\n",
      " PPO  rlhf \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " sft\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " sft \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " PT \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " N \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ELO  \n",
      "\n",
      "\n",
      "\n",
      " gpt4 \n",
      "\n",
      " Claude gvt 3.5 \n",
      "\n",
      " \n",
      "\n",
      " kuna \n",
      "\n",
      "\n",
      "\n",
      "rohf \n",
      "\n",
      " sft\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " GPT \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 3 9.2\n",
      "\n",
      " 0.74 \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53\n",
      "\n",
      "\n",
      "\n",
      "  53\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " GPT \n",
      "\n",
      " gpt \n",
      "\n",
      "\n",
      "\n",
      " GPT \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " token \n",
      "\n",
      " Transformer \n",
      "\n",
      " 80\n",
      "\n",
      " 80 \n",
      "\n",
      " Transformer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " llms\n",
      "\n",
      "\n",
      "\n",
      "GPT \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Transformer \n",
      "\n",
      " \n",
      "\n",
      " Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " llm \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Transformer \n",
      "\n",
      " Transformer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "' \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "llms\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " gpt4 \n",
      "\n",
      "\n",
      "\n",
      " gpt4 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " python \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " python \n",
      "\n",
      "\n",
      "\n",
      " while \n",
      "\n",
      "\n",
      " alphago \n",
      "\n",
      " alphago \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "alphago \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Python\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " rogpt \n",
      "\n",
      " rgbt \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " llm \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " llms  llms\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Transformer \n",
      "\n",
      " Transformer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "120\n",
      "\n",
      "\n",
      "\n",
      " 400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " U \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " LLMS \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " llama \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "llms \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " llms \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "llm  Json\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Transformer \n",
      "\n",
      " \n",
      "\n",
      " Transformer \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Laura \n",
      "\n",
      " \n",
      "\n",
      " Spar \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " llama \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  sft \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " rlhref \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " rlh \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " GT4 \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "LLM\n",
      "\n",
      "\n",
      "\n",
      "LLM\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " llms \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " rlhf\n",
      "\n",
      " sft \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " llms \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " llms \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " 2021  9  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Twitter \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " llms\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " gpt4\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GPT \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Microsoft build 2023 \n",
      "\n",
      "\n",
      " Python \n",
      "\n",
      " gpt4 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 2023\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for reading lines from a file\n",
    "with open('build.txt', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        print(line, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76720dfc-afd8-4ffc-b50d-c7281266ad84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"foreign  [Music]  researcher and founding member of open  AI Andre carpathy  [Applause]  hi everyone  I'm happy to be here to tell you about  the state of gbt and more generally  about the rapidly growing ecosystem of  large language models  so I would like to partition the talk  into two parts  in the first part I would like to tell  you about how we train GPT assistance  and then in the second part we will we  are going to take a look at how we can  use these assistants effectively for  your applications  so first let's take a look at the  emerging recipe for how to train these  assistants and keep in mind that this is  all very new and still rapidly evolving  but so far the recipe looks something  like this  now this is kind of a complicated slide  so I'm going to go through it piece by  piece but roughly speaking we have four  major stages free training supervised  fine tuning reward modeling  reinforcement learning and they follow  each other serially  now in each stage we have a data set  that is that powers that stage we have  an algorithm that for our purposes will  be a objective and a and over for uh for  training the neural network and then we  have a resulting model and then there's  some notes on the bottom so the first  stage we're going to start with is the  pre-training stage now this stage is  kind of special in this diagram and this  diagram is not to scale because this  stage is where all of the computational  work basically happens this is 99 of the  training uh compute time and also flops  and so this is where we are dealing with  internet scale data sets with thousands  of gpus in the store computer and also  months of training potentially the other  three stages are fine-tuning stages that  are much more along the lines of a small  few number of gpus and hours or days  so let's take a look at the pre-training  stage to achieve a base model  first we're going to gather a large  amount of data here's an example of what  we call a data mixture that comes from  this uh paper that was released by meta  where they released this llama based  model now you can see roughly the kinds  of data sets that enter into these  collections so we have common crawl  which is just a web scrape C4 which is  also a common crawl and then some high  quality data sets as well so for example  GitHub Wikipedia books archive stack  exchange and so on these are all mixed  up together and then they are sampled  According to some given proportions and  that forms the training set for the  neural net for the GPT  now before we can actually train on this  data we need to go through one more  pre-processing step and that is  tokenization and this is basically a  translation of the raw text that we  scrape from the internet into sequences  of integers because that's the native  representation over which gpts function  now this is a lossless kind of  translation between pieces of text and  tokens and integers and there are a  number of algorithms for this stage  typically for example you could use  something like byte pair encoding which  iteratively merges little text chunks  and groups them into tokens  and so here I'm showing some example  chunks of these tokens and then this is  the raw integer sequence that will  actually feed into a Transformer  now here I'm showing two uh sort of like  examples for hyper parameters that  govern this stage so gpt4 we did not  release too much information about how  it was trained and so on so I'm using  gpt3's numbers but gpt3 is of course a  little bit old by now about three years  ago but llama is a fairly recent model  from meta so these are roughly the  orders of magnitude that we're dealing  with when we're doing pre-training  the vocabulary size is usually a couple  10 000 tokens the context length is  usually something like two thousand four  thousand or nowadays even 100 000 and  this governs the maximum number of  integers that the GPT will look at when  it's trying to predict the next integer  in a sequence  um you can see that roughly the number  of parameters is say 65 billion for  llama now even though llama has only 65b  parameters compared to gvt3's 175  billion parameters llama is a  significantly more powerful model and  intuitively that's because the model is  trained for significantly longer in this  case 1.4 trillion tokens instead of just  300 billion tokens so you shouldn't  judge the power of a model just by the  number of parameters that it contains  below I'm showing some tables of rough  number of rough hyper parameters that  typically go into specifying the  Transformer neural network so the number  of heads the dimension size number of  layers and so on and on the bottom I'm  showing some training hyper parameters  so for example to train the 65b model  meta used 2000 gpus roughly 21 days  train of training and and roughly  several million dollars and so that's  the rough orders of magnitude that you  should have in mind for the pre-training  stage  now when we're actually pre-chaining  what happens roughly speaking we are  going to take our tokens and we're going  to lay them out into Data batches so we  have these arrays that will feed into  the Transformer and these arrays are B  the batch size and these are all  independent examples stacked up in rows  and B by T T being the maximum context  length so in my picture I only have 10.  the context length so this could be two  thousand four thousand Etc so these are  extremely long rows and what we do is we  take these documents and we pack them  into rows and we delimit them with these  Special end of text tokens uh basically  it's on the Transformer where a new  document begins  and so here I have a few examples of  documents and then I've stretched them  out into uh into this input  now we're going to feed all of these  numbers into Transformer and let me let  me just focus on a single particular  cell but the same thing will happen at  every every cell in this diagram so  let's look at the green cell the green  cell is going to take a look at all of  the tokens before it so all of the  tokens in yellow and we're going to feed  that entire context into the Transformer  neural network  and the Transformer is going to try to  predict the next token in the sequence  in this case in red  now the Transformer I don't have too  much time to unfortunately go into the  full details of this neural network  architecture is just a large blob of  neural net stuff for our purposes and  it's got several 10 billion parameters  typically or something like that and of  course as they tune these parameters  you're getting slightly different uh  predicted distributions for every single  one of these cells  and so for example if our vocabulary  size is 50 257 tokens then we're going  to have that many numbers because we  need to specify a probability  distribution for what comes next so  basically we have a probability for  whatever may follow now in this specific  example for the specific cell 513 will  come next and so we can use this as a  source of supervision to update our  Transformers weights and so we're  applying this basically on every single  cell in parallel and we keep swap  swapping batches and we're trying to get  the Transformer to make the correct  predictions over what token comes next  in a sequence  so let me show you more concretely what  this looks like when you train one of  these models this is actually coming  from a New York Times and they trained a  small GPT on Shakespeare and so here's a  small snippet of Shakespeare and they  trained a GPT on it now in the beginning  at initialization the GPT starts with  completely random weights so you're just  getting completely random outputs as  well  but over time as you train uh the GPT  longer and longer you are getting more  and more coherent and consistent uh sort  of samples from the model and the way  you sample from it of course is you uh  predict what comes next you sample from  that distribution and you keep feeding  that back into the process and you can  basically uh sample large sequences and  so by the end you see that the  Transformer has learned about words and  where to put spaces and where to put  commas and so on and so we're making  more and more consistent predictions  over time  these are the kinds of plots that you're  looking at when you're doing model  pre-training effectively we're looking  at the loss function over time as you  train and low loss means that our  Transformer is predicting the correct uh  is giving a higher probability to the  correct next integer in a sequence  now what are we going to do with this  model once we've trained it after a  month well the first thing that we  noticed we the field is that these  models basically in the process of  language modeling learn very powerful  General representations and it's  possible to very efficiently fine-tune  them for any arbitrary Downstream task  you might be interested in so as an  example if you're interested in  sentiment classification the approach  used to be that you collect a bunch of  positives and negatives and then you  train some kind of an NLP model for for  that but the new approach is ignore  sentiment classification  go off and do large language model  pre-training train the large Transformer  and then you can only you may only have  a few examples and you can very  efficiently fine tune your model for  that task and so this works very well in  practice and the reason for this is that  basically the Transformer is forced to  multitask a huge amount of tasks in the  language modeling task because just just  in terms of predicting the next token  it's forced to understand a lot about  the structure of the of the text and all  the different concepts they're in  so that was GPT one now around the time  of gpt2 people noticed that actually  even better than fine tuning you can  actually prompt these models very  effectively so these are language models  and they want to complete documents so  you can actually trick them into  performing tasks just by arranging these  fake documents so in this example for uh  for example we have some passage and  then we sort of like do QA qaqa this is  called a few shot prompt and then we do  q and then as the Transformer is trying  to complete the document it's actually  answering our question and so this is an  example of prompt engineering a base  model making the belief that it's sort  of imitating a document and getting it  to perform a task  and so this kicked off I think the era  of I would say prompting over  fine-tuning and seeing that this  actually can work extremely well on a  lot of problems even without training  any neural networks fine-tuning or so on  now since then we've seen an entire  evolutionary tree of Base models that  everyone has trained not all of these  models are available for example the  gpt4 base model was never released the  gpt4 model that you might be interacting  with over API is not a base model it's  an assistant model and we're going to  cover how to get those in a bit gpt3  based model is available via the API  under the name DaVinci and gpt2 base  model is available even as weights on  our GitHub repo but currently the best  available base model probably is the  Llama series from meta although it is  not commercially licensed  now one thing to point out is base  models are not assistance they don't  want to answer to they don't want to  make answers to your questions they just  want to complete documents so if you  tell them write a poem about the brand  cheese it will just you know it will  answer questions with more questions  it's just completing what it thinks as a  document  however you can prompt them in a  specific way for base models that that  is more likely to work so as an example  here's a poem about bread and cheese and  in that case it will auto-complete  correctly  you can even trick base models into  being assistance and the way you would  do this is you would create like a  specific few shot prompt that makes it  look like there's some kind of a  document between a human and assistant  and they're exchanging sort of um  information and then at the bottom you  sort of put your query at the end and  the base model will sort of like  condition itself into being like a  helpful assistant and kind of answer but  this is not very reliable and doesn't  work super well in practice although it  can be done  so instead we have a different path to  make actual gbt assistance not just base  model document completers and so that  takes us into supervised fine-tuning so  in the supervised fine tuning stage we  are going to collect small but high  quality data sets and in this case we're  going to ask human contractors to gather  data of the format of the form prompt  and ideal response and we're going to  collect lots of these typically tens of  thousands or something like that  and then we're going to still do  language modeling on this data so  nothing change algorithmically we're  just swapping out a training set so it  used to be internet documents which is a  high quantity local  four basically QA prompt response kind  of data and that is low quantity high  quality  so we still do language modeling and  then after training we get an sfd model  and you can actually deploy these models  and they are actual assistants and they  work to some extent let me show you what  an example demonstration might look like  so here's something that a human  contractor might come up with here's  some random prompt can you write a short  introduction about the relevance of the  term monopsony or something like that  and then the contractor also writes out  an ideal response and when they write  out these responses they are following  extensive labeling documentations and  they're being asked to be helpful  truthful and harmless and those labeling  instructions here you probably can't  read it another can I but they're long  and this is just people following  instructions and trying to complete  these prompts  so that's what the data set looks like  and you can train these models and this  works to some extent  now you can actually continue the  pipeline from here on and go into rlhf  reinforcement learning from Human  feedback that consists of both reward  modeling and reinforcement learning so  let me cover that and then I'll come  back to why you may want to go through  the extra steps and how that compares to  just sft models  so in the reward modeling step what  we're going to do is we're now going to  shift our data collection to be of the  form of comparisons so here's an example  of what our data set will look like  I have the same prompt identical prompt  on the top which is asking the assistant  to write a program or function that  checks if a given string is a palindrome  and then what we do is we take the sft  model which we've already trained and we  create multiple completions so in this  case we have three completions that the  model has created  and then we ask people to rank these  completions so if you stare at this for  a while and by the way these are very  difficult things to do to compare some  of these predictions and this can take  people even hours for a single prompt  completion pairs but let's say we  decided that one of these is much better  than the others and so on so we rank  them  then we can follow that with something  that looks very much kind of like a  binary classification on all the  possible pairs between these completions  so what we do now is we lay out our  prompt in rows and the prompts is  identical across all three rows here so  it's all the same prompt but the  completion of this vary and so the  yellow tokens are coming from the sft  model then what we do is we append  another special reward uh readout token  at the end and we basically only  supervise the Transformer at this single  green token and the Transformer will  predict some reward for how good that  completion is for that prompt  and so basically it makes a guess about  the quality of each completion and then  once it makes a guess for every one of  them we also have the ground truth which  is telling us the ranking of them and so  we can actually enforce that some of  these numbers should be much higher than  others and so on WE formulate this into  a loss function and we train our model  to make reward predictions that are  consistent with the ground truth coming  from the comparisons from all these  contractors so that's how we train our  reward model and that allows us to score  how good a completion is for a prompt  once we have a reward model this is we  can't deploy this because this is not  very useful as an assistant by itself  but it's very useful for the reinforce  reinforcement learning stage that  follows now because we have a reward  model we can score the quality of any  arbitrary completion for any given  prompt so what we do during  reinforcement learning is we basically  get again a large collection of prompts  and now we do reinforcement learning  with respect to the reward model so  here's what that looks like  we we take a single prompt we lay it out  in rows and now we use the sft we use  the basically the model we'd like to  train which is initialized at sft model  to create some completions in yellow  and then we append the reward token  again and we read off the reward  according to the reward model which is  now kept fixed it doesn't change anymore  and now the reward model tells us the  quality of every single completion for  each for these prompts and so what we  can do is we can now just basically  apply the same language modeling loss  function but we're currently training on  the yellow tokens and we are weighing  the language modeling objective by the  rewards indicated by the reward model so  as an example in the first row the  reward model said that this is a fairly  high score in completion and so all the  tokens that we happen to sample on the  first row are going to get reinforced  and they're going to get higher  probabilities for the future  conversely on the second row the reward  model really did not like this  completion negative 1.2 and so therefore  every single token that we sampled in  that second row is going to get a  slightly higher probability for the  future and we do this over and over on  many prompts on many batches and  basically we get a policy which creates  yellow tokens here and it basically all  of them all of the completions here will  score high according to the reward model  that we trained in the previous stage  so that's how we train uh that's what  the rohf pipeline is  now and then at the end you get a model  that you could deploy and so and as an  example chat GPT is an rlhf model but  some other models that you might come  across like for example of the kuna 13B  and so on these are sft models so we  have base models sft models and rlh  models and that's kind of like the state  of things there  now why would you want to do rlhf so one  answer that is kind of not that exciting  is that it just works better so this  comes from the instructor gbt paper  according to these experiments a while  ago now these PPO models are rlhf and we  see that they are basically just  preferred in a lot of comparisons when  we give them to humans so humans just  prefer out basically tokens that come  from our religion models compared to sft  models compared to base model that is  prompted to be an assistant and so it  just works better but you might ask why  why does it work better and I don't  think that there's a single like amazing  answer that the community has really  like agreed on but I will just offer one  uh one reason potentially and it has to  do with the asymmetry between how easy  computationally it is to compare versus  generate so let's take an example of  generating a haiku suppose I ask a model  to write a haiku about paper clips if  you're a contractor trying to give train  data then imagine being a contractor  collecting basically data for the sft  stage how are you supposed to create a  nice haiku for a paperclip you might  just not be very good at that but if I  give you a few examples of haikus you  might be able to appreciate some of  these haikus a lot more than others and  so judging which one of these is good is  much easier task and so basically this  asymmetry makes it so that comparisons  are a better way to potentially leverage  yourself as a human and your judgment to  create a slightly better model  now our religious models are not  strictly an improvement on the base  models in some cases so in particular  we've noticed for example that they lose  some entropy so that means that they  give More PT results they can output  lower variations uh like they can output  samples with lower variation than base  model so base model has lots of entropy  and we'll go with lots of diverse  outputs  so for example one one kind of place  where I still prefer to use a base model  is in the setup where  you basically have N Things and you want  to generate more things like it and so  here is an example that I just cooked up  I want to generate cool Pokemon names I  gave it seven Pokemon names and I asked  the base model to complete the document  and it gave me a lot more Pokemon names  uh these are fictitious I tried to look  them up I don't believe they're actual  Pokemons and uh this is the kind of task  that I think base model would be good at  because it still has lots of entropy it  will give you lots of diverse cool kind  of more things that look like whatever  you give it before  so this is what uh this is number having  all said all that these are kind of like  the assistant models that are probably  available to you at this point uh that  there's a team at Berkeley that ranked a  lot of the available assistant models  and gave them basically ELO ratings So  currently some of the best models of  course are gpt4 by far I would say  followed by Claude gvt 3.5 and then a  number of models some of these might be  available as weights like the kuna koala  Etc and the first three rows here are  all they're all rohf models and all of  the other models to my knowledge are sft  models I believe  okay so that's how we train these models  on the high level now I'm going to  switch gears and let's look at how we  can best apply and the GPT assistant  model to your problems now I would like  to work  in a setting of a concrete example so  let's  let's work with a concrete example here  let's say that you are working on an  article or a blog post and you're going  to write this sentence at the end  California's population is 53 times that  of Alaska so for some reason you want to  compare the populations of these two  states  think about the rich internal monologue  and Tool use and how much work actually  goes computationally in your brain to  generate this one final sentence so  here's maybe what that could look like  in your brain okay for this next step  let me blog uh for my blog let me  compare these two populations  okay first I'm going to obviously need  to get both of these populations now I  know that I probably don't know these  populations off the top of my head so  I'm kind of like aware of what I know or  don't know of my self-knowledge right  so I go I do some tool use and I go to  Wikipedia and I look up California's  population and Alaska's population  now I know that I should divide the two  but again I know that dividing three 9.2  by 0.74 is very unlikely to succeed  that's not the kind of thing that I can  do in my head and so therefore I'm gonna  rely on the calculator so I'm going to  use a calculator punch it in and see  that the output is roughly 53.  and then maybe I do some reflection and  Sanity checks in my brain so does 53  make sense well that's quite quite a  large fraction but then California is  the most popular state so maybe that  looks okay  so then I have all the information I  might need and now I get to the sort of  creative portion of writing so I might  start to write something like California  has 53 x times greater and then I think  to myself that's actually like really  awkward phrasing so let me actually  delete that and let me try again and so  as I'm writing I have this separate  process almost inspecting what I'm  writing and judging whether it looks  good or not  and then maybe I delete and maybe I  reframe it and then maybe I'm happy with  what comes out  so basically long story short a ton  happens under the hood in terms of your  internal monologue when you create  sentences like this but what does a  sentence like this look like when we are  training a GPT on it  From gpt's perspective this is just a  sequence of tokens  so GPT when it's reading or generating  these tokens it just goes  and each chunk is roughly the same  amount of computational work for each  token and these Transformers are not  very shallow networks they have about 80  layers of reasoning but 80 is still not  like too much and so this Transformer is  going to do its best to imitate but of  course the process here looks very very  different from the process that you took  so in particular in our final artifacts  in the data sets that we create and then  eventually feed to llms all of that  internal dialogue is completely stripped  and uh and unlike you the GPT will look  at every single token and spend the same  amount of compute on every one of them  and so you can't expect it to actually  like well you can't expect it to do to  uh sort of do too much work per token  so and also in particular basically  these Transformers are just like token  simulators so they don't know what they  don't know like they just imitate the  next token they don't know what they're  good at or not good at they just tried  their best imitate the next token they  don't reflect in the loop they don't  sanity check anything they don't correct  their mistakes along the way by default  they just uh sample token sequences  they don't have separate inner monologue  streams in their head right that are  evaluating what's happening now they do  have some sort of cognitive advantages I  would say and that is that they do  actually have a very large fact-based  knowledge across a vast number of areas  because they have say several 10 billion  parameters so it's a lot of storage for  a lot of facts  but and they also I think have a  relatively large and perfect working  memory so whatever fixed into the  whatever fits into the context window is  immediately available to the Transformer  through its internal self-attention  mechanism and so it's kind of like um  Perfect Memory but it's got that finite  size but the Transformer has a very  direct access to it and so it can like  losslessly remember anything that is  inside its context window  so that's kind of how I would compare  those two and the reason I bring all of  this up is because I think to a large  extent prompting is just making up for  this sort of cognitive difference  between these two kind of architectures  um like our brains here and llm brains  you can look at it that way almost  so here's one thing that people found  for example works pretty well in  practice especially if your tasks  require reasoning you can't expect the  Transformer to make to to do too much  reasoning per token and so you have to  really spread out the reasoning across  more and more tokens so for example you  can't give a Transformer a very  complicated question and expect it to  get the answer in a single token there's  just not enough time for it these  Transformers need tokens to think quote  unquote I like to say sometimes and so  this is some of the things that work  well you may for example have a few shot  prompt that shows the Transformer that  it should like show its work when it's  answering a question when it's answering  the question and if you give a few  examples the Transformer will imitate  that template and it will just end up  working out better in terms of its  evaluation  Additionally you can elicit this kind of  behavior from the Transformer by saying  let's think step by step because this  condition is the transformer into sort  of like showing its work and because it  kind of snaps into a mode of showing its  work it's going to do less computational  work per token and so it's more likely  to succeed as a result because it's  making  um slower reasoning over time  here's another example this one is  called self-consistency we saw that we  had the ability to start writing and  then if it didn't work out I can try  again and I can try multiple times and  uh and maybe  um select the one that worked best so in  these kinds of approaches you may sample  not just once but you may sample  multiple times and then have some  process for finding the ones that aren't  good and then keeping just those samples  or doing a majority vote or something  like that so basically these  Transformers in the process as they  predict the next token just like you  they can get unlucky and they can they  could sample and not a very good token  and they can go down sort of like a  blind alley in terms of reasoning and so  unlike you they cannot recover from that  they are stuck with every single token  they sample and so they will continue  the sequence even if they even know that  this sequence is not going to work out  so give them the ability to look back  inspect or try to find uh try to  basically sample around it  here's one technique also you could it  turns out that actually llms like they  know when they've screwed up so as an  example  say you asked the model to generate a  poem that does not rhyme and it might  give you a poem but it actually Rhymes  but it turns out that especially for the  bigger models like gpt4 you can just ask  it did you meet the assignment and  actually gpt4 knows very well that it  did not meet the assignment it just kind  of got unlucky in its sampling and so it  will tell you no I didn't actually meet  the assignment here's let me try again  but without you prompting it it doesn't  even like it doesn't know it doesn't  know to revisit and uh and so on so you  have to make up for that in your prompt  you have to get it to check if you don't  ask it to check it's not going to check  by itself it's just a token simulator  I think more generally a lot of these  techniques fall into the bucket of what  I would say recreating our system too so  you might be familiar with the system  one system to thinking for humans system  one is a fast automatic process and I  think kind of corresponds to like an llm  just sampling tokens  and system two is the slower deliberate  planning sort of part of your brain  and so this is a paper actually from  just last week because this space is  pretty quickly evolving it's called tree  of thought and in tree of thought uh the  authors of this paper propose  maintaining multiple completions for any  given prompt and then they are also  scoring them along the way and keeping  the ones that are going well if that  makes sense and so a lot of people are  like really playing around with kind of  um prompt engineering to um to basically  bring back some of these abilities that  we sort of have in our brain for llms  now one thing I would like to note here  is that this is not just a prompt this  is actually prompts that are together  used with some python glue code because  you don't you actually have to maintain  multiple prompts and you also have to do  some tree search algorithm here um to  like figure out which prompt to expand  Etc so it's a symbiosis of python glue  code and individual prompts that are  called in a while loop or in a bigger  algorithm  I also think there's a really cool  parallel here to alphago alphago has a  policy for placing the next Stone when  it plays go and its policy was trained  originally by imitating humans but in  addition to this policy it also does  Monte Carlo tree search and basically it  will play out a number of possibilities  in its head and evaluate all of them and  only keep the ones that work well and so  I think this is kind of an equivalent of  alphago but for text  if that makes sense  so just like tree of thought I think  more generally people are starting to  like really Explore More General  techniques of not just the simple  question answer prompt but something  that looks a lot more like python blue  code stringing together many problems so  on the right I have an example from this  paper called react where they structure  the answer to a prompt as a sequence of  thought action observation thought  action observation and it's a full  rollout a kind of a thinking process to  answer the query and in these actions  the model is also allowed to to lose  on the left I have an example of rogpt  and now rgbt by the way became as a  project that I think got a lot of hype  recently and I think uh but I think I  still find it kind of inspirationally  interesting it's um it's a project that  allows an llm to sort of keep task list  and continue to recursively break down  tasks and I don't think this currently  works very well and I would not advise  people to use it in Practical  applications I just think it's something  to generally take inspiration from in  terms of where this is going I think  over time  so that's kind of like giving our model  system to thinking the next thing that I  find kind of interesting is  this following survey would say almost  psychological Quirk of llms is that llms  don't want to succeed they want to  imitate  you want to succeed and you should ask  for it  so what I mean by that is  when Transformers are trained they have  training sets and there can be an entire  spectrum of performance qualities in  their training data so for example there  could be some kind of a prompt for some  Physics question or something like that  and there could be a student solution  that is completely wrong but there can  also be an expert answer that is  extremely right and Transformers can't  tell the difference between like look I  mean they know they know about low  quality Solutions and high quality  Solutions but by default they want to  imitate all of it because they're just  trained on language modeling and so at  test time you actually have to ask for a  good performance so in this example in  this paper it's um they tried various  prompts and let's think step by step was  very powerful because it's sort of like  spread out the reasoning of remaining  tokens but what worked even better is  let's work this out in a step-by-step  way to be sure we have the right answer  and so it's kind of like a conditioning  on getting a right answer and this  actually makes the Transformer work  better because the Transformer doesn't  have to now hedge its probability Mass  on low quality Solutions as ridiculous  as that sounds and so basically don't be  feel free to ask for a strong solution  say something like you are a leading  expert on this topic pretend you have  iq120 Etc but don't try to ask for too  much IQ because if you ask for IQ of  like 400 you might be out of data  distribution or even worse you could be  in data distribution for some like  sci-fi stuff and it will start to like  take on some sci-fi or like role-playing  or something like that so you have to  find like the right amount of IQ I think  it's got some U-shaped curve there  next up  as we saw when we are trying to solve  problems we know we are good at and what  we're not good at and we Lean On Tools  computationally you want to do the same  potentially with your uh llms so in  particular we may want to give them  calculators code interpreters and so on  the ability to do search and uh there's  a lot of techniques for doing that one  thing to keep in mind again is that  these Transformers by default may not  know what they not don't know so you may  even want to tell the transformer in the  prompt you are not very good at mental  arithmetic whenever you need to do very  large number addition multiplication or  whatever instead use this calculator  here's how you use the calculator use  this token combination etc etc so you  have to actually like spell it out  because the model by default doesn't  know what it's good at or not good at  necessarily just like you and I you and  I might be  next up I think something that is very  interesting is we went from a world that  was retrieval only all the way the  pendulum has swung to The Other Extreme  where it's memory only in lens but  actually there's this entire space in  between of these retrieval augmented  models and this works extremely well in  practice  as I mentioned the context window of a  transformer is its working memory if you  can load the working memory with any  information that is relevant to the task  the model will work extremely well  because it can immediately access all  that memory  and so I think a lot of people are  really interested in  uh basically retrieval augmented  generation and on the bottom I have like  an example of llama index which is one  sort of data connector to lots of  different types of data and you can uh  all you can make it you can index all of  that data and you can make it accessible  to llms and the emerging recipe there is  you take relevant documents you split  them up into chunks you embed all of  them and you basically get embedding  vectors that represent that data you  store that in a vector store and then at  test time you make some kind of a query  to your vector store and you fetch  chunks that might be relevant to your  task and you stuff them into the prompt  and then you generate so this can work  quite well in practice so this is I  think similar to when you're and I solve  problems you can do everything from your  memory and Transformers have very large  and extensive memory but also it really  helps to reference some primary  documents so when you whenever you find  yourself going back to a textbook to  find something or whenever you find  yourself going back to the documentation  of a library to look something up the  transform transformers definitely when I  do that too you you have a mem you have  some memory over how some documentation  of a library works but it's much better  to look it up so the same same applies  here  next I wanted to briefly talk about  constraint prompting I also find this  very interesting  uh this is basically techniques for  um  forcing a certain template in the  outputs of llms so guidance is one  example from Microsoft actually and uh  here we are enforcing that the output  from the llm will be Json and this will  actually guarantee that the output will  take on this form because they go in and  they mess with the probabilities of all  the different tokens that come out of  the Transformer and if they clamp those  tokens and then the Transformer is only  filling in the blanks here and then you  can enforce additional restrictions on  what could go into those blanks so this  might be really helpful and I think this  kind of constraint sampling is also  extremely interesting  I also wanted to say a few words about  fine tuning it is the case that you can  get really far with prompt engineering  but it's also possible to think about  fine-tuning your models now fine-tuning  models means that you are actually going  to change the weights of the model  it is becoming a lot more accessible to  do this in practice and that's because  of a number of techniques that have been  developed and have libraries for uh very  recently so for example parameter  efficient fine-tuning techniques like  Laura make sure that you're only Trend  you're only training small Spar species  of your model so most of the model is  kept clamped at the base model and some  pieces of it are allowed to change and  this still works pretty well empirically  and makes it much cheaper to sort of  tune only small pieces of your model  uh there's also it also means that  because most of your model is clamped  you can use very low Precision inference  for computing those parts because they  are not going to be updated by gradient  descent and so that makes everything a  lot more efficient as well and in  addition we have a number of Open Source  high quality based models currently as I  mentioned I think llama is quite nice  although it is not commercially licensed  I believe right now  some things to keep in mind is that  basically fine tuning is is a lot more  technically involved it requires a lot  more I think technical expertise to do  to do right it requires human data  contractors for data sets and or  synthetic data pipelines that can be  pretty complicated this will definitely  slow down your iteration cycle by a lot  and I would say on a high level sft is  achievable because it is just your  continuing language modeling task It's  relatively straightforward but rlhref I  would say is very much research  territory and is even much harder to get  to work and so I would probably not  advise that someone just tries to roll  their own rlh of implementation these  things are pretty unstable very  difficult to train uh not something that  is I think very beginner friendly right  now and it's also potentially likely  also to to change pretty rapidly still  so I think these are my sort of default  recommendations right now  I would break up your task into two  major parts number one achieve your top  performance and number two optimize your  performance in that order  number one the best performance will  currently come from GT4 model it is the  most capable of by far  use prompts that are very detailed they  have lots of task contents relative  relevant information and instructions  think along the lines of what would you  tell a task contractor if they can't  email you back but then also keep in  mind that a task contractor is a human  and they have inner monologue and  they're very clever Etc llms do not  possess those qualities so make sure to  think through the psychology of the llm  almost and cater prompts to that  retrieve and add any relevant context  and information to these prompts  basically refer to a lot of the prompt  engineering techniques some of them are  highlighted in the slides above but also  this is a very large space and I would  just advise you to look for prompt  engineering techniques online there's a  lot to cover there  experiment with few shop examples what  this refers to is you don't just want to  tell you want to show whenever it's  possible so give it examples of  everything that helps it really  understand what you mean if you can  tools and plugins to offload a tasks  that are difficult for llms natively uh  and then think about not just a single  prompt and answer think about potential  chains and reflection and how you glue  them together and how you could  potentially make multiple samples and so  on  finally if you think you've squeezed out  prompt engineering which I think you  should stick with for a while look at  some potentially fine-tuning a model to  your application but expect this to be a  lot more slower and evolved and then  there's an expert fragile research Zone  here and I would say that is rlhf which  currently does work a bit better than  sft if you can get it to work but again  this is uh pretty involved I would say  and to optimize your costs try to  explore lower capacity models or shorter  prompts and so on  I also wanted to say a few words about  the use cases in which I think llms are  currently well suited for so in  particular note that there's a large  number of limitations to llms today and  uh so I would keep that definitely in  mind for all your applications models  and this by the way could be an entire  talk so I don't have time to cover it in  full detail models may be biased they  may fabricate hallucinate information  they may have reasoning errors they may  struggle in entire classes of  applications they have knowledge cutoffs  so they might not know any information  above say September 2021 they are  susceptible to a large range of attacks  which are sort of like coming out on  Twitter daily including prompt injection  jailbreak attacks data poisoning attacks  and so on so my recommendation right now  is use llms in low stakes applications  combine them with always with human  oversight use them as a source of  inspiration and suggestions and think  co-pilots instead of completely  autonomous agents that are just like  performing a task somewhere it's just  not clear that the models are there  right now  so I wanted to close by saying that gpt4  is an amazing artifact I'm very thankful  that it exists and it's it's beautiful  it has a ton of knowledge across so many  areas it can do math code and so on and  in addition there's this thriving  ecosystem of everything else that is  being built and incorporated into into  the ecosystem uh some of some of the  things these things I've talked about  and all of this power is accessible at  your fingertips  so here's everything that's needed in  terms of code to ask GPT for a question  to prompt it and get a response in this  case I said uh can you say something to  inspire the audience of Microsoft build  2023  and I just punched this into Python and  verbatim gpt4 said the following  uh and by the way I did not know that  they used this trick in the keynote so I  thought I was being clever but  but it is really good at this it says  ladies and gentlemen Invaders and  Trailblazers Microsoft build 2023  welcome to the Gathering of Brilliant  Minds like no other you are The  Architects of the future The Visionaries  molding the digital realm in which  Humanity thrives embrace the Limitless  possibilities of Technologies and let  your ideas soar as high as your  imagination together let's create a more  connected remarkable and inclusive world  for generations to come get ready to  unleash your creativity canvas the  unknown and turn dreams into reality  your Journey Begins today  okay\\n\",\n",
       " '\\n',\n",
       " '\\n',\n",
       " '[]\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '[]\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'GBT \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' GPT \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' 99\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' GPU\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' GPU \\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Meta \\n',\n",
       " '\\n',\n",
       " ' llama \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' C4\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'GitHub  \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'GPT \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'gpts \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' Transformer \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' gpt4 \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " 'gpt3  gpt3 \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' llama  meta \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' pre   - \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '10 000 \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '204000  100000 \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'GPT \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' llama \\n',\n",
       " '\\n',\n",
       " '650 \\n',\n",
       " '\\n',\n",
       " ' llama\\n',\n",
       " '\\n',\n",
       " ' gvt3  1750\\n',\n",
       " '\\n',\n",
       " ' 65b  llama \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " ' 1.4 \\n',\n",
       " '\\n',\n",
       " '3000 \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Transformer \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' 65b \\n',\n",
       " '\\n',\n",
       " ' 2000 GPU  21 \\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Transformer  B\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'B  T T \\n',\n",
       " '\\n',\n",
       " '  10.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " ' Transformer \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' Transformer\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " ' Transformer\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Transformer \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' Transformer \\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' 50 257 \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' 513\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Transformers \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " 'Transformer \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " ' GPT\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' GPT\\n',\n",
       " '\\n',\n",
       " 'GPT \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '  GPT \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Transformer \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Transformer \\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' NLP \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' Transformer\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' Transformer \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' GPT \\n',\n",
       " '\\n',\n",
       " 'gpt2 \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' QA qaqa \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'q Transformer \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'gpt4 \\n',\n",
       " '\\n',\n",
       " ' API  gpt4 \\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' gpt3 \\n',\n",
       " '\\n',\n",
       " ' DaVinci  API  gpt2 \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' GitHub \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'metaLlama\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' GBT \\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' QA \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' sfd \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " ' rlhf \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' sft \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'top \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' sft \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' sft\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' Transformer\\n',\n",
       " '\\n',\n",
       " ' Transformer \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " ' sft \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' sft \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' 1.2 \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' rohf \\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " ' GPT  rlhf \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' kuna 13B \\n',\n",
       " '\\n',\n",
       " ' sft \\n',\n",
       " '\\n',\n",
       " ' sft  rlh\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' rlhf \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all the lines of a file in a list, use list(f) or f.readlines()\n",
    "with open('build.txt', encoding=\"utf-8\") as f:\n",
    "    f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2f7b1ae-07fd-4962-a7be-3178b7c7a51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f.write(string) writes the contents of string to the file, returning the number of characters written\n",
    "with open('build.txt', 'r+', encoding=\"utf-8\") as f:\n",
    "    f.write('This is a test\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf9c3cba-f8bc-41b7-ace5-d90e2820a917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"('the answer', 42)\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Other types of objects need to be converted\n",
    "value = ('the answer', 42)\n",
    "s = str(value)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aeffb784-9413-488d-a3c8-b908b1ea9575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f.tell()?\n",
    "with open('build.txt', encoding=\"utf-8\") as f:\n",
    "    f.tell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a1b11c0-76bb-40fa-92c8-31c0c6533d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1, \"simple\", \"list\"]'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving structured data with json (JavaSript Object Notation)\n",
    "import json\n",
    "x = [1, 'simple', 'list']\n",
    "json.dumps(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8658d8b4-7148-4bbb-8385-707ba48083f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump() simply serializes the object to a text file\n",
    "with open('build.txt', 'r+', encoding=\"utf-8\") as f:\n",
    "    json.dump(x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e91e99-e470-48af-9d01-e6a80dc46543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
