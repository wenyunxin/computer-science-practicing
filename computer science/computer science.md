[toc]



计算机基础知识

# [Crash Course Computer Science](https://www.bilibili.com/video/BV1EW411u7th?p=3&vd_source=0e5acab5147017a01775d15356db9209)

## Preview

<iframe width="560" height="315" src="https://www.youtube.com/embed/tpIctyqH29Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hello world! I’m Carrie Anne Philbin and welcome to Crash Course Computer Science! So, computers really have allowed us to do some pretty amazing things - think global **telecommunications**, international **commerce**, global transportation, breakthroughs in medicine, distributed education, online shopping, online dating, just the Internet in general. Computers are allowing us to explore our own world and other worlds, and of course some seemingly **mundane** things like permitting us to spy on our pets from work or communicate with our friends in a nearly **indecipherable** stream of emoji! But don’t call computers magical. They are not, I repeat ARE NOT, magical. 

> 大家好，我是Carrie Anne Philbin，欢迎来到Crash Course Computer Science! 计算机真的可以让我们做大量不可思议的事情——比如全球通信，国际贸易，全球运输，医学突破，在线教育，在线购物，这些都是互联网的功劳。计算机让我们探索我们自己的世界和其他世界，当然也包括一些看起来平常的事情，比如可以让我们在工作的时候监视我们的宠物，或者跟我们的朋友用难以看懂的表情符号流聊天！但是不能说计算机是魔法，不是的，我重复：不是魔法。

So before we get into what we are going to talk about in this course, it might be useful to tell you what we aren’t going to talk about. We aren’t going to teach you how to program. Programming is a really **crucial** aspect of computer **software**, and we will get to the rules that guide the **logic of hardware** and **software design**. But we aren’t going to teach you how to program an Arduino Uno to water your plant or how to change the CSS on your grandma’s sewing blog so visitors’ cursors turn into kittens. This also isn’t a computing course. Or at least how computing is thought of in the U.S. **Computing here is a goal** - it’s **what computers do**. And we’ll talk about some of that for sure, but OUR goal for this course is much broader. But computing means other things in other countries. It’s all pretty confusing. But what we are going to **look are the history of computers**… even before we had electricity. We’re going retrace the design decisions that have given us our president-day **component**s. We’re going to talk about **how Operating Systems work**… or don’t work… **how the YouTubes get to you over the Internet**, **how our smartphones and other smart devices are**... well getting smarter, and of course **mysterious** futuristic stuff like **quantum** computing and **frustrating** present-day stuff like **hacking**! 

> 在我们开始讲述这门课之前，先讲清楚我们不会讲什么会对听课有帮助。我们不会教你如何编程。编程是计算机软件非常重要的部分。但我们不会教你如何编写用微型控制器给你的植物浇水或者如何在你奶奶的缝纫网站上改变CSS因而可以访客的光标变成小猫的程序。这不是一门计算课。或者说，至少在美国，人们是如何看待计算的。计算在这里是目标——它是计算机所做的事情。当然我们会在课程中谈论一些计算内容，但是我们这门课程的目标更广泛。但计算在其他国家意味着其他事情。这一切都很让人困惑。但是我们会讲到计算机的历史……甚至在我们拥有电力之前。我们要回溯那些为我们提供了现在的**组件**的设计决定。我们会谈论操作系统如何工作，或者不工作……你如何通过互联网触达YouTube，我们的智能手机和其他智能设备如何变得更加智能。还有什么的未来事物，比如量子计算，也有现在困扰我们的事物——黑客

It’s a lot to cover, but I suppose before we get started I should introduce myself. I’m Carrie Anne Philbin! Hello! I'm an award winning secondary Computing teacher, author of 'Adventures in Raspberry Pi' and the creator of a YouTube video series for teenagers called the **Geek** Gurl Diaries, which includes stuff like interviews with women working in technology, computer science based tutorials, and hands on digital maker style projects. In my day job, I help people learn about technology and how to make things with computers as Director of Education for the Raspberry Pi Foundation, which is a charity based in Cambridge in the UK. 

> 这有很多内容要讲，但我想在我们开始之前，我应该介绍一下自己。我是卡丽-安-菲尔宾! 大家好！我是卡丽-安-菲尔宾。我是一名获奖的中学计算机教师，是《树莓派历险记》的作者，也是YouTube青少年视频系列 "**极客**女孩日记 "的创作者，其中包括对从事技术工作的女性的采访，基于计算机科学的教程，以及动手制作数字创客风格的项目。在我的日常工作中，我作为树莓派基金会的教育总监，帮助人们学习技术和如何用电脑制作东西，该基金会是一个位于英国剑桥的慈善机构。

Needless to say, I am **passionate** about this stuff, but not because computers are these amazing devices that are always making our lives easier (sometimes that’s **debatable**) but because computers **inarguably** have become **pivotal** in our society. From our cars and **thermostat**s to **pacemaker**s and cellphones, computers are everywhere, and it’s my hope that by the end of this course you’ll have a better understanding and appreciation for how far we’ve come and how far they may take us. I’ll see you next week. 

> 不用说，我对这些东西是**热情的**，但不是因为计算机是这些神奇的设备，总是让我们的生活更容易（有时这是**值得商榷的**），而是因为计算机**可以说**已经成为我们社会中**关键的**东西。从我们的汽车和**恒温器**到**起搏器**和手机，计算机无处不在，我希望在本课程结束时，你能更好地理解和欣赏我们已经走了多远，以及它们可能带我们走多远。下周见。



### history

device: 算盘—手摇计算——机械计算机——制表机——电子机械计算机（继电器计算机）——electronic computer电子计算机（真空管计算机）——IBM 608, released in 1957 – the first fully transistor-powered, commercially-available computer（晶体管计算机）

conponent: relay——vaccum tube——transistor

first computer programmer： [Ada Lovelace](https://en.wikipedia.org/wiki/Ada_Lovelace)

father of the computer 计算机之父：[Charles Babbage](https://en.wikipedia.org/wiki/Charles_Babbage)

father of computer science and artificial intelligence 计算机科学，人工智能之父：[Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing)

first truly general purpose, programmable, electronic computer: ENIAC

Noyce is widely regarded as the father of modern ICs, ushering in the electronics era... 

second generation of electronic computing: transistorized computers

third generation of electronic computing: era of integrated circuits, especially microprocessors

forth generation of electronic computing: [VLSI](https://en.wikipedia.org/wiki/Very_Large_Scale_Integration) software

evolution of promgramming languages: machine code — assembly languages — high-level programming languages

translation order: high-level programming languages  — translated into assembly languages by compiler— translated into machine code by assembler

compiler:  a specialized program that transforms “source” code written in a programming language into a low-level language, like assembly or the binary “machine code” that the CPU can directly process.

assembler: read in text-based instructions, and assemble them into the corresponding binary instructions automatically.

promgramming languages:

FORTRAN, COBOL, In the 1960s, we had languages like ALGOL, LISP and BASIC. In the 70’s: Pascal, C and Smalltalk were released. The 80s gave us C++, Objective-C, and Perl. And the 90’s: python, ruby, and Java. And the new millennium has seen the rise of Swift, C#, and Go

move up a new level of abstractions: Logic Gates; half adder & full adder; ALU-V; Gtated latch; memory; Control Unit; CPU; program; assemble languages; function; Data Structure; hide complexity by encapsulating low-level details in higher-order components; virtual memory; file system; use **graphics libraries** with ready-to-go functions for drawing lines, curves, shapes, text, and other cool stuff; 

continual abstractions—>files looked like pieces of paper, and they could be stored in little folders, all of which could sit on your desktop, or be put away into digital filing cabinets. It’s a metaphor that sits ontop of the underlying file system; DNS servers trees; World Wide Web;



### method

official website, wikepedia, hack news & other learning website

learning every basic book that learning AI need, including math, programming languages...

#1~ #10 #17 #19 are about hardware

#11 ~ #16 #18 #20~#23, #26~#37 are about software:

- #11~16 & #18are about soft engineering;

- #20~#21 are about files & files technics

- #22~#23, #26~#27 are about graphics

- #28~#30 are about network

- #31~#33 are about cybersecurity

- #34~#37 are about [**Artificial intelligence**](https://en.wikipedia.org/wiki/Artificial_intelligence#Learning)

#24~#25 are about the history of electronic computer

#38~#40 Technology and Humanities?



系统性地封装成各个组件是现代编程的重要思想



## #1 Early computing

<iframe width="560" height="315" src="https://www.youtube.com/embed/O5nskjZ_GoI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hello world, I’m Carrie Anne, and welcome to CrashCourse Computer Science! Over the course of this series, we’re going to go from [[**bit**s]], [[**byte**s]], [[**transistor**s]] and [[**logic gate**s]], all the way to [[**Operating System**s]], [[**Virtual Reality**]] and [[**Robot**s]]! We’re going to cover a lot, but just to clear things up - we ARE NOT going to teach you how to program. Instead, we’re going to explore a range of computing topics as a [[discipline]] and a technology. Computers are the [[**lifeblood**]] of today’s world. If they were to suddenly turn off, all at once, the power grid would shut down, cars would crash, planes would fall, water **treatment** plants would stop, stock markets would freeze, trucks with food wouldn’t know where to deliver, and employees wouldn’t get paid. Even many non-computer objects - like DFTBA shirts and the chair I’m sitting on – are made in factories run by computers. Computing really has **transformed** nearly every aspect of our lives. 

> Hello world, I’m Carrie Anne, and welcome to CrashCourse Computer Science! 在这门课程中，我们将从比特，字节，晶体管和逻辑门一路讲到操作系统，虚拟现实和机器人！我们会覆盖很多内容，让我们讲清楚，我们不会教你如何编程。相反，我们将探索一系列作为一门**学科**和技术的计算主题。计算机是今天世界的命脉。如果他们一下子突然关闭，电力网讲关闭，车子会相撞，飞机会坠毁，水处理工厂会停工，股票市场会冻结，食物卡车不知道去哪里送货，工人不会获得薪水。即使是非计算机对象，比如DFTBAT恤和我现在坐的椅子，都是在被计算机运行的工厂生产。计算已经改变几乎我们生活的方方面面。

And this isn’t the first time we’ve seen this sort of technology-driven global change. Advances in manufacturing during the Industrial Revolution brought a new scale to human civilization - in agriculture, industry and **domestic** life. **Mechanization** meant **superior** harvests and more food, mass produced goods, cheaper and faster travel and communication, and usually a better quality of life. And computing technology is doing the same right now – from automated farming and medical equipment, to global telecommunications and educational opportunities, and new **frontiers** like **Virtual Reality** and **Self Driving Cars**. We are living in a time likely to be remembered as the Electronic Age. With billions of transistors in just your smartphones, computers can seem pretty complicated, but really, they’re just simple machines that perform complex actions through many **layers of abstraction**. So in this series, we’re going break down those layers, and build up from simple 1’s and 0’s, to **logic units**, **CPUs**, operating systems, the entire internet and beyond. 

> 而这并不是我们第一次看到这种技术驱动的全球变化。工业革命期间制造业的进步给人类文明带来了新的规模--在农业、工业和**家庭**生活中。**机械化**意味着的**丰厚的**收成和更多的食物，大规模生产的商品，更便宜和更快的旅行和通信，以及通常更好的生活质量。而计算技术现在也在做同样的事情--从自动耕作和医疗设备，到全球电信和教育机会，以及像**虚拟现实**和**自动驾驶汽车**这样的新**前沿技术**。我们正生活在一个可能**被称为**电子时代的时代。仅仅在你的智能手机中就有**数十亿个**晶体管，计算机看起来相当复杂，但实际上，它们只是简单的机器，通过许多**层的抽象**来执行复杂的动作。因此，在这个系列中，我们将打破这些层级，从简单的1和0，到**逻辑单元**、**中央处理器**、操作系统、整个互联网以及其他。

And don’t worry, in the same way someone buying t-shirts on a webpage doesn’t need to know how that webpage was programmed, or the web designer doesn’t need to know how all the packets are routed, or router engineers don’t need to know about transistor logic, this series will build on previous episodes but not be dependent on them. By the end of this series, I hope that you can better **contextualize** computing’s role both in your own life and society, and how **humanity**'s (arguably) greatest invention is just in its [[**infancy**]], with its biggest impacts yet to come. But before we get into all that, we should start at computing’s origins, because although electronic computers are relatively new, the need for computation is not. 

> 而且不用担心，就像在网页上买T恤的人不需要知道这个网页是如何编程的，或者网页设计师不需要知道所有数据包是如何路由的，或者路由器工程师不需要知道晶体管逻辑一样，这个系列将建立在以前的剧集之上，但不依赖于它们。在本系列结束时，我希望你能更好地了解计算在你自己的生活和社会中的作用，以及**人类**的（可以说是）最伟大的发明是如何只是处于其[[**初期阶段**]]中，其最大的影响还未到来。但在我们进入这一切之前，我们应该从计算的起源开始，因为尽管电子计算机相对较新，但对计算的需求却不是。

INTRO The earliest recognized device for computing was the [[**abacus**]], invented in **Mesopotamia** around 2500 BCE. It’s essentially a hand operated calculator, that helps add and subtract many numbers. It also **stores the current state of the computation**, much like your hard drive does today. The abacus was created because, the scale of society had become greater than what a single person could keep and manipulate in their mind. There might be thousands of people in a village or tens of thousands of cattle. There are many variants of the abacus, but let’s look at a really basic version with each row representing a different power of ten. So each bead on the bottom row represents a single unit, in the next row they represent 10, the row above 100, and so on. Let’s say we have 3 heads of cattle represented by 3 beads on the bottom row on the right side. If we were to buy 4 more cattle we would just slide 4 more [[beads]] to the right for a total of 7. But if we were to add 5 more after the first 3 we would run out of beads, so we would slide everything back to the left, slide one bead on the second row to the right, representing ten, and then add the final 2 beads on the bottom row for a total of 12. This is particularly useful with large numbers. So if we were to add 1,251 we would just add 1 to the bottom row, 5 to the second row, 2 to the third row, and 1 to the fourth row - we don’t have to add in our head and the abacus stores the total for us. 

> 简介：公认的最早的计算设备是**算盘**，于公元前2500年左右在**美索不达米亚**发明的。它本质上是一个手控计算器，可以帮助加减许多数字。它还可以**存储当前的计算状态**，就像今天的硬盘一样。算盘之所以被创造出来，是因为社会的规模已经超过了一个人在头脑中所能保持和操纵的范围。一个村子里可能有几千人，或者有几万头**牛**。算盘有很多变体，但让我们看看一个非常基本的版本，每一行代表不同的十次方。因此，最下面一行的每颗珠子代表一个单位，往上一行它们代表10，再上一行代表100，以此类推。比方说，我们有3头牛，由底排右侧的3颗珠子代表。如果我们要再买4头牛，我们只需再向右滑动4个[[**珠子**]]，总共是7头。但如果我们要在前3头之后再加5头，我们的珠子就用完了，所以我们要把所有的东西滑回左边，把第二行的一个珠子滑到右边，代表10，然后再加上底行的最后两个珠子，总共是12。这对大数字来说特别有用。因此，如果我们要加1,251，我们只需在最下面一行加1，第二行加5，第三行加2，第四行加1--我们不必在脑子里加，算盘会为我们储存总数。

Over the next 4000 years, humans developed all sorts of clever computing devices, like the **astrolabe**, which enabled ships to calculate their [[**latitude**]] at sea. Or the slide rule, for assisting with multiplication and division. And there are literally hundred of types of clocks created that could be used to calculate sunrise, **tides**, positions of [[**celestial**]] bodies, and even just the time. Each one of these devices made something that was previously **laborious** to calculate much faster, easier, and often more accurate –– it lowered the barrier to entry, and at the same time, **amplified our mental abilities** –– take note, this is a theme we’re going to touch on a lot in this series. As early computer pioneer Charles Babbage said: “At each increase of knowledge, as well as on the [[**contrivance**]] of every new tool, human labour becomes abridged.” However, none of these devices were called “computers”. The earliest documented use of the word “computer” is from 1613, in a book by Richard Braithwait. And it wasn’t a machine at all - it was a job title. Braithwait said, “I have read the truest computer of times, and the best arithmetician that ever breathed, and he **reduceth** thy dayes into a short number”. 

> 在接下来的4000年里，人类开发了各种聪明的计算设备，比如**星盘**，它使船只能够在海上计算其**纬度**。或者滑尺，用于辅助乘法和除法。还有数以百计的时钟，可以用来计算日出、**潮汐**、**天体**的位置，甚至只是计算时间。这些设备中的每一个都使以前**费力的**计算变得更快、更容易，而且往往更准确--它降低了入门门槛，同时，**放大了我们的思维能力**--注意，这是我们在这个系列中要经常触及的主题。正如早期的计算机先驱查尔斯-巴贝奇所说。"知识的每一次增长，以及每一种新工具的**发明**，都会使人类的劳动变得节俭。" 然而，这些设备都不被称为 "计算机"。最早使用 "计算机"这个词的记录是在1613年，由Richard Braithwait写的一本书中。而且这根本不是一台机器--是一个工作头衔。Braithwait说："我读到了时代最真实的计算机，以及有史以来最好的算术师，他把你的日子**缩减**为短暂的数目"。

In those days, computer was a person who did calculations, sometimes with the help of machines, but often not. This job title persisted until the late 1800s, when the meaning of computer started shifting to refer to devices. Notable among these devices was the Step Reckoner, built by German polymath Gottfried Leibniz in 1694. Leibniz said “... it is beneath the dignity of excellent men to waste their time in calculation when any [[peasant]] could do the work just as accurately with the aid of a machine.” It worked kind of like the [[**odometer**]] in your car, which is really just a machine for adding up the number of miles your car has driven. The device had a series of gears that turned; each gear had ten teeth, to represent the digits from 0 to 9. Whenever a gear bypassed nine, it [[**rotated**]] back to 0 and advanced the **adjacent** gear by one tooth. Kind of like when hitting 10 on that basic abacus. This worked in reverse when doing subtraction, too. With some clever mechanical tricks, the Step Reckoner was also able to multiply and divide numbers. Multiplications and divisions are really just many additions and subtractions. For example, if we want to divide 17 by 5, we just subtract 5, then 5, then 5 again, and then we can’t subtract any more 5’s… so we know 5 goes into 17 three times, with 2 left over. The Step Reckoner was able to do this in an automated way, and was the first machine that could do all four of these operations. And this design was so successful it was used for the next three centuries of calculator design. 

> 在那些日子里，计算机是一个做计算的人，有时在机器的帮助下，但往往不是。这个工作头衔一直持续到19世纪末，当时计算机的含义开始转变为指设备。这些设备中值得一提的是德国多面手戈特弗里德-莱布尼茨（Gottfried Leibniz）在1694年建造的Step Reckoner。莱布尼茨说："......当任何一个**农民**可以借助机器做同样精确的工作时，把时间浪费在计算上是有损于优秀人士的尊严的。" 它的工作原理有点像你车上的**里程器**——它实际上只是一台用于增加你的汽车行驶里程数的机器。该设备有一系列的齿轮转动；每个齿轮有十个齿，代表从0到9的数字。每当一个齿轮绕过9，它就会**旋转**回到0，并将**相邻的**齿轮前进一个齿。有点像在基本算盘上打10的时候。在做减法时，这也是反过来的。通过一些巧妙的机械技巧，"步步算盘"还能进行数字的乘除运算。乘法和除法实际上只是许多加法和减法。例如，如果我们想用5除以17，我们只需减去5，然后是5，再减去5，然后我们就不能再减去任何5了......所以我们知道5进入17三次，还剩下2。Step Reckoner能够以自动化的方式完成这些操作，并且是第一台能够完成所有这四种操作（四则运算）的机器。这个设计是如此成功，以至于在接下来的三个世纪里，计算器的设计都是如此。

Unfortunately, even with mechanical calculators, most real world problems required many steps of computation before an answer was determined. It could take hours or days to generate a single result. Also, these hand-crafted machines were expensive, and not accessible to most of the population. So, before 20th century, most people experienced computing through pre-computed tables assembled by those amazing “human computers” we talked about. So if you needed to know the square root of 8 million 6 hundred and 75 thousand 3 hundred and 9, instead of spending all day hand-[[cranking]] your step reckoner, you could look it up in a huge book full of square root tables in a minute or so. Speed and accuracy is particularly important on the battlefield, and so militaries were among the first to apply computing to complex problems. A particularly difficult problem is accurately firing [[artillery]] shells, which by the 1800s could travel well over a kilometer (or a bit more than half a mile). Add to this varying wind conditions, temperature, and atmospheric pressure, and even hitting something as large as a ship was difficult. Range Tables were created that allowed gunners to look up environmental conditions and the distance they wanted to fire, and the table would tell them the angle to set the canon. These Range Tables worked so well, they were used well into World War Two. 

> 不幸的是，即使有机械计算器，大多数现实世界的问题在确定答案之前都需要许多计算步骤。产生一个结果可能需要几个小时或几天。而且，这些手工制作的机器很昂贵，大多数人都无法使用。因此，在20世纪之前，大多数人都是通过我们谈到的那些神奇的 "人类计算机 "预先计算出来的表格来体验计算。所以，如果你需要知道八百万六百七十五万三千九百的平方根，而不是花一整天的时间用手摇动你的步算机，你可以在一分钟左右的时间内从一本充满平方根表的大书中查到它。速度和准确性在**战场**上尤其重要，因此军队是最早将计算应用于复杂问题的国家之一。一个特别困难的问题是准确地发射大炮炮弹，到19世纪，炮弹的飞行距离可能远远超过一公里（或略多于半英里）。再加上不同的风况、温度和大气压力，即使击中像船一样大的东西也很困难。射程表应运而生，它允许炮手查看环境条件和他们想要射击的距离，而表格会告诉他们设置火炮的角度。这些测距表的效果非常好，它们一直被用于第二次世界大战。

The problem was, if you changed the design of the cannon or of the shell, a whole new table had to be computed, which was massively time consuming and inevitably led to errors. Charles Babbage acknowledged this problem in 1822 in a paper to the Royal Astronomical Society entitled: “Note on the application of machinery to the computation of astronomical and mathematical tables". Let’s go to the thought bubble. Charles Babbage proposed a new mechanical device called the **Difference Engine**, a much more complex machine that could [[approximate]] polynomials. Polynomials describe the relationship between several variables - like range and air pressure, or amount of pizza Carrie Anne eats and happiness. Polynomials could also be used to approximate **logarithmic** and **trigonometric** functions, which are a real [[**hassle**]] to calculate by hand. Babbage started construction in 1823, and over the next two decades, tried to [[**fabricate**]] and **assemble** the 25,000 components, collectively weighing around 15 tons. Unfortunately, the project was ultimately abandoned. But, in 1991, historians finished constructing a Difference Engine based on Babbage's drawings and writings - and it worked! But more importantly, during construction of the Difference Engine, Babbage imagined an even more complex machine - the **Analytical Engine**. Unlike the Difference Engine, Step Reckoner and all other computational devices before it - the Analytical Engine was a “general purpose computer”. It could be used for many things, not just one particular computation; it could be given data and run operations in sequence; it had memory and even a primitive printer. Like the Difference Engine, it was ahead of its time, and was never fully constructed. 

> 问题是，如果你改变了大炮或炮弹的设计，就必须计算一个全新的表格，这需要耗费大量的时间，并不可避免地导致错误。查尔斯-巴贝奇在1822年给皇家天文学会的一篇论文中承认了这个问题，该论文题为："关于机械在天文和数学表格计算中的应用的说明"。让我们去看看这个思想的泡沫。查尔斯-巴贝奇提出了一种新的机械装置，称为**差分引擎**，这是一种更为复杂的机器，可以近似多项式。多项式描述了几个变量之间的关系--比如范围和气压，或者卡丽-安吃的比萨饼的数量和幸福感。多项式也可用于近似**对数**和**三角**函数，这些函数用手计算起来确实**麻烦**。巴贝奇于1823年开始建造，在接下来的20年里，试图**制造**和**组装**25,000个部件，总共重约15吨。不幸的是，该项目最终被放弃了。但是，在1991年，历史学家根据巴贝奇的图纸和著作完成了差分引擎的建造工作--它成功了！这就是巴贝奇。但更重要的是，在建造差分机的过程中，巴贝奇想象出一种更复杂的机器--**分析机**。与差分机、步骤记录器和之前的所有其他计算设备不同，分析引擎是一台 "通用计算机"。它可以用于许多事情，而不仅仅是一个特定的计算；它可以被赋予数据并按顺序运行操作；它有内存，甚至还有一个**原始的**打印机。像差分引擎一样，它领先于它的时代，而且从未被完全建造。
>

However, the idea of an “automatic computer” – one that could guide itself through a series of operations automatically, was a huge deal, and would foreshadow computer programs. English mathematician Ada Lovelace wrote [[hypothetical]] programs for the Analytical Engine, saying, “A new, a vast, and a powerful language is developed for the future use of analysis.” For her work, Ada is often considered the world’s first programmer. The Analytical Engine would inspire, arguably, the first generation of computer scientists, who incorporated many of Babbage’s ideas in their machines. This is why Babbage is often considered the "father of computing". Thanks Thought Bubble! 

> 然而，"自动计算机"的想法--一个能够自动指导自己完成一系列操作的计算机，是一个巨大的交易，并将预示着计算机程序。英国数学家艾达-洛夫莱斯为分析引擎写了假设程序，她说："为未来的分析使用开发了一种新的、庞大的、强大的语言"。由于她的工作，艾达通常被认为是世界上第一个程序员。可以说，分析引擎将激励第一代计算机科学家，他们将巴贝奇的许多想法融入他们的机器中。这就是为什么巴贝奇经常被认为是 "计算之父"。谢谢思想泡泡! 

So by the end of the 19th century, computing devices were used for special purpose tasks in the sciences and engineering, but rarely seen in business, government or domestic life. However, the US government faced a serious problem for its 1890 census that demanded the kind of efficiency that only computers could provide. The US Constitution requires that a [[census]] be conducted every ten years, for the purposes of distributing federal funds, representation in congress, and good stuff like that. And by 1880, the US population was booming, mostly due to immigration. That census took seven years to manually compile and by the time it was completed, it was already out of date – and it was predicted that the 1890 census would take 13 years to compute. That’s a little problematic when it’s required every decade! The Census bureau turned to Herman Hollerith, who had built a tabulating machine. His machine was “electro-mechanical” – it used traditional mechanical systems for keeping count, like Leibniz’s Step Reckoner –– but coupled them with electrically-powered components. Hollerith’s machine used punch cards which were paper cards with a grid of locations that can be punched out to represent data. For example, there was a series of holes for marital status. If you were married, you would punch out the married spot, then when the card was inserted into Hollerith’s machine, little metal pins would come down over the card – if a spot was punched out, the pin would pass through the hole in the paper and into a little vial of mercury, which completed the circuit. This now completed circuit powered an electric motor, which turned a gear to add one, in this case, to the “married” total. 

> 所以到19世纪末，计算设备被用于科学和工程领域的特殊目的任务，但在商业、政府或家庭生活中很少见到。然而，美国政府在1890年的人口普查中面临着一个严重的问题，它需要只有计算机才能提供的那种效率。美国宪法要求每十年进行一次人口普查，以便分配联邦资金，在国会的代表权，以及类似的好东西。而到了1880年，美国的人口正在蓬勃发展，主要是由于移民。那次人口普查花了七年时间手工编制，当它完成时，它已经过时了--据预测，1890年的人口普查将需要13年时间来计算。当每十年都需要这样做的时候，这就有点问题了! 人口普查局求助于赫尔曼-霍勒里思，他制造了一台**制表机**。他的机器是 "电子机械"--它使用传统的机械系统进行计数，如莱布尼茨的Step Reckoner--但将它们与电力驱动的部件相结合。霍利斯的机器使用打孔卡，这是一种纸卡，上面有一个可以打出的位置网格，以表示数据。例如，有一系列的孔代表婚姻状况。如果你结婚了，你就打出已婚的位置，然后当卡片被插入霍勒里思的机器时，小金属针会在卡片上落下--如果一个位置被打出，针会穿过纸上的孔，进入一个小瓶中的水银，从而完成电路。这个已经完成的电路为一个电动机提供动力，电动机转动齿轮，在这种情况下，在 "结婚"总数上增加一个。

Hollerith’s machine was roughly 10x faster than manual tabulations, and the Census was completed in just two and a half years - saving the census office millions of dollars. Businesses began recognizing the value of computing, and saw its potential to **boost** profits by improving labor- and data-intensive tasks, like accounting, insurance [[**appraisals**]], and inventory management. To meet this demand, Hollerith founded The Tabulating Machine Company, which later merged with other machine makers in 1924 to become The International Business Machines Corporation or IBM - which you’ve probably heard of. These electro-mechanical “business machines” were a huge success, transforming commerce and government, and by the mid-1900s, the explosion in world population and the rise of globalized trade demanded even faster and more flexible tools for processing data, setting the stage for digital computers, which we’ll talk about next week. 

> 霍利斯的机器比人工制表快了大约10倍，而人口普查只用了两年半的时间就完成了，为人口普查办公室节省了**数百万**美元。企业开始认识到计算机的价值，并看到它有可能通过改善劳动和数据密集型任务，如会计、保险**评估**和库存管理，来提高利润。为了满足这一需求，Hollerith成立了制表机公司，该公司后来在1924年与其他机器制造商合并，成为国际商业机器公司或IBM--你可能已经听说过。这些电子机械式的 "商业机器 "取得了巨大的成功，改变了商业和政府。到了20世纪中期，世界人口的**爆炸**和全球化贸易的兴起，需要更快、更灵活的工具来处理数据，为数字计算机创造了条件，我们下周将讨论这个问题。
>



## #2 Electronic computing

<iframe width="560" height="315" src="https://www.youtube.com/embed/LN0ucKNX0hc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Our last episode brought us to the start of the 20th century, where early, special purpose computing devices, like tabulating machines, were a huge **boon** to governments and business - aiding, and sometimes replacing, rote manual tasks. But the scale of human systems continued to increase at an **unprecedented** rate. The first half of the 20th century saw the world’s population almost double. World War 1 **mobilized** 70 million people, and World War 2 involved more than 100 million. Global trade and transit networks became interconnected like never before, and the **sophistication** of our engineering and scientific endeavors reached new heights – we even started to seriously consider visiting other planets. And it was this explosion of complexity, **bureaucracy**, and ultimately data, that drove an increasing need for automation and computation. Soon those cabinet-sized electro-mechanical computers grew into room-sized behemoths that were expensive to maintain and prone to errors. And it was these machines that would set the stage for future innovation. 

> 我们的上一集把我们带到了20世纪初，早期的特殊用途的计算设备，如制表机，对政府和企业来说是一个巨大的--**帮助**，有时甚至取代了传统的手工作业。但人类系统的规模继续以**前所未有的**速度增加。在20世纪上半叶，世界人口几乎翻了一番。第一次世界大战**动员**了7000万人，第二次世界大战涉及1亿多人。全球贸易和运输网络变得前所未有的相互联系，我们的工程和科学努力的**先进性**达到了新的高度 - 我们甚至开始认真考虑访问其他星球。正是这种复杂性、**官僚主义**以及最终数据的爆炸性增长，推动了对自动化和计算的需求不断增加。很快，那些柜子大小的电子机械计算机发展成为房间大小的庞然大物，维护成本高，容易出错。而正是这些机器为未来的创新创造了条件。

INTRO One of the largest electro-mechanical computers built was the Harvard Mark I, completed in 1944 by IBM for the Allies during World War 2. It contained 765,000 components, three million connections, and five hundred miles of wire. To keep its internal mechanics synchronized, it used a 50-foot **shaft** running right through the machine driven by a five **horsepower** motor. One of the earliest uses for this technology was running simulations for the Manhattan Project. 

> 简介 最大的电子机械计算机之一是1944年由IBM公司在第二次世界大战期间为**盟军**完成的哈佛马克一号。它包含765,000个部件，300万个连接点，以及500英里的电线。为了保持其内部机械的同步，它使用了一条50英尺长的**轴**，由一个5**马力的**马达驱动，直接穿过机器。这项技术最早的用途之一是为曼哈顿项目进行模拟。
>

The brains of these huge electro-mechanical beasts were **[[relays]]**: electrically-controlled mechanical switches. In a relay, there is a control wire that determines whether a circuit is opened or closed. The control wire connects to a coil of wire inside the relay. When current flows through the **coil**, an electromagnetic field is created, which in turn, attracts a metal arm inside the relay, snapping it shut and completing the circuit. You can think of a relay like a water **faucet**. The control wire is like the faucet handle. Open the faucet, and water flows through the pipe. Close the faucet, and the flow of water stops. Relays are doing the same thing, just with electrons instead of water. The controlled circuit can then connect to other circuits, or to something like a motor, which might increment a count on a gear, like in Hollerith's tabulating machine we talked about last episode.

> 这些巨大的电动机械兽的大脑是**继电器**：电控机械开关。在一个继电器中，有一条控制线决定电路是打开还是关闭。控制线连接到继电器内部的电线线圈。当电流流经**线圈**时，就会产生一个电磁场，反过来吸引继电器内的金属臂，啪嗒一声闭合，完成电路。你可以把继电器想象成一个水**龙头**。控制线就像水龙头的把手。打开水龙头，水就会流经管道。关闭水龙头，水流就会停止。继电器也在做同样的事情，只是用电子而不是水。受控电路然后可以连接到其他电路，或连接到像电机这样的东西，它可能会增加齿轮上的计数，就像我们上一集谈到的霍勒里思的制表机。

Unfortunately, the mechanical arm inside of a relay *has **mass***, and therefore can’t move instantly between opened and closed states. A good relay in the 1940’s might be able to **flick** back and forth fifty times in a second. That might seem pretty fast, but it’s not fast enough to be useful at solving large, complex problems. The Harvard Mark I could do 3 additions or subtractions per second; multiplications took 6 seconds, and divisions took 15. And more complex operations, like a **trigonometric** function, could take over a minute. In addition to slow switching speed, another limitation was **wear** and tear. Anything mechanical that moves will wear over time. Some things break entirely, and other things start getting sticky, slow, and just plain unreliable. And as the number of relays increases, the probability of a failure increases too. The Harvard Mark I had roughly 3500 relays. Even if you assume a relay has an operational life of 10 years, this would mean you’d have to replace, on average, one faulty relay every day! That’s a big problem when you are in the middle of running some important, multi-day calculation. And that’s not all engineers had to contend with. 

> 不幸的是，继电器内部的机械臂有***质量***，因此不能在打开和关闭状态之间瞬间移动。在20世纪40年代，一个好的继电器可能能够在一秒钟内来回五十次。这可能看起来相当快，但它的速度不足以在解决大型复杂问题时发挥作用。哈佛Mark I每秒可以做3次加减法；乘法需要6秒，除法需要15秒。而更复杂的运算，如**三角**函数，可能需要1分钟以上。除了开关速度慢之外，另一个限制是**磨损**和撕裂。任何移动的机械都会随着时间的推移而磨损。有些东西会完全坏掉，而其他东西则开始变得粘稠、缓慢，而且就是不可靠的。而且随着继电器数量的增加，发生故障的概率也会增加。哈佛大学的马克一世大约有3500个继电器。即使你假设一个继电器的工作寿命为10年，这也意味着你必须平均每天更换一个有问题的继电器! 当你正在进行一些重要的、多天的计算时，这是一个大问题。而这还不是工程师们不得不面对的全部问题。

These huge, dark, and warm machines also attracted insects. In September 1947, operators on the Harvard Mark II pulled a dead **moth** from a **malfunctioning** relay. Grace Hopper who we’ll talk more about in a later episode noted, “From then on, when anything went wrong with a computer, we said it had bugs in it.” And that’s where we get the term computer bug. It was clear that a faster, more reliable alternative to electro-mechanical relays was needed if computing was going to advance further, and fortunately that alternative already existed! In 1904, English physicist John Ambrose Fleming developed a new electrical component called a **thermionic** valve, which housed two electrodes inside an **airtight** glass bulb - this was the first vacuum tube. One of the **electrodes** could be heated, which would cause it to emit electrons – a process called thermionic emission. The other electrode could then attract these electrons to create the flow of our electric faucet, but only if it was positively charged - if it had a negative or **neutral** charge, the electrons would no longer be attracted across the vacuum so no current would flow. 

> 这些巨大、黑暗和温暖的机器也吸引了昆虫。1947年9月，哈佛大学Mark II的操作人员从一个**故障的**继电器中拉出一只死**蛾**。格雷斯-霍珀（Grace Hopper）（我们将在以后的节目中进一步讨论）指出："从那时起，当计算机出现任何问题时，我们就说它有虫子。" 这就是我们的计算机bug一词的由来。很明显，如果计算机要进一步发展，就需要一个更快、更可靠的电子机械继电器的替代品，幸运的是，这种替代品已经存在了! 1904年，英国物理学家约翰-安布罗斯-弗莱明开发了一种被称为**热离子**阀的新电气元件，它将两个电极装在一个**密闭的**玻璃球内--这就是第一个真空管。其中一个**电极**可以被加热，这将导致它发射电子--这一过程称为热离子发射。然后，另一个电极可以吸引这些电子，形成我们的电龙头的流量，但前提是它带正电--如果它带负电或**中性**电荷，电子将不再被吸引穿过真空，所以不会有电流流动。

An electronic component that permits the one-way flow of current is called a **diode**, but what was really needed was a switch to help turn this flow on and off. Luckily, shortly after, in 1906, American inventor Lee de Forest added a third “control” electrode that sits between the two electrodes in Fleming’s design. By applying a positive charge to the control electrode, it would permit the flow of electrons as before. But if the control electrode was given a negative charge, it would prevent the flow of electrons. So by manipulating the control wire, one could open or close the circuit. It’s pretty much the same thing as a relay - but importantly, vacuum tubes have no moving parts. This meant there was less wear, and more importantly, they could switch thousands of times per second. These **triode vacuum tubes** would become the basis of radio, long distance telephone, and many other electronic devices for nearly a half century. I should note here that vacuum tubes weren’t perfect - they’re kind of **fragile**, and can burn out like light bulbs, they were a big improvement over mechanical relays. Also, initially vacuum tubes were expensive – a radio set often used just one, but a computer might require hundreds or thousands of electrical switches. 

> 一个允许电流单向流动的电子元件被称为**二极管**，但真正需要的是一个开关来帮助打开和关闭这种流动。幸运的是，不久之后，在1906年，美国发明家李-德-弗雷斯特增加了第三个 "控制 "电极，该电极位于弗莱明设计的两个电极之间。通过向控制电极施加正电荷，它将允许电子像以前一样流动。但是如果控制电极被赋予一个负电荷，它将阻止电子的流动。因此，通过操纵控制线，人们可以打开或关闭电路。这与继电器几乎是一样的 - 但重要的是，真空管没有移动部件。这意味着磨损较少，更重要的是，它们可以每秒开关数千次。这些**三极管真空管**将成为近半个世纪以来无线电、长途电话和许多其他电子装置的基础。我应该在这里指出，真空管并不完美--它们有点**脆弱**，可以像灯泡一样烧坏，但它们比机械继电器有很大的改进。另外，最初真空管很昂贵--一套收音机通常只用一个，但一台计算机可能需要数百或数千个电子开关。

But by the 1940s, their cost and reliability had improved to the point where they became **feasible** for use in computers…. at least by people with deep pockets, like governments. This marked the shift from electro-mechanical computing to **electronic computing**. Let’s go to the Thought Bubble. The first large-scale use of vacuum tubes for computing was the Colossus Mk 1 designed by engineer Tommy Flowers and completed in December of 1943. The Colossus was installed at Bletchley Park, in the UK, and helped to decrypt Nazi communications. This may sound familiar because two years prior Alan Turing, often called the **father of computer science**, had created an electromechanical device, also at Bletchley Park, called the Bombe. It was an electromechanical machine designed to break Nazi **Enigma** codes, but the Bombe wasn’t technically a computer, and we’ll get to Alan Turing’s contributions later. 

> 但到了20世纪40年代，它们的成本和可靠性已经提高到了一个地步，即它们在计算机中的使用变得**可行**，....，至少是由政府等腰缠万贯的人使用。这标志着从电子机械计算到**电子计算**的转变。让我们去看看《思想泡影》。第一次大规模使用真空管进行计算是由工程师汤米-弗劳尔斯设计的Colossus Mk 1，于1943年12月完成。Colossus被安装在英国的布莱切利公园，并帮助解密纳粹的通信。这听起来很熟悉，因为在两年前，通常被称为计算机科学之父的阿兰-图灵创造了一个机电设备，也是在布莱切利公园，名为 "炸弹"。这是一台旨在破解纳粹**英格玛**密码的机电设备，但从技术上讲，"炸弹 "并不是一台计算机，我们将在后面讨论艾伦-图灵的贡献。

Anyway, the first version of Colossus contained 1,600 vacuum tubes, and in total, ten Colossi were built to help with code-breaking. Colossus is regarded as the **first programmable, electronic computer**. Programming was done by plugging hundreds of wires into plugboards, sort of like old school telephone switchboards, in order to set up the computer to perform the right operations. So while “programmable”, it still had to be configured to perform a specific computation. Enter the The Electronic Numerical Integrator and Calculator – or ENIAC – completed a few years later in 1946 at the University of Pennsylvania. Designed by John Mauchly and J. Presper Eckert, this was the world's **first truly general purpose, programmable, electronic computer**. ENIAC could perform 5000 ten-digit additions or subtractions per second, many, many times faster than any machine that came before it. It was operational for ten years, and is estimated to have done more arithmetic than the entire human race up to that point. But with that many vacuum tubes failures were common, and ENIAC was generally only operational for about half a day at a time before breaking down. Thanks Thought Bubble. 

> 总之，第一个版本的Colossus包含1,600个真空管，总共建造了10个Colossi来帮助破译密码。Colossus被认为是**第一台可编程的电子计算机**。编程是通过将数以百计的电线插入插板来完成的，有点像老式的电话交换机，以便将计算机设置为执行正确的操作。因此，虽然 "可编程"，但它仍然必须被配置为执行特定的计算。进入电子数字积分器和计算器--或ENIAC--几年后于1946年在宾夕法尼亚大学完成。由John Mauchly和J. Presper Eckert设计，这是世界上**首个真正的通用、可编程的电子计算机**。ENIAC每秒可以进行5000次10位数的加减运算，比之前的任何机器都快很多很多倍。它运行了十年，据估计，它所做的算术运算比整个人类到那时为止所做的还要多。但是这么多真空管的故障是很常见的，ENIAC一般每次只运行半天就坏了。谢谢思想泡泡。

By the 1950’s, even vacuum-tube-based computing was reaching its limits. The US Air Force’s AN/FSQ-7 computer, which was completed in 1955, was part of the “SAGE” air defense computer system we’ll talk more about in a later episode. To reduce cost and size, as well as improve reliability and speed, a **radical** new electronic switch would be needed. In 1947, Bell Laboratory scientists John Bardeen, Walter Brattain, and William Shockley invented the **transistor**, and with it, **a whole new era of computing was born**! The physics behind transistors is pretty complex, relying on **quantum mechanics**, so we’re going to stick to the basics. A transistor is just like a relay or vacuum tube - it’s a switch that can be opened or closed by applying electrical power via a control wire. Typically, transistors have two electrodes separated by a material that sometimes can conduct electricity, and other times resist it – a **semiconductor**. In this case, the control wire attaches to a “gate” electrode. By changing the electrical charge of the gate, the conductivity of the semiconducting material can be manipulated, allowing current to flow or be stopped – like the water faucet analogy we discussed earlier. 

> 到20世纪50年代，即使是基于真空管道的计算也达到了极限。美国空军的AN/FSQ-7计算机于1955年完成，是我们将在以后的节目中进一步讨论的 "SAGE "防空计算机系统的一部分。为了降低成本和尺寸，以及提高可靠性和速度，需要一个**彻底**新的电子开关。1947年，贝尔实验室的科学家约翰-巴丁、沃尔特-布拉坦和威廉-肖克利发明了**晶体管**，随着它的出现，**一个全新的计算时代诞生了**! 晶体管背后的物理学是相当复杂的，依赖于**量子力学**，所以我们关注基础知识。晶体管就像一个继电器或真空管--它是一个开关，可以通过控制线施加电力来打开或关闭。通常情况下，晶体管有两个电极，被一种有时能导电、有时能抗电的材料隔开--一种**半导体**。在这种情况下，控制线连接到一个 "门"电极。通过改变栅极的电荷，可以操纵半导体材料的导电性，允许电流流动或停止--就像我们之前讨论的水龙头的比喻。

Transistor_Simple_Circuit_Diagram_with_NPN_Labels：

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\Transistor_Simple_Circuit_Diagram_with_NPN_Labels.svg.png" style="zoom:67%;" />

Even the very first transistor at Bell Labs showed tremendous promise – it could switch between on and off states 10,000 times per second. Further, unlike vacuum tubes made of glass and with carefully suspended, fragile components, transistors were solid material known as a **solid state component**. Almost immediately, transistors could be made smaller than the smallest possible relays or vacuum tubes. This led to dramatically smaller and cheaper computers, like the IBM 608, released in 1957 – the **first fully transistor-powered, commercially-available computer**. It contained 3000 transistors and could perform 4,500 additions, or roughly 80 multiplications or divisions, every second. IBM soon transitioned all of its computing products to transistors, bringing transistor-based computers into offices, and eventually, homes. 

> 甚至贝尔实验室的第一个晶体管也显示出巨大的前景--它可以在每秒10000次的开和关状态之间切换。此外，与玻璃制成的真空管和精心悬挂的易碎部件不同，晶体管是固体材料，被称为**固态元件**。几乎在第一时间，晶体管就可以做得比最小的继电器或真空管还要小。这导致了计算机的急剧缩小和廉价，如1957年发布的IBM 608--**第一台完全由晶体管驱动的商业化计算机**。它包含3000个晶体管，每秒钟可以进行4500次加法，或大约80次乘法或除法。IBM很快将其所有的计算产品过渡到晶体管，将基于晶体管的计算机带入办公室，并最终带入家庭。

Today, computers use transistors that are smaller than 50 **nanometers** in size – for reference, a sheet of paper is roughly 100,000 nanometers thick. And they’re not only incredibly small, they’re super fast – they can switch states millions of times per second, and can run for decades. A lot of this transistor and semiconductor development happened in the Santa Clara  Valley, between San Francisco and San Jose, California. As the most common material used to create semiconductors is **silicon**, this region soon became known as Silicon Valley. Even William Shockley moved there, founding Shockley Semiconductor, whose employees later founded Fairchild Semiconductors, whose employees later founded Intel - the world’s largest computer chip maker today. Ok, so we’ve gone **from relays to vacuum tubes to transistors**. We can turn electricity on and off really, really, really fast. But how do we get from transistors to actually computing something, especially if we don’t have motors and gears? That’s what we’re going to cover over the next few episodes. Thanks for watching. See you next week. 

> 今天，计算机使用的晶体管尺寸小于50**纳米**--作为参考，一张纸的厚度大约为100,000纳米。它们不仅令人难以置信地小，而且速度超快--它们每秒可以切换数百万次状态，并且可以运行几十年。这种晶体管和半导体的发展很多都发生在加利福尼亚州旧金山和圣何塞之间的圣克拉拉谷地。由于制造半导体最常用的材料是**硅**，这个地区很快就被称为硅谷。甚至威廉-肖克利也搬到了那里，创立了肖克利半导体公司，其员工后来又创立了飞兆半导体公司，其员工后来又创立了英特尔--当今世界最大的计算机芯片制造商。好的，所以我们已经了解了**从继电器到真空管再到晶体管的**发展。我们可以非常、非常、非常快地开启和关闭电力。但是，我们如何从晶体管到实际计算的东西，特别是如果我们没有电机和齿轮？这就是我们在接下来的几期节目中要介绍的内容。谢谢你的观看。下周见。



## #3 Boolean Logic & Logic Gates

<iframe width="560" height="315" src="https://www.youtube.com/embed/gI-qXk7XojA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne and welcome to Crash Course Computer Science! Today we start our journey up the **ladder** of abstraction, where we leave behind the simplicity of being able to see every switch and gear, but gain the ability to assemble increasingly complex systems. 

> 大家好，我是卡丽-安，欢迎来到《计算机科学速成班》。今天我们开始了抽象的**阶梯**之旅，在这里我们抛开了能够看到每一个开关和齿轮的简单性，但获得了组装越来越复杂的系统的能力。
>
> （如果说前面更多是从硬件的层面来讲述，那么接下来的课程是从软件的层面来探讨）

INTRO Last episode, we talked about how computers evolved from electromechanical devices, that often had decimal representations of numbers – like those represented by teeth on a gear – to electronic computers with transistors that can turn the flow of electricity on or off. And fortunately, even with just two states of electricity, we can represent important information. We call this representation **Binary** -- which literally means “of two states”, in the same way a bicycle has two wheels or a biped has two legs. You might think two states isn’t a lot to work with, and you’d be right! But, it’s exactly what you need for representing the values “true” and “false”. **In computers, an “on” state, when electricity is flowing, represents true**. The off state, no electricity flowing, represents false. We can also write binary as 1’s and 0’s instead of true’s and false’s – they are just different expressions of the same signal – but we’ll talk more about that in the next episode. 

> 引言 上一集，我们谈到了计算机是如何从机电设备演变而来的，这些设备通常有十进制的数字表示--就像齿轮上的齿表示的数字--到有晶体管的电子计算机，可以打开或关闭电流。而幸运的是，即使只有两种电的状态，我们也可以表示重要的信息。我们称这种表示方式为**二进制**--字面意思是 "两种状态"，就像自行车有两个轮子或双足动物有两条腿一样。你可能会认为两个状态并不是一个很大的工作，你是对的！但是，这正是你的工作。但是，这正是你所需要的，用来表示 "真 "和 "假 "的值。**在计算机中，"开 "的状态，当电流流过时，代表真**。关闭状态，没有电流，代表假。我们也可以把二进制写成1和0，而不是真和假--它们只是同一信号的不同表达方式--但我们将在下一集里详细讨论这个问题。

Now it is actually possible to use transistors for more than just turning electrical current on and off, and to allow for different levels of current. Some early electronic computers were **ternary**, that's three states, and even **quinary**, using 5 states. The problem is, the more intermediate states there are, the harder it is to keep them all seperate -- if your smartphone battery starts running low or there’s electrical noise because someone's running a **microwave** nearby, the signals can get mixed up... and this problem only gets worse with transistors changing states millions of times per second! So, placing two signals as far apart as possible - using just ‘on and off’ - gives us the most distinct signal to minimize these issues. Another reason computers use binary is that an entire branch of mathematics already existed that dealt exclusively with true and false values. And it had figured out all of the necessary rules and operations for manipulating them. It's called Boolean Algebra! 

> 现在，实际上有可能使用晶体管，而不仅仅是打开和关闭电流，并允许不同级别的电流。一些早期的电子计算机是**三态的**，也就是三种状态，甚至还有**四态的**，使用五种状态。问题是，中间状态越多，就越难将它们分开 -- 如果你的智能手机电池开始耗尽，或者因为附近有人在使用**微波炉**而出现电噪声，信号就会被混淆......而这个问题只会随着晶体管每秒数百万次的状态变化而变得更加严重！因此，将两个信号放在尽可能远的地方--只使用 "开和关"--给我们提供最明显的信号，以尽量减少这些问题。计算机使用二进制的另一个原因是，整个数学的一个分支已经存在，专门处理真假值。而且它已经想出了所有必要的规则和操作来处理它们。这就是所谓的**布尔代数（Boolean Algebra）**。

**George Boole**, from which Boolean Algebra later got its name, was a self-taught English mathematician in the 1800s. He was interested in representing logical statements that went “under, over, and beyond” Aristotle’s approach to logic, which was, unsurprisingly, grounded in philosophy. Boole’s approach allowed truth to be systematically and formally proven, through logic equations which he introduced in his first book, “**The Mathematical Analysis of Logic**” in 1847. In “regular” algebra -- the type you probably learned in high school -- the values of variables are numbers, and operations on those numbers are things like addition and multiplication. But in Boolean Algebra, the values of variables are true and false, and the operations are logical. 

> **乔治-布尔（George Boole）**是19世纪自学成才的英国数学家，布尔代数的名字就来自于他。他对表现逻辑语句感兴趣，这些语句 "在亚里士多德的逻辑方法之下、之上和之上"，而亚里士多德的逻辑方法是以哲学为基础的，这并不奇怪。布尔的方法允许真理被系统地和正式地证明，通过逻辑方程，他在1847年的第一本书《**逻辑的数学分析**》中介绍了这些方程。在 "正则"代数中--你可能在高中学到的类型--变量的值是数字，对这些数字的运算是像加法和乘法。但在[布尔代数](https://en.wikipedia.org/wiki/Boolean_algebra)中，变量的值是true和false，而运算是逻辑性的。

There are three fundamental operations in Boolean Algebra: a **NOT**, an **AND**, and an **OR** operation. And these operations turn out to be really useful so we’re going to look at them individually. A NOT takes a single boolean value, either true or false, and negates it. It **flips** true to false, and false to true. We can write out a little logic table that shows the original value under Input, and the outcome after applying the operation under Output. Now here’s the cool part -- we can easily build boolean logic out of transistors. As we discussed last episode, transistors are really just little electrically controlled switches. They have three wires: two electrodes and one control wire. When you apply electricity to the control wire, it lets current flow through from one electrode, through the transistor, to the other electrode. This is a lot like a **spigot** on a pipe -- open the tap, water flows, close the tap, water shuts off. You can think of the control wire as an input, and the wire coming from the bottom electrode as the output. So with a single transistor, we have one input and one output. If we turn the input on, the output is also on because the current can flow through it. If we turn the input off, the output is also off and the current can no longer pass through. Or in boolean terms, when the input is true, the output is true. And when the input is false, the output is also false. Which again we can show on a logic table. 

> 在布尔代数中，有三种基本操作：**NOT**，**AND**，和**OR**操作。这些操作都是非常有用的，所以我们要单独看一下它们。一个 "NOT"操作需要一个单一的布尔值，要么是true，要么是false，并对其进行否定。它把true**翻转**为false，把false翻为true。我们可以写出一个小的逻辑表，在输入项下显示原始值，在输出项下显示应用操作后的结果。现在，最酷的部分来了--我们可以很容易地用晶体管来构建布尔逻辑。正如我们在上一集所讨论的那样，晶体管实际上只是小的电控开关。它们有三条线：两条电极和一条控制线。当你给控制线通电时，它让电流从一个电极流过，通过晶体管，流向另一个电极。这很像管道上的**水龙头**--打开水龙头，水就流出来了，关闭水龙头，水就关闭了。你可以把控制线看作是一个输入，而来自底部电极的线是输出。因此，对于单个晶体管，我们有一个输入和一个输出。如果我们把输入打开，输出也会打开，因为电流可以流过它。如果我们关闭输入端，输出端也会关闭，电流不能再通过。或者用布尔术语来说，当输入为true时，输出为true。而当输入为false时，输出也为false。我们可以再次在逻辑表上显示。

 NOT Boolean operation table: 

| input | output |
| ----- | ------ |
| true  | false  |
| false | true   |

This isn’t a very exciting circuit though because its not doing anything -- the input and output are the same. But, we can modify this circuit just a little bit to create a NOT. Instead of having the output wire at the end of the transistor, we can move it before. If we turn the input on, the transistor allows current to pass through it to the “ground”, and the output wire won’t receive that current - so it will be off. In our water **metaphor** grounding would be like if all the water in your house was flowing out of a huge hose so there wasn’t any water pressure left for your shower. So in this case if the input is on, output is off. When we turn off the transistor, though, current is prevented from flowing down it to the ground, so instead, current flows through the output wire. So the input will be off and the output will be on. And this matches our logic table for NOT, so congrats, we just built a circuit that computes NOT! We call them **NOT gates** - we call them gates because they’re controlling the path of our current. 

> 这并不是一个非常令人兴奋的电路，因为它没有做任何事情--输入和输出是一样的。但是，我们可以稍微修改一下这个电路，创造一个NOT。我们可以把输出线移到晶体管的前面，而不是把它放在末端。如果我们把输入端打开，晶体管允许电流通过它到 "地"，而输出线不会收到这个电流--所以它将被关闭。在我们的水**比喻**中，接地就像如果你房子里所有的水都从一个巨大的软管中流出来，那么你的淋浴就没有任何水压。所以在这种情况下，如果输入是打开的，输出就是关闭的。不过，当我们关闭晶体管时，电流被阻止顺着它流向地面，所以相反，电流流经输出线。因此，输入将被关闭，输出将被打开。这与我们的NOT逻辑表相吻合，所以恭喜你，我们刚刚建立了一个可以计算NOT的电路。我们称它们为NOT门--我们称它们为门是因为它们在控制我们的电流路径。

The AND Boolean operation takes two inputs, but still has a single output. In this case the output is only true if both inputs are true. Think about it like telling the truth. You’re only being completely honest if you don’t lie even a little. For example, let’s take the statement, “My name is Carrie Anne AND I’m wearing a blue dress". Both of those facts are true, so the whole statement is true. But if I said, “My name is Carrie Anne AND I’m wearing pants” that would be false, because I’m not wearing pants. Or trousers. If you’re in England. The Carrie Anne part is true, but a true AND a false, is still false. If I were to reverse that statement it would still obviously be false, and if I were to tell you two complete lies that is also false, and again we can write all of these combinations out in a table. 

> AND布尔运算需要两个输入，但仍有一个输出。在这种情况下，只有当两个输入都是真的时候，输出才是真的。想想看，这就像说实话。只有在你不撒谎的情况下，你才是完全诚实的。例如，让我们来看看这句话："我的名字叫卡丽-安，我穿着一件蓝色的衣服"。这两个事实都是真的，所以整个陈述是真的。但如果我说，"我的名字是卡丽-安，而且我穿着裤子"，那将是错误的，因为我没有穿裤子。或长裤。如果你是在英国。卡丽-安的部分是真的，但真和假，仍然是假的。如果我把这句话倒过来，显然还是假的，如果我告诉你两个完全的谎言，那也是假的，我们可以把所有这些组合写在一个表格里。

AND Boolean operation table

| Input A | Input B | Output |
| ------- | ------- | ------ |
| true    | true    | true   |
| true    | false   | false  |
| false   | true    | false  |
| false   | false   | false  |

To build an AND gate, we need two transistors connected together so we have our two inputs and one output. If we turn on just transistor A, current won’t flow because the current is stopped by transistor B. Alternatively, if transistor B is on, but the transistor A is off, the same thing, the current can’t get through. Only if transistor A AND transistor B are on does the output wire have current. 

> 为了建立一个AND门，我们需要两个晶体管连接在一起，这样我们就有两个输入和一个输出。如果我们只打开晶体管A，电流就不会流动，因为电流被晶体管B阻止了。只有当晶体管A和晶体管B都开启时，输出线才有电流。
>
> （这种情况下，transistor A, transistor B是处于串联状态）

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\and gate.jpg" style="zoom: 67%;" />

The last boolean operation is OR -- where only one input has to be true for the output to be true. For example, my name is Margaret Hamilton OR I’m wearing a blue dress. This is a true statement because although I’m not Margaret Hamilton unfortunately, I am wearing a blue dress, so the overall statement is true. An OR statement is also true if both facts are true. The only time an OR statement is false is if both inputs are false. 

> 最后一个布尔运算是OR--只有一个输入是真的，输出才是真的。例如，我的名字是玛格丽特-汉密尔顿，或者我穿着一件蓝色的衣服。这是一个真实的语句，因为虽然我不是玛格丽特-汉密尔顿，但我穿着一件蓝色的衣服，所以整个语句是真实的。如果两个事实都是真的，那么一个OR语句也是真的。只有当两个输入都是假的时候，OR语句才是假的。

OR Boolean operation table:

| Input A | Input B | Output |
| ------- | ------- | ------ |
| true    | true    | true   |
| true    | false   | true   |
| false   | true    | true   |
| false   | false   | false  |

Building an OR gate from transistors needs a few extra wires. Instead of having two transistors in series -- one after the other -- we have them in **parallel**. We run wires from the current source to both transistors. We use this little **arc** to note that the wires jump over one another and aren’t connected, even though they look like they cross. If both transistors are turned off, the current is prevented from flowing to the output, so the output is also off. Now, if we turn on just Transistor A, current can flow to the output. Same thing if transistor A is off, but Transistor B in on. Basically if A OR B is on, the output is also on. Also, if both transistors are on, the output is still on. 

> 用晶体管建立一个OR门需要一些额外的电线。我们不是将两个晶体管串联在一起--一个在另一个之后--而是将它们**并联**在一起。我们把电线从电流源接到两个晶体管。我们用这个小**弧**来指出，这些电线互相跳过，并没有连接，尽管它们看起来像是交叉的。如果两个晶体管都关闭，电流被阻止流向输出，所以输出也是关闭的。现在，如果我们只打开晶体管A，电流可以流向输出。如果晶体管A是关闭的，但晶体管B是打开的，情况也一样。基本上，如果A或B被打开，输出也是打开的。此外，如果两个晶体管都打开了，输出仍然是打开的。

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\or gate.jpg" style="zoom:67%;" />

Ok, now that we’ve got NOT, AND, and OR gates, and we can leave behind the **constituent** transistors and move up a layer of abstraction. The standard engineers use for these gates are a triangle with a dot for a NOT, a D for the AND, and a spaceship for the OR. Those aren’t the official names, but that's how I like to think of them. Representing them and thinking about them this way allows us to build even bigger components while keeping the overall complexity relatively the same - just remember that that mess of transistors and wires is still there. 

> 好了，现在我们已经有了NOT、AND和OR门，我们可以抛开构成的晶体管，向上移动一层抽象。工程师对这些门使用的标准是一个三角形，其中一个点代表NOT，一个D代表AND，一个飞船代表OR。这些并不是官方名称，但我喜欢这样去想它们。用这种方式表示它们并思考它们，可以让我们在保持整体复杂度相对不变的情况下制造出更大的元件--只要记住，那些乱七八糟的晶体管和电线仍然存在。
>

For example, another useful boolean operation in computation is called an **Exclusive OR** - or XOR for short. XOR is like a regular OR, but with one difference: if both inputs are true, the XOR is false. The only time an XOR is true is when one input is true and the other input is false. It’s like when you go out to dinner and your meal comes with a side salad OR a soup – sadly, you can’t have both! 

> 例如，计算中另一个有用的布尔运算被称为**排他性OR**--或简称XOR。XOR就像普通的OR，但有一个区别：如果两个输入都是真，XOR就是假。只有当一个输入为真，另一个输入为假的时候，XOR才是真的。这就像你出去吃饭，你的饭菜里有一份沙拉或一份汤--不幸的是，你不可能同时吃到这两样东西！"。

XOR table: 

| Input A | Input B | Output |
| ------- | ------- | ------ |
| true    | true    | false  |
| true    | false   | true   |
| false   | true    | true   |
| false   | false   | false  |

And building this from transistors is pretty confusing, but we can show how an XOR is created from our three basic boolean gates. We know we have two inputs again -- A and B -- and one output. Let’s start with an OR gate, since the logic table looks almost identical to an OR. There’s only one problem - when A and B are true, the logic is different from OR, and we need to output “false”. To do this we need to add some additional gates. If we add an AND gate, and the input is true and true, the output will be true. This isn’t what we want. But if we add a NOT immediately after this will flip it to false. Okay, now if we add a final AND gate and send it that value along with the output of our original OR gate, the AND will take in “false” and “true”, and since AND needs both values to be true, its output is false. That’s the first row of our logic table. If we work through the remaining input combinations, we can see this boolean logic circuit does implement an Exclusive OR. 

> 而从晶体管中建立这个是相当混乱的，但我们可以展示如何从我们的三个基本布尔门中创建一个XOR。我们知道我们又有两个输入--A和B--和一个输出。让我们从OR门开始，因为其逻辑表看起来与OR门几乎相同。只有一个问题--当A和B为真时，逻辑与OR不同，我们需要输出 "假"。要做到这一点，我们需要添加一些额外的门。如果我们添加一个AND门，并且输入为真和真，输出将为真。这并不是我们想要的。但如果我们在这之后立即添加一个NOT，就会把它翻转为假。好了，现在如果我们添加一个最后的AND门，并把这个值和我们原来的OR门的输出一起送给它，AND将接收 "false "和 "true"，由于AND需要两个值都是真的，它的输出是false。这就是我们逻辑表的第一行。如果我们通过其余的输入组合，我们可以看到这个布尔逻辑电路确实实现了排他性OR。

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\xor gate.jpg" style="zoom:67%;" />

And XOR turns out to be a very useful component, and we’ll get to it in another episode, so useful in fact engineers gave it its own symbol too -- an OR gate with a smile :) But most importantly, we can now put XOR into our metaphorical toolbox and not have to worry about the individual logic gates that make it up, or the transistors that make up those gates, or how electrons are flowing through a semiconductor. 

> XOR被证明是一个非常有用的组件，我们将在另一集里讨论它，事实上，工程师们也给了它自己的符号--一个带着微笑的OR门：) 但最重要的是，我们现在可以把XOR放到我们的隐喻工具箱中，而不必担心组成它的各个逻辑门，或组成这些门的晶体管，或电子如何在半导体中流动。

Moving up another layer of abstraction. When computer engineers are designing processors, they rarely work at the transistor level, and instead work with much larger blocks, like logic gates, and even larger components made up of logic gates, which we’ll discuss in future episodes. And even if you are a professional computer programmer, it’s not often that you think about how the logic that you are programming is a ctually implemented in the physical world by these **teeny** tiny components. We’ve also moved from thinking about raw electrical signals to our first representation of data - true and false - and we’ve even gotten a little taste of computation. With just the logic gates in this episode, we could build a machine that evaluates complex logic statements, like if “Name is John Green AND after 5pm OR is Weekend AND near Pizza Hut”, then “John will want pizza” equals true. And with that, I'm starving, I'll see you next week. 

> 再往上走一层抽象。当计算机工程师设计处理器时，他们很少在晶体管水平上工作，而是使用更大的块，如逻辑门，甚至是由逻辑门组成的更大的组件，我们将在未来的节目中讨论这些。即使你是一个专业的计算机程序员，你也不会经常考虑你正在编程的逻辑是如何通过这些**极小**的微小元件在物理世界中实现的。我们也从对原始电信号的思考转向了对数据的首次表述--真和假--我们甚至还尝到了计算的滋味。仅仅用这一集的逻辑门，我们就可以建造一台机器来评估复杂的逻辑语句，例如，如果 "名字是约翰-格林，下午5点以后，或者是周末，在必胜客附近"，那么 "约翰会想要比萨 "就等于真的。就这样，我饿了，下周见。



## #4 Representing numbers and letters with binary

<iframe width="560" height="315" src="https://www.youtube.com/embed/1GSjbWt0c9M" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi I’m Carrie Anne, this is Crash Course Computer Science and today we’re going to talk about **how computers store and represent numerical data**. Which means we’ve got to talk about Math! But don’t worry. Every single one of you already knows exactly what you need to know to follow along. So, last episode we talked about how transistors can be used to build logic gates, which can evaluate boolean statements. And in boolean algebra, there are only two, binary values: true and false. But if we only have two values, how in the world do we represent information beyond just these two values? That’s where the Math comes in. 

> 大家好，我是卡丽-安，这里是计算机科学速成班，今天我们要讲的是**计算机如何存储和表示数字数据**。这意味着我们要谈数学！但不要担心。但是不要担心。你们中的每一个人都已经知道了你们需要知道的东西，可以跟着学。所以，上一集我们谈到了晶体管如何被用来建立逻辑门，它可以求布尔语句的值。而在布尔代数中，只有两个，二进制值：true和false。但是，如果我们只有两个值，那么我们到底如何表示这两个值以外的信息呢？这就是数学的作用。

INTRO So, as we mentioned last episode, a single binary value can be used to represent a number. Instead of true and false, we can call these two states 1 and 0 which is actually incredibly useful. And **if we want to represent larger things we just need to add more binary digits**. This works exactly the same way as the decimal numbers that we’re all familiar with. With decimal numbers there are "only" 10 possible values a single **digit** can be; 0 through 9, and to get numbers larger than 9 we just start adding more digits to the front. We can do the same with binary. For example, let’s take the number two hundred and sixty three. What does this number actually represent? Well, it means we’ve got 2 one-hundreds, 6 tens, and 3 ones. If you add those all together, we’ve got 263. Notice how each column has a different multiplier. In this case, it’s 100, 10, and 1. Each multiplier is ten times larger than the one to the right. That's because each column has ten possible digits to work with, 0 through 9, after which you have to **carry** one to the next column. For this reason, it’s called **base-ten notation**, also called **decimal** since deci means ten. 

> 简介：所以，正如我们上一集提到的，一个单一的二进制值可以用来代表一个数字。我们可以把这两个状态称为1和0，而不是真和假，这实际上是非常有用的。而**如果我们想表示更大的东西，我们只需要添加更多的二进制数字**。这与我们所熟悉的十进制数的工作方式完全相同。对于十进制数字，一个数位（十进制的数位是0~9）"只有"10个可能的值；0到9，为了得到大于9的数字，我们只需在前面添加更多的数字。我们可以用二进制做同样的事情。例如，让我们来看看263这个数字。这个数字实际上代表什么？嗯，这意味着我们有2个100，6个10，和3个1。如果你把这些加在一起，我们就有263。注意每一列都有一个不同的乘数。在这种情况下，它是100、10和1。每个乘数都比右边的乘数大10倍（这里正确的表述应该是“每个乘数都是右边的乘数的10倍”，英文阐述：Each multiplier is ten times as larger as the one to the right）。这是因为每一列都有十个可能的数字，从0到9，之后你必须把一个数字进位到下一列。由于这个原因，它被称为**十进制记数法**，也被称为**十进制**，因为deci是指十。

AND Binary works exactly the same way, it’s just base-two. That’s because there are only two possible digits in binary – 1 and 0. This means that each multiplier has to be two times larger than the column to its right. Instead of hundreds, tens, and ones, we now have fours, twos and ones. Take for example the binary number: 101. This means we have 1 four, 0 twos, and 1 one. Add those all together and we’ve got the number 5 in base ten. But to represent larger numbers, binary needs a lot more **digits**. Take this number in binary 10110111. We can convert it to decimal in the same way. We have 1 x 128, 0 x 64, 1 x 32, 1 x 16, 0 x 8, 1 x 4, 1 x 2, and 1 x 1. Which all adds up to 183. Math with binary numbers isn’t hard either. Take for example decimal addition of 183 plus 19. First we add 3 + 9, that’s 12, so we put 2 as the sum and carry 1 to the ten’s column. Now we add 8 plus 1 plus the 1 we carried, thats 10, so the sum is 0 carry 1. Finally we add 1 plus the 1 we carried, which equals 2. So the total sum is 202. Here’s the same sum but in binary. Just as before, we start with the ones column. Adding 1+1 results in 2, even in binary. But, there is no symbol "2" so we use 10 and put 0 as our sum and carry the 1. Just like in our decimal example. 1 plus 1, plus the 1 carried, equals 3 or 11 in binary, so we put the sum as 1 and we carry 1 again, and so on. We end up with 11001010, which is the same as the number 202 in base ten. 

> 二进制的工作方式也是一样，只是基数为2。这是因为二进制中只有两个可能的数字--1和0。这意味着每个乘数必须比其右边的列大两倍。（这里正确的表述应该是“每个乘数必须是其右边列的两倍”，英文阐述：each multiplier has to be two times as larger as the column to its right）我们现在没有百位、十位和一位，而是有四位、二位和一位。以二进制数字为例：101。这意味着我们有1个4，0个2，和1个1。把这些加在一起，我们就得到了基数为10的数字5。但是为了表示更大的数字，二进制需要更多的**数位**。以这个二进制数字10110111为例。我们可以用同样的方法将其转换成十进制。我们有1×128，0×64，1×32，1×16，0×8，1×4，1×2和1×1，加起来就是183。二进制数字的数学也不难。以183加19的十进制加法为例。首先，我们把3+9加起来，就是12，所以我们把2作为总和，并把1进位到10的那一列。现在我们把8加1再加上我们进位的1，就是10，所以和是0进位1。最后我们把1加上我们进位的1，等于2。所以总和是202。下面是同样的和，但是是二进制的。就像以前一样，我们从1列开始。加1+1的结果是2，即使是二进制的。但是，没有符号 "2"，所以我们用10，并把0作为我们的总和，并进位1，就像我们的十进制例子。1加1，再加上进位的1，在二进制中等于3或11，所以我们把总和作为1，并再次进位1，如此循环。我们最后得到11001010，这与十进制中的数字202相同。

add 183 to 10 in binary:

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\add in binary.jpg" style="zoom:67%;" />

Each of these binary **digits**, 1 or 0, is called a “**bit**”. So in these last few examples, we were using 8-bit numbers with their lowest value of zero and highest value is 255, which requires all 8 bits to be set to 1. Thats 256 different values, or 2 to the 8th power. You might have heard of 8-bit computers, or 8-bit graphics or audio. These were computers that did most of their operations **in chunks of 8 bits**. But 256 different values isn’t a lot to work with, so it meant things like 8-bit games were limited to 256 different colors for their graphics. And **8-bits is such a common size in computing, it has a special word: a byte**. A byte is 8 bits. If you’ve got 10 bytes, it means you’ve really got 80 bits. You’ve heard of kilobytes, megabytes, gigabytes and so on. These prefixes denote different scales of data. Just like one kilogram is a thousand grams, 1 kilobyte is a thousand bytes…. or really 8000 bits. Mega is a million bytes (MB), and giga is a billion bytes (GB). Today you might even have a hard drive that has 1 terabyte (TB) of storage. That's 8 trillion ones and zeros. 

> 这些二进制的**数位**，1或0，被称为 "**位（比特）**"。因此，在最后几个例子中，我们使用的是8位数，其最低值为0，最高值为255，这需要将所有8位都设置为1。 这就是256个不同的值，或2的8次方。你可能听说过8位计算机，或8位图形或音频。这些计算机的大部分操作都是**以8位为单位**进行的。但是，256个不同的值并不是很多，所以这意味着像8位游戏的图形被限制在256种不同的颜色上。8比特在计算中是一个很常见的大小，它有一个特殊的词：**字节**。一个字节是8比特。如果你有10个字节，这意味着你真的有80个比特。你已经听说过千字节、兆字节、千兆字节等等。这些前缀表示数据的不同尺度。就像一公斤是一千克一样，1千字节是一千字节....，或者真的是8000比特。Mega是一百万字节（MB），giga是十亿字节（GB）。今天，你甚至可能有一个拥有1兆字节（TB）存储量的硬盘。那是8万亿个1和0。

But hold on! That’s not always true. **In binary, a kilobyte has two to the power of 10 bytes, or 1024**. 1000 is also right when talking about kilobytes, but we should acknowledge it isn’t the only correct definition. You’ve probably also heard the term 32-bit or 64-bit computers – you’re almost certainly using one right now. What this means is that they operate **in chunks of 32 or 64 bits**. That’s a lot of bits! **The largest number you can represent with 32 bits is just under 4.3 billion**. Which is **thirty-two 1's in binary**. This is why our Instagram photos are so smooth and pretty – they are composed of millions of colors, because computers today use 32-bit color graphics. 

> 但等一下! 这并不总是真的。**在二进制中，一个千字节有2的10次方个字节，即1024**。在谈论千字节时，1000也是正确的，但我们应该承认它并不是唯一正确的定义。你可能也听说过32位或64位计算机这个术语--你现在几乎肯定在使用一个。这意味着它们以**32或64位的块状**形式运行。这是一个很大的位数! **你能用32位表示的最大数字是不到43亿**。也就是**32个二进制的1**。这就是为什么我们的Instagram照片如此流畅和漂亮--它们是由数百万种颜色组成的，因为今天的计算机使用32位彩色图形。

Of course, not everything is a positive number - like my bank account in college. So we need a way to represent positive and negative numbers. Most computers use the first bit for the sign: 1 for negative, 0 for positive numbers, and then use the remaining 31 bits for the number itself. That gives us a range of roughly plus or minus two billion. While this is a pretty big range of numbers, it’s not enough for many tasks. There are 7 billion people on the earth, and the US national debt is almost 20 trillion dollars after all. This is why 64-bit numbers are useful. The largest value a 64-bit number can represent is around 9.2 **quintillion**! That’s a lot of possible numbers and will hopefully stay above the US national debt for a while! 

> 当然，不是所有的东西都是正数--比如我大学时的银行账户。所以我们需要一种方法来表示正数和负数。大多数计算机使用第一位作为符号：1代表负数，0代表正数，然后用剩下的31位表示数字本身。这给了我们一个大约正负20亿的范围。虽然这是一个相当大的数字范围，但对于许多任务来说是不够的。地球上有70亿人，而美国的国债毕竟有近20万亿美元。这就是为什么64位数字是有用的。一个64位的数字所能代表的最大数值大约是9.2×百万的三次方（亿）! 这是一个很多可能的数字，希望能在美国国债之上保持一段时间! 

Most importantly, as we’ll discuss in a later episode, computers must **label locations in their memory**, known as **addresses**, in order to store and retrieve values. As computer memory has grown to gigabytes and terabytes – that’s trillions of bytes – it was necessary to have 64-bit memory addresses as well. In addition to negative and positive numbers, computers must deal with numbers that are not whole numbers, like 12.7 and 3.14, or maybe even **stardate**: 43989.1. These are called “floating point” numbers, because the decimal point can float around in the middle of number. Several methods have been developed to represent floating point numbers. The most common of which is the **IEEE 754 standard**. And you thought historians were the only people bad at naming things! In essence, this standard stores decimal values sort of like **scientific notation**. For example, 625.9 can be written as 0.6259 x 10^3. There are two important numbers here: the .6259 is called the **significand**. And 3 is the **exponent**. In a 32-bit floating point number, the first bit is used for the sign of the number -- positive or negative. **The next 8 bits are used to store the exponent** and **the remaining 23 bits are used to store the significand**. 

> 最重要的是，正如我们将在以后的节目中讨论的那样，计算机必须对其内存中的位置进行**标记**，称为**地址**，以便存储和检索值。随着计算机内存发展到千兆字节和万兆字节--也就是数万亿字节--也有必要拥有64位内存地址。除了负数和正数之外，计算机还必须处理非整数的数字，如12.7和3.14，甚至可能是**星际日期**：43989.1. 这些数字被称为 "浮点"，因为小数点可以在数字的中间浮动。已经开发了几种方法来表示浮点数。其中最常见的是**IEEE 754标准**。你以为历史学家是唯一不擅长命名的人吗？从本质上讲，这个标准存储的十进制数值有点像**科学计数法**。例如，625.9可以被写成0.6259 x 10^3。这里有两个重要的数字：0.6259被称为**有效位数**。而3则是**指数**。在一个32位的浮点数中，第一位用来表示数字的符号--正或负。**接下来的8位用于存储指数**，**剩下的23位用于存储有效位数**。

an example of float point number is represented by the computer:

![{\displaystyle 1.2345=\underbrace {12345} _{\text{significand}}\times \underbrace {10} _{\text{base}}\!\!\!\!\!\!^{\overbrace {-4} ^{\text{exponent}}}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ae814346939ac31086e1d0286c41d98e6b053102)

Ok, we’ve talked a lot about numbers, but your name is probably composed of letters, so it’s really useful for computers to also have a way to represent text. However, rather than have a special form of storage for letters, **computers simply use numbers to represent letters**. The most straightforward approach might be to simply number the letters of the alphabet: A being 1, B being 2, C 3, and so on. In fact, Francis Bacon, the famous English writer, used five-bit sequences to encode all 26 letters of the English alphabet to send secret messages back in the 1600s. And five bits can store 32 possible values – so that’s enough for the 26 letters, but not enough for **punctuation**, digits, and upper and lower case letters. Enter ASCII, the American Standard Code for Information Interchange. Invented in 1963, ASCII was a 7-bit code, enough to store 128 different values. With this expanded range, it could encode capital letters, lowercase letters, digits 0 through 9, and symbols like the @ sign and punctuation marks. For example, a lowercase ‘a’ is represented by the number 97, while a capital ‘A’ is 65. A colon is 58 and a closed parenthesis is 41. ASCII even had a selection of special **command codes**, such as a newline character to tell the computer where to **wrap** a line to the next row. In older computer systems, the line of text would literally continue off the edge of the screen if you didn’t include a new line character! Because ASCII was such an early standard, it became widely used, and critically, allowed different computers built by different companies to exchange data. This ability to universally exchange information is called “**interoperability**”. 

> 好吧，我们已经谈了很多关于数字的问题，但是你的名字可能是由字母组成的，所以对于计算机来说，有一种表示文字的方法确实很有用。然而，**计算机并没有为字母提供特殊的存储形式，而是简单地用数字来表示字母**。最直接的方法可能是为字母表中的字母简单编号。A是1，B是2，C是3，以此类推。事实上，英国著名作家弗朗西斯-培根（Francis Bacon）早在16世纪就用五位数序列来编码所有26个英文字母，以发送秘密信息。而5位可以存储32个可能的值--所以这对26个字母来说已经足够了，但对**标点符号**、数字以及大写和小写字母来说还不够。进入ASCII，美国信息交换标准代码。发明于1963年，ASCII是一个7位的代码，足以存储128个不同的值。随着范围的扩大，它可以对大写字母、小写字母、数字0到9，以及@符号和标点符号等符号进行编码。例如，小写字母 "a "用数字97表示，而大写字母 "A "是65。冒号是58，闭合小括号是41。ASCII甚至还有一些特殊的**命令代码**，如换行符（new line），用来告诉计算机在哪里**包裹**一行，以切换到下一行。在旧的计算机系统中，如果你不加入换行符，文本行就会继续离开屏幕的边缘。由于ASCII是如此早期的标准，它被广泛使用，关键是它允许不同公司制造的不同计算机交换数据。这种普遍交换信息的能力被称为 "**互操作性**"。
>

However, it did have a major limitation: it was really only designed for English. Fortunately, there are 8 bits in a byte, not 7, and it soon became popular to use codes 128 through 255, previously unused, for "national" characters. In the US, those extra numbers were largely used to encode additional symbols, like mathematical notation, graphical elements, and common **accented** characters. On the other hand, while the Latin characters were used universally, Russian computers used the extra codes to encode **Cyrillic** characters, and Greek computers, Greek letters, and so on. And national character codes worked pretty well for most countries. The problem was, if you opened an email written in Latvian on a Turkish computer, the result was completely **incomprehensible**. And things totally broke with the rise of computing in Asia, as languages like Chinese and Japanese have thousands of characters. There was no way to encode all those characters in 8-bits! In response, each country invented multi-byte encoding **schemes**, all of which were mutually incompatible. The Japanese were so familiar with this encoding problem that they had a special name for it: "**mojibake**", which means "**scrambled** text". 

> 然而，它确实有一个主要的限制：它确实只为英语设计。幸运的是，一个字节有8位，而不是7位，而且很快就流行使用编码128至255，以前没有使用过，用于 "国家"字符。在美国，这些额外的数字主要用于编码额外的符号，如数学符号、图形元素和常见的**口音的**字符。另一方面，虽然拉丁字符被普遍使用，但俄罗斯的计算机使用额外的代码来编码**西里尔**字符，以及希腊计算机、希腊字母等。而国家字符编码对大多数国家来说都很好用。问题是，如果你在土耳其的电脑上打开一封用拉脱维亚语写的电子邮件，结果是完全**无法理解的**。随着计算机在亚洲的兴起，事情就完全破灭了，因为像中文和日文这样的语言有成千上万的字符。没有办法用8位编码所有这些字符! 作为回应，每个国家都发明了多字节的编码**方案**，这些方案都是相互不兼容的。日本人对这个编码问题非常熟悉，以至于他们有一个特殊的名字。"**mojibake**"，意思是 "**scrambled**文本"。

And so it was born – **Unicode** – one format to rule them all. Devised in 1992 to finally do away with all of the different international schemes it replaced them with one universal encoding scheme. **The most common version of Unicode uses 16 bits with space for over a million codes** - enough for every single character from every language ever used – more than 120,000 of them in over 100 types of script plus space for mathematical symbols and even graphical characters like Emoji. And in the same way that ASCII defines a scheme for encoding letters as binary numbers, other file formats – like MP3s or GIFs – use binary numbers to encode sounds or colors of a pixel in our photos, movies, and music. Most importantly, under the **hood** it all comes down to long sequences of bits. Text messages, this YouTube video, every webpage on the internet, and even your computer’s operating system, are nothing but **long sequences of 1s and 0s**. So next week, we’ll start talking about how your computer starts manipulating those binary sequences, for our first true taste of computation. Thanks for watching. See you next week. 

> 因此，它诞生了--**Unicode**--一种统治它们的格式。1992年，为了最终消除所有不同的国际方案，它用一个通用的编码方案取代它们。**最常见的Unicode版本使用16位，可容纳超过一百万个代码**--足以容纳有史以来使用的每一种语言的每一个字符--超过100种字体中的12万个字符，加上数学符号的空间，甚至像Emoji一样的图形字符。正如ASCII定义了一个将字母编码为二进制数字的方案一样，其他文件格式--如MP3或GIF--也使用二进制数字来编码声音或我们照片、电影和音乐中的像素颜色。最重要的是，在**表面**下，一切都归结为长的比特序列。短信、这个YouTube视频、互联网上的每一个网页，甚至你的电脑操作系统，都不过是个1和0的**长序列**。因此，下周，我们将开始讨论你的计算机如何开始操纵这些二进制序列，让我们第一次真正尝到计算的滋味。谢谢你的观看。下周见。



## #5 How computer calculate - the ALU

<iframe width="560" height="315" src="https://www.youtube.com/embed/1I5ZMmrOfnA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Ann and this is Crash Course Computer Science. So last episode, we talked about how numbers can be represented in binary. Representing Like, 00101010 is 42 in decimal. Representing  and storing numbers is an important function of a computer, but the real goal is computation, or manipulating numbers in a structured and purposeful way, like adding two numbers together. These operations are handled by a computer’s **Arithmetic and Logic Unit**, but most people call it by its street name: the ALU. The ALU is the mathematical brain of a computer. When you understand an ALU’s **design and function**, you’ll understand a fundamental part of modern computers. It is THE thing that does all of the computation in a computer, so basically everything uses it. 

> 嗨，我是Carrie Ann，这里是计算机科学速成班。上一集，我们谈到了数字如何用二进制表示。表示像，00101010在十进制中是42。表示和存储数字是计算机的一个重要功能，但真正的目标是计算，或以结构化和有目的的方式操作数字，比如把两个数字加在一起。这些操作是由计算机的**算术和逻辑单元**处理的，但大多数人都叫它的街名：ALU。ALU是计算机的数学大脑。当你了解了ALU的**设计和功能**，你就会明白现代计算机的一个基本部分。它是计算机中进行所有计算的东西，所以基本上所有东西都使用它。

First though, look at this beauty. This is perhaps the most famous ALU ever, the Intel 74181. When it was released in 1970, it was It was the first complete ALU that fit entirely inside of a single **chip** - Which was a huge engineering **feat** at the time. So today we’re going to take those Boolean logic gates we learned about last week to build a simple ALU circuit with much of the same functionality as the 74181. And over the next few episodes we’ll use this to construct a computer from **scratch**. So it’s going to get a little bit complicated, but I think you guys can handle it. 

> 不过，首先看看这个美丽的东西吧。这可能是有史以来最著名的ALU，英特尔74181。当它在1970年发布时，它是第一个完全装在单个**芯片**内的完整ALU--这在当时是一个巨大的工程**成就**。因此，今天我们将利用上周学到的那些布尔逻辑门来构建一个简单的ALU电路，其功能与74181基本相同。在接下来的几期节目中，我们将用它来构建一个**从零开始**的计算机。因此，这将变得有点复杂，但我认为你们可以处理它。

INTRO An ALU is really two units in one -- there’s an **arithmetic unit** and a **logic unit**. Let's start with the arithmetic unit, which is responsible for handling all numerical operations in a computer, like addition and subtraction. It also does a bunch of other simple things like add one to a number, which is called an **increment** operation, but we’ll talk about those later. Today, we’re going to focus on the **pièce de résistance**, the crème de la crème of operations that underlies almost everything else a computer does - adding two numbers together. We could build this circuit entirely out of **individual transistors**, but that would get confusing really fast. So instead as we talked about in Episode 3 – we can use a high-level of abstraction and build our components out of logic gates, in this case: AND, OR, NOT and XOR gates. The simplest adding circuit that we can build takes two binary digits, and adds them together. So we have two inputs, A and B, and one output, which is the sum of those two digits. Just to clarify: A, B and the output are all single bits. There are only four possible input combinations. The first three are: 0+0 = 0 1+0 = 1 0+1 = 1 Remember that in binary, 1 is the same as true, and 0 is the same as false. So this set of inputs exactly matches the boolean logic of an **XOR gate**, and we can use it as our **1-bit adder**. But the fourth input combination, 1 + 1, is a special case. 1 + 1 is 2 (obviously) but there’s no 2 digit in binary, so as we talked about last episode, the result is 0 and the 1 is carried to the next column. So the sum is really 10 in binary. Now, the output of our XOR gate is partially correct - 1 plus 1, outputs 0. But, we need an extra **output wire** for that carry bit. The carry bit is only “true” when the inputs are 1 AND 1, because that's the only time when the result (two) is bigger than 1 bit can store… and conveniently we have a gate for that! An AND gate, which is only true when both inputs are true, so we’ll add that to our circuit too. And that's it. This circuit is called **a half adder**. It’s It's not that complicated - just two logic gates - but let’s abstract away even this level of detail and encapsulate our newly **minted** half adder as its own component, with two inputs - bits A and B - and two outputs, the sum and the carry bits. This takes us to another level of abstraction… heh… I feel like I say that a lot. I wonder if this is going to become a thing. 

> 简介 ALU实际上是两个单元组成的--有一个**算术单元**和一个**逻辑单元**。让我们从算术单元开始，它负责处理计算机中的所有数字运算，如加法和减法。它还做其他一些简单的事情，比如在一个数字上加一，这被称为**增量**操作，但我们稍后会讨论这些。今天，我们将重点讨论**把两个数字相加**，也就是计算机几乎所有其他操作的基础--两个数字相加。我们可以完全用**独立的晶体管**来构建这个电路，但这很快就会变得混乱。因此，正如我们在第3篇中所谈到的那样，我们可以使用高层次的抽象，用逻辑门来构建我们的组件，在这种情况下。AND, OR, NOT和XOR门。我们可以建立的最简单的加法电路需要两个二进制数字，并把它们加在一起。所以我们有两个输入，A和B，和一个输出，就是这两个数字的总和。只是为了澄清。A、B和输出都是单比特。只有四种可能的输入组合。前三个是：0+0=0 1+0=1 0+1=1 记住，在二进制中，1与真相同，0与假相同。因此，这组输入与**XOR门**的布尔逻辑完全匹配，我们可以用它作为我们的**1位加法器**。但是第四个输入组合，1+1，是一个特殊情况。1+1等于2（很明显），但二进制中没有2位，所以正如我们上一集所说，结果是0，1被带到下一列。所以总和实际上是二进制的10。现在，我们的XOR门的输出是部分正确的--1加1，输出0。但是，我们需要一个额外的**输出线**这个进位。只有当输入为1和1时，进位才是 "真 "的，因为那是唯一的一次，当结果（2）大于1位可以存储的时候......而我们有一个门可以方便地处理这个问题 - AND门，只有当两个输入都是真的时候才是真的，所以我们也要把它加入我们的电路中。就这样了。这个电路被称为**半加器**。它并不复杂--只有两个逻辑门--但是让我们把这一层次的细节也抽象出来，把我们新**创建**的半加器封装成自己的组件，有两个输入--比特A和B--和两个输出，即"总和"和“进位”比特。这使我们进入了另一个抽象层次......嘿......我觉得我说了很多。我想知道这是否会成为一个梗。

four possible input combinations & half adder:

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\four kinds of addition and half adder.jpg" style="zoom: 80%;" />

half adder:

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\half adder.jpg" style="zoom: 80%;" />

Anyway, If you want to add more than 1 + 1 we’re going to need a “Full Adder.” That half-adder left us with a carry bit as output. That means that when we move on to the next column in a multi-column addition, and every column after that, we are going to have to add three bits together, no two. A full adder is a bit more complicated - it takes three bits as inputs: A, B and C. So the maximum possible input is 1 + 1 + 1, which equals 1 carry out 1, so we still only need two output wires: sum and carry. We can build a full adder using half adders. To do this, we use a half adder to add A plus B just like before – but then feed that result and input C into a second half adder. Lastly, we need a OR gate to check if either one of the carry bits was true. That’s it, we just made a **full adder**! Again,we can go up a level of abstraction and **wrap up** this full adder as its own component. It takes three inputs, adds them, and outputs the sum and the carry, if there is one. 

> 总之，如果你想加多于1+1，我们就需要一个 "全加器"。那个半加器给我们留下了一个进位作为输出。这意味着，当我们在多列加法中进入下一列，以及之后的每一列，我们将不得不把三个比特加在一起，而不是两个。全加器就比较复杂了--它需要三个比特作为输入。因此，最大可能的输入是1+1+1，等于1进位1，所以我们仍然只需要两个输出线：“总和”与“进位”。我们可以用半加器建立一个全加器。要做到这一点，我们使用一个半加器，像以前一样，将A加B - 但然后将结果和输入C送入第二个半加器。最后，我们需要一个OR门来检查进位中的任何一个是否为真。就这样，我们刚刚做了一个**全加器**! 同样，我们可以上升到一个抽象的层次，把这个全加器作为自己的组件。它接受三个输入，将它们相加，然后输出“总和”和“进位”（如果有的话）。

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\full adder table.jpg" style="zoom: 80%;" />

building a full adder: A(input A), B(input B), C(carry 进位)， S(sum, 总和)

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\full adder.jpg" style="zoom:80%;" />



Armed with our new components, we can now build a circuit that takes two, 8-bit numbers – Let’s call them A and B – and adds them together. Let’s start with the very first bit of A and B, which we’ll call A0 and B0. At this point, there is no carry bit to deal with, because this is our first addition. So we can use our half adder to add these two bits together. The output is sum0. Now we want to add A1 and B1 together. It's possible there was a carry from the previous addition of A0 and B0, so this time we need to use a full adder that also inputs the carry bit. We output this result as sum1. Then, we take any carry from this full adder, and run it into the next full adder that handles A2 and B2. And we just keep doing this in a big chain until all 8 bits have been added. Notice how the carry bits **ripple** forward to each subsequent adder. For this reason, this is called an 8-bit ripple carry adder. Notice how our last full adder has a carry out. 

> 有了新的元件，我们现在可以建立一个电路，将两个8位数字--我们称它们为A和B--加在一起。让我们从A和B的第一个位开始，我们称之为A0和B0。在这一点上，没有进位需要处理，因为这是我们的第一个加法。所以我们可以用我们的半加器将这两个位加在一起。其输出为sum0。现在我们要把A1和B1加在一起。可能在之前的A0和B0的加法中存在一个进位，所以这次我们需要使用一个全加器，同时输入进位。我们把这个结果输出为sum1。然后，我们从这个全加器中取出任何一个进位，并将其运行到下一个处理A2和B2的全加器中。我们就这样一直做下去，直到所有的8位都被加上。请注意进位是如何**连**到每个后续加法器的。由于这个原因，这个加法器被称为8位波进位加法器。注意我们的最后一个全加器有一个“进位”输出。

8-bit ripple adder:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\8-bit ripple carry adder.jpg)

If there is a carry into the 9th bit, it means the sum of the two numbers is too large to fit into 8-bits. This is called an **overflow**. In general, an overflow occurs when the result of an addition is too large to be represented by the number of bits you are using. This can usually cause errors and unexpected behavior. Famously, the original PacMan **arcade** game used 8 bits to keep track of what level you were on. This meant that if you made it past level 255 – the largest number **storablein** 8 bits – to level 256, the ALU overflowed. This caused a bunch of errors and **glitches** making the level unbeatable. The bug became a **rite** of passage for the greatest PacMan players. So if we want to avoid overflows, we can extend our circuit with more full adders, allowing us to add 16 or 32 bit numbers. This makes overflows less likely to happen, but at the expense of more gates. 

> 如果有一个进位到第9位，这意味着两个数字的总和太大，无法装入8位。这被称为**溢出**。一般来说，当加法的结果太大，无法用你所使用的比特数字表示时，就会发生溢出。这通常会导致错误和意外行为。著名的是，最初的吃豆人**街机**游戏使用8位来跟踪你所处的级别。这意味着，如果你超过了255级--8位中可存储的最大数字--到256级，ALU就会溢出。这就造成了一堆错误和**故障**，使得这个关卡无法被打败。这个错误成为最伟大的吃豆人玩家的一个**仪式**。因此，如果我们想避免溢出，我们可以用更多的全加器来扩展我们的电路，允许我们添加16或32位数字。这使得溢出不太可能发生，但代价是更多的门。

An additional **downside** is that it takes a little bit of time for each of the carries to ripple forward. **Admittedly**, not very much time, electrons move pretty fast, so we’re talking about **billionths** of a second, but that’s enough to make a difference in today’s fast computers. For this reason, modern computers use a slightly different adding circuit called a ‘carry-look-ahead’ adder which is faster, but ultimately does exactly the same thing-- adds binary numbers. The ALU’s arithmetic unit also has circuits for other math operations and in general these 8 operations are always supported. And like our adder, these other operations are built from individual logic gates. Interestingly, you may have noticed that there are no multiply and divide operations. That's because simple ALUs don’t have a circuit for this, and instead just perform a series of additions. Let’s say you want to multiply 12 by 5. That’s the same thing as **adding 12 to itself 5 times**. So it would take 5 passes through the ALU to do this one multiplication. And this is how many simple processors, like those in your **thermostat**, TV remote, and microwave, do multiplication. It’s slow, but it gets the job done. However, fancier processors, like those in your laptop or smartphone, have arithmetic units with dedicated circuits for multiplication. And as you might expect, the circuit is more complicated than addition -- there’s no magic, it just takes a lot more **logic gates** – which is why less expensive processors don’t have this feature. 

> 另外一个**缺点**是，每一个进位都需要一点时间才能向前连接。**诚然，**所需的时间并不多，电子的移动速度相当快，所以我们谈论的是**几十亿分之一**秒，但这足以让今天的快速计算机产生差异。由于这个原因，现代计算机使用了一个稍微不同的加法电路，称为 "超前进位"加法器，它的速度更快，但最终做的是完全相同的事情--相加二进制数字。ALU的算术单元也有用于其他数学运算的电路，一般来说，这8种运算总是被支持。就像我们的加法器一样，这些其他的操作是由单独的逻辑门建立的。有趣的是，你可能已经注意到，这里没有乘法和除法运算。这是因为简单的ALU没有这方面的电路，而只是进行一系列的加法运算。比方说，你想用12乘以5，这就相当于**将12加到自身5次**。所以需要通过5次ALU来完成这个乘法。这就是许多简单的处理器，比如你的**恒温器**、电视遥控器和微波炉中的处理器，就是这样做乘法的。这很慢，但它能完成工作。然而，更高级的处理器，如你的笔记本电脑或智能手机中的处理器，有专门用于乘法的算术单元。正如你所期望的，乘法电路比加法更复杂--没有魔法，只是需要更多的**逻辑门**--这就是为什么不太昂贵的处理器没有这个功能。

8 math operations ALU sopported:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\8 operation.jpg)

Ok, let’s move on to the other half of the ALU: **the Logic Unit**. Instead of arithmetic operations, the Logic Unit performs… well... logical operations, like AND, OR and NOT, which we’ve talked about previously. It also performs simple numerical tests, like checking if a number is negative. For example, here’s a circuit that tests if the output of the ALU is zero. It does this using a bunch of OR gates to see if any of the bits are 1. Even if one single bit is 1, we know the number can’t be zero and then we use a final NOT gate to flip this input so the output is 1 only if the input number is 0. So that’s a high level overview of what makes up an ALU. We even built several of the main components from scratch, like our ripple adder. As you saw, it’s just a big bunch of logic gates connected in clever ways. Which brings us back to that ALU you admired so much at the beginning of the episode. 

> 好了，让我们继续讨论ALU的另一半：**逻辑单元**。逻辑单元不进行算术运算，而是进行......嗯......逻辑运算，比如我们之前讲过的AND、OR和NOT。它还执行简单的数字测试，比如检查一个数字是否为负数。例如，这里有一个测试ALU的输出是否为零的电路。它使用一堆OR门来检测是否有任何位是1。即使有一个位是1，我们也知道这个数字不可能是0，然后我们使用最后的NOT门来翻转这个输入，所以只有当输入的数字是0时，输出才是1。 所以这就是对组成ALU的高层次概述。我们甚至从头开始建立了几个主要的组件，比如我们的纹波加法器。正如你所看到的，它只是一大堆逻辑门以巧妙的方式连接起来。这让我们回到了你在本集开始时非常欣赏的那个ALU。

a circuit test if the output of the ALU is zero:

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\test zero.png" style="zoom: 67%;" />

The Intel 74181. Unlike the 8-bit ALU we made today, the 74181 could only handle 4-bit inputs, which means YOU BUILT AN ALU THAT’S LIKE TWICE AS GOOD AS THAT SUPER FAMOUS ONE. WITH YOUR MIND! Well.. sort of. We didn’t build the whole thing… but you get the idea. The 74181 used about 70 logic gates, and it couldn’t multiply or divide. But it was a huge step forward in **miniaturization**, opening the doors to more capable and less expensive computers. This 4-bit ALU circuit is already a lot to take in, but our 8-bit ALU would require hundreds of logic gates to fully build and engineers don’t want to see all that complexity when using an ALU, so they came up with a special symbol to wrap it all up, which looks like a big ‘V’. Just another level of abstraction! 

> 英特尔74181。与我们今天制造的8位ALU不同，74181只能处理4位输入，这意味着你制造的ALU比那个超级著名的ALU要好两倍。用你的头脑！？嗯......算是吧。我们没有建立整个东西......但你知道这些概念。74181使用了大约70个逻辑门，而且它不能进行乘法或除法。但它在**小型化**方面迈出了一大步，为更有能力、更便宜的计算机打开了大门。这个4位ALU电路已经需要逻辑门了，但是我们的8位ALU需要数百个逻辑门来完全建立，工程师们不想在使用ALU时看到所有这些复杂的东西，所以他们想出了一个特殊的符号来包装这一切，看起来像一个大的 "V"。只是另一个层次的抽象! 

The [combinational logic](https://en.wikipedia.org/wiki/Combinational_logic) circuitry of the [74181](https://en.wikipedia.org/wiki/74181) integrated circuit, which is a simple four-bit ALU:

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\74181aluschematic.png" style="zoom: 80%;" />

Our 8-bit ALU has two inputs, A and B, each with 8 bits. We also need a way to specify what operation the ALU should perform, for example, addition or subtraction. For that, we use a 4-bit operation code. We’ll talk about this more in a later episode, but in brief, 1000 might be the command to add, while 1100 is the command for subtract. Basically, the **operation code tells the ALU what operation to perform**. And the result of that operation on inputs A and B is an 8-bit output. ALUs also output a series of Flags, which are 1-bit outputs for particular states and statuses. For example, if we subtract two numbers, and the result is 0, our zero-testing circuit, the one we made earlier, sets the **Zero Flag** to True (1). This is useful if we are trying to determine if two numbers are are equal. If we wanted to test if A was less than B, we can use the ALU to calculate A subtract B and look to see if the **Negative Flag** was set to true. If it was, we know that A was smaller than B. And finally, there’s also a wire attached to the carry out on the adder we built, so if there is an overflow, we’ll know about it. This is called the **Overflow Flag**. Fancier ALUs will have more flags, but **these three flags are universal and frequently used**. In fact, we’ll be using them soon in a future episode. So now you know how your computer does all its basic mathematical operations digitally with no gears or levers required. We’re going to use this ALU when we construct our CPU two episodes from now. But before that, our computer is going to need some memory! We'll talk about that next week. 

> 我们的8位ALU有两个输入，A和B，每个都有8位。我们还需要一种方法来指定ALU应该执行什么操作，例如，加法或减法。为此，我们使用一个4位的操作代码。我们将在以后的节目中更多地讨论这个问题，但简单地说，1000可能是加法的命令，而1100是减法的命令。基本上，**操作码告诉ALU要执行什么操作**。而该操作在输入A和B上的结果是一个8位输出。ALU也会输出一系列的标志(Flags)，这些标志是特定状态和状态s的1位输出。例如，如果我们把两个数字相减，结果是0，我们的零点测试电路，也就是我们之前做的那个，会把**零点标志**设置为真（1）。如果我们试图确定两个数字是否相等，这很有用。如果我们想测试A是否小于B，我们可以用ALU来计算A减去B，然后看看**负标志**是否被设置为真。如果是，我们就知道A小于B。最后，在我们建立的加法器上还有一条线连接到加法器的进位，所以如果有溢出，我们就会知道。这被称为**溢出标志**。更高级的ALU会有更多的标志，但**这三个标志是通用的，而且经常使用**。事实上，我们很快就会在未来的节目中使用它们。所以现在你知道你的计算机是如何以数字方式完成所有基本的数学运算的，不需要齿轮或杠杆。从现在开始的两集里，我们将在构建我们的CPU时使用这个ALU。但在此之前，我们的计算机需要一些内存！下周我们将讨论这个问题。我们将在下周讨论这个问题。

8-bit Arithmetic & Logic Unit

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\alu.jpg" style="zoom:80%;" />



## #6 Registers and RAM

<iframe width="560" height="315" src="https://www.youtube.com/embed/fpnE6UAfbtU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne and welcome to Crash Course Computer Science. So last episode, using just logic gates, we built a simple ALU, which performs arithmetic and logic operations, **hence the ‘A’ and the ‘L’**. But of course, there’s not much point in calculating a result only to throw it away - it would be useful to store that value somehow, and maybe even run several operations in a row. That's where computer memory comes in! If you've ever been in the middle of a long RPG **campaign** on your console, or **slogging** through a difficult level on **Minesweeper** on your desktop, and your dog came by, tripped and pulled the power **cord** out of the wall, you know the **agony** of losing all your progress. **Condolences**. But the reason for your loss is that your console, your laptop and your computers make use of **Random Access Memory, or RAM**, which stores things like game state - as long as the power stays on. Another type of memory, called **persistent memory**, can survive without power, and it’s used for different things; We'll talk about the persistence of memory in a later episode. Today, we’re going to start small - literally by building a circuit that can store one.. single.. bit of information. After that, we’ll scale up, and build our very own memory module, and we’ll combine it with our ALU next time, when we finally build our very own CPU! 

> 大家好，我是卡丽-安，欢迎来到计算机科学速成班。上一集，我们只用了逻辑门，就建立了一个简单的ALU，它可以进行算术和逻辑运算，**所以有 "A "和 "L "**。但当然，计算出一个结果后就把它扔掉是没有什么意义的--以某种方式存储这个值会很有用，甚至可以连续运行几个运算。这就是计算机内存的作用! 如果你曾经在你的游戏机上正在进行一场漫长的RPG**战役**，或者在你的台式机上**艰难地**通过**扫雷**的困难关卡，而你的狗走过来，被绊倒并把电源线**从墙上拉下来**，你就知道失去所有进展的**痛苦**了。**哀悼**。但你损失的原因是，你的控制台、你的笔记本电脑和你的电脑都使用了**随机存取存储器，或称RAM**，它储存了游戏状态等东西--只要电源保持开启。另一种类型的内存，称为**持久性内存**，可以在没有电源的情况下生存，它用于不同的事情；我们将在以后的节目中谈论内存的持久性。今天，我们将从小处着手--从字面上看，就是建立一个可以存储一个......单一......位信息的电路。之后，我们将扩大规模，建立我们自己的内存模块，下次当我们最终建立我们自己的CPU时，我们将把它与我们的ALU结合起来 

INTRO All of the logic circuits we've discussed so far go in one direction - always flowing forward - like our 8-bit ripple adder from last episode. But we can also create circuits that loop back on themselves. Let’s try taking an ordinary OR gate, and feed the output back into one of its inputs and see what happens. First, let’s set both inputs to 0. So 0 OR 0 is 0, and so this circuit always outputs 0. If we were to flip input A to 1. 1 OR 0 is 1, so now the output of the OR gate is 1. A **fraction** of a second later, that loops back around into input B, so the OR gate sees that both of its inputs are now 1. 1 OR 1 is still 1, so there is no change in output. If we flip input A back to 0, the OR gate still outputs 1. So now we've got a circuit that records a “1” for us. Except, we've got a **teensy** tiny problem - this change is **permanent**! No matter how hard we try, there’s no way to get this circuit to flip back from a 1 to a 0. Now let’s look at this same circuit, but with an AND gate instead. We'll start inputs A and B both at 1. 1 AND 1 outputs 1 forever. But, if we then flip input A to 0, because it’s an AND gate, the output will go to 0. So this circuit records a 0, the opposite of our other circuit. Like before, no matter what input we apply to input A afterwards, the circuit will always output 0. Now we’ve got circuits that can record both 0s and 1s. The key to making this a useful piece of memory is to combine our two circuits into what is called the **AND-OR Latch**. It has two inputs, a "set" input, which sets the output to a 1, and a "reset" input, which resets the output to a 0. If set and reset are both 0, the circuit just outputs whatever was last put in it. In other words, it remembers a single bit of information! Memory! 

> 引言 到目前为止，我们所讨论的所有逻辑电路都是单向的--总是向前流动--就像上一集的8位纹波加法器。但是，我们也可以创造出能够自我循环的电路。让我们试试用一个普通的OR门，把输出反馈到它的一个输入，看看会发生什么。首先，让我们把两个输入端都设为0，所以0 OR 0是0，所以这个电路总是输出0。如果我们把输入端A翻转为1，1 OR 0是1，所以现在OR门的输出是1。几**分之**秒后，它又循环到输入端B，所以OR门看到它的两个输入端现在都是1，1 OR 1仍然是1，所以输出没有变化。如果我们把输入A翻转到0，OR门仍然输出1。所以现在我们**有一个电路，为我们记录了一个 "1"**。但是，我们有一个**小**的问题--这个变化是**永久的**！无论我们如何努力，都没有办法让这个电路从1翻转到0。我们把输入端A和B都设为1，1和1永远输出1。但是，如果我们把输入A翻转到0，因为它是一个AND门，输出将变成0。所以这个电路记录了一个0，与上一个电路相反。就像之前一样，无论之后我们对输入A施加什么输入，电路都会输出0。现在我们已经有了可以记录0和1的电路。要使这段记忆成为有用的东西，关键是要把我们的两个电路合并成所谓的**AND-OR锁存器**。它有两个输入，一个是 "设置"输入，将输出设置为1，另一个是 "复位 "输入，将输出复位为0。如果set和reset都是0，那么电路就只输出最后放进去的东西。换句话说，它记住了一个位的信息! 记忆! 

a circuit that records a "1":

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\store1.jpg" style="zoom:80%;" />

a circuit that records a "0":

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\records0.jpg" style="zoom:80%;" />

**AND-OR latch:**

a "set" input, which sets the output to a "1":

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\sets the outout to a 1.jpg" style="zoom:80%;" />

a "reset" input, which resets the output to a "0": 

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\reset the output to a 0.jpg" style="zoom:80%;" />

if set and reset are both 0, the circuit outputs whatever was last put in it (In other words, it remembers a single bit of information! Memory! ): 

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\and-or latch.jpg" style="zoom:80%;" />

This is called a “latch” because it “latches onto” a particular value and stays that way. **The action of putting data into memory is called writing**, whereas **getting the data out is called reading**. Ok, so we’ve got a way to store a single bit of information! Great! Unfortunately, having two different wires for input – set and reset – is a bit confusing. To make this a little easier to use, we really want a single wire to input data, that we can set to either 0 or 1 to store the value. Additionally, we are going to need a wire that enables the memory to be either available for writing or “locked” down --which is called the write enable line. By adding a few extra logic gates, we can build this circuit, which is called a Gated Latch since the “gate” can be opened or closed. Now this circuit is starting to get a little complicated. We don’t want to have to deal with all the individual logic gates... so as before, we’re going to **bump up** a level of abstraction, and put our whole Gated Latch circuit in a box -- a box that stores one bit. Let’s test out our new component! Let’s start everything at 0. If we **toggle** the Data wire from 0 to 1 or 1 to 0, nothing happens - the output stays at 0. That’s because the write enable wire is off, which prevents any change to the memory. So we need to “open” the “gate” by turning the write enable wire to 1. Now we can put a 1 on the data line to save the value 1 to our latch. Notice how the output is now 1. Success! We can turn off the enable line and the output stays as 1. Once again, we can toggle the value on the data line all we want, but the output will stay the same. The value is saved in memory. Now let’s turn the enable line on again use our data line to set the latch to 0. Done. Enable line off, and the output is 0. And it works! 

> 这被称为 "锁存器"，因为它 "锁存"在一个特定的数值上并保持这种状态。**将数据放入存储器的动作被称为写**，而**将数据取出的动作被称为读**。好了，我们已经有了一种方法来存储一个单一的信息! 很好! 不幸的是，有两条不同的输入线--设置和复位--有点令人困惑。为了使其更容易使用，我们希望只有一条数据输入线，我们可以将其设置为0或1来存储该值。此外，我们还需要一条线，使存储器可用于写入或 "锁定"--这被称为“允许写入线”。通过添加一些额外的逻辑门，我们可以建立这个电路，它被称为门锁，因为 "门"可以打开或关闭。现在这个电路开始变得有点复杂了。我们不想处理所有单独的逻辑门......所以像以前一样，我们要**提升**一个抽象级别，把整个门锁存电路放在一个盒子里--一个存储一个比特的盒子。让我们来测试一下我们的新组件吧! 如果我们把数据线从0切换到1或从1切换到0，什么也不会发生--输出保持在0。这是因为“允许写入线”是关闭的，它阻止了对内存的任何改变。因此，我们需要通过将“允许写入线”转为1来 "打开 "这个 "门"。现在我们可以在数据线上加一个1，把值1保存到我们的锁存器中。注意现在的输出是1。我们可以关闭“允许写入线”，输出保持为1。再一次，我们可以随意切换数据线上的值，但输出将保持不变。该值被保存在内存中。现在，让我们再次打开“允许写入线”，用我们的数据线将锁存器设置为0，完成。“允许写入线”关闭，输出为0。

gate latch:

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\gate latchjpg.jpg" style="zoom:80%;" />

a abstraction component of a gated latch:

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\a abstraction of gate latch oponent.jpg" style="zoom:80%;" />

Now, of course, computer memory that only stores one bit of information isn’t very useful -- definitely not enough to run **Frogger**. Or anything, really. But we’re not limited to using only one latch. If we put 8 latches side-by-side, we can store 8 bits of information like an 8-bit number. A group of latches operating like this is called a **register**, which holds a single number, and the number of bits in a register is called its **width**. Early computers had 8-bit registers, then 16, 32, and today, many computers have registers that are 64-bits wide. To write to our register, we first have to enable all of the latches. We can do this with a single wire that connects to all of their enable inputs, which we set to 1. We then send our data in using the 8 data wires, and then set enable back to 0, and the 8 bit value is now saved in memory. 

> 现在，当然，只存储一比特信息的计算机内存并不十分有用--绝对不足以运行**蛙人**。或者任何东西，真的。但是我们并不局限于只使用一个锁存器。如果我们把8个锁存器并排放在一起，我们就可以像8位数一样存储8位信息。像这样操作的一组锁存器被称为**寄存器**，它可以保存一个数字，寄存器中的位数被称为其**位宽**。早期的计算机有8位寄存器，然后是16位、32位，今天，许多计算机的寄存器宽度为64位。要写到我们的寄存器，我们首先要启用所有的锁存器。我们可以用一根线连接所有的“允许输入线”，并将其设置为1。然后我们用8根数据线把数据送进去，再把“允许输入线”设置为0，这时8位的值就保存在内存中了。

8-bit register: D:(date in), E(enable), Q(data out)

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\8bit register.jpg" style="zoom: 67%;" />

Putting latches side-by-side works ok for a small-ish number of bits. A 64-bit register would need 64 wires running to the data pins, and 64 wires running to the outputs. Luckily we only need 1 wire to enable all the latches, but that’s still 129 wires. For 256 bits, we end up with 513 wires! The solution is a **matrix**! In this matrix, we don’t arrange our latches in a row, we put them in a **grid**. For 256 bits, we need a 16 by 16 grid of latches with 16 rows and columns of wires. To activate any one latch, we must turn on the corresponding row AND column wire. 

> 把锁存器并排放在一起，对少量的比特数来说是可以的。一个64位的寄存器需要64条线连接到数据引脚，64条线连接到输出。幸运的是，我们只需要一条线来启用所有的锁存器，但这仍然有129条线。对于256位来说，我们最终需要513条线! 解决方案是一个**矩阵**! 在这个矩阵中，我们不把锁存器排成一排，而是把它们放在一个**网格**中。对于256位，我们需要一个16乘16的锁存器网格，有16行和16列的导线。要激活任何一个锁存器，我们必须打开相应的行线和列线。

16×16 latches matrix(256 bit register):

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\latch matrix.jpg"  />

Let’s zoom in and see how this works. We only want the latch at the **intersection** of the two active wires to be enabled, but all of the other latches should stay disabled. For this, we can use our **trusty** AND gate! The AND gate will output a 1 only if the row and the column wires are both 1. So we can use this signal to uniquely select a single latch. This row/column setup connects all our latches with a single, shared, write enable wire. In order for a latch to become write enabled, the row wire, the column wire, and the write enable wire must all be 1. That should only ever be true for one single latch at any given time. This means we can use a single, shared wire for data. Because only one latch will ever be write enabled, only one will ever save the data -- the rest of the latches will simply ignore values on the data wire because they are not write enabled. We can use the same trick with a read enable wire to read the data later, to get the data out of one specific latch. This means in total, for 256 bits of memory, we only need 35 wires - 1 data wire, 1 write enable wire, 1 read enable wire, and 16 rows and columns for the selection. That’s significant wire savings! 

> 让我们把它放大，看看它是如何工作的。我们只想使用两根激活源线的**交汇处**的锁存器，但其他所有的锁存器应该保持禁用。为此，我们可以使用我们的**信任的**AND门! 只有当行线和列线都是1时，AND门才会输出1。所以我们可以用这个信号来唯一地选择一个锁存器。这种行/列设置将我们所有的锁存器与一条共享的"允许写入线"相连。为了使锁存器成为写启用，行线、列线和"允许写入线"都必须是1。在任何给定的时间里，只有一个锁存器是这样的。这意味着我们可以使用一个单一的、共享的数据线。因为只有一个锁存器会被写入，所以只有一个锁存器会保存数据--其余的锁存器会简单地忽略数据线上的值，因为它们没有激活“允许写入线”。我们可以使用同样的技巧，用一条读使能线来读取数据，以便从一个特定的锁存器中获取数据。这意味着，对于256比特的存储器，我们总共只需要35条线--1条数据线，1条"允许写入线"，1条读"允许读取线"，以及16行和列的选择。这就大大节省了电线! 

zoom in:

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\zoom in.jpg"  />

But we need a way to uniquely specify each intersection. We can think of this like a city, where you might want to meet someone at 12th **avenue** and 8th street -- that's an address that defines an intersection. The latch we just saved our one bit into has an address of row 12 and column 8. Since there is a maximum of 16 rows, we store the row address in a 4 bit number. 12 is 1100 in binary. We can do the same for the column address: 8 is 1000 in binary. So the address for the particular latch we just used can be written as 11001000. To convert from an address into something that selects the right row or column, we need a special component called a **multiplexer** -- which is the computer component with a pretty cool name at least compared to the ALU. Multiplexers come in all different sizes, but because we have 16 rows, we need a 1 to 16 multiplexer. It works like this. You feed it a 4 bit number, and it connects the input line to a corresponding output line. So if we pass in 0000, it will select the very first column for us. If we pass in 0001, the next column is selected, and so on. We need one multiplexer to handle our rows and another multiplexer to handle the columns. 

> 但是我们需要一种方法来唯一地指定每个交叉口。我们可以把它想象成一个城市，你可能想在12号**大道**和8号大街上和某人见面--那是一个定义交叉点的地址。我们刚刚保存的那个锁存器的地址是第12行和第8列。因为最多只有16行，所以我们把行的地址存储为4位数。12在二进制中是1100。我们可以对列的地址做同样的处理。8在二进制中是1000。因此，我们刚刚使用的特定锁存器的地址可以写成11001000。为了将地址转换为选择正确的行或列，我们需要一个特殊的组件，称为**多路复用器**--至少与ALU相比，这是一个名字相当酷的计算机组件。多路复用器有各种不同的尺寸，但由于我们有16行，我们需要一个1到16的多路复用器。它的工作原理是这样的。你给它输入一个4位数字，它就把输入线连接到相应的输出线。因此，如果我们输入0000，它将为我们选择第一列。如果我们输入0001，就会选择下一列，以此类推。我们需要一个复用器来处理我们的行，另一个复用器来处理列。

multiplexer:

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\multiplexer.jpg"  />

Ok, it’s starting to get complicated again, so let’s make our 256-bit memory its own component. Once again a new level of abstraction! It takes an 8-bit address for input - the 4 bits for the column and 4 for the row. We also need write and read enable wires. And finally, we need just one data wire, which can be used to read or write data. Unfortunately, even 256-bits of memory isn’t enough to run much of anything, so we need to scale up even more! We’re going to put them in a row. Just like with the **registers**. We’ll make a row of 8 of them, so we can store an 8 bit number - also known as a byte. To do this, we feed the exact same address into all 8 of our 256-bit memory components at the same time, and each one saves one bit of the number. That means the component we just made can store 256 bytes at 256 different addresses. Again, to keep things simple, we want to leave behind this inner complexity. Instead of thinking of this as a series of individual memory modules and circuits, we’ll think of it as a **uniform** bank of addressable memory. We have 256 addresses, and at each address, we can read or write an 8-bit value. We’re going to use this memory component next episode when we build our CPU. 

> 好吧，这又开始变得复杂了，所以让我们把256位内存变成自己的组件。这又是一个新的抽象层次！它需要一个8位的输入地址--列的4位和行的4位。我们还需要写和读的“启动线”。最后，我们只需要一条数据线，它可以用来读取或写入数据。不幸的是，即使是256位的内存也不足以运行很多东西，所以我们需要扩大规模 我们要把它们放在一排。就像对待**寄存器**一样。我们将把它们做成一排8个，这样我们就可以存储一个8位的数字--也被称为字节。为了做到这一点，我们将完全相同的地址同时输入到所有8个256位的内存组件中，每个组件保存数字的一个比特。这意味着我们刚刚制作的组件可以在256个不同的地址存储256个字节。同样，为了使事情简单，我们要抛开这种内在的复杂性。我们不要把它看成是一系列单独的内存模块和电路，而是把它看成是一个**统一的**整体可寻址内存。我们有256个地址，在每个地址，我们可以读或写一个8位的值。下一集我们将在建造CPU时使用这个内存组件。

256-bit memory:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\256 bit memory.jpg)

256 bytes memory(store 256 bytes at 256 different addresses):

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\256byte register.jpg)

a **uniform** bank of addressable memory:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\address bank component.jpg)

The way that modern computers scale to megabytes and gigabytes of memory is by doing the same thing we’ve been doing here -- keep packaging up little **bundles** of memory into larger, and larger, and larger arrangements. As the number of memory locations grow, our addresses have to grow as well. 8 bits hold enough numbers to provide addresses for 256 bytes of our memory, but that’s all. To address a gigabyte – or a billion bytes of memory – we need 32-bit addresses. An important property of this memory is that we can access any memory location, at any time, and in a random order. For this reason, it’s called Random-Access Memory or RAM. When you hear people talking about how much **RAM** a computer has - **that's the computer’s memory**. RAM is like a human’s short term or working memory, where you keep track of things going on right now - like whether or not you had lunch or paid your phone bill. Here’s an actual stick of RAM - **with 8 memory modules soldered onto the board**. If we carefully opened up one of these modules and zoomed in, The first thing you would see are 32 squares of memory. Zoom into one of those squares, and we can see each one is **comprised of 4 smaller blocks**. If we zoom in again, we get down to the **matrix of individual bits**. This is a matrix of 128 by 64 bits. That’s 8192 bits in total. Each of our 32 squares has 4 matrices, so that’s 32 thousand, 7 hundred and 68 bits. And there are 32 squares in total. So all in all, that’s roughly 1 million bits of memory in each chip. Our RAM stick has 8 of these chips, so in total, this RAM can store 8 millions bits, otherwise known as 1 megabyte. That’s not a lot of memory these days -- this is a RAM module from the 1980’s. 

> 现代计算机扩展到兆字节和千兆字节内存的方式是通过做我们一直在做的同样的事情--不断将小**捆绑**的内存打包成更大、更大、更多的排列。随着内存位置数量的增加，我们的地址也必须增加。8位的数字足以为256字节的内存提供地址，但这就是全部。为了给一千兆字节--或十亿字节的内存寻址，我们需要32位地址。这种内存的一个重要属性是，我们可以在任何时间以随机的顺序访问任何内存位置。由于这个原因，它被称为随机存取存储器或RAM。当你听到人们谈论一台电脑有多少**RAM**时--**那是电脑的内存**。RAM就像人的短期或工作记忆，在那里你可以跟踪现在正在发生的事情--比如你是否吃了午饭或支付了电话费。这是一个实际的内存条--**有8个内存模块焊接在电路板上**。如果我们小心翼翼地打开其中一个模块并放大，你首先会看到32个内存方块。放大其中一个方块，我们可以看到每个方块是由**4个小块组成的**。如果我们再次放大，我们就可以看到各个比特的**矩阵**。这是一个128×64位的矩阵。总共有8192个比特。我们的32个方块中的每一个都有4个矩阵，所以这就是32,700和68位。而且总共有32个方块。因此，总的来说，每个芯片中大约有100万比特的内存。我们的RAM棒有8个这样的芯片，所以总的来说，这个RAM可以存储800万比特，也就是所谓的1兆字节（MB）。这在现在并不是一个很大的内存 -- 这是一个来自1980年代的RAM模块。

a stick of RAM with 1 megabyte:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\1 mb memory stick.jpg)

Today you can buy RAM that has a gigabyte or more of memory - that’s billions of bytes of memory. So, today, we built a piece of **SRAM** - Static Random-Access Memory – which uses latches. There are other types of RAM, such as **DRAM**, **Flash memory**, and **NVRAM**. These are very similar in function to SRAM, but use different circuits to store the individual bits -- for example, using different logic gates, **capacitors**, charge traps, or **memristors**. But fundamentally, all of these technologies store bits of information in massively nested **matrices** of memory cells. Like many things in computing, the fundamental operation is relatively simple.. it’s the layers and layers of abstraction that’s mind blowing -- like a **russian doll** that keeps getting smaller and smaller and smaller. I’ll see you next week. Credits 

> 今天，你可以买到拥有一千兆字节（GB）或更多内存的RAM--那是数十亿字节的内存。所以，今天，我们建造了一块**SRAM**--静态随机存取存储器--它使用锁存器。还有其他类型的RAM，如**DRAM**、**Flash存储器**和**NVRAM**。这些在功能上与SRAM非常相似，但使用不同的电路来存储各个比特--例如，使用不同的逻辑门、**电容**、电荷陷阱或**记忆体**。但从根本上说，所有这些技术都将信息位存储在大规模嵌套的**矩阵**的存储单元中。就像计算领域的许多事情一样，基本操作相对简单，但层层叠叠的抽象才是令人震惊的 -- 就像一个不断变小的**俄罗斯娃娃**。下周见。 



## #7 The central processing unit(CPU)

<iframe width="560" height="315" src="https://www.youtube.com/embed/FZGugFqdr60" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, this is Crash Course Computer Science, and today, we’re talking about processors. Just a warning though - this is probably the most complicated episode in the series. So once you get this, you’re golden. We’ve already made a Arithmetic and Logic Unit, which takes in binary numbers and performs calculations, and we’ve made two types of computer memory: **Registers -- small, linear chunks of memory**, useful for **storing a single value** -- and then we scaled up, and made some **RAM, a larger bank of memory that can store a lot of numbers located at different addresses**. Now it’s time to put it all together and build ourselves the heart of any computer, but without any of the emotional **baggage** that comes with human hearts. For computers, this is the **Central Processing Unit**, most commonly called the **CPU**. 

> 大家好，我是Carrie Anne，这里是《计算机科学速成班》，今天我们讨论的是处理器。不过要提醒大家的是--这可能是本系列中最复杂的一集。所以，一旦你掌握了这个，你是极其厉害的。我们已经做了一个算术和逻辑单元，它接收二进制数字并进行计算，我们还做了两种类型的计算机存储器。**寄存器--小的、线性的内存块**，对**存储单个数值**很有用--然后我们扩大规模，制作了一些**RAM，这是一个较大的内存库，可以存储位于不同地址的大量数字**。现在是时候把这一切放在一起，为我们自己建造任何计算机的心脏，但没有任何与人类心脏有关的情感**包袱**。对于计算机来说，这就是**中央处理单元**，最常见的叫法是**CPU**。

INTRO A CPU’s job is to execute programs. Programs, like Microsoft Office, Safari, or your beloved copy of Half Life: 2, are made up of a series of individual operations, called **instructions**, because they “instruct” the computer what to do. If these are **mathematical instructions**, like add or subtract, the CPU will configure its ALU to do the mathematical operation. Or it might be a **memory instruction**, in which case the CPU will talk with memory to read and write values. There are a lot of parts in a CPU, so we’re going to lay it out piece by piece, building up as we go. We’ll focus on functional blocks, rather than showing every single wire. When we do connect two components with a line, this is an abstraction for all of the necessary wires. This high level view is called the **microarchitecture**. 

> 引言 CPU的工作是执行程序。程序，如Microsoft Office、Safari，或你心爱的《半条命：2》，是由一系列单独的操作组成的，称为**指令**，因为它们 "指示 "计算机做什么。如果这些是**数学指令**，如加法或减法，CPU将配置其ALU来完成数学运算。或者它可能是一个**内存指令**，在这种情况下，CPU将与内存对话，以读取和写入值。CPU中有很多部件，所以我们将逐个进行介绍，边介绍边学习。我们将专注于功能块，而不是展示每一条线。当我们用一条线连接两个部件时，这是一个对所有必要的线的抽象。这种高层次的观点被称为**微架构**。

OK, first, we’re going to need some memory. Lets drop in the RAM module we created last episode. To keep things simple, we’ll assume it only has 16 memory locations, each containing 8 bits. Let’s also give our processor four, 8-bit **memory registers**, labeled A, B, C and D which will be used to temporarily store and manipulate values. We already know that data can be stored in memory as binary values and **programs can be stored in memory too**. We can **assign an ID to each instruction** supported by our CPU. In our hypothetical example, we use the first four bits to store the “operation code”, or **opcode** for short. The final four bits specify where the data for that operation should come from - this could be registers or an address in memory. We also need two more registers to complete our CPU. First, we need a register to keep track of where we are in a program. For this, we use an instruction **address register**, which as the name suggests, stores the memory address of the current instruction. And then we need the other register to store the current instruction, which we’ll call the **instruction register**. When we first **boot up** our computer, all of our registers start at 0. 

> 好的，首先，我们需要一些内存。让我们把我们上一集创建的RAM模块放进去。为了简单起见，我们假设它只有16个内存位置，每个位置包含8位。让我们也给我们的处理器四个8位的**内存寄存器**，标记为A、B、C和D，将用于临时存储和操作数值。我们已经知道，数据可以作为二进制值存储在内存中，**程序也可以存储在内存中**。我们可以为CPU支持的**每个指令分配一个ID**。在我们假设的例子中，我们用前四位来存储 "操作代码"，简称**操作码**。最后四位指定该操作的数据来自何处--这可能是寄存器或内存中的一个地址。我们还需要另外两个寄存器来完成我们的CPU。首先，我们需要一个寄存器来记录我们在程序中的位置。为此，我们使用一个指令**地址寄存器**，顾名思义，它存储了当前指令的内存地址。然后我们需要另一个寄存器来存储当前指令，我们称之为**指令寄存器**。当我们第一次**启动**计算机时，我们所有的寄存器都是从0开始的。

instrction table:

<img src="H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\instruction table.jpg"  />

As an example, we’ve initialized our RAM with a simple computer program that we’ll to through today. The first phase of a CPU’s operation is called the **fetch phase**. This is where we retrieve our first instruction. First, we wire our Instruction Address Register to our RAM module. The register’s value is 0, so the RAM returns whatever value is stored in address 0. In this case, 0010 1110. Then this value is copied into our **instruction register**. Now that we’ve fetched an instruction from memory, we need to figure out what that instruction is so we can execute it. That is run it. Not kill it. This is called the **decode phase**. In this case the opcode, which is the first four bits, is: 0010. This opcode corresponds to the “LOAD A” instruction, which loads a value from RAM into Register A. The RAM address is the last four bits of our instruction which are 1110, or 14 in decimal. Next, instructions are **decoded and interpreted by a Control Unit**. Like everything else we’ve built, it too is **made out of logic gates**. For example, to recognize a LOAD A instruction, we need a circuit that checks if the opcode matches 0010 which we can do with a handful of logic gates. Now that we know what instruction we’re dealing with, we can go ahead and perform that instruction which is the beginning of the **execute phase**! 

> 作为一个例子，我们已经用一个简单的计算机程序初始化了我们的RAM，今天我们将过一遍这个程序。CPU运行的第一个阶段被称为**取阶段**。这就是我们获取第一条指令的地方。首先，我们将指令地址寄存器连接到我们的RAM模块。寄存器的值是0，所以RAM会返回存储在地址0的任何值。在这种情况下，就是0010 1110。然后这个值被复制到我们的**指令寄存器**中。现在我们已经从内存中获取了一条指令，我们需要弄清楚这条指令是什么，以便我们能够执行它。也就是运行它。而不是杀死它。这被称为**解码阶段**。在这个例子中，操作码，也就是前四位，是：0010。这个操作码对应的是 "LOAD A "指令，它从RAM中加载一个值到寄存器A中。RAM地址是我们指令的最后四位，即1110，或十进制的14。接下来，指令由控制单元**解码和解释**。就像我们建造的其他东西一样，它也是**由逻辑门组成的**。例如，为了识别LOAD A指令，我们需要一个电路来检查操作码是否符合0010，我们可以用少量的逻辑门来完成。现在我们知道了我们正在处理的指令，我们可以继续执行该指令，这是**执行阶段**的开始 
>

Using the output of our LOAD_A checking circuit, we can turn on the RAM’s read enable line and send in address 14. The RAM retrieves the value at that address, which is 00000011, or 3 in decimal. Now, because this is a LOAD_A instruction, we want that value to only be saved into Register A and not any of the other registers. So if we connect the RAM’s data wires to our four data registers, we can use our LOAD_A check circuit to enable the write enable only for Register A. And there you have it -- we’ve successfully loaded the value at RAM address 14 into Register A. We’ve completed the instruction, so we can turn all of our wires off, and we’’re ready to fetch the next instruction in memory. To do this, **we increment the Instruction Address Register by 1 which completes the execute phase**. LOAD_A is just one of several possible instructions that our CPU can execute. Different instructions are decoded by different logic circuits, which configure the CPU’s components to perform that action. Looking at all those individual decode circuits is too much detail, so since we looked at one example, we’re going to go head and package them all up as a single **Control Unit** to keep things simple. That’s right a new level of abstraction. 

> 使用我们的LOAD_A检查电路的输出，我们可以打开RAM的读取“允许线”并送入地址14。RAM检索到该地址的值，即0000000011，或十进制的3。现在，因为这是一条LOAD_A指令，我们希望这个值只保存在寄存器A中，而不是任何其他的寄存器中。所以如果我们把RAM的数据线连接到我们的四个数据寄存器，我们就可以使用我们的LOAD_A检查电路来使寄存器A的允许写入生效。成功了 -- 我们已经成功的把RAM地址14的值加载到寄存器A。为此，**我们将指令地址寄存器增量加1，完成执行阶段**。LOAD_A只是我们的CPU可以执行的几种可能的指令之一。不同的指令由不同的逻辑电路进行解码，这些逻辑电路配置CPU的组件来执行该动作。看所有这些单独的解码电路太详细了，所以既然我们观察一个例子，我们就要把它们全部打包成一个**控制单元**来保持简单。是的，一个新的抽象层次。

The Control Unit is comparable to the **conductor** of an orchestra, directing all of the different parts of the CPU. Having completed one full **fetch/decode/execute cycle**, we’re ready to start all over again, beginning with the fetch phase. The Instruction Address Register now has the value 1 in it, so the RAM gives us the value stored at address 1, which is 0001 1111. On to the decode phase! 0001 is the “LOAD B” instruction, which moves a value from RAM into Register B. The memory location this time is 1111, which is 15 in decimal. Now to the execute phase! The Control Unit configures the RAM to read address 15 and configures Register B to receive the data. Bingo, we just saved the value 00001110, or the number 14 in decimal, into Register B. Last thing to do is increment our instruction address register by 1, and we’re done with another cycle. 

> 控制单元相当于一个**管弦乐队**的**指挥**，指挥着CPU的所有不同部分。在完成了一个完整的**取/解码/执行周期**后，我们准备重新开始，从取的阶段开始。指令地址寄存器现在的值是1，所以RAM给我们存储在地址1的值，也就是0001 1111。进入解码阶段! 0001是 "LOAD B "指令，它将一个值从RAM移到寄存器B中。现在进入执行阶段! 控制单元将RAM配置为读取地址15，并将寄存器B配置为接收数据。宾果，我们刚刚把数值00001110，或者十进制的数字14，保存到寄存器B中。

Our next instruction is a bit different. Let’s fetch it. 1000 01 00. That opcode 1000 is an ADD instruction. Instead of an 4-bit RAM address, this instruction uses two sets of 2 bits. Remember that 2 bits can encode 4 values, so 2 bits is enough to select any one of our 4 registers. The first set of 2 bits is 01, which in this case corresponds to Register B, and 00, which is Register A. So “1000 01 00” is the instruction for adding the value in Register B into the value in register A. So to execute this instruction, we need to integrate the ALU we made in Episode 5 into our CPU. The Control Unit is responsible for selecting the right registers to pass in as inputs, and configuring the ALU to perform the right operation. For this ADD instruction, the Control Unit enables Register B and feeds its value into the first input of the ALU. It also enables Register A and feeds it into the second ALU input. As we already discussed, the ALU itself can perform several different operations, so the Control Unit must configure it to perform an ADD operation by passing in the ADD opcode. Finally, the output should be saved into Register A. But it can’t be written directly because the new value would **ripple** back into the ALU and then keep adding to itself. So the Control Unit uses an internal register to temporarily save the output, turn off the ALU, and then write the value into the proper destination register. In this case, our inputs were 3 and 14, and so the sum is 17, or 00010001 in binary, which is now sitting in Register A. As before, the last thing to do is increment our instruction address by 1, and another cycle is complete. 

> 我们的下一条指令有点不同。让我们来获取它。1000 01 00. 这个操作码1000是一条ADD指令。这条指令使用了两组2比特，而不是4比特的RAM地址。记住，2位可以编码4个值，所以2位足以选择我们4个寄存器中的任何一个。第一组2位是01，在本例中对应的是寄存器B，00是寄存器A，所以 "1000 01 00 "是将寄存器B中的值加入到寄存器A中的指令。控制单元负责选择正确的寄存器作为输入，并配置ALU以执行正确的操作。对于这个ADD指令，控制单元启用寄存器B，并将其值输入ALU的第一个输入端。它还启用了寄存器A并将其输入到ALU的第二个输入端。正如我们已经讨论过的，ALU本身可以执行几种不同的操作，所以控制单元必须通过传递ADD操作码来配置它来执行ADD操作。最后，输出应该被保存到寄存器A中，但是不能直接写入，因为新的数值会**波纹**回到ALU中，然后不断加到自己身上。所以控制单元使用一个内部寄存器来暂时保存输出，关闭ALU，然后将数值写入适当的目标寄存器。在这种情况下，我们的输入是3和14，所以总和是17，即二进制的0001，现在就在寄存器A中。像以前一样，最后要做的是将我们的指令地址增加1，另一个周期就完成了。
>

Okay, so let’s fetch one last instruction: 01001101. When we decode it we see that 0100 is a STORE_A instruction, with a RAM address of 13. As usual, we pass the address to the RAM module, but instead of read-enabling the memory, we write-enable it. At the same time, we read-enable Register A. This allows us to use the data line to pass in the value stored in register A. Congrats, we just ran our first computer program! It loaded two values from memory, added them together, and then saved that sum back into memory. 

> 好的，那么我们来取最后一条指令。01001101. 当我们解码的时候，我们看到0100是一条STORE_A指令（把寄存器A的值放入内存），RAM地址是13。像往常一样，我们把地址传给RAM模块，但是我们不是“允许”读取内存，而是“允许”写入它。同时，我们对寄存器A进行“允许”读取，这样我们就可以使用数据线来传递存储在寄存器A中的值。祝贺你，我们刚刚运行了我们的第一个计算机程序! 它从内存中加载了两个数值，将它们相加，然后将这个和保存到内存中。

first execute program:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\first program.jpg)

Of course, by me talking you through the individual steps, I was manually transitioning the CPU through its fetch, decode and execute phases. But there isn’t a mini Carrie Anne inside of every computer. So the responsibility of **keeping the CPU ticking along falls to a component called the clock**. As it’s name suggests, the clock triggers an electrical signal at a precise and regular **interval**. Its signal is used by the Control Unit to advance the internal operation of the CPU, keeping everything in lock-step - like the dude on a Roman galley **drumming** rhythmically at the front, keeping all the rowers synchronized... or a **metronome**. Of course you can’t go too fast, because even electricity takes some time to travel down wires and for the signal to settle. The speed at which a CPU can carry out each step of the fetch-decode-execute cycle is called its **Clock Speed**. This speed is measured in **Hertz** - a unit of frequency. One Hertz means one cycle per second. Given that it took me about 6 minutes to talk you through 4 instructions -- LOAD, LOAD, ADD and STORE -- that means I have an effective clock speed of roughly .03 Hertz. 

> 当然，在我告诉你各个步骤的时候，我是在手动将CPU转变通过其获取、解码和执行阶段。但是，每台计算机内部并没有一个迷你的卡里安。因此，**维持CPU运转的责任落在了一个叫做时钟的部件上**。顾名思义，时钟以精确而有规律的**间隔**触发一个电信号。它的信号被控制单元用来推进CPU的内部操作，使一切保持同步--就像罗马船队的鼓手在前面有节奏地**鼓动**，使所有划船者保持同步......或者一个**节拍器**。当然，你不能走得太快，因为即使是电，也需要一些时间沿着电线传输，并使信号稳定下来。CPU执行取-解码-执行周期的每一步的速度被称为其**时钟速度**。这个速度是以**赫兹**为单位测量的，这是一个频率单位。一赫兹意味着每秒钟一个周期。鉴于我花了大约6分钟来告诉你4条指令--LOAD、LOAD、ADD和STORE--这意味着我的有效时钟速度大约为0.03赫兹。

Admittedly, I’m not a great computer but even someone handy with math might only be able to do one calculation in their head every second or 1 Hertz. The very first, single-chip CPU was the Intel 4004, a 4-bit CPU released in 1971. It’s microarchitecture is actually pretty similar to our example CPU. Despite being the first processor of its kind, it had a mind-blowing clock speed of 740 Kilohertz -- that’s 740 thousand cycles per second. You might think that’s fast, but it’s nothing compared to the processors that we use today. **One megahertz is one million clock cycles per second**, and the computer or even phone that you are watching this video on right now is no doubt a few gigahertz -- that's BILLIONs of CPU cycles every… single... second. Also, you may have heard of people **overclocking** their computers. This is when you modify the clock to speed up the **tempo** of the CPU -- like when the drummer speeds up when the Roman Galley needs to ram another ship. Chip makers often design CPUs with enough tolerance to handle a little bit of overclocking, but too much can either **overheat** the CPU, or produce **gobbledygook** as the signals fall behind the clock. And although you don’t hear very much about **underclocking**, it’s actually super useful. Sometimes it’s not necessary to run the processor at full speed... maybe the user has stepped away, or just not running a particularly demanding program. 

> 诚然，我不是一个伟大的计算师，但即使是一个精通数学的人，可能也只能在他们的头脑中每秒钟或1赫兹做一次计算。最早的单芯片CPU是英特尔4004，一个在1971年发布的4位CPU。它的微架构实际上与我们的示例CPU很相似。尽管它是同类产品中的第一个处理器，但它的时钟速度达到了令人震惊的740千赫兹--即每秒740千次。你可能认为这已经很快了，但与我们今天使用的处理器相比，这根本不算什么。**一兆赫兹是每秒一百万个时钟周期**，而你现在正在看这个视频的电脑甚至手机无疑是几千兆赫兹 -- 这意味着每......一......秒都有几十亿个CPU周期。此外，你可能听说过有人**超频**他们的电脑。这是当你修改时钟以加快CPU的**节奏**时--就像当罗马船队需要追上另一艘船时，鼓手加快了速度。芯片制造商在设计CPU时通常有足够的容忍度来处理一点点超频，但过多的超频会使CPU**过热**，或者在信号落后于时钟的情况下产生**乱码**。虽然你没有听到很多关于**降频**的说法，但它实际上是非常有用的。有时没有必要以全速运行处理器......也许用户已经离开了，或者只是没有运行一个特别苛刻的程序。

By slowing the CPU down, you can save a lot of power, which is important for computers that run on batteries, like laptops and smartphones. To meet these needs, many modern processors can increase or decrease their clock speed based on demand, which is called **dynamic frequency scaling**. So, with the addition of a clock, our CPU is complete. We can now put a box around it, and make it its own component. Yup. A new level of abstraction! 

> 通过降低CPU的速度，你可以节省大量的电力，这对于靠电池运行的计算机，如笔记本电脑和智能手机非常重要。为了满足这些需求，许多现代处理器可以根据需求增加或减少其时钟速度，这被称为**动态频率调整**。因此，随着时钟的增加，我们的CPU就完成了。我们现在可以把它放进工具盒，让它成为自己的组件。是的。一个新的抽象层次! 

a cup chip communication with RAM:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\cpu chip and RAM.jpg)

RAM, as I showed you last episode, lies outside the CPU as its own component, and **they communicate with each other using address, data and enable wires**. Although the CPU we designed today is a simplified example, many of the basic mechanics we discussed are still found in modern processors. Next episode, we’re going to **beef up** our CPU, extending it with more instructions as we take our first baby steps into software. I’ll see you next week. 

> 正如我上一集所展示的那样，RAM作为自己的组件位于CPU之外，**它们使用地址、数据和“允许线”相互通信**。虽然我们今天设计的CPU是一个简化的例子，但我们讨论的许多基本机制仍然可以在现代处理器中找到。下一集，我们将**改进**我们的CPU，用更多的指令来扩展它，同时我们向软件迈出第一步。下周见。
>



## #8 Instruction & Programs

<iframe width="560" height="315" src="https://www.youtube.com/embed/zltgXvg6r3k" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne and this is Crash Course Computer Science! Last episode, we combined an ALU, control unit, some memory, and a clock together to make a basic, but functional Central Processing Unit – or CPU – the beating, ticking heart of a computer. We’ve done all the hard work of building many of these components from the electronic circuits up, and now it’s time to give our CPU some actual instructions to process! The thing that makes a CPU powerful is the fact that it is **programmable** – if you write a different sequence of instructions, then the CPU will perform a different task. So **the CPU is a piece of hardware which is controlled by easy-to-modify software**! 

>  上一集，我们把一个ALU、控制单元、一些存储器和一个时钟组合在一起，组成了一个基本的、但功能齐全的中央处理器--或称CPU--计算机的跳动、滴答的心脏。我们已经完成了从电子电路开始构建这些组件的所有艰苦工作，现在是时候给我们的CPU一些实际的指令来进行处理了！"。CPU之所以强大，是因为它可以**编程**--如果你写了不同的指令序列，那么CPU将执行不同的任务。因此，**CPU是一块硬件，由易于修改的软件控制**! 

INTRO Let’s quickly revisit the simple program that we stepped through last episode. **The computer memory looked like this. Each address contained 8 bits of data**. For our hypothetical CPU, the first four bits specified the operation code, or opcode, and the second set of four bits specified an address or registers. In memory address zero we have 0010 1110. Again, those first four bits are our opcode which corresponds to a “LOAD_A” instruction. This instruction reads data from a location of memory specified in those last four bits of the instruction and saves it into Register A. In this case, 1110, or 14 in decimal. So let’s not think of this of memory address 0 as “0010 1110”, but rather as the instruction “LOAD_A 14”. That’s much easier to read and understand! And for me to say! And we can do the same thing for the rest of the data in memory. In this case, our program is just four instructions long, and we’ve put some numbers into memory too, 3 and 14. So now let’s step through this program: First is LOAD_A 14, which takes the value in address 14, which is the number 3, and stores it into Register A. Then we have a “LOAD_B 15” instruction, which takes the value in memory location 15, which is the number 14, and saves it into Register B. Okay. Easy enough. But now we have an “ADD” instruction. This tells the processor to use the ALU to add two registers together, in this case, B and A are specified. The ordering is important, because the resulting sum is saved into the second register that’s specified. So in this case, the resulting sum is saved into Register A. And finally, our last instruction is “STORE_A 13”, which instructs the CPU to write whatever value is in Register A into memory location 13. Yesss! Our program adds two numbers together. That’s about as exciting as it gets when we only have four instructions to play with. 

> 引言 让我们快速重温一下上一集我们踩过的简单程序。**计算机的内存是这样的。每个地址包含8位数据**。对于我们假设的CPU来说，前四位指定的是操作代码，或者说是操作码，第二组四位指定的是一个地址或寄存器。在内存地址0，我们有0010 1110。同样，这前四位是我们的操作码，对应于 "LOAD_A "指令。这条指令从指令的最后四位指定的内存位置读取数据，并将其保存在寄存器A中，本例中为1110，即十进制的14。所以我们不要把内存地址0看作是 "0010 1110"，而要看作是 "LOAD_A 14 "指令。这就更容易阅读和理解了! 而且对我来说也是如此! 我们也可以对内存中的其他数据做同样的事情。在这种情况下，我们的程序只有四条指令，我们把一些数字也放进了内存，3和14。那么现在让我们来看看这个程序的步骤。首先是 "LOAD_A 14 "指令，它获取地址14中的数值，也就是数字3，并将其存入寄存器A。然后我们有一条 "LOAD_B 15 "指令，它获取内存位置15中的数值，也就是数字14，并将其存入寄存器B。很简单。但现在我们有一条 "ADD "指令。这告诉处理器使用ALU将两个寄存器加在一起，在这种情况下，B和A是指定的。顺序是很重要的，因为产生的和被保存到指定的第二个寄存器中。最后，我们的最后一条指令是 "STORE_A 13"，它指示CPU将寄存器A中的任何数值写到内存位置13。是的! 我们的程序将两个数字加在一起。当我们只有四条指令可以玩的时候，这已经是最令人兴奋的了。

So let’s add some more! Now we’ve got a subtract function, which like ADD, specifies two registers to operate on. We’ve also got a fancy new instruction called **JUMP**. As the name implies, this causes the program to “jump” to a new location. This is useful if we want to **change the order of instructions, or choose to skip some instructions**. For example, a JUMP 0, would cause the program to go back to the beginning. At a low level, this is done by writing the value specified in the last four bits into the instruction address register, overwriting the current value. We’ve also added a special version of JUMP called JUMP_NEGATIVE. This only jumps the program if the ALU’s negative flag is set to true. As we talked about in Episode 5, the negative flag is only set when the result of an arithmetic operation is negative. If the result of the arithmetic was zero or positive, the negative flag would not be set. So the JUMP NEGATIVE won’t jump anywhere, and the CPU will just continue on to the next instruction. And finally, computers need to be told when to stop processing, so we need a **HALT instruction**. Our previous program really should have looked like this to be correct, otherwise the CPU would have just continued on after the STORE instruction, processing all those 0’s. But there is no instruction with an opcode of 0, and so the computer would have **crashed**! It’s important to point out here that we’re storing both instructions and data in the same memory. There is no difference fundamentally -- it’s all just binary numbers. So the HALT instruction is really important because it allows us to separate the two. 

> 所以，让我们再加一些吧! 现在我们有一个减法函数，它和ADD一样，指定了两个寄存器来操作。我们还有一个新的指令叫**JUMP**。顾名思义，它使程序 "跳"到一个新的位置。如果我们想**改变指令的顺序，或者选择跳过某些指令**，这是很有用的。例如，一个JUMP 0会使程序回到起点。在低级别的情况下，这是通过将最后四位指定的值写入指令地址寄存器，覆盖当前值来实现的。我们还增加了一个特殊版本的JUMP，叫做JUMP_NEGATIVE。它只在ALU的负标志被设置为 "true "时才会跳转程序。正如我们在第5章中谈到的，只有当算术运算的结果为负数时，负标志才会被设置。如果算术的结果是零或正，负标志就不会被设置。所以JUMP NEGATIVE不会跳到任何地方，CPU只会继续下一条指令。最后，计算机需要被告知何时停止处理，所以我们需要一个**HALT指令**。我们之前的程序确实应该是这样才对，否则CPU就会在STORE指令后继续处理所有的0。但是没有操作码为0的指令，所以计算机就会**崩溃**! 这里必须指出，我们在同一内存中同时存储指令和数据。从根本上说没有什么区别 -- 都是二进制数字而已。所以HALT指令真的很重要，因为它允许我们把这两者分开。

Okay, so let’s make our program a bit more interesting, by adding a JUMP. We’ll also modify our two starting values in memory to 1 and 1. Lets step through this program just as our CPU would. First, LOAD_A 14 loads the value 1 into Register A. Next, LOAD_B 15 loads the value 1 into Register B. As before, we ADD registers B and A together, with the sum going into Register A. 1+1 = 2, so now Register A has the value 2 in it (stored in binary of course) Then the STORE instruction saves that into memory location 13. Now we hit a “JUMP 2” instruction. This causes the processor to overwrite the value in the instruction address register, which is currently 4, with the new value, 2. Now, on the processor’s next fetch cycle, we don’t fetch HALT, instead we fetch the instruction at memory location 2, which is ADD B A. We’ve jumped! Register A contains the value 2, and register B contains the value 1. So 1+2 = 3, so now Register A has the value 3. We store that into memory. And we’ve hit the JUMP again, back to ADD B A. 1+3 = 4. So now register A has the value 4. See what's happening here? Every loop, we’re adding one. Its **counting up**! Cooooool. 

> 好的，让我们通过添加一个JUMP来使我们的程序更有趣一些。我们还将把内存中的两个起始值修改为1和1.让我们像CPU那样来完成这个程序。首先，LOAD_A 14将数值1加载到寄存器A中。接下来，LOAD_B 15将数值1加载到寄存器B中。现在我们执行一个 "JUMP 2 "指令。现在，在处理器的下一个取值周期，我们没有取到HALT，而是取到了内存位置2的指令，即ADD B A。寄存器A包含数值2，而寄存器B包含数值1。所以1+2=3，所以现在寄存器A的值是3。我们将其存储到内存中。我们再一次点击JUMP，回到ADD B A，1+3=4。所以现在寄存器A的值是4。看到这里发生了什么？每一个循环都增1。不断**增加**! 爽啊。

But notice there’s no way to ever escape. We’re never.. ever.. going to get to that halt instruction, because we’re always going to hit that JUMP. This is called an **infinite** loop – a program that runs forever… ever… ever… ever… ever To break the loop, we need a **conditional jump**. A jump that only happens if a certain condition is met. Our JUMP_NEGATIVE is one example of a conditional jump, but computers have other types too - like JUMP IF EQUAL and JUMP IF GREATER. So let’s make our code a little fancier and step through it. Just like before, the program starts by loading values from memory into registers A and B. In this example, the number 11 gets loaded into Register A, and 5 gets loaded into Register B. Now we subtract register B from register A. That’s 11 minus 5, which is 6, and so 6 gets saved into Register A. Now we hit our JUMP NEGATIVE. The last ALU result was 6. That’s a positive number, so the the negative flag is false. That means the processor does not jump. So we continue on to the next instruction... ...which is a JUMP 2. No conditional on this one, so we jump to instruction 2 no matter what. 

> 但是请注意，我们没有办法逃脱。我们永远......永远......都无法到达停止指令，因为我们总是要碰到JUMP。这就是所谓的**无限**循环--一个永远运行的程序......永远......永远......为了打破这个循环，我们需要一个**条件跳跃**。一个只有在满足特定条件时才会发生的跳转。我们的JUMP_NEGATIVE是条件性跳转的一个例子，但计算机也有其他类型--比如JUMP IF EQUAL和JUMP IF GREATER。因此，让我们把我们的代码弄得更漂亮一点，并逐步完成它。在这个例子中，数字11被装入寄存器A，5被装入寄存器B，现在我们用寄存器B减去寄存器A，11减去5，就是6，所以6被存入寄存器A。最后的ALU结果是6，这是一个正数，所以负标志是假的。这意味着处理器没有跳转。所以我们继续下一条指令......这是一个JUMP 2.这个指令没有条件，所以我们无论如何都要跳到指令2。

Ok, so we’re back at our SUBTRACT Register B from Register A. 6 minus 5 equals 1. So 1 gets saved into register A. Next instruction. We’re back again at our JUMP NEGATIVE. 1 is also a positive number, so the CPU continues on to the JUMP 2, looping back around again to the SUBTRACT instruction. This time is different though. 1 minus 5 is negative 4. And so the ALU sets its negative flag to true for the first time. Now, when we advance to the next instruction, JUMP_NEGATIVE 5, the CPU executes the jump to memory location 5. We’re out of the infinite loop! 

> 好了，我们又回到了从寄存器A减去寄存器B的指令。所以1被保存到了寄存器A中，下一条指令。我们又回到了我们的JUMP NEGATIVE。1也是一个正数，所以CPU继续执行JUMP 2，再次循环到SUBTRACT指令。但这次不同了。1减去5是负4。所以ALU第一次将其负数标志设置为真。现在，当我们前进到下一条指令JUMP_NEGATIVE 5时，CPU执行跳转到内存位置5。我们已经走出了无限循环! 

Now we have a ADD B to A. Negative 4 plus 5, is positive 1, and we save that into Register A. Next we have a STORE instruction that saves Register A into memory address 13. Lastly, we hit our HALT instruction and the computer rests. So even though this program is only 7 instructions long, the CPU ended up executing 13 instructions, and that's because it looped twice internally. This code calculated the remainder if we divide 5 into 11, which is one. With a few extra lines of code, we could also keep track of how many loops we did, the count of which would be how many times 5 went into 11… we did two loops, so that means 5 goes into 11 two times... with a remainder of 1. And of course this code could work for any two numbers, which we can just change in memory to whatever we want: 7 and 81, 18 and 54, it doesn’t matter -- that’s the power of software! 

> 现在我们有一条ADD B到A的指令，负4加5，就是正1，我们把它存入寄存器A。最后，我们打出HALT指令，计算机就休息了。因此，尽管这个程序只有7条指令，但CPU最终执行了13条指令，这是因为它内部循环了两次。这段代码计算了如果我们将5除以11的余数，也就是1。如果多写几行代码，我们还可以记录我们做了多少次循环，计算结果是5进11的次数......我们做了两次循环，所以这意味着5进11两次......余数是1。 当然，这段代码可以适用于任何两个数字，我们可以在内存中随便改成：7和81，18和54，这都不重要--这就是软件的力量! 

Software also allowed us to do something our hardware could not. Remember, our ALU didn’t have the functionality to divide two numbers, instead it’s the program we made that gave us that functionality. And then other programs can use our divide program to do even fancier things. And you know what that means. New levels of abstraction! 

> 软件也允许我们做一些硬件无法做到的事情。请记住，我们的ALU并没有除以两个数字的功能，相反，是我们做的程序给了我们这个功能。然后其他程序可以使用我们的除法程序来做更高级的事情。而你知道这意味着什么。新的抽象层次! 
>

So, our hypothetical CPU is very basic – all of its instructions are 8 bits long, with the opcode occupying only the first four bits. So even if we used every combination of 4 bits, our CPU would only be able to support a maximum of 16 different instructions. On top of that, several of our instructions used the last 4 bits to specify a memory location. But again, 4 bits can only encode 16 different values, meaning we can address a maximum of 16 memory locations - that’s not a lot to work with. For example, we couldn’t even JUMP to location 17, because we literally can’t fit the number 17 into 4 bits. 

> 因此，我们假设的CPU是非常基本的--它的所有指令都是8位的，操作码只占前4位。因此，即使我们使用每一个4位的组合，我们的CPU也只能支持最多16条不同的指令。除此之外，我们的几条指令使用最后4位来指定一个内存位置。但同样，4位只能编码16个不同的值，这意味着我们最多只能寻址16个内存位置--这不是一个很大的工作空间。例如，我们甚至不能JUMP到17号位置，因为我们实际上无法将数字17装入4比特。

For this reason, real, modern CPUs use two strategies. The most straightforward approach is just to have bigger instructions, with more bits, like 32 or 64 bits. This is called the instruction length. Unsurprisingly. The second approach is to use variable length instructions. For example, imagine a CPU that uses 8 bit opcodes. When the CPU sees an instruction that needs no extra values, like the HALT instruction, it can just execute it immediately. However, if it sees something like a JUMP instruction, it knows it must also fetch the address to jump to, which is saved immediately behind the JUMP instruction in memory. This is called, logically enough, an Immediate Value. In such processor designs, instructions can be any number of bytes long, which makes the fetch cycle of the CPU a tad more complicated. Now, our example CPU and instruction set is **hypothetical**, designed to illustrate key working principles. So I want to leave you with a real CPU example. 

> 由于这个原因，真正的、现代的CPU使用了两种策略。最直接的方法是拥有更大的指令，有更多的比特，如32或64比特。这就是所谓的指令长度。不出所料。第二种方法是使用可变长度的指令。例如，设想一个使用8位操作码的CPU。当CPU看到一条不需要额外数值的指令，如HALT指令，它可以立即执行。但是，如果它看到像JUMP指令这样的指令，它知道它还必须获取要跳转的地址，这个地址紧紧保存在内存中JUMP指令的后面。从逻辑上讲，这被称为 "即时值"。在这样的处理器设计中，指令可以是任何数量的字节，这使得CPU的取值周期变得更加复杂。现在，我们的CPU和指令集例子是**假设的**，旨在说明关键的工作原理。所以我想给你留下一个真实的CPU例子。

In 1971, Intel released the 4004 processor. It was the first CPU put all into a single chip and **paved** the path to the intel processors we know and love today. It supported 46 instructions, shown here. Which was enough to build an entire working computer. And it used many of the instructions we’ve talked about like JUMP ADD SUBTRACT and LOAD. It also uses 8-bit immediate values, like we just talked about, for things like JUMPs, in order to address more memory. And processors have come a long way since 1971. A modern computer processor, like an Intel Core i7, has thousands of different instructions and instruction **variants**, ranging from one to fifteen bytes long. For example, there’s over a dozens different opcodes just for variants of ADD! And this huge growth in instruction set size is due in large part to extra bells and **whistles** that have been added to processor designs overtime, which we’ll talk about next episode. See you next week! 

> 1971年，英特尔发布了4004处理器。它是第一个全部放在一个芯片中的CPU，并为我们今天所知道和喜爱的英特尔处理器铺平了道路。它支持46条指令，如图所示。这足以建立一个完整的工作计算机。它使用了许多我们已经谈到的指令，如JUMP ADD SUBTRACT和LOAD。它还使用8位即时值，就像我们刚刚谈到的，用于像JUMP这样的事情，以便寻址更多的内存。自1971年以来，处理器已经有了长足的进步。一个现代计算机处理器，如英特尔酷睿i7，有数千条不同的指令和指令**变体**，长度从1到15个字节不等。例如，光是ADD的变体就有几十种不同的操作码。这种指令集规模的巨大增长在很大程度上是由于处理器设计中增加了额外的功能，我们将在下一集讨论这些问题。下周见! 

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\interl 4004 instruction set.jpg)



## #9 Advanced CPU designs

<iframe width="560" height="315" src="https://www.youtube.com/embed/rtAlC5J1U40" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne and welcome to CrashCourse Computer Science! As we’ve discussed throughout the series, computers have come a long way from mechanical devices capable of maybe one calculation per second, to CPUs running at kilohertz and megahertz speeds. The device you’re watching this video on right now is almost certainly running at Gigahertz speeds - that’s billions of instructions executed every second. Which, trust me, is a lot of computation! In the early days of electronic computing, processors were typically made faster by improving the switching time of the transistors inside the chip - the ones that make up all the **logic gates**, **ALUs** and other stuff we’ve talked about over the past few episodes. But just making transistors faster and more efficient only went so far, so processor designers have developed various techniques to **boost** performance allowing not only simple instructions to run fast, but also performing much more **sophisticated** operations. 

> 大家好，我是卡丽-安，欢迎来到《计算机科学速成班》! 正如我们在整个系列中所讨论的那样，计算机已经从每秒能够进行一次计算的机械装置，发展到以千赫兹和兆赫兹速度运行的CPU。你现在观看这段视频的设备几乎可以肯定是以千赫兹的速度运行的--那就是每秒执行数十亿条指令。相信我，这是一个很大的计算量! 在电子计算的早期，处理器通常是通过改善芯片内部晶体管的开关时间来提高速度的--这些晶体管构成了所有的**逻辑门**、**ALU**和其他我们在过去几期节目中谈到的东西。但是，仅仅使晶体管更快、更有效还不够，所以处理器设计者开发了各种技术来**提高**性能，不仅使简单指令快速运行，而且还能执行更多**高级**复杂的操作。

INTRO Last episode, we created a small program for our CPU that allowed us to divide two numbers. We did this by doing many subtractions in a row... so, for example, 16 divided by 4 could be broken down into the smaller problem of 16 minus 4, minus 4, minus 4, minus 4. When we hit zero, or a negative number, we knew that we we’re done. But this approach gobbles up a lot of clock cycles, and isn’t particularly efficient. So most computer processors today have divide as one of the instructions that the ALU can perform in hardware. Of course, this extra circuitry makes the ALU bigger and more complicated to design, but also more capable - a complexity-for-speed tradeoff that has been made many times in computing history. 

> 上一集，我们为我们的CPU创建了一个小程序，使我们能够对两个数字进行除法。我们通过连续做许多减法来做到这一点......因此，例如，16除以4可以被分解成16减4、减4、减4、减4这样的小问题。当我们遇到零或负数时，我们知道我们已经完成了。但这种方法占用了大量的时钟周期，并不是特别有效。所以今天大多数计算机处理器都将除法作为ALU可以在硬件中执行的指令之一。当然，这种额外的电路使ALU更大，设计更复杂，但也更有能力--在计算历史上，这种复杂性与速度的权衡已经进行了很多次。

For instance, modern computer processors now have special circuits for things like graphics operations, decoding compressed video, and encrypting files - all of which are operations that would take many many many clock cycles to perform with standard operations. You may have even heard of processors with **MMX**, **3D**Now!, or **SSE**. These are processors with additional, fancy circuits that allow them to execute additional, fancy instructions - for things like gaming and encryption. These extensions to the instruction set have grown, and grown over time, and once people have written programs to take advantage of them, it’s hard to remove them. So instruction sets tend to keep getting larger and larger keeping all the old opcodes around for backwards compatibility. 

> 例如，现代计算机处理器现在有专门的电路，用于图形操作、解码压缩视频和加密文件等，所有这些操作都需要花费很多很多的时钟周期来执行标准操作。你甚至可能听说过带有**MMX**、**3DNow！**或**SSE**的处理器。这些处理器具有额外的、花哨的电路，使它们能够执行额外的、花哨的指令--用于游戏和加密等事情。这些指令集的扩展随着时间的推移不断增加，一旦人们编写程序来利用它们，就很难将其删除。因此，指令集往往会越来越大，为了向后兼容，所有的旧操作码都被保留下来。

The Intel 4004, the first truly integrated CPU, had 46 instructions - which was enough to build a fully functional computer. But a modern computer processor has thousands of different instructions, which utilize all sorts of clever and complex internal circuitry. Now, high clock speeds and fancy instruction sets lead to another problem - getting data in and out of the CPU quickly enough. It’s like having a powerful steam locomotive, but no way to shovel in coal fast enough. In this case, the bottleneck is RAM. RAM is typically a memory module that lies outside the CPU. This means that data has to be transmitted to and from RAM along sets of data wires, called a bus. This bus might only be a few centimeters long, and remember those electrical signals are traveling near the speed of light, but when you are operating at gigahertz speeds – that’s billionths of a second – even this small delay starts to become problematic. It also takes time for RAM itself to lookup the address, retrieve the data, and configure itself for output. So a “load from RAM” instruction might take dozens of clock cycles to complete, and during this time the processor is just sitting there idly waiting for the data. 

> 英特尔4004，第一个真正的集成CPU，有46条指令--这足以建立一个功能齐全的计算机。但是，现代计算机处理器有数千条不同的指令，它们利用各种巧妙而复杂的内部电路。现在，高时钟速度和花哨的指令集导致了另一个问题--让数据足够快地进入和离开CPU。这就像有一个强大的蒸汽机车，但没有办法足够快地铲入煤炭。在这种情况下，瓶颈是RAM。RAM通常是一个位于CPU之外的内存模块。这意味着，数据必须沿着几组数据线（称为总线）在RAM之间传输。这条总线可能只有几厘米长，记住这些电信号的速度接近光速，但是当你以千兆赫兹的速度运行时--那是十亿分之一秒--即使这个小延迟也开始变得有问题。RAM本身也需要时间来查找地址，检索数据，并为输出配置自己。因此，一条 "从RAM加载 "的指令可能需要几十个时钟周期才能完成，而在这段时间里，处理器只是坐在那里闲置地等待数据。

One solution is to put a little piece of RAM right on the CPU -- called a **cache**. There isn’t a lot of space on a processor’s chip, so most caches are just kilobytes or maybe megabytes in size, where RAM is usually gigabytes. Having a cache speeds things up in a clever way. When the CPU requests a memory location from RAM, the RAM can transmit not just one single value, but a whole block of data. This takes only a little bit more time than transmitting a single value, but it allows this data block to be saved into the cache. This tends to be really useful because computer data is often arranged and processed sequentially. For example, let say the processor is totalling up daily sales for a restaurant. It starts by fetching the first transaction from RAM at memory location 100. The RAM, instead of sending back just that one value, sends a block of data, from memory location 100 through 200, which are then all copied into the cache. Now, when the processor requests the next transaction to add to its running total, the value at address 101, the cache will say “Oh, I’ve already got that value right here, so I can give it to you right away!” And there’s no need to go all the way to RAM. Because the cache is so close to the processor, it can typically provide the data in a single clock cycle -- no waiting required. This speeds things up tremendously over having to go back and forth to RAM every single time. When data requested in RAM is already stored in the cache like this it’s called a cache hit, and if the data requested isn’t in the cache, so you have to go to RAM, it’s a called a cache miss. The cache can also be used like a scratch space, storing intermediate values when performing a longer, or more complicated calculation. 

> 一个解决方案是在CPU上放一小块RAM--称为**缓存**。处理器的芯片上没有太多的空间，所以大多数缓存只有几千字节或几兆字节大小，而RAM通常是几千字节。有了缓存，就能以一种巧妙的方式加快事情的进展。当CPU向RAM请求一个内存位置时，RAM可以传输的不仅仅是一个单一的值，而是整个数据块。这只需要比传输单个数值多花一点时间，但它允许这个数据块被保存到缓冲区。这往往是非常有用的，因为计算机数据往往是按顺序排列和处理的。例如，假设处理器正在计算一家餐馆的每日销售总额。它首先从内存位置100的RAM中获取第一笔交易。RAM不是只发回一个值，而是发回一个数据块，从内存位置100到200，然后全部复制到缓存中。现在，当处理器要求将下一个事务添加到它的运行总数，即地址101的值时，缓存会说："哦，我已经在这里得到了这个值，所以我可以马上给你！" 而且不需要一直到RAM去。由于高速缓存离处理器很近，它通常可以在一个时钟周期内提供数据--不需要等待。这比每次都去RAM来回跑要快得多。当在RAM中请求的数据已经存储在高速缓存中时，这被称为高速缓存命中，而如果请求的数据不在高速缓存中，所以你必须去RAM，这被称为高速缓存缺失。缓存也可以像临时空间一样使用，在进行较长或较复杂的计算时存储中间值。

Continuing our restaurant example, let’s say the processor has finished totalling up all of the sales for the day, and wants to store the result in memory address 150. Like before, instead of going back all the way to RAM to save that value, it can be stored in cached copy, which is faster to save to, and also faster to access later if more calculations are needed. But this introduces an interesting problem -- the cache’s copy of the data is now different to the real version stored in RAM. This mismatch has to be recorded, so that at some point everything can get synced up. For this purpose, the cache has a special flag for each block of memory it stores, called the dirty bit -- which might just be the best term computer scientists have ever invented. Most often this synchronization happens when the cache is full, but a new block of memory is being requested by the processor. Before the cache erases the old block to free up space, it checks its dirty bit, and if it’s dirty, the old block of data is written back to RAM before loading in the new block. 

> 继续我们餐厅的例子，假设处理器已经完成了当天所有销售额的合计，并想把结果存储在内存地址150。像以前一样，与其一直回到RAM中去保存这个值，不如把它保存在缓存的副本中，这样保存的速度更快，而且以后如果需要更多的计算，也可以更快地访问。但这也带来了一个有趣的问题--缓存中的数据副本现在与存储在RAM中的真实版本不同。这种不匹配必须被记录下来，以便在某些时候所有的东西都能被同步起来。为此，高速缓存对其存储的每个内存块都有一个特殊的标志，称为脏位--这可能是计算机科学家发明的最好的术语。大多数情况下，这种同步发生在高速缓存已满，但处理器正在请求一个新的内存块。在高速缓存擦除旧块以释放空间之前，它会检查它的脏位，如果它是脏的，在加载新块之前，旧的数据块会被写回RAM。
>

Another trick to boost cpu performance is called **instruction pipelining**. Imagine you have to wash an entire hotel’s worth of sheets, but you’ve only got one washing machine and one dryer. One option is to do it all sequentially: put a batch of sheets in the washer and wait 30 minutes for it to finish. Then take the wet sheets out and put them in the dryer and wait another 30 minutes for that to finish. This allows you to do one batch of sheets every hour. Side note: if you have a dryer that can dry a load of laundry in 30 minutes, please tell me the brand and model in the comments, because I’m living with 90 minute dry times, minimum. But, even with this magic clothes dryer, you can speed things up even more if you parallelize your operation. As before, you start off putting one batch of sheets in the washer. You wait 30 minutes for it to finish. Then you take the wet sheets out and put them in the dryer. But this time, instead of just waiting 30 minutes for the dryer to finish, you simultaneously start another load in the washing machine. Now you’ve got both machines going at once. Wait 30 minutes, and one batch is now done, one batch is half done, and another is ready to go in. This effectively doubles your throughput. Processor designs can apply the same idea. In episode 7, our example processor performed the fetch-decode-execute cycle sequentially and in a continuous loop: Fetch-decode-execute, fetch-decode-execute, fetch-decode-execute, and so on. This meant our design required three clock cycles to execute one instruction. But each of these stages uses a different part of the CPU, meaning there is an opportunity to parallelize! While one instruction is getting executed, the next instruction could be getting decoded, and the instruction beyond that fetched from memory. All of these separate processes can overlap so that all parts of the CPU are active at any given time. In this pipelined design, an instruction is executed every single clock cycle which triples the throughput. But just like with caching this can lead to some tricky problems. A big hazard is a dependency in the instructions. For example, you might fetch something that the currently executing instruction is just about to modify, which means you’ll end up with the old value in the pipeline. To compensate for this, pipelined processors have to look ahead for data dependencies, and if necessary, stall their pipelines to avoid problems. High end processors, like those found in laptops and smartphones, go one step further and can dynamically reorder instructions with dependencies in order to minimize stalls and keep the pipeline moving, which is called out-of-order execution. As you might imagine, the circuits that figure this all out are incredibly complicated. Nonetheless, pipelining is tremendously effective and almost all processors implement it today. 

> 另一个提高cpu性能的技巧被称为**指令管道化**。想象一下，你要洗一整个酒店的床单，但你只有一台洗衣机和一台烘干机。一种选择是按顺序进行：将一批床单放入洗衣机，等待30分钟后完成。然后把湿的床单拿出来，放到烘干机里，再等30分钟就可以了。这样，你就可以每小时洗一批床单。题外话：如果你有一台能在30分钟内烘干一批衣服的烘干机，请在评论中告诉我品牌和型号，因为我现在的生活是至少90分钟的烘干时间。但是，即使有了这个神奇的干衣机，如果你把你的操作平行化，你还可以把事情加快。像以前一样，你开始把一批床单放入洗衣机。你等待30分钟，让它完成。然后你把湿床单拿出来，放到烘干机里。但是这一次，你不是只等30分钟烘干机完成，而是同时在洗衣机里开始洗另一批。现在你已经让两台机器同时运转。等待30分钟后，一批衣服已经完成，一批衣服已经完成一半，另一批衣服也准备好了。这有效地使你的吞吐量增加了一倍。处理器设计可以应用同样的想法。在第7集中，我们的示例处理器以连续循环的方式依次执行取-解码-执行的循环。取数-解码-执行，取数-解码-执行，取数-解码-执行，如此反复。这意味着我们的设计需要三个时钟周期来执行一条指令。但这些阶段中的每一个都使用了CPU的不同部分，这意味着有机会进行并行化 当一条指令被执行时，下一条指令可能正在被解码，而更多的指令则从内存中获取。所有这些独立的过程都可以重叠，因此CPU的所有部分在任何时候都是活跃的。在这种流水线设计中，每一个时钟周期都有一条指令被执行，从而使吞吐量增加三倍。但就像缓存一样，这也会导致一些棘手的问题。一个很大的危险是指令中的依赖性。例如，你可能会获取当前执行的指令正要修改的东西，这意味着你最终会在流水线上得到旧的值。为了弥补这一点，流水线处理器必须提前寻找数据的依赖性，如果有必要，可以让他们的流水线停顿，以避免问题。高端处理器，如笔记本电脑和智能手机中的处理器，更进一步，可以动态地重新排列有依赖关系的指令，以尽量减少停顿并保持管道的移动，这被称为失序执行。正如你可能想象的那样，计算这一切的电路是非常复杂的。然而，流水线是非常有效的，今天几乎所有的处理器都实现了它。

Another big hazard are conditional **jump instructions** -- we talked about one example, a JUMP NEGATIVE, last episode. These instructions can change the execution flow of a program depending on a value. A simple pipelined processor will perform a long stall when it sees a jump instruction, waiting for the value to be finalized. Only once the jump outcome is known, does the processor start refilling its pipeline. But, this can produce long delays, so high-end processors have some tricks to deal with this problem too. Imagine an upcoming jump instruction as a fork in a road - a branch. Advanced CPUs guess which way they are going to go, and start filling their pipeline with instructions based off that guess – a technique called speculative execution. When the jump instruction is finally resolved, if the CPU guessed correctly, then the pipeline is already full of the correct instructions and it can motor along without delay. However, if the CPU guessed wrong, it has to discard all its speculative results and perform a pipeline flush - sort of like when you miss a turn and have to do a u-turn to get back on route, and stop your GPS’s insistent shouting. To minimize the effects of these flushes, CPU manufacturers have developed sophisticated ways to guess which way branches will go, called branch prediction. Instead of being a 50/50 guess, today’s processors can often guess with over 90% accuracy! In an ideal case, pipelining lets you complete one instruction every single clock cycle, but then superscalar processors came along which can execute more than one instruction per clock cycle. During the execute phase even in a pipelined design, whole areas of the processor might be totally idle. For example, while executing an instruction that fetches a value from memory, the ALU is just going to be sitting there, not doing a thing. So why not fetch-and-decode several instructions at once, and whenever possible, execute instructions that require different parts of the CPU all at the same time!? But we can take this one step further and add duplicate circuitry for popular instructions. For example, many processors will have four, eight or more identical ALUs, so they can execute many mathematical instructions all in parallel! 

> 另一个大的危险是有条件的**跳转指令**--我们上一集讲过一个例子，即JUMP NEGATIVE。这些指令可以根据一个值来改变程序的执行流程。一个简单的流水线处理器在看到跳转指令时，会进行长时间的停顿，等待数值的最终确定。只有在知道跳转结果后，处理器才开始重新填充其流水线。但是，这可能会产生长时间的延迟，所以高端处理器也有一些技巧来处理这个问题。想象一下，即将到来的跳转指令是一条岔路--一个分支。先进的CPU会猜测他们要走哪条路，并根据猜测开始用指令填充管道--这种技术称为推测执行。当跳转指令最终被解决时，如果CPU猜对了，那么管道里已经装满了正确的指令，它可以毫不迟疑地行驶。然而，如果CPU猜错了，它就必须丢弃所有的推测结果，并进行管道刷新--这有点像你错过了一个转弯，不得不调头回到原路，并停止GPS的持续叫喊。为了尽量减少这些冲刷的影响，CPU制造商已经开发了复杂的方法来猜测分支的走向，称为分支预测。今天的处理器不再是50/50的猜测，而是经常可以猜测到90%以上的准确率 在理想的情况下，流水线可以让你在每个时钟周期完成一条指令，但后来超标量处理器出现了，它可以在每个时钟周期执行一条以上的指令。在执行阶段，即使在流水线设计中，处理器的整个区域也可能是完全闲置的。例如，在执行从内存取值的指令时，ALU就会坐在那里，不做任何事情。那么，为什么不一次取回并解码几条指令，并尽可能在同一时间执行需要CPU不同部分的指令呢！？但我们可以更进一步，为流行的指令添加重复的电路。例如，许多处理器会有四个、八个或更多相同的ALU，因此它们可以全部并行地执行许多数学指令! 

Pipelined superscalar CPU:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\pipelined superscalar.jpg)

Ok, the techniques we’ve discussed so far primarily optimize the execution throughput of a single stream of instructions, but another way to increase performance is to run several streams of instructions at once with multi-core processors. You might have heard of dual core or quad core processors. This means there are multiple independent processing units inside of a single CPU chip. In many ways, this is very much like having multiple separate CPUs, but because they’re tightly integrated, they can share some resources, like cache, allowing the cores to work together on shared computations. But, when more cores just isn’t enough, you can build computers with multiple independent CPUs! High end computers, like the servers streaming this video from YouTube’s datacenter, often need the extra horsepower to keep it silky smooth for the hundreds of people watching simultaneously. Two- and four-processor configuration are the most common right now, but every now and again even that much processing power isn’t enough. 

> 好吧，到目前为止我们讨论的技术主要是优化单个指令流的执行吞吐量，但另一种提高性能的方法是用多核处理器同时运行几个指令流。你可能听说过双核或四核处理器。这意味着在一个CPU芯片内有多个独立的处理单元。在许多方面，这很像拥有多个独立的CPU，但由于它们是紧密集成的，它们可以共享一些资源，如缓存，允许核心一起工作，进行共享计算。但是，当更多的内核不够用时，你可以用多个独立的CPU来建造计算机! 高端计算机，如从YouTube数据中心传输视频的服务器，往往需要额外的马力，以保持数百人同时观看时的流畅性。目前，两个和四个处理器的配置是最常见的，但时不时地，即使这么多处理能力也是不够的。

quad-core(四核):

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\quad core.jpg)

So we humans get extra ambitious and build ourselves a supercomputer! If you’re looking to do some really monster calculations – like simulating the formation of the universe - you’ll need some pretty serious compute power. A few extra processors in a desktop computer just isn’t going to cut it. You’re going to need a lot of processors. No.. no... even more than that. A lot more! When this video was made, the world’s fastest computer was located in The National Supercomputing Center in Wuxi, China. The Sunway TaihuLight contains a brain-melting 40,960 CPUs, each with 256 cores! Thats over ten million cores in total... and each one of those cores runs at 1.45 gigahertz. In total, this machine can process 93 Quadrillion -- that’s 93 million-billions -- floating point math operations per second, knows as FLOPS. And trust me, that’s a lot of FLOPS!! No word on whether it can run Crysis at max settings, but I suspect it might. So long story short, not only have computer processors gotten a lot faster over the years, but also a lot more sophisticated, employing all sorts of clever tricks to squeeze out more and more computation per clock cycle. Our job is to wield that incredible processing power to do cool and useful things. That’s the essence of programming, which we’ll start discussing next episode. See you next week. 

> 因此，我们人类变得更加雄心勃勃，为自己建造了一台超级计算机 如果你想做一些真正的巨型计算--比如模拟宇宙的形成--你将需要一些相当严重的计算能力。一台台式电脑中的几个额外的处理器是无法满足的。你将需要大量的处理器。不...不...甚至比这更多。多得多! 制作这段视频时，世界上最快的计算机位于中国无锡的国家超级计算中心。双威太湖之光包含40,960个CPU，每个CPU有256个内核！这意味着超过1000万个内核。总共超过1000万个内核......而且每一个内核都以1.45千兆赫的速度运行。总的来说，这台机器可以每秒处理93 Quadrillion--也就是9300万亿次的浮点数学运算，即FLOPS。相信我，这是一个很大的FLOPS! 没有关于它是否能在最大设置下运行《孤岛惊魂》的消息，但我怀疑它可能会。因此，长话短说，这些年来，计算机处理器不仅变得更快，而且更加复杂，采用了各种巧妙的技巧，在每个时钟周期内挤出越来越多的计算。我们的工作是挥舞这些令人难以置信的处理能力，做一些很酷和有用的事情。这就是编程的本质，我们将在下一集开始讨论这个问题。下周见。



## #10 Early Programming

<iframe width="560" height="315" src="https://www.youtube.com/embed/nwDq4adJwzM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I'm Carrie Anne and welcome to Crash Course Computer Science. Over the last few episodes, We've talked a lot about the mechanics of how computers work. How they use complex circuits to save and retrieve values from memory, and perform operations on those values, like adding two numbers together. We've even briefly talked about sequences of operations, which is a computer program. What we haven't talked about is how a program gets into a computer. You might remember in episode 7 and 8 , when we stepped through some simple example programs for the CPU that we had created. For simplicity, we just waved our hands and said that the program was already magically in memory. But in reality, programs have to be loaded into a computer's memory. It's not magic. It's computer science.

> 大家好，我是卡丽-安，欢迎来到《计算机科学速成班》。在过去的几期节目中，我们已经谈了很多关于计算机如何工作的原理。它们如何使用复杂的电路来保存和检索内存中的值，并对这些值进行操作，比如把两个数字加在一起。我们甚至简单地谈到了操作序列，这是一个计算机程序。我们没有谈到的是，程序是如何进入计算机的。你可能还记得在第7和第8集，我们通过一些简单的例子程序，为我们创建的CPU踩点。为了简单起见，我们只是挥了挥手，说程序已经神奇地进入了内存。但实际上，程序必须被加载到计算机的内存中。这不是魔术。这是计算机科学。

 [Theme Music] The need to programme machines existed way before the development of computers. The most famous example of this was in textile manufacturing. if you just wanted to weave a big red tablecloth, you could simply feed red thread into a loom and let it run. But what about if you wanted the cloth to have a pattern like stripes or plaid? Workers would have to periodically reconfigure the loom as dictated by the pattern, but this was labor intensive which made patterned fabrics expensive. In response, Joseph Marie Jacquard developed a programmable textile loom, which he first demonstrated in 1801. The pattern for each row of the cloth was defined by a punched card. The presence or absence of a hole in the card determined if a specific thread was held high or low in the loom. Such that the cross thread called the weft passed above or below the thread. To vary the pattern across rows these punch cards were arranged in long chains, forming a sequence of commands for the loom. Sound familiar? Many consider Jacquard's loom to be one of the earliest forms of programming. **Punched cards**, turned out to be a cheap , reliable, fairly human readable way to store data. 

> 在计算机发展之前，对机器进行编程的需求就已经存在。这方面最有名的例子是纺织业。如果你只想织一块大红桌布，你可以简单地把红线送进织布机，让它运行。但是，如果你想让布有一个像条纹或格子的图案呢？工人们必须根据图案的要求定期重新配置织机，但这是劳动密集型的，使图案织物变得昂贵。作为回应，约瑟夫-玛丽-雅各布开发了一种可编程的纺织机，他在1801年首次展示了这种纺织机。每一排布的图案由打孔的卡片来定义。卡片上是否有孔，决定了一个特定的线在织机上是高还是低。这样，被称为纬线的交叉线就会在该线的上方或下方通过。为了改变各行的图案，这些打孔卡片被排列成长链，形成织机的命令序列。听起来很熟悉吧？许多人认为提花的织机是最早的编程形式之一。**打孔卡片**，被证明是一种廉价、可靠、相当可读的存储数据的方式。

Nearly a century later, punch cards were use to help tabulate the 1890 newest census which we talked about in episode 1. Each card held an individual person's data. Things like race, marital status, number of children, country of birth, and so on. For each demographic question, a census worker would punch out a hole of the appropriate position. When a card was fed into the tabulating machine, a hole would cause the running total for that specific answer to be increased by one. In this way you could feed the entire county's worth of people and at the end you'd have running totals for all of the questions that you asked. It is important to note here that early tabulating machines were not truly computers as they can only do one thing-tabulate. Their operation was fixed and not programmable. Punched cards stored data, but not a program. 

> 将近一个世纪后，打卡机被用来帮助统计1890年最新的人口普查，我们在第一集谈到过。每张卡都有一个人的数据。诸如种族、婚姻状况、子女数量、出生国等等。对于每个人口统计问题，人口普查工作人员会在适当的位置打出一个洞。当一张卡片被送入制表机时，孔会导致该特定答案的运行总数增加一个。通过这种方式，你可以把整个县的人的数据都送进去，最后你会得到你所问的所有问题的运行总数。这里需要注意的是，早期的制表机并不是真正的计算机，因为它们只能做一件事--制表。它们的操作是固定的，而不是可编程的。打卡纸储存了数据，但不是程序。

Over the next 60 years, these business machines grew in capability, adding features to subtract, multiply, divide, and even make simple decisions about when to perform certain operations. To trigger these functions appropriately, so that different calculations could be performed, a programmer accessed a control panel. This panel was full of little sockets, into which a programmer would plug cables to pass values and signals between different parts of the machine. For this reason they were also called plug boards. Unfortunately, this meant having to rewire the machine each time a different program needed to be run. And so by the 1920s, these plug boards were made swappable. This not only made programming a lot more comfortable, but also allowed for different programs to be plugged into a machine. For example, one board might be wired to calculate sales tax, while another helps with payroll. But plug boards were fiendishly complicated to program. This tangle of wires is a program for calculating a profit loss summary, using an IBM 402 accounting machine, which were popular in the 1940s. And this style of plug board programming wasn't unique to electromechanical computers. The world's first General-Purpose electronic computer, the Eniac, completed in 1946, used a ton of them. Even after a program had been completely figured out on paper, physically wiring up the Eniac and getting the program to run could take upwards of three weeks. 

> 在接下来的60年里，这些商业机器的能力不断增强，增加了减法、乘法、除法的功能，甚至对何时进行某些操作做出了简单的决定。为了适当地触发这些功能，以便进行不同的计算，程序员进入一个控制面板。这个面板上有很多小插座，程序员将电缆插入其中，在机器的不同部分之间传递数值和信号。由于这个原因，它们也被称为插板。不幸的是，这意味着每次需要运行不同的程序时，都必须为机器重新接线。因此，到了20世纪20年代，这些插线板被制成可替换的。这不仅使编程变得更加舒适，而且还允许将不同的程序插入机器。例如，一块插板可以用来计算销售税，而另一块则帮助处理工资。但插线板的编程复杂得令人发指。这个纠结的电线是一个计算利润损失摘要的程序，使用的是IBM 402会计机，这在20世纪40年代很流行。这种插板式编程方式并不是机电式计算机所独有的。世界上第一台通用电子计算机，即1946年完成的Eniac，就使用了大量的插件板。即使在一个程序在纸上已经完全想好之后，在Eniac上进行物理布线并使程序运行可能需要三个星期以上。
>

Given the enormous cost of these early computers, weeks of downtime simply to switch programs was unacceptable and the new, faster, more flexible way to programme machines was badly needed. Fortunately by the late 1940s and into the 50s, **electronic memory** was becoming feasible. As costs fell, memory size grew. Instead of storing a program as a physical plug board of wires, it became possible to store a program entirely in a computer's memory. Where it could be easily changed by programmers and quickly accessed by the CPU. These computers were called stored-program computers. With enough computer memory, you could store not only the program you wanted to run, but also any data your program would need. 

> 考虑到这些早期计算机的巨大成本，仅仅为了切换程序而停工数周是不可接受的，因此非常需要新的、更快、更灵活的机器编程方式。幸运的是，在20世纪40年代末和50年代，**电子存储器**开始变得可行。随着成本的下降，存储器的规模也在不断扩大。与其将一个程序存储为一个由电线组成的物理插板，不如将一个程序完全存储在计算机的存储器中。在那里，程序员可以很容易地改变程序，而CPU则可以快速访问。这些计算机被称为存储程序计算机。有了足够的计算机内存，你不仅可以存储你想运行的程序，还可以存储你的程序需要的任何数据。
>

Including new values it created along the way, Unifying the progrmming data into a single shared memory is called the **von Neumann architecture**. Named after John von Neumann, a prominent mathematician and physicist, who worked on the Manhattan project and several early electronic computers. And once said, "I'm thinking about something much more important than bombs, I'm thinking about computers". The hallmarks of a von Neumann computer are a processing unit containing an arithmetic logic unit, data registers, an instruction register, and an instruction address register. And finally, a memory to store both data and instructions. Hopefully, this sounds familiar, because we actually built a von Neumann computer in episode 7. The very first von Neumann architecture stored program computer was constructed in 1948 by the University of Manchester, nicknamed "Baby". And even the computer you are watching this video right now uses the same architecture. 

> 包括它沿途创造的新值，将程序数据统一到一个共享内存中，被称为**冯-诺伊曼架构**。以约翰-冯-诺伊曼命名，他是一位著名的数学家和物理学家，曾参与曼哈顿计划和几个早期的电子计算机。他曾经说过："我在思考比炸弹更重要的东西，我在思考计算机"。冯-诺依曼计算机的标志是一个处理单元，包含一个算术逻辑单元、数据寄存器、一个指令寄存器和一个指令地址寄存器。最后，还有一个存储数据和指令的存储器。希望这听起来很熟悉，因为我们在第7集实际建造了一台冯-诺依曼计算机。第一台冯-诺依曼结构的存储程序计算机是在1948年由曼彻斯特大学建造的，绰号为 "宝贝"。甚至你现在正在看的这段视频中的计算机也采用了同样的架构。

Now electronic computer memory is great and all, but you still have to load the programming data in to the computer before it can run. And for this reason, punch cards were used. Let's get to the Thought Bubble. Well into the 1980s, almost all computers had a punch card reader. Which could suck in a single punch card at a time and write the contents of the card into the computer's memory. If you loaded in a stack of punch cards, the reader would load them all into memory sequentially, as a big block. Once the programming data were in memory, the computer would be told to execute it. Of course, even simple computer programs might have hundreds of instructions, which meant that programs were stored as stacks of punch cards. So if you ever have the misfortune of accidentally dropping your program on the floor, it could take you hours, days, or even weeks to put the code back in the right order. A common trick was to draw a diagonal line on the side of the card stack called striping, so you'd have at least some clue how to get it back into the right order. The largest program ever punched into punch cards was the US Air Force's sage air defense system, completed in 1955. At its peak, the product is said to have employed 20% of the world's programmers. Its main control program was stored on a whopping 62,500 punch cards, which is equivalent to roughly 5 megabytes of data. Pretty underwhelming by today's standards. And punch cards weren't only useful for getting data into computers, but also getting data out of them. At the end of a program, results could be written out of computer memory and onto punch cards by, well, punching cards. Then this data could be analyzed by humans or loaded into a second program for additional computation. Thanks, Thought Bubble. 

> 现在，电子计算机内存是伟大的，但你仍然必须在计算机运行之前将编程数据加载到计算机中。由于这个原因，打卡机被使用。让我们来谈谈 "思想泡泡"。进入20世纪80年代，几乎所有的计算机都有一个打卡器。它可以一次吸入一张打孔卡，并将卡上的内容写入计算机的内存中。如果你装入一叠打孔卡，读卡器会把它们作为一个大块，按顺序装入内存。一旦编程数据进入内存，计算机将被告知执行它。当然，即使是简单的计算机程序也可能有数百条指令，这意味着程序被存储为一叠打卡。因此，如果你不幸不小心把程序掉在地上，你可能要花几小时、几天、甚至几周的时间才能把代码按正确的顺序放回去。一个常见的技巧是在卡片堆的一侧画上一条对角线，称为条纹，这样你至少会有一些线索，如何把它恢复到正确的顺序。有史以来打入打卡机的最大项目是美国空军的圣人防空系统，于1955年完成。在其高峰期，据说该产品雇用了全世界20%的程序员。它的主要控制程序存储在高达62,500张打卡机上，这相当于大约5兆字节的数据。以今天的标准来看，这很不容易。打孔卡不仅有助于将数据输入计算机，也有助于从计算机中获取数据。在一个程序结束时，结果可以从计算机内存中写出来，并通过打卡机写到打卡机上。然后，这些数据可以由人类进行分析，或者加载到第二个程序中进行额外的计算。谢谢你，思想泡泡。

A close cousin to punch cards was punched paper tape, which is basically the same idea, but continuously instead of being on individual cards. And of course, we haven't talked about hard drives, CD-Roms, DVDs, USB thumb drives, and other similar goodies. We'll get to those more advanced types of data storage in a future episode. Finally, in addition to plug boards and punch paper, there was another common way to program and control computers pre-1980: panel programming. Rather than having to physically plug in cables to activate certain functions, this could also be done with huge panels full of switches and buttons. And there were indicator lights to display the status of various functions and values in memory. Computers of the 50s and 60s often featured huge control consoles that look like this. Although it was rare to input a whole program using just switches, it was possible. And early home computers made for the hobbyist market used switches extensively, because most home users couldn't afford expensive peripherals like punch card readers. 

> 打孔卡的一个近亲是打孔纸带，这基本上是同样的想法，但是是连续的，而不是在单个卡片上。当然，我们还没有谈及硬盘、CD-Roms、DVD、USB拇指驱动器和其他类似的好东西。我们将在以后的节目中讨论这些更先进的数据存储类型。最后，除了插板和打孔纸之外，1980年以前还有一种常见的计算机编程和控制方式：面板编程。与其用物理方式插入电缆来激活某些功能，还可以用充满开关和按钮的巨大面板来完成。而且有指示灯显示各种功能的状态和内存中的数值。50年代和60年代的计算机通常具有巨大的控制盘，看起来像这样。虽然只用开关输入整个程序的情况很少，但这是可能的。早期为爱好者市场制造的家用电脑广泛使用开关，因为大多数家庭用户买不起昂贵的外围设备，如打卡器。

The first commercially successful home computer was the Altair 8800, which sold in two versions: preassembled and as a kit. The kit, which was popular with amateur computing enthusiasts, sold for the then unprecedented low price of around 、\$400 in 1975 or about $2,000 in 2017. To program 8800 you'd literally toggle the switches on the front panel to enter the binary Opcodes for the instruction you wanted. Then you press the deposit button to write that value into memory. Then in the next location in memory, you toggle the switches again for your next instruction, deposit it and so on. When you had finally entered your whole program into memory, you would toggle the switches, move back to memory address zero, press the run button, and watch the little lights blink. That was home computing in 1975, wow. 

> 第一台商业上成功的家用电脑是Altair 8800，它以两个版本出售：预装和套件。该套件受到业余计算机爱好者的欢迎，在1975年以前所未有的低价出售，约为400美元，在2017年约为2000美元。要对8800进行编程，你只需拨动前面板上的开关，为你想要的指令输入二进制操作码。然后你按下寄存按钮，将该值写入内存。然后在内存的下一个位置，你再拨动开关，输入下一条指令，存入内存，如此反复。当你最终将整个程序输入内存时，你将拨动开关，移回内存地址0，按下运行按钮，并看着小灯闪烁。这就是1975年的家庭计算，哇。

Whether it was plug board switches or punched paper Programming these early computers was the realm of experts. Either professionals who did this for a living, or technology enthusiasts. You needed intimate knowledge of the underlying hardware, So things like processor Opcodes and register wits to write programs. This meant programming was hard and tedious. And even professional engineers and scientists struggle to take full advantage of what computing could offer. What was needed was a simpler way to tell computers what to do; a simpler way to write programs. And that brings us to programming languages which we'll talk about next episode. See you next week. 

> 无论是插板开关还是打孔纸，这些早期计算机的编程都是专家的领域。要么是以此为生的专业人士，要么是技术爱好者。你需要对底层硬件有深入的了解，所以像处理器操作码和寄存器的智慧来编写程序。这意味着编程是困难和乏味的。即使是专业的工程师和科学家，也很难充分利用计算所能提供的优势。我们需要的是一种更简单的方法来告诉计算机该做什么；一种更简单的方法来编写程序。这就给我们带来了编程语言，我们将在下一集谈及。下周见。



## #11 The First Programming Languages

<iframe width="560" height="315" src="https://www.youtube.com/embed/RU1u-js7db8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

So far, for most of this series, we’ve focused on **hardware** -- the physical components of computing -- things like: electricity and circuits, registers and RAM, ALUs and CPUs. But programming at the hardware level is cumbersome and inflexible, so programmers wanted a more versatile way to program computers - what you might call a “softer” medium. That’s right, we’re going to talk about **Software**! 

> 到目前为止，在本系列的大部分内容中，我们一直专注于**硬件**--计算的物理组件--诸如：电力和电路、寄存器和RAM、ALU和CPU。但是，在硬件层面上的编程是繁琐和不灵活的，所以程序员希望有一种更多的方式来为计算机编程--你可以称之为 "软"媒介。对的，我们将讨论**软件**！

INTRO In episode 8, we walked through a simple program for the CPU we designed. The very first instruction to be executed, the one at memory address 0, was 0010 1110. As we discussed, the first four bits of an instruction is the operation code, or OPCODE for short. On our hypothetical CPU, 0010 indicated a LOAD_A instruction -- which moves a value from memory into Register A. The second set of four bits defines the memory location, in this case, 1110, which is 14 in decimal. So what these eight numbers really mean is “LOAD Address 14 into Register A”. We’re just using two different languages. You can think of it like English and Morse Code. “Hello” and “.... . .-.. .-.. ---” mean the same thing -- hello! -- they’re just encoded differently. English and Morse Code also have different levels of complexity. English has 26 different letters in its alphabet and way more possible sounds. Morse only has dots and dashes. But, they can convey the same information, and computer languages are similar. 

> 在第8集，我们浏览了我们设计的CPU的一个简单程序。第一条要执行的指令，即内存地址为0的那条，是0010 1110。正如我们所讨论的，一条指令的前四位是操作代码，简称OPCODE。在我们假设的CPU上，0010表示一条LOAD_A指令--它将一个值从内存移到寄存器A中。第二组四位定义了内存位置，在本例中是1110，十进制为14。所以这八个数字的真正含义是 "将地址14加载到寄存器A"。我们只是在使用两种不同的语言。你可以把它想象成英语和摩斯密码。"你好 "和".... .-...-...-...-... "意思是一样的--你好！--它们只是编码不同。英语和摩斯密码也有不同的复杂程度。英语的字母表里有26个不同的字母，还有更多可能的声音。莫尔斯码只有点和破折号。但是，它们可以传达相同的信息，而且计算机语言也是类似的。

As we've seen, computer hardware can only handle raw, **binary instructions**. This is the “language” computer processors natively speak. In fact, it’s the only language they’re able to speak. It’s called Machine Language or **[Machine Code](https://en.wikipedia.org/wiki/Machine_code)**. In the early days of computing, people had to write entire programs in machine code. More specifically, they’d first write a high-level version of a program on paper, in English, for example... “retrieve the next sale from memory, then add this to the running total for the day, week and year, then calculate any tax to be added” ...and so on. An informal, high-level description of a program like this is called **Pseudo-Code**. Then, when the program was all figured out on paper, they’d painstakingly expand and translate it into binary machine code by hand, using things like opcode tables. After the translation was complete, the program could be fed into the computer and run. As you might imagine, people quickly got fed up with this process. 

> 正如我们所看到的，计算机硬件只能处理原始的、**[二进制指令](https://en.wikipedia.org/wiki/Machine_code)**。这是计算机处理器天生的 "语言"。事实上，这也是它们唯一能说的语言。它被称为机器语言或**机器代码**。在计算机的早期，人们不得不用机器代码编写整个程序。更确切地说，他们会先在纸上用英语写出一个程序的高级版本，例如 "从内存中检索下一笔销售，然后将其添加到一天、一周和一年的运行总数中，然后计算要添加的任何税款"......等等。像这样的程序的非正式、高层次的描述被称为**伪代码**。然后，当程序在纸面上全部想好后，他们会不厌其烦地用手工将其扩展并翻译成二进制机器码，使用的东西包括操作码表。翻译完成后，程序就可以输入计算机并运行。正如你所想象的那样，人们很快就对这个过程感到厌倦了。

So, by the late 1940s and into the 50s, programmers had developed slightly higher-level languages that were more human-readable. Opcodes were given simple names, called mnemonics, which were followed by operands, to form instructions. So instead of having to write instructions as a bunch of 1’s and 0’s, programmers could write something like “LOAD_A 14”. We used this mnemonic in Episode 8 because it’s so much easier to understand! Of course, a CPU has no idea what “LOAD_A 14” is. It doesn’t understand text-based language, only binary. And so programmers came up with a clever trick. They created reusable helper programs, in binary, that read in text-based instructions, and assemble them into the corresponding binary instructions automatically. This program is called -- you guessed it -- an **Assembler**. It reads in a program written in an **[Assembly Language](https://en.wikipedia.org/wiki/Assembly_language)** and converts it to native machine code. “LOAD_A 14” is one example of an **assembly instruction**. Over time, Assemblers gained new features that made programming even easier. One nifty feature is automatically figuring out JUMP addresses. This was an example program I used in episode 8:Notice how our JUMP NEGATIVE instruction jumps to address 5, and our regular JUMP goes to address 2. The problem is, if we add more code to the beginning of this program, all of the addresses would change. That’s a huge pain if you ever want to update your program! 

> 因此，到了20世纪40年代末和50年代，程序员们已经开发出了稍微高级的语言，这些语言更容易被人阅读。操作码被赋予简单的名称，称为助记符，后面是操作数，形成指令。因此，程序员不必把指令写成一堆1和0，而是可以写成 "LOAD_A 14 "这样的文字。我们在第8篇中使用了这种记忆法，因为它非常容易理解。当然，CPU不知道 "LOAD_A 14 "是什么。它不懂基于文本的语言，只懂二进制。因此，程序员们想出了一个聪明的技巧。他们创建了可重复使用的二进制辅助程序，读取基于文本的指令，并将其自动组装成相应的二进制指令。这个程序被称为--你猜对了--**汇编器**。它读入用**[汇编语言](https://en.wikipedia.org/wiki/Assembly_language)**编写的程序，并将其转换为本地机器码。"LOAD_A 14 "就是一个**汇编指令**的例子。随着时间的推移，汇编程序获得了新的功能，使编程更加容易。一个有趣的功能是自动计算出JUMP地址。这是我在第8集使用的一个例子：注意我们的JUMP NEGATIVE指令如何跳到地址5，而我们的普通JUMP则跳到地址2。问题是，如果我们在这个程序的开头添加更多的代码，所有的地址都会改变。如果你想更新你的程序，这将是一个巨大的痛苦! 

And so an assembler does away with raw jump addresses, and lets you insert little labels that can be jumped to. When this program is passed into the assembler, it does the work of figuring out all of the jump addresses. Now the programmer can focus more on programming and less on the underlying mechanics under the hood enabling more sophisticated things to be built by hiding unnecessary complexity. As we’ve done many times in this series, we’re once again moving up another level of abstraction. A NEW LEVEL OF ABSTRACTION! 

> 因此，汇编器取消了原始跳转地址，而让你插入可以跳转的小标签。当这个程序被传递到汇编器中时，它就完成了计算所有跳转地址的工作。现在，程序员可以把精力更多地放在编程上，而不是放在引擎盖下的底层机械上，通过隐藏不必要的复杂性来建立更复杂的东西。正如我们在这个系列中多次所做的那样，我们又一次提升了抽象的层次。一个新的抽象层次! 

However, even with nifty assembler features like auto-linking JUMPs to labels, Assembly Languages are still a thin veneer over machine code. In general, each assembly language instruction converts directly to a corresponding machine instruction – a one-to-one mapping – so it’s inherently tied to the underlying hardware. And the assembler still forces programmers to think about which registers and memory locations they will use. If you suddenly needed an extra value, you might have to change a lot of code to fit it in. 

> 然而，即使有了像自动连接JUMP和标签这样的巧妙的汇编功能，汇编语言仍然是机器代码的一层薄薄的外衣。一般来说，每条汇编语言指令都会直接转换为相应的机器指令--一对一的映射--所以它与底层硬件有着内在的联系。而且，汇编程序仍然迫使程序员考虑他们将使用哪些寄存器和内存位置。如果你突然需要一个额外的值，你可能不得不改变大量的代码来适应它。
>

Let’s go to the Thought Bubble. This problem did not escape Dr. Grace Hopper. As a US naval officer, she was one of the first programmers on the Harvard Mark 1 computer, which we talked about in Episode 2. This was a colossal, electro-mechanical beast completed in 1944 as part of the allied war effort. Programs were stored and fed into the computer on punched paper tape. By the way, as you can see, they “patched” some bugs in this program by literally putting patches of paper over the holes on the punch tape. The Mark 1’s instruction set was so primitive, there weren’t even JUMP instructions. To create code that repeated the same operation multiple times, you’d tape the two ends of the punched tape together, creating a physical loop. In other words, programming the Mark 1 was kind of a nightmare! After the war, Hopper continued to work at the forefront of computing. To unleash the potential of computers, she designed a high-level programming language called “Arithmetic Language Version 0”, or A-0 for short. Assembly languages have direct, one-to-one mapping to machine instructions. But, a single line of a high-level programming language might result in dozens of instructions being executed by the CPU. To perform this complex translation, Hopper built the first **[compiler](https://en.wikipedia.org/wiki/Compiler)** in 1952. This is a specialized program that transforms “source” code written in a programming language into a low-level language, like assembly or the binary “machine code” that the CPU can directly process. Thanks, Thought Bubble. 

> 让我们去看看 "思想泡泡"。这个问题并没有逃过格雷斯-霍珀博士的眼睛。作为一名美国海军军官，她是哈佛大学马克1号计算机的首批程序员之一，我们在第二集里谈到了这一点。这是一个巨大的电子机械野兽，于1944年完成，是盟军战争努力的一部分。程序被储存在打孔纸带上并输入计算机。顺便说一下，正如你所看到的，他们在这个程序中 "修补 "了一些错误，方法是在打孔带上的孔上贴上纸补丁。马克1的指令集是如此原始，甚至没有JUMP指令。为了创建多次重复相同操作的代码，你会把打孔带的两端绑在一起，形成一个物理循环。换句话说，为马克1号编程是一场噩梦。战后，霍普继续在计算机的最前沿工作。为了释放计算机的潜力，她设计了一种名为 "0版算术语言 "的高级编程语言，简称A-0。汇编语言对机器指令有直接的、一对一的映射。但是，高级编程语言的一行可能导致几十条指令被CPU执行。为了进行这种复杂的转换，霍普在1952年建立了第一个**[编译器](https://en.wikipedia.org/wiki/Compiler)**。这是一个专门的程序，将用编程语言编写的 "源码 "转换为低级语言，如汇编或CPU可以直接处理的二进制 "机器码"。谢谢，思想泡泡。

So, despite the promise of easier programming, many people were skeptical of Hopper’s idea. She once said, “I had a running compiler and nobody would touch it. … they carefully told me, computers could only do arithmetic; they could not do programs.” But the idea was a good one, and soon many efforts were underway to craft new **[programming languages](https://en.wikipedia.org/wiki/Programming_language)** -- today there are hundreds! Sadly, there are no surviving examples of A-0 code, so we’ll use **[Python](https://en.wikipedia.org/wiki/Python_(programming_language))**, a modern programming language, as an example. 

> 因此，尽管承诺了更容易的编程，但许多人对霍珀的想法持怀疑态度。她曾说："我有一个正在运行的编译器，但没有人愿意碰它。......他们小心翼翼地告诉我，计算机只能做算术，他们不能做程序。" 但是这个想法很好，很快就有很多人开始努力制作新的**[编程语言](https://en.wikipedia.org/wiki/Programming_language)** -- 今天有数百种！可悲的是，没有现存的A-0代码的例子，所以我们将使用**[Python](https://en.wikipedia.org/wiki/Python_(programming_language))**，一种现代编程语言，作为例子。

Let’s say we want to add two numbers and save that value. Remember, in assembly code, we had to fetch values from memory, deal with registers, and other low-level details. But this same program can be written in python like so: Notice how there are no registers or memory locations to deal with -- the compiler takes care of that stuff, abstracting away a lot of low-level and unnecessary complexity. The programmer just creates abstractions for needed memory locations, known as **[variables](https://en.wikipedia.org/wiki/Variable_(computer_science))**, and gives them names. So now we can just take our two numbers, store them in variables we give names to -- in this case, I picked a and b but those variables could be anything - and then add those together, saving the result in c, another variable I created. It might be that the compiler assigns Register A under the hood to store the value in a, but I don’t need to know about it! Out of sight, out of mind! It was an important historical milestone, but A-0 and its later variants weren’t widely used. 

> 比方说，我们想把两个数字相加并保存该值。记住，在汇编代码中，我们必须从内存中取值，处理寄存器，以及其他低级别的细节。但同样的程序可以像这样用python编写。请注意，没有任何寄存器或内存位置需要处理 -- 编译器处理了这些东西，抽象出了很多低级和不必要的复杂性。程序员只是为需要的内存位置创建抽象，称为**[变量](https://en.wikipedia.org/wiki/Variable_(computer_science))**，并给它们命名。因此，现在我们只需将两个数字，存储在我们命名的变量中--在本例中，我选择了a和b，但这些变量可以是任何东西--然后将它们相加，将结果保存在我创建的另一个变量c中。可能是编译器在引擎下指定了寄存器A来存储a中的值，但我不需要知道它！我只需要知道它是什么。看不见，摸不着! 这是一个重要的历史里程碑，但A-0和它后来的变体并没有被广泛使用。

**FORTRAN**, derived from "Formula Translation", was released by IBM a few years later, in 1957, and came to dominate early computer programming. John Backus, the FORTRAN project director, said: "Much of my work has come from being lazy. I didn't like writing programs, and so ... I started work on a programming system to make it easier to write programs." You know, typical lazy person. They’re always creating their own programming systems. Anyway, on average, programs written in FORTRAN were 20 times shorter than equivalent handwritten assembly code. Then the FORTRAN Compiler would translate and expand that into native machine code. The community was skeptical that the performance would be as good as hand written code, but the fact that programmers could write more code more quickly, made it an easy choice economically: trading a small increase in computation time for a significant decrease in programmer time. Of course, IBM was in the business of selling computers, and so initially, FORTRAN code could only be compiled and run on IBM computers. And most programing languages and compilers of the 1950s could only run on a single type of computer. So, if you upgraded your computer, you’d often have to re-write all the code too! 

> **FORTRAN**，源于 "公式翻译"，几年后，即1957年，由IBM发布，并在早期计算机编程中占据主导地位。FORTRAN项目主管John Backus说。"我的很多工作都来自于懒惰。我不喜欢写程序，所以......。我开始了一个编程系统的工作，以使编写程序更容易"。你知道，典型的懒人。他们总是在创造自己的编程系统。总之，平均而言，用FORTRAN编写的程序比同等的手写汇编代码短20倍。然后，FORTRAN编译器会将其翻译并扩展为本地机器代码。社区对其性能是否与手写代码一样好持怀疑态度，但事实上，程序员可以更快编写更多的代码，这在经济上是一个简单的选择：用计算时间的少量增加换取程序员时间的大幅减少。当然，IBM的业务是销售计算机，所以最初，FORTRAN代码只能在IBM计算机上编译和运行。而50年代的大多数编程语言和编译器只能在单一类型的计算机上运行。因此，如果你升级了你的电脑，你往往也要重新写所有的代码。

In response, computer experts from industry, academia and government formed a consortium in 1959 -- the Committee on Data Systems Languages, advised by our friend Grace Hopper -- to guide the development of a common programming language that could be used across different machines. The result was the high-level, easy to use, Common Business-Oriented Language, or **COBOL** for short. To deal with different underlying hardware, each computing architecture needed its own COBOL compiler. But critically, these compilers could all accept the same COBOL source code, no matter what computer it was run on. This notion is called write once, run anywhere. It’s true of most programming languages today, a benefit of moving away from assembly and machine code, which is still CPU specific. The biggest impact of all this was reducing computing’s barrier to entry. Before high level programming languages existed, it was a realm exclusive to computer experts and enthusiasts. And it was often their full time profession. But now, scientists, engineers, doctors, economists, teachers, and many others could incorporate computation into their work. 

> 作为回应，来自工业界、学术界和政府的计算机专家在1959年组成了一个联盟--数据系统语言委员会，由我们的朋友格雷斯-霍珀提供建议--以指导开发一种可以在不同机器上使用的通用编程语言。其结果是高层次的、易于使用的、面向商业的通用语言，简称**COBOL**。为了处理不同的底层硬件，每个计算架构都需要自己的COBOL编译器。但关键的是，这些编译器都可以接受相同的COBOL源代码，无论在什么计算机上运行。这个概念被称为 "一次写入，随处运行"。今天，大多数编程语言都是如此，这是摆脱汇编和机器代码的好处，因为汇编和机器代码仍然是针对CPU的。所有这些的最大影响是降低了计算机的进入门槛。在高级编程语言出现之前，它是计算机专家和爱好者的专属领域。而且这往往是他们的全职职业。但现在，科学家、工程师、医生、经济学家、教师和许多其他人都可以将计算纳入他们的工作。

Thanks to these languages, computing went from a cumbersome and esoteric discipline to a general purpose and accessible tool. At the same time, abstraction in programming allowed those computer experts – now “professional programmers” – to create increasingly sophisticated programs, which would have taken millions, tens of millions, or even more lines of assembly code. Now, this history didn’t end in 1959. In fact, a golden era in programming language design jump started, evolving in lockstep with dramatic advances in computer hardware. In the 1960s, we had languages like **ALGOL**, **LISP** and **BASIC**. In the 70’s: **Pascal**, **C** and **Smalltalk** were released. The 80s gave us **C++**, **Objective-C**, and **Perl**. And the 90’s: **python**, **ruby**, and **Java**. And the new millennium has seen the rise of **Swift**, **C#**, and **Go** - not to be confused with Let it Go and Pokemon Go. Anyway, some of these might sound familiar -- many are still around today. It’s extremely likely that the web browser you’re using right now was written in C++ or Objective-C. That list I just gave is the tip of the iceberg. And languages with fancy, new features are proposed all the time. Each new language attempts to leverage new and clever abstractions to make some aspect of programming easier or more powerful, or take advantage of emerging technologies and platforms, so that more people can do more amazing things, more quickly. Many consider the holy grail of programming to be the use of “plain ol’ English”, where you can literally just speak what you want the computer to do, it figures it out, and executes it. This kind of intelligent system is science fiction… for now. And fans of 2001: A Space Odyssey may be okay with that. 

> 由于这些语言，计算从一个繁琐和深奥的学科变成了一个通用的和可获得的工具。同时，编程中的抽象化使那些计算机专家--现在的 "专业程序员"--能够创建越来越复杂的程序，而这些程序本来需要数百万、数千万甚至更多行的汇编代码。现在，这段历史并没有在1959年结束。事实上，一个编程语言设计的黄金时代已经开始，与计算机硬件的巨大进步同步发展。在20世纪60年代，我们有**ALGOL**、**LISP**和**BASIC**等语言。在70年代。发布了**Pascal**、**C**和**Smalltalk**。80年代：**C++**，**Objective-C**，和**Perl**。到了90年代。**python**，**ruby**，和**Java**。而在新的千年里，**Swift**、**C#**和**Go**--不要与Let it Go和Pokemon Go混淆。总之，其中一些可能听起来很熟悉 -- 许多今天仍然存在。你现在使用的网络浏览器极有可能是用C++或Objective-C编写的。我刚才列举的只是冰山一角。而具有花哨的新功能的语言一直在被提出。每一种新的语言都试图利用新的和聪明的抽象来使编程的某些方面更容易或更强大，或利用新兴技术和平台的优势，以便更多的人可以做更多惊人的事情，更快。许多人认为编程的圣杯是使用 "普通英语"，在那里你可以真正地说出你想让计算机做的事情，它可以找出并执行它。这种智能系统是科幻小说......目前。而《2001年：太空漫游》的粉丝们可能对这一点没有意见。

Now that you know all about programming languages, we’re going to deep dive for the next couple of episodes, and we’ll continue to build your understanding of how programming languages, and the software they create, are used to do cool and unbelievable things. See you next week. 

> 现在你已经知道了所有关于编程语言的知识，我们将在接下来的几期节目中进行深入探讨，我们将继续建立你对编程语言以及它们所创造的软件是如何被用来做很酷和不可思议的事情的理解。下周见。



## #12 Programming Basics: Statements & Functions

<iframe width="560" height="315" src="https://www.youtube.com/embed/l26oaHV7D40" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science! Last episode we discussed how writing programs in native machine code, and having to contend with so many low level details, was a huge impediment to writing complex programs. To abstract away many of these low-level details, Programming Languages were developed that let programmers concentrate on solving a problem with computation, and less on nitty gritty hardware details. So today, we’re going to continue that discussion, and introduce some fundamental building blocks that almost all programming languages provide. 

> 大家好，我是Carrie Anne，欢迎来到《计算机科学速成班》! 上一集我们讨论了用本地机器代码编写程序，以及不得不与许多低层次的细节作斗争，这是编写复杂程序的一个巨大障碍。为了抽象出这些低层次的细节，人们开发了编程语言，让程序员集中精力解决计算问题，而不去关注那些琐碎的硬件细节。所以今天，我们将继续讨论，并介绍一些几乎所有编程语言都提供的基本构件。

INTRO Just like spoken languages, programming languages have **[statements](https://en.wikipedia.org/wiki/Statement_(computer_science))**. These are individual complete thoughts, like “I want tea” or “it is raining”. By using different words, we can change the meaning; for example, “I want tea” to “I want unicorns”. But we can’t change “I want tea” to “I want raining” - that doesn’t make grammatical sense. The set of rules that govern the structure and composition of statements in a language is called **syntax**. The English language has syntax, and so do all programming languages. “A equals 5” is a programming language statement. In this case, the statement says a variable named A has the number 5 stored in it. This is called an assignment statement because we're assigning a value to a variable. To express more complex things, we need a series of statements, like “A is 5, B is ten, C equals A plus B” This program tells the computer to set variable ‘A’ equal to 5, variable ‘B’ to 10, and finally to add ‘A’ and ‘B’ together, and put that result, which is 15, into -- you guessed it -- variable C. Note that we can call variables whatever we want. Instead of A, B and C, it could be apples, pears, and fruits. The computer doesn’t care, as long as variables are uniquely named. But it’s probably best practice to name them things that make sense in case someone else is trying to understand your code. 

> 简介 就像口语一样，编程语言有**[语句](https://en.wikipedia.org/wiki/Statement_(computer_science))**。这些是单独的完整的想法，如 "我想喝茶 "或 "下雨了"。通过使用不同的词，我们可以改变其含义；例如，"我想喝茶 "变成 "我想要独角兽"。但我们不能把 "我想喝茶 "改为 "我想下雨"--这在语法上是说不通的。管理一种语言中语句的结构和构成的一系列规则被称为**语法**。英语有语法，所有的编程语言也都有语法。"A等于5 "是一种编程语言的语句。在这种情况下，该语句表示一个名为A的变量中存储有数字5。这被称为赋值语句，因为我们在给一个变量赋值。为了表达更复杂的东西，我们需要一系列的语句，比如 "A是5，B是10，C等于A加B。"这个程序告诉计算机将变量'A'设为5，变量'B'设为10，最后将'A'和'B'相加，并将结果，也就是15，放入--你猜对了--变量C。可以不叫A、B和C，而叫苹果、梨和水果。只要变量的名字是唯一的，计算机就不会在意。但最好的做法是给它们起一个有意义的名字，以防别人尝试理解你的代码。
>

A program, which is a list of instructions, is a bit like a recipe: boil water, add noodles, wait 10 minutes, drain and enjoy. In the same way, the program starts at the first statement and runs down one at a time until it hits the end. So far, we’ve added two numbers together. Boring. Let’s make a video game instead! Of course, it’s way too early to think about coding an entire game, so instead, we’ll use our example to write little snippets of code that cover some programming fundamentals. Imagine we’re building an old-school arcade game where Grace Hopper has to capture bugs before they get into the Harvard Mark 1 and crash the computer! On every level, the number of bugs increases. Grace has to catch them before they wear out any relays in the machine. Fortunately, she has a few extra relays for repairs. To get started, we’ll need to keep track of a bunch of values that are important for gameplay, like what level the player is on, the score, the number of bugs remaining, as well as the number of spare relays in Grace’s inventory. So, we must “initialize” our variables, that is, set their initial value: “level equals 1, score equals 0, bugs equals 5, spare relays equals 4, and player name equals “Andre”. 

> 一个程序，也就是一串指令，有点像菜谱：烧开水，加面条，等10分钟，沥水，享用。以同样的方式，程序从第一条语句开始，一条条往下跑，直到跑到最后。到目前为止，我们已经把两个数字加在一起。很无聊。让我们做一个视频游戏来代替吧! 当然，现在考虑为整个游戏编码还为时过早，因此，我们将用我们的例子来编写涵盖一些编程基础知识的小段代码。想象一下，我们正在构建一个老式的街机游戏，格雷斯-霍珀必须在虫子进入哈佛Mark1并使计算机崩溃之前捕获虫子 在每一关，虫子的数量都会增加。格蕾丝必须在它们耗尽机器中的继电器之前抓住它们。幸运的是，她有一些额外的继电器用于维修。为了开始游戏，我们需要跟踪一些对游戏很重要的数值，比如玩家所处的级别、分数、剩余的虫子数量，以及格蕾丝库存中的备用继电器数量。因此，我们必须 "初始化 "我们的变量，也就是说，设置它们的初始值。"等级等于1，分数等于0，虫子等于5，备用继电器等于4，玩家名字等于 "Andre"。

To create an interactive game, we need to control the flow of the program beyond just running from top to bottom. To do this, we use **Control Flow Statements**. There are several types, but If Statements are the most common. You can think of them as “If X is true, then do Y”. An English language example is: “If I am tired, then get tea” So if “I am tired” is a true statement, then I will go get tea If “I am tired” is false, then I will not go get tea. An IF statement is like a fork in the road. Which path you take is conditional on whether the expression is true or false -- so these **[expressions](https://en.wikipedia.org/wiki/Expression_(computer_science))** are called **Conditional Statements**. In most programming languages, an if statement looks something like …. “If, expression, then, some code, then end the if statement”. For example, if “level” is 1, then we set the score to zero, because the player is just starting. We also set the number of bugs to 1, to keep it easy for now. Notice the lines of code that are conditional on the if-statement are nested between the IF and END IF. Of course, we can change the conditional expression to whatever we want to test, like “is score greater than 10” 5 or “is bugs less than 1”. And If-Statements can be combined with an ELSE statement, which acts as a catch-all if the expression is false. If the level is not 1, the code inside the ELSE block will be executed instead, and the number of bugs that Grace has to battle is set to 3 times the level number. So on level 2, it would be six bugs, and on level 3 there’s 9, and so on. Score isn’t modified in the ELSE block, so Grace gets to keep any points earned. Here are some examples of if-then-else statements from some popular programming languages -- you can see the syntax varies a little, but the underlying structure is roughly the same. 

> 为了创建一个交互式游戏，我们需要控制程序的流程，而不是仅仅从上到下运行。为了做到这一点，我们使用**控制流语句**。有几种类型，但If语句是最常见的。你可以把它们想象成 "如果X为真，则做Y"。一个英语的例子是。"如果我累了，就去喝茶"，所以如果 "我累了 "是一个真实的语句，那么我就会去喝茶，如果 "我累了 "是假的，那么我就不会去喝茶。一个IF语句就像道路上的一个分岔口。你走哪条路是以表达式是真还是假为条件的--所以这些**[表达式](https://en.wikipedia.org/wiki/Expression_(computer_science))**被称为**条件语句**。在大多数编程语言中，if语句看起来像.... "如果，表达式，然后，一些代码，然后结束if语句"。例如，如果 "水平 "是1，那么我们将分数设置为零，因为玩家刚刚开始。我们还将虫子的数量设置为1，以保持目前的简单。请注意，在if-statement上有条件的代码行嵌套在IF和END IF之间。当然，我们可以将条件表达式改为我们想要测试的任何内容，比如 "分数是否大于10"5或 "错误是否小于1"。而且If-Statements可以和ELSE语句结合起来，如果表达式是假的，ELSE语句就会充当万能钥匙。如果级别不是1，ELSE块内的代码将被执行，而Grace需要战斗的bug数被设置为级别数的3倍。因此，在第2层，将有6个虫子，在第3层，有9个，以此类推。分数在ELSE块中没有被修改，所以Grace可以保留所有获得的分数。下面是一些流行的编程语言中的if-then-else语句的例子--你可以看到其语法有一些不同，但基本结构大致相同。

if-else statements examples:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\if-statement.jpg)

If-statements are executed once, a conditional path is chosen, and the program moves on. To repeat some statements many times, we need to create a **conditional loop**. One way is a while statement, also called a **while loop**. As you might have guessed, this loops a piece of code “while” a condition is true. Regardless of the programming language, they look something like this: In our game, let’s say at certain points, a friendly colleague restocks Grace with relays! Hooray! To animate him replenishing our stock back up to a maximum of 4, we can use a while loop. Let’s walk through this code. First we’ll assume that Grace only has 1 tube left when her colleague enters. When we enter the while loop, the first thing the computer does is test its conditional…is relays less than 4? Well, relays is currently 1, so yes. Now we enter the loop! Then, we hit the line of code: “relays equals relays plus 1”. This is a bit confusing because the variable is using itself in an assignment statement, so let's unpack it. You always start by figuring out the right side of the equals sign first, so what does “relays plus 1” come out to be? Well, relays is currently the value 1, so 1 plus 1 equals 2. Then, this result gets saved back into the variable relays, writing over the old value, so now relays stores the value 2. We’ve hit the end of the while loop, which jumps the program back up. Just as before, we test the conditional to see if we’re going to enter the loop. Is relays less than 4? Well, yes, relays now equals 2, so we enter the loop again! 2 plus 1 equals 3. So 3 is saved into relays. Loop again. Is 3 less than 4? Yes it is! Into the loop again. 3 plus 1 equals 4. So we save 4 into relays. Loop again. Is 4 less than 4?.... No! So the condition is now false, and thus we exit the loop and move on to any remaining code. That’s how a while loop works! 

> if-statements被执行一次，一个条件路径被选择，然后程序继续前进。为了多次重复某些语句，我们需要创建一个**条件循环**。一种方法是while语句，也叫**while循环**。你可能已经猜到了，这是在一个while条件为真的情况下循环一段代码。不管是什么编程语言，它们看起来都是这样的。在我们的游戏中，假设在某些时候，一个友好的同事给格雷斯补充了继电器！这就是我们的游戏。万岁! 为了使他将我们的库存补充到最多4个，我们可以使用一个while循环。让我们来看看这段代码。首先，我们假设当她的同事进来时，Grace只剩下1个管子。当我们进入while循环时，计算机做的第一件事就是测试其条件......relays是否小于4？嗯，继电器目前是1，所以是的。现在我们进入了循环! 然后，我们打出了这行代码。"relays等于relays加1"。这有点令人困惑，因为这个变量在一个赋值语句中使用自己，所以让我们来解开它。你总是先弄清楚等号的右边，那么 "relays plus 1 "是什么意思？嗯，relays目前的值是1，所以1加1等于2。然后，这个结果又被保存到变量relays中，写入旧值，所以现在relays存储的是2。我们已经到了while循环的末端，程序又跳了起来。就像以前一样，我们测试条件，看我们是否要进入循环。relays是否小于4？嗯，是的，继电器现在等于2，所以我们再次进入循环! 2加1等于3。所以3被保存到relays中。再次循环。3是不是小于4？是的，它是! 再次进入循环。3加1等于4。所以我们把4存进继电器。再次循环。4是不是小于4？.... 不！不！不！不 所以现在的条件是假的，因此我们退出循环，继续进行任何剩余的代码。这就是while循环的工作原理! 

while loop:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\while loop.jpg)

There’s also the common **For Loop**. Instead of being a **condition-controlled** loop that can repeat forever until the condition is false, a FOR loop is **count-controlled**; it repeats a specific number of times. They look something like this: Now, let’s put in some real values. This example loops 10 times, because we’ve specified that variable ‘i’ starts at the value 1 and goes up to 10. The unique thing about a FOR loop is that each time it hits NEXT, it adds one to ‘i’. When ‘i’ equals 10, the computer knows it’s been looped 10 times, and the loop exits. We can set the number to whatever we want -- 10, 42, or a billion -- it’s up to us. Let’s say we want to give the player a bonus at the end of each level for the number of vacuum relays they have left over. As the game gets harder, it takes more skill to have unused relays, so we want the bonus to go up exponentially based on the level. We need to write a piece of code that calculates exponents - that is, multiplying a number by itself a specific number of times. A loop is perfect for this! First lets initialize a new variable called “bonus” and set it to 1. Then, we create a FOR loop starting at 1, and looping up to the level number. Inside that loop, we multiply bonus times the number of relays, and save that new value back into bonus. For example, let’s say relays equals 2, and level equals 3. So the FOR loop will loop three times, which means bonus is going to get multiplied by relays... by relays... by relays. Or in this case, times 2, times 2, times 2, which is a bonus of 8! That’s 2 to the 3rd power! 

> 还有一个常见的**For循环**。FOR循环不是一个可以永远重复直到条件为假的**条件控制的**循环，而是一个**计数控制的**；它重复特定的次数。它们看起来像这样。现在，让我们输入一些真实的数值。这个例子循环了10次，因为我们指定了变量'i'从数值1开始，一直到10。FOR循环的独特之处在于，每一次点击NEXT，它都会给'i'加一。当'i'等于10时，计算机知道它已经循环了10次，循环就退出了。我们可以把这个数字设置为我们想要的任何数字--10，42，或者10亿--这取决于我们。比方说，我们想在每个关卡结束时根据玩家剩余的真空继电器的数量给他们一个奖励。随着游戏越来越难，拥有未使用的继电器需要更多的技巧，所以我们希望奖金能根据关卡呈指数上升。我们需要写一段计算指数的代码--也就是说，将一个数字乘以其自身的特定次数。循环是完美的选择。首先，让我们初始化一个名为 "奖金 "的新变量，并将其设置为1。然后，我们创建一个FOR循环，从1开始，一直循环到级别数。在这个循环中，我们将奖金乘以继电器的数量，并将这个新的数值保存到奖金中。例如，我们假设继电器等于2，等级等于3。所以FOR循环将循环三次，这意味着奖金将被乘以继电器......乘以继电器......。或者在这种情况下，乘以2，乘以2，乘以2，这是一个8的奖金！这是2的3次方。那是2的3次方! 

for loop:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\for loop.jpg)

This exponent code is useful, and we might want to use it in other parts of our code. It’d be annoying to copy and paste this everywhere, and have to update the variable names each time. Also, if we found a bug, we’d have to hunt around and update every place we used it. It also makes code more confusing to look at. Less is more! What we want is a way to package up our exponent code so we can use it, get the result, and not have to see all the internal complexity. We’re once again moving up a new level of abstraction! 

> 这个指数代码很有用，我们可能想在我们代码的其他部分使用它。如果到处复制和粘贴，而且每次都要更新变量名，那就太烦人了。另外，如果我们发现了一个错误，我们就必须四处寻找并更新我们使用它的每一个地方。这也使得代码看起来更加混乱。少即是多! 我们想要的是一种方法来打包我们的指数代码，这样我们就可以使用它，得到结果，而不必看到所有内部的复杂性。我们再一次提升了一个新的抽象层次 !

To compartmentalize and hide complexity, programming languages can package pieces of code into named **functions**, also called **methods** or **subroutines** in different programming languages. These functions can then be used by any other part of that program just by calling its name. Let’s turn our exponent code into a function! First, we should name it. We can call it anything we want, like HappyUnicorn, but since our code calculates exponents, let’s call it exponent. Also, instead of using specific variable names, like “relays” and “levels”, we specify generic variable names, like Base and Exp, whose initial values are going to be “passed” into our function from some other part of the program. The rest of our code is the same as before, now tucked into our function and with new variable names. Finally, we need to send the result of our exponent code back to the part of the program that requested it. For this, we use a RETURN statement, and specify that the value in ‘result’ be returned. So our full function code looks like this: Now we can use this function anywhere in our program, simply by calling its name and passing in two numbers. For example, if we want to calculate 2 to the 44th power, we can just call “exponent 2 comma 44.” and like 18 trillion comes back. Behind the scenes, 2 and 44 get saved into variables Base and Exp inside the function, it does all its loops as necessary, and then the function returns with the result. 

> 为了分割和隐藏复杂性，编程语言可以将代码片断打包成命名的**函数**，在不同的编程语言中也称为**方法**或**子程序**。然后，这些函数可以被该程序的任何其他部分使用，只需调用其名称即可。让我们把我们的指数代码变成一个函数! 首先，我们应该给它命名。我们可以叫它任何名字，比如HappyUnicorn，但由于我们的代码是计算指数的，我们就叫它exponent吧。另外，我们不使用特定的变量名，如 "继电器 "和 "水平"，而是指定通用的变量名，如Base和Exp，其初始值将从程序的其他部分 "传递 "到我们的函数中。我们其余的代码和以前一样，现在被塞进我们的函数中，并使用新的变量名。最后，我们需要将我们的指数代码的结果送回程序中请求它的部分。为此，我们使用RETURN语句，并指定返回 "result "中的值。所以我们的完整函数代码看起来像这样。现在，我们可以在程序中的任何地方使用这个函数，只需调用它的名字并传入两个数字。例如，如果我们想计算2的44次方，我们可以直接调用 "exponent(2, 44) "，这样就会返回18万亿。在幕后，2和44被保存到函数内部的变量Base和Exp中，它进行所有必要的循环，然后函数带着结果返回。

function example:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\function example.jpg)

Let’s use our newly minted function to calculate a score bonus. First, we initialize bonus to 0. Then we check if the player has any remaining relays with an if-statement. If they do, we call our exponent function, passing in relays and level, which calculates relays to the power of level, and returns the result, which we save into bonus. This bonus calculating code might be useful later, so let’s wrap it up as a function too! Yes, a function that calls a function! And then, wait for it…. we can use this function in an even more complex function. Let’s write one that gets called everytime the player finishes a level. We’ll call it “levelFinished” - it needs to know the number of relays left, what level it was, and the current score; those values have to get passed in. Inside our function, we’ll calculate the bonus, using our calcBonus function, and add that to the running score. Also, if the current score is higher than the game’s high score, we save the new high score and the players name. Finally, we return the current score. Now we’re getting pretty fancy. Functions are calling functions are calling functions! When we call a single line of code, like this the complexity is hidden. We don’t see all the internal loops and variables, we just see the result come back as if by magic…. a total score of 53. But it’s not magic, it’s the power of abstraction! If you understand this example, then you understand **the power of functions, and the entire essence of modern programming**. 

> 让我们使用我们新的函数来计算得分奖金。首先，我们将奖金初始化为0。然后我们用if语句检查玩家是否还有剩余的继电器。如果他们有，我们就调用我们的指数函数，输入relays和level，计算relays到level的幂，并返回结果，我们将其保存到奖金中。这段计算奖金的代码以后可能会很有用，所以我们也把它作为一个函数来包装一下吧 是的，一个调用函数的函数! 然后，请等待....，我们可以在一个更复杂的函数中使用这个函数。让我们写一个在玩家完成一个关卡时被调用的函数。我们把它叫做 "levelFinished"--它需要知道还剩下多少个继电器，是哪一关，以及当前的分数；这些值必须被传进去。在我们的函数中，我们将使用我们的calcBonus函数来计算奖金，并将其添加到运行分数中。此外，如果当前分数高于游戏的高分，我们将保存新的高分和玩家的名字。最后，我们返回当前分数。现在我们变得很花哨了。函数在调用函数在调用函数! 当我们调用一行代码时，像这样的复杂性是隐藏的。我们看不到所有的内部循环和变量，我们只看到结果回来了，就像变魔术一样....，总分是53分。但这并不是魔术，而是抽象的力量! 如果你理解了这个例子，那么你就理解了**函数的力量，以及现代编程的全部本质**。

It’s not feasible to write, for example, a web browser as one gigantically long list of statements. It would be millions of lines long and impossible to comprehend! So instead, software consists of thousands of smaller functions, each responsible for different features. In modern programming, it’s uncommon to see functions longer than around 100 lines of code, because by then, there’s probably something that should be pulled out and made into its own function. **Modularizing programs into functions** not only allows a single programmer to write an entire app, but also allows teams of people to work efficiently on even bigger programs. Different programmers can work on different functions, and if everyone makes sure their code works correctly, then when everything is put together, the whole program should work too! 

> 例如，将一个网络浏览器写成一个巨大的长的语句列表是不可行的。它将会有几百万行之长，而且不可能被理解！因此，软件由成千上万的小函数组成。因此，软件是由成千上万的小函数组成的，每个函数负责不同的功能。在现代编程中，很少看到超过100行代码的函数，因为到那时，可能有一些东西应该被拉出来，做成自己的函数。**将程序模块化为函数**，不仅可以让一个程序员编写整个应用程序，而且还可以让团队有效地工作在更大的程序上。不同的程序员可以在不同的函数上工作，如果每个人都能确保他们的代码正常工作，那么当所有东西放在一起时，整个程序也应该是正常的！

And in the real world, programmers aren’t wasting time writing things like exponents. Modern programming languages come with huge bundles of pre-written functions, called **[Libraries](https://en.wikipedia.org/wiki/Library_(computing))**. These are written by expert coders, made efficient and rigorously tested, and then given to everyone. There are libraries for almost everything, including networking, graphics, and sound -- topics we’ll discuss in future episodes. But before we get to those, we need to talk about Algorithms. Intrigued? You should be. I’ll see you next week. 

> 而在现实世界中，程序员不会浪费时间去写指数之类的东西。现代编程语言带有大量的预写函数，称为**[库](https://en.wikipedia.org/wiki/Library_(computing))**。这些都是由专业的编码员编写的，效率很高，经过严格的测试，然后给大家使用。几乎所有的东西都有库，包括网络、图形和声音--我们将在未来的节目中讨论这些话题。但在我们讨论这些之前，我们需要先谈谈算法。感到好奇吗？你应该很感兴趣。下周见。



## #13 Intro to algorithms

<iframe width="560" height="315" src="https://www.youtube.com/embed/rL8X2mlNHPM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science! Over the past two episodes, we got our first taste of programming in a high-level language, like Python or Java. We talked about different types of programming language statements – like assignments, ifs, and loops – as well as putting statements into functions that perform a computation, like calculating an exponent. Importantly, the function we wrote to calculate exponents is only one possible solution. There are other ways to write this function – using different statements in different orders – that achieve exactly the same numerical result. The difference between them is the **[algorithm](https://en.wikipedia.org/wiki/Algorithm)**, that is the specific steps used to complete the computation. Some algorithms are better than others even if they produce equal results. Generally, the fewer steps it takes to compute, the better it is, though sometimes we care about other factors, like how much memory it uses. The term algorithm comes from Persian polymath Muḥammad ibn Mūsā al-Khwārizmī who was one of the fathers of algebra more than a millennium ago. The crafting of efficient algorithms – a problem that existed long before modern computers – led to a whole science surrounding computation, which evolved into the modern discipline of… you guessed it! Computer Science! 

> 在过去的两期节目中，我们第一次尝到了用高级语言编程的滋味，如Python或Java。我们谈到了不同类型的编程语言语句--如赋值、ifs和循环--以及将语句放入进行计算的函数中，如计算指数。重要的是，我们写的计算指数的函数只是一种可能的解决方案。还有其他写这个函数的方法--以不同的顺序使用不同的语句--可以达到完全相同的数字结果。它们之间的区别在于**[算法](https://en.wikipedia.org/wiki/Algorithm)**，也就是用于完成计算的具体步骤。有些算法比其他算法更好，即使它们产生相同的结果。一般来说，计算的步骤越少，就越好，尽管有时我们会关心其他因素，比如它使用多少内存。算法一词来自波斯多面手穆罕默德-伊本-穆萨-哈瓦兹米，他是一千多年前代数之父之一。高效算法的制作--这个问题早在现代计算机之前就存在了--导致了一门围绕着计算的完整科学，它演变成了现代的学科......你猜对了! 计算机科学! 

INTRO One of the most storied algorithmic problems in all of computer science is **sorting**… as in sorting names or sorting numbers. Computers sort all the time. Looking for the cheapest airfare, arranging your email by most recently sent, or scrolling your contacts by last name -- those all require sorting. You might think “sorting isn’t so tough… how many algorithms can there possibly be?” The answer is: a lot. Computer Scientists have spent decades inventing algorithms for sorting, with cool names like Bubble Sort and Spaghetti Sort. Let’s try sorting! 

> 在所有的计算机科学中，最具有传奇色彩的算法问题之一是**排序**......如对名字的排序或对数字的排序。计算机一直在进行排序。寻找最便宜的机票，按最近发送的邮件排列，或按姓氏滚动你的联系人--这些都需要进行排序。你可能会想："排序并不难......可能有多少种算法？" 答案是：很多。计算机科学家们已经花了几十年的时间来发明排序的算法，其名字很酷，如泡沫排序和意大利面条排序。让我们试试排序吧！

Imagine we have a set of airfare prices to Indianapolis. We’ll talk about how data like this is represented in memory next week, but for now, a series of items like this is called an array. Let’s take a look at these numbers to help see how we might sort this programmatically. We’ll start with a simple algorithm. First, let’s scan down the array to find the smallest number. Starting at the top with 307. It’s the only number we’ve seen, so it’s also the smallest. The next is 239, that’s smaller than 307, so it becomes our new smallest number. Next is 214, our new smallest number. 250 is not, neither is 384, 299, 223 or 312. So we’ve finished scanning all numbers, and 214 is the smallest. To put this into ascending order, we swap 214 with the number in the top location. Great. We sorted one number! 

> 想象一下，我们有一组到印第安纳波利斯的机票价格。我们下周会讨论像这样的数据如何在内存中表示，但现在，像这样的一系列项目被称为数组。让我们来看看这些数字，以帮助我们了解如何以编程方式对其进行排序。我们将从一个简单的算法开始。首先，让我们向下扫描数组，找到最小的数字。从顶部的307开始。这是我们唯一看到的数字，所以它也是最小的。接下来是239，它比307小，所以它成为我们新的最小的数字。接下来是214，我们新的最小的数字。250不是，384、299、223和312也不是。所以我们已经完成了对所有数字的扫描，214是最小的。为了把这个数字放到升序中，我们把214和最上面的数字交换。太好了。我们对一个数字进行了排序! 
>

Now we repeat the same procedure, but instead of starting at the top, we can start one spot below. First we see 239, which we save as our new smallest number. Scanning the rest of the array, we find 223 is the next smallest, so we swap this with the number in the second spot. Now we repeat again, starting from the third number down. This time, we swap 239 with 307. This process continues until we get to the very last number, and voila, the array is sorted and you’re ready to book that flight to Indianapolis! The process we just walked through is one way – or one algorithm – for sorting an array. It’s called **Selection Sort** -- and it’s pretty basic. 

> 现在我们重复同样的程序，但不是从顶部开始，而是从下面一个位置开始。首先我们看到239，我们把它作为我们新的最小的数字。扫描数组的其余部分，我们发现223是下一个最小的数字，所以我们将其与第二个位置的数字交换。现在我们再次重复，从第三个数字开始。这一次，我们将239与307互换。这个过程一直持续到最后一个数字，瞧，数组已经分类了，你已经准备好预订飞往印第安纳波利斯的航班了 我们刚刚经历的过程是对数组进行排序的一种方法，或者说是一种算法。它被称为**选择排序**--它是非常基本的。

pseudo-code of selection sort:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\selection sort.jpg)

Here’s the pseudo-code. This function can be used to sort 8, 80, or 80 million numbers - and once you've written the function, you can use it over and over again. With this sort algorithm, we loop through each position in the array, from top to bottom, and then for each of those positions, we have to loop through the array to find the smallest number to swap. You can see this in the code, where one FOR loop is nested inside of another FOR loop. This means, very roughly, that if we want to sort N items, we have to loop N times, inside of which, we loop N times, for a grand total of roughly N times N loops... Or N squared. **This relationship of input size to the number of steps the algorithm takes to run characterizes the complexity of the Selection Sort algorithm. It gives you an approximation of how fast, or slow, an algorithm is going to be**. Computer Scientists write this order of growth in something known as – no joke – “**big O notation**”. N squared is not particularly efficient. Our example array had n = 8 items, and 8 squared is 64. If we increase the size of our array from 8 items to 80, the running time is now 80 squared, which is 6,400. So although our array only grew by 10 times - from 8 to 80 – the running time increased by 100 times – from 64 to 6,400! This effect magnifies as the array gets larger. That’s a big problem for a company like Google, which has to sort arrays with millions or billions of entries. So, you might ask, as a burgeoning computer scientist, is there a more efficient sorting algorithm? 

> 这里是伪代码。这个函数可以用来对8个、80个或8000万个数字进行排序--一旦你写好了这个函数，你就可以反复使用它。通过这种排序算法，我们在数组中从上到下循环每个位置，然后对于每个位置，我们都要在数组中循环寻找最小的数字来交换。你可以在代码中看到这一点，一个FOR循环嵌套在另一个FOR循环中。这意味着，非常粗略，如果我们想对N个项目进行排序，我们必须循环N次，在这些循环中，我们又循环N次，总的来说是N次N次的循环...... 或N的平方。**这种输入大小与算法运行步骤的关系表征了选择排序算法的复杂性。它给你一个算法的快慢的近似值**。计算机科学家用一种被称为--不是玩笑--"**大O符号** "的东西来写这个增长顺序。O(N^2^)并不是特别有效。我们的例子数组有n=8项，O(8^2^)是64。如果我们把数组的大小从8项增加到80项，现在的运行时间是O(80^2^)，也就是6400。因此，尽管我们的数组只增加了10倍--从8到80，但运行时间却增加了100倍--从64到6400！这种影响随着数组的增大而放大。这种影响会随着数组的变大而放大。对于像谷歌这样的公司来说，这是一个大问题，因为它必须对有数百万或数十亿条目的数组进行排序。因此，作为一个未来的计算机科学家你可能会问，是否有更有效的排序算法？

Let’s go back to our old, unsorted array and try a different algorithm, **merge sort**. The first thing merge sort does is check if the size of the array is greater than 1. If it is, it splits the array into two halves. Since our array is size 8, it gets split into two arrays of size 4. These are still bigger than size 1, so they get split again, into arrays of size 2, and finally they split into 8 arrays with 1 item in each. Now we are ready to merge, which is how “merge sort” gets its name. Starting with the first two arrays, we read the first – and only – value in them, in this case, 307 and 239. 239 is smaller, so we take that value first. The only number left is 307, so we put that value second. We’ve successfully merged two arrays. We now repeat this process for the remaining pairs, putting them each in sorted order. Then the merge process repeats. Again, we take the first two arrays, and we compare the first numbers in them. This time its 239 and 214. 214 is lowest, so we take that number first. Now we look again at the first two numbers in both arrays: 239 and 250. 239 is lower, so we take that number next. Now we look at the next two numbers: 307 and 250. 250 is lower, so we take that. Finally, we’re left with just 307, so that gets added last. In every case, we start with two arrays, each individually sorted, and merge them into a larger sorted array. We repeat the exact same merging process for the two remaining arrays of size two. Now we have two sorted arrays of size 4. Just as before, we merge, comparing the first two numbers in each array, and taking the lowest. We repeat this until all the numbers are merged, and then our array is fully sorted again! 

> 让我们回到我们的旧的、未排序的数组，尝试一种不同的算法，**合并排序**。合并排序的第一件事是检查数组的大小是否大于1，如果是，它将数组分成两半。由于我们的数组大小为8，它被分割成两个大小为4的数组。这些数组仍然大于1的大小，所以它们又被分割成2的数组，最后它们被分割成8个数组，每个数组中只有一个项目。现在我们准备进行合并，这就是 "合并排序 "的名称由来。从前两个数组开始，我们读取其中的第一个--也是唯一的--值，在本例中，307和239。239比较小，所以我们先读取这个值。唯一剩下的数字是307，所以我们把这个值放在第二个。我们已经成功地合并了两个数组。现在我们对剩下的数组重复这个过程，把它们分别按排序顺序排列。然后重复合并的过程。再次，我们取前两个数组，并比较其中的第一个数字。这一次是239和214。214是最低的，所以我们先取这个数字。现在我们再看看两个数组中的前两个数字：239和250。239是较低的，所以我们接下来取这个数字。现在我们看下两个数字。307和250。250更低，所以我们取它。最后，我们只剩下307，所以最后再加上它。在每一种情况下，我们都从两个数组开始，每个数组都是单独排序的，然后将它们合并成一个更大的排序数组。我们对剩下的两个大小为2的数组重复完全相同的合并过程。现在我们有两个大小为4的排序数组。就像之前一样，我们进行合并，比较每个数组中的前两个数字，并取其最低的。我们重复这个过程，直到所有的数字都被合并，然后我们的数组又被完全排序了! 
>

The bad news is: no matter how many times we sort these, you’re still going to have to pay $214 to get to Indianapolis. Anyway, the “Big O” computational complexity of merge sort is N times the Log of N. The N comes from the number of times we need to compare and merge items, which is directly proportional to the number of items in the array. The Log N comes from the number of merge steps. In our example, we broke our array of 8 items into 4, then 2, and finally 1. That’s 3 splits. Splitting in half repeatedly like this has a logarithmic relationship with the number of items - trust me! Log base 2 of 8 equals 3 splits. If we double the size of our array to 16 – that's twice as many items to sort – it only increases the number of split steps by 1 since log base 2 of 16 equals 4. Even if we increase the size of the array more than a thousand times, from 8 items to 8000 items, the number of split steps stays pretty low. Log base 2 of 8000 is roughly 13. That's more, but not much more than 3 -- about four times larger – and yet we’re sorting a lot more numbers. For this reason, merge sort is much more efficient than selection sort. And now I can put my ceramic cat collection in name order MUCH faster! 

> 坏消息是：无论我们对这些东西进行多少次排序，你还是得花214美元才能到印第安纳波利斯。总之，合并排序的 "大O "计算复杂度是O(Nlog<sub>2</sub>^N^)。N来自我们需要比较和合并项目的次数，这与数组中的项目数量**成正比**。对数N来自于合并步骤的数量。在我们的例子中，我们把8个项目的数组分成4个，然后是2个，最后是1个，这就是3次分割。像这样反复地分割成两半，与项目的数量有对数关系--相信我 log<sub>2</sub>^8^等于3次分割。如果我们把数组的大小增加一倍到16--这是要排序的项目数量的两倍--它只增加了1个分割步骤，因为log<sub>2</sub>^16^等于4。即使我们将数组的大小增加一千倍以上，从8个项目增加到8000个项目，分割步骤的数量也保持相当低。log<sub>2</sub>^8000^大约是13。这就更多了，但并不比3多多少--大约大四倍--但我们却要分拣更多的数字。由于这个原因，合并排序比选择排序要有效得多。现在我可以把我的陶瓷猫收藏品按名字顺序排列，速度快多了! 
>

There are literally dozens of sorting algorithms we could review, but instead, I want to move on to my other favorite category of classic algorithmic problems: **graph search**! A graph is a network of nodes connected by lines. You can think of it like a map, with cities and roads connecting them. Routes between these cities take different amounts of time. We can label each line with what is called a cost or weight. In this case, it’s weeks of travel. Now let’s say we want to find the fastest route for an army at Highgarden to reach the castle at Winterfell. The simplest approach would just be to try every single path exhaustively and calculate the total cost of each. That’s a brute force approach. We could have used a brute force approach in sorting, by systematically trying every permutation of the array to check if it’s sorted. This would have an N factorial complexity - that is the number of nodes, times one less, times one less than that, and so on until 1. Which is way worse than even N squared. 

> 我们可以回顾几十种排序算法，但相反，我想转到我最喜欢的另一类经典算法问题上。**图搜索**! 图是一个由线连接的节点网络。你可以把它想象成一张地图，有城市和道路连接它们。这些城市之间的路线需要不同的时间。我们可以给每条线贴上所谓的成本或权重。在这种情况下，它是几周的旅行。现在，假设我们想找到高庭的军队到达临冬城城堡的最快路线。最简单的方法就是详尽地尝试每一条路径，并计算每一条的总成本。这就是一种蛮力方法。我们可以在排序中使用蛮力方法，通过系统地尝试数组的每一个排列组合来检查它是否被排序。这将有一个O(N!)复杂度--也就是节点的数量N，乘以(N-1)，乘以(N-2)，以此类推，直到1。这甚至比O(N^2^)还要糟糕。

But, we can be way more clever! The classic algorithmic solution to this graph problem was invented by one of the greatest minds in computer science practice and theory, Edsger Dijkstra, so it’s appropriately named **Dijkstra's algorithm**. We start in Highgarden with a cost of 0, which we mark inside the node. For now, we mark all other cities with question marks - we don’t know the cost of getting to them yet. Dijkstra's algorithm always starts with the node with lowest cost. In this case, it only knows about one node, Highgarden, so it starts there. It follows all paths from that node to all connecting nodes that are one step away, and records the cost to get to each of them. That completes one round of the algorithm. We haven’t encountered Winterfell yet, so we loop and run Dijkstra's algorithm again. With Highgarden already checked, the next lowest cost node is King's Landing. Just as before, we follow every unvisited line to any connecting cities. The line to The Trident has a cost of 5. However, we want to keep a running cost from Highgarden, so the total cost of getting to The Trident is 8 plus 5, which is 13 weeks. Now we follow the offroad path to Riverrun, which has a high cost of 25, for a total of 33. But we can see inside of Riverrun that we’ve already found a path with a lower cost of just 10. So we disregard our new path, and stick with the previous, better path. We’ve now explored every line from King's Landing and didn’t find Winterfell, so we move on. The next lowest cost node is Riverrun, at 10 weeks. First we check the path to The Trident, which has a total cost of 10 plus 2, or 12. That’s slightly better than the previous path we found, which had a cost of 13, so we update the path and cost to The Trident. There is also a line from Riverrun to Pyke with a cost of 3. 10 plus 3 is 13, which beats the previous cost of 14, and so we update Pyke's path and cost as well. That’s all paths from Riverrun checked... so... you guessed it, Dijkstra's algorithm loops again. The node with the next lowest cost is The Trident and the only line from The Trident that we haven’t checked is a path to Winterfell! It has a cost of 10, plus we need to add in the cost of 12 it takes to get to The Trident, for a grand total cost of 22. We check our last path, from Pyke to Winterfell, which sums to 31. Now we know the lowest total cost, and also the fastest route for the army to get there, which avoids King’s Landing! 

> 但是，我们可以做得更聪明! 这个图问题的经典算法解决方案是由计算机科学实践和理论中最伟大的人物之一Edsger Dijkstra发明的，所以它被恰当地命名为**Dijkstra算法**。我们从高庭开始，成本为0，我们将其标记在节点内。现在，我们用问号标记所有其他城市--我们还不知道到达它们的成本。Dijkstra的算法总是从成本最低的节点开始。在这种情况下，它只知道一个节点，即Highgarden，所以它从那里开始。它沿着所有从该节点到所有一步之遥的连接节点的路径，并记录到达每个节点的成本。这就完成了一轮的算法。我们还没有遇到临冬城，所以我们再次循环并运行Dijkstra算法。由于已经检查过高庭，下一个成本最低的节点是君临城。就像以前一样，我们沿着每条未访问过的线路去寻找任何连接城市。到三叉戟的线路成本为5。然而，我们想保持从高庭出发的运行成本，所以到达三叉戟的总成本是8加5，也就是13周。现在我们沿着越野路去奔流城，它的成本很高，为25，总共是33。但我们可以看到在奔流城内，我们已经找到了一条成本较低的道路，只有10周。所以我们不考虑我们的新路，而坚持走之前那条更好的路。我们现在已经探索了从君临城出发的每一条线路，但没有找到临冬城，所以我们继续前进。下一个成本最低的节点是奔流城，为10周。首先我们查看通往三叉戟的路径，总成本为10加2，即12。这比我们之前找到的路径要好一点，后者的成本是13，所以我们更新了到三叉戟的路径和成本。还有一条从奔流城到派克的线路，成本为3。10加3是13，比之前的14成本要高，所以我们也更新派克的路径和成本。这就是所有从Riverrun出发的路径，所以......你猜对了，Dijkstra的算法再次循环。成本最低的节点是三叉戟，而唯一一条我们没有检查过的从三叉戟出发的线路是通往临冬城的路径！它的成本是10，再加上一个 "小"。它的成本是10，再加上去三叉戟的12成本，总成本是22。我们检查最后一条路，从派克到临冬城，总成本为31。现在我们知道了最低的总成本，也是军队到达那里的最快路线，它避开了君临城。

Dijkstra's algorithm to graph problem example:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\graph search example.jpg)

Dijkstra's original algorithm, conceived in 1956, had a complexity of the number of nodes in the graph squared. And squared, as we already discussed, is never great, because it means the algorithm can’t scale to big problems - like the entire road map of the United States. Fortunately, Dijkstra's algorithm was improved a few years later to take the number of nodes in the graph, times the log of the number of nodes, PLUS the number of lines. Although this looks more complicated, it’s actually quite a bit faster. Plugging in our example graph, with 6 cities and 9 lines, proves it. Our algorithm drops from 36 loops to around 14. As with sorting, there are innumerable graph search algorithms, with different pros and cons. Every time you use a service like Google Maps to find directions, an algorithm much like Dijkstra's is running on servers to figure out the best route for you. Algorithms are everywhere and the modern world would not be possible without them. We touched only the very tip of the algorithmic iceberg in this episode, but a central part of being a computer scientist is leveraging existing algorithms and writing new ones when needed, and I hope this little taste has intrigued you to SEARCH further. I’ll see you next week. 

14：

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\14.jpg)

> Dijkstra在1956年构思的原始算法，其复杂度为图中节点数的平方，即O(N^2^)。而平方，正如我们已经讨论过的，从来都不是很好，因为它意味着该算法不能扩展到大问题--比如美国的整个路线图。幸运的是，Dijkstra的算法在几年后被改进为取图中的节点数，乘以节点数的对数，再加上线的数量。即O(Nlog^N^+L)。虽然这看起来更复杂，但实际上快了不少。插入我们的例子图，有6个城市和9条线，证明了这一点。我们的算法从36个循环下降到14个左右。与排序一样，有无数的图形搜索算法，有不同的优点和缺点。每次你使用像谷歌地图这样的服务来寻找方向时，一个很像Dijkstra的算法正在服务器上运行，为你找出最佳路线。算法无处不在，没有它们，现代世界就不可能存在。在这一集里，我们只触及了算法的冰山一角，但作为一名计算机科学家，其核心部分是利用现有的算法，并在需要时编写新的算法，我希望这一小段经历能让你对搜索产生兴趣。我们下周见。



## #14 Data structures

<iframe width="560" height="315" src="https://www.youtube.com/embed/DuDz6B4cqVc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I'm Carrie Anne, and welcome to Crash Course Computer Science! Last episode, we discussed a few example classic algorithms, like sorting a list of numbers and finding the shortest path in a graph. What we didn’t talk much about, is how the data the algorithms ran on was stored in computer memory. You don’t want your data to be like John Green’s college dorm room, with food, clothing and papers strewn everywhere. Instead, we want our data to be structured, so that it’s organized, allowing things to be easily retrieved and read. For this, computer scientists use **[Data Structures](https://en.wikipedia.org/wiki/Data_structure)**! 

> 上一集，我们讨论了几个经典算法的例子，如对数字列表进行排序和寻找图形中最短路径。上一集，我们讨论了几个经典算法的例子，比如对数字列表进行排序和寻找图形中的最短路径。我们没有多谈的是，这些算法运行的数据是如何存储在计算机内存中的。你不希望你的数据像约翰-格林的大学宿舍一样，食物、衣服和文件散落一地。相反，我们希望我们的数据是结构化的，这样它就会被组织起来，让事情容易被检索和阅读。为此，计算机科学家使用**[数据结构](https://en.wikipedia.org/wiki/Data_structure)**! 
>

INTRO We already introduced one basic data structure last episode, **Arrays**, also called lists or Vectors in some languages. These are a series of values stored in memory. So instead of just a single value being saved into a variable, like ‘j equals 5’, we can define a whole series of numbers, and save that into an array variable. To be able to find a particular value in this array, we have to specify an index. Almost all programing languages start arrays at index 0, and use a square bracket syntax to denote array access. So, for example, if we want to add the values in the first and third spots of our array ‘j’, and save that into a variable ‘a’, we would write a line of code like this. How an array is stored in memory is pretty straightforward. For simplicity, let’s say that the compiler chose to store ours at memory location 1,000. The array contains 7 numbers, and these are stored one after another in memory, as seen here. So when we write “j index of 0”, the computer goes to memory location 1,000, with an offset of 0, and we get the value 5. If we wanted to retrieve “j index of 5”, our program goes to memory location 1000, plus an offset of 5, which in this case, holds a value of 4. It’s easy to confuse the fifth number in the array with the number at index 5. They are not the same. Remember, the number at index 5 is the 6th number in the array because the first number is at index 0. Arrays are extremely versatile data structures, used all the time, and so there are many functions that can handle them to do useful things. For example, pretty much every programming language comes with a built-in sort function, where you just pass in your array, and it comes back sorted. So there’s no need to write that algorithm from scratch. 

> 引言 我们在上一集已经介绍了一种基本的数据结构，**数组**，在某些语言中也称为列表或矢量。这些是存储在内存中的一系列数值。因此，我们可以定义一连串的数字，并将其保存在一个数组变量中，而不是仅仅将一个单一的值保存在一个变量中，例如 "j等于5"。为了能够在这个数组中找到一个特定的值，我们必须指定一个索引。几乎所有的编程语言都从索引0开始，并使用方括号语法来表示数组访问。因此，举例来说，如果我们想把数组 "j "中的第一个和第三个位置的值相加，并将其保存到变量 "a "中，我们会写下这样一行代码。数组如何存储在内存中是非常简单的。为了简单起见，我们假设编译器选择将我们的数组存储在内存位置1,000。数组包含7个数字，这些数字在内存中一个接一个地存储，如图所示。因此，当我们写下 "j索引为0 "时，计算机会进入内存位置1,000，偏移量为0，而我们得到的数值为5。如果我们想检索 "j索引为5"，我们的程序就会进入内存位置1000，加上5的偏移量，在这种情况下，内存中的值为4。它们是不一样的。记住，索引5的数字是数组中的第6个数字，因为第一个数字是在索引0。数组是非常通用的数据结构，一直在使用，因此有很多函数可以处理它们，做一些有用的事情。例如，几乎每一种编程语言都有一个内置的排序功能，你只需传入数组，它就会回来排序。所以没有必要从头开始写这种算法。

array in memory:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\array.jpg)

Very closely related are **Strings**, which are just arrays of characters, like letters, numbers, punctuation and other written symbols. We talked about how computers store characters way back in Episode 4. Most often, to save a string into memory, you just put it in quotes, like so. Although it doesn’t look like an array, it is. Behind the scenes, the memory looks like this. Note that the string ends with a zero in memory. It’s not the character zero, but the binary value 0. This is called the **null character**, and denotes the end of the string in memory. This is important because if I call a function like “print quote”, which writes the string to the screen, it prints out each character in turn starting at the first memory location, but it needs to know when to stop! Otherwise, it would print out every single thing in memory as text. The zero tells string functions when to stop. Because computers work with text so often, there are many functions that specifically handle strings. For example, many programming languages have a string concatenation function, or “strcat”, which takes in two strings, and copies the second one to the end of the first. 

> 与此密切相关的是**字符串**，它只是字符的数组，如字母、数字、标点符号和其他书面符号。我们在第4章中谈到了计算机如何存储字符。大多数情况下，要把一个字符串保存到内存中，你只需把它放在引号中，就像这样。虽然它看起来不像是一个数组，但它确实是。在幕后，内存看起来像这样。注意，字符串在内存中以零结尾。这不是零这个字符，而是二进制值0。这被称为**空字符**，表示内存中字符串的结束。这一点很重要，因为如果我调用 "print quote "这样的函数，将字符串写到屏幕上，它会从第一个内存位置开始，依次打印出每个字符，但它需要知道何时停止！否则，它将打印出每一个字符。否则，它将把内存中的每一个东西都打印成文本。零告诉字符串函数何时停止。因为计算机经常与文本打交道，所以有许多函数专门处理字符串。例如，许多编程语言都有一个字符串连接函数，即 "strcat"，它接收两个字符串，并将第二个字符串复制到第一个字符串的末尾。

string in memory:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\string.jpg)

We can use arrays for making one dimensional lists, but sometimes you want to manipulate data that is two dimensional, like a grid of numbers in a spreadsheet, or the pixels on your computer screen. For this, we need a **Matrix**. You can think of a Matrix as an array of arrays! So a 3 by 3 matrix is really an array of size 3, with each index storing an array of size 3. We can initialize a matrix like so. In memory, this is packed together in order like this. To access a value, you need to specify two indexes, like “J index of 2, then index of 1” - this tells the computer you’re looking for the item in subarray 2 at position 1. And this would give us the value 12. 

> 我们可以使用数组来制作一维的列表，但有时你想操作二维的数据，比如电子表格中的数字网格，或电脑屏幕上的像素。为此，我们需要一个**矩阵**。你可以把矩阵看成是一个数组的数组! 所以，一个3乘3的矩阵实际上是一个大小为3的数组，每个索引存储一个大小为3的数组。我们可以像这样初始化一个矩阵。在内存中，这是按照这样的顺序打包的。要访问一个值，你需要指定两个索引，比如 "J索引为2，然后索引为1" - 这告诉计算机你要找的是子阵列2中位置为1的项目。而这将给我们带来12这个值。

matrix of 3 arrays of size 3:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\matrix.jpg)

matrix of 3 arrays of size 3 in memory:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\index of matrix.jpg)

The cool thing about matrices is we’re not limited to 3 by 3 -- we can make them any size we want -- and we can also make them any number of dimensions we want. For example, we can create a five dimensional matrix and access it like this. That’s right, you now know how to access a five dimensional matrix -- tell your friends! 

> 矩阵最酷的地方在于，我们并不局限于3乘3 -- 我们可以让它们变成我们想要的任何大小 -- 我们也可以让它们变成我们想要的任何维数。例如，我们可以创建一个五维的矩阵，并像这样访问它。没错，你现在知道如何访问一个五维矩阵了--告诉你的朋友吧！

access five dimensional matrix:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\access five dimensions matrix.jpg)

So far, we’ve been storing individual numbers or letters into our arrays or matrices. But often it’s useful to store a block of related variables together. Like, you might want to store a bank account number along with its balance. Groups of variables like these can be bundled together into a **Struct**. Now we can create variables that aren’t just single numbers, but are compound data structures, able to store several pieces of data at once. We can even make arrays of structs that we define, which are automatically bundled together in memory. If we access, for example, J index of 0, we get back the whole struct stored there, and we can pull the specific account number and balance data we want. This array of structs, like any other array, gets created at a fixed size that can’t be enlarged to add more items. Also, arrays must be stored in order in memory, making it hard to add a new item to the middle. But, the struct data structure can be used for building more complicated data structures that avoid these restrictions. Let’s take a look at this struct that’s called a “**node**”. 

> 到目前为止，我们一直在将单个数字或字母存储到我们的数组或矩阵中。但通常情况下，将一组相关的变量存储在一起是很有用的。比如，你可能想存储一个银行账户的号码和它的余额。像这样的变量组可以被捆绑在一起变成一个**Struct**。现在我们可以创建的变量不仅仅是单一的数字，而是复合的数据结构，能够同时存储几块数据。我们甚至可以把我们定义的结构体做成数组，在内存中自动捆绑起来。例如，如果我们访问J索引为0的结构，我们会得到存储在那里的整个结构，我们可以拉出我们想要的特定账号和余额数据。这个结构数组和其他数组一样，被创建时有一个固定的大小，不能被扩大以增加更多的项目。而且，数组必须在内存中按顺序存储，因此很难在中间添加新的项目。但是，struct数据结构可以用来构建更复杂的数据结构，避免这些限制。让我们来看看这个被称为 "**节点**"的结构。

It stores a variable, like a number, and also a **pointer**. A pointer is a special variable that points, hence the name, to a location in memory. Using this struct, we can create a **linked list**, which is a flexible data structure that can store many nodes. It does this by having each node point to the next node in the list. Let’s imagine we have three node structs saved in memory, at locations 1000, 1002 and 1008. They might be spaced apart, because they were created at different times, and other data can sit between them. So, you see that the first node contains the value 7, and the location 1008 in its “next” pointer. This means that the next node in the linked list is located at memory location 1008. Looking down the linked list, to the next node, we see it stores the value 112 and points to another node at location 1002. If we follow that, we find a node that contains the value 14 and points back to the first node at location 1000. So this linked list happened to be circular, but it could also have been terminated by using a next pointer value of 0 -- the null value -- which would indicate we’ve reached the end of the list. 

> 它存储了一个变量，像一个数字，也是一个**指针**。指针是一个特殊的变量，它指向内存中的一个位置，因此被称为指针。使用这个结构，我们可以创建一个**链表**，这是一个灵活的数据结构，可以存储许多节点。它通过让每个节点指向列表中的下一个节点来实现这一点。让我们想象一下，我们有三个节点结构保存在内存中，分别位于1000、1002和1008位置。它们可能会有一定的间隔，因为它们是在不同的时间创建的，而且其他数据也可能位于它们之间。因此，你看到第一个节点包含数值7，其 "下一个 "指针的位置为1008。这意味着链接列表中的下一个节点位于内存位置1008。向下看链接列表中的下一个节点，我们看到它存储了值112，并指向另一个节点的位置1002。如果我们沿着这个方向走，我们会发现一个包含数值14的节点，并指向位置1000的第一个节点。所以这个链表碰巧是循环的，但它也可以通过使用下一个指针值0--空值来终止，这将表明我们已经到达了列表的末端。

struct node:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\node.jpg)

circuilar linked list:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\circuilar link list.jpg)

When programmers use linked lists, they rarely look at the memory values stored in the next pointers. Instead, they can use an abstraction of a linked list, that looks like this, which is much easier to conceptualize. Unlike an array, whose size has to be pre-defined, linked lists can be dynamically extended or shortened. For example, we can allocate a new node in memory, and insert it into this list, just by changing the next pointers. Linked Lists can also easily be re-ordered, trimmed, split, reversed, and so on. Which is pretty nifty! And pretty useful for algorithms like sorting, which we talked about last week.

> 当程序员使用链表时，他们很少看存储在下一个指针中的内存值。相反，他们可以使用一个抽象的链表，看起来像这样，这在概念上要容易得多。与数组不同，数组的大小必须预先定义，而链表可以动态地扩展或缩短。例如，我们可以在内存中分配一个新的节点，并将其插入这个列表中，只需改变下一个指针即可。链接列表也可以很容易地被重新排序、修剪、分割、反转，等等。这是很有趣的! 而且对我们上周谈到的排序等算法相当有用。

linked list abstraction:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\linked list abstraction.jpg)

Owing to this flexibility, many more-complex data structures are built on top of linked lists. The most famous and universal are **queues** and **stacks**. A queue – like the line at your post office – goes in order of arrival. The person who has been waiting the longest, gets served first. No matter how frustrating it is that all you want to do is buy stamps and the person in front of you seems to be mailing 23 packages. But, regardless, this behavior is called First-In First-Out, or FIFO. That’s the first part. Not the 23 packages thing. Imagine we have a pointer, named “post office queue”, that points to the first node in our linked list. Once we’re done serving Hank, we can read Hank’s next pointer, and update our “post office queue” pointer to the next person in the line. We’ve successfully dequeued Hank -- he’s gone, done, finished. If we want to enqueue someone, that is, add them to the line, we have to traverse down the linked list until we hit the end, and then change that next pointer to point to the new person. 

> 由于这种灵活性，许多更复杂的数据结构被建立在链表之上。最著名和最普遍的是**队列**和**堆栈**。一个队列--就像在邮局排队一样--按照到达的顺序进行。等待时间最长的人，会先得到服务。不管你只想买邮票，而在你前面的人似乎在邮寄23个包裹，这多么令人沮丧。但是，无论如何，这种行为被称为先入先出，或FIFO。那是第一部分。而不是23个包裹的事情。想象一下，我们有一个指针，名为 "邮局队列"，它指向我们链接列表中的第一个节点。一旦我们为汉克服务完毕，我们就可以读取汉克的下一个指针，并将我们的 "邮局队列 "指针更新到下一个人的队列中。我们已经成功地取消了Hank的排队--他已经走了，完成了，结束了。如果我们想排队，也就是把他们加入到队伍中，我们必须在链接列表中向下遍历，直到我们到达终点，然后改变下一个指针，指向新的人。

post-office queue:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\postoffice queue.jpg)

With just a small change, we can use linked lists as **stacks**, which are LIFO… **Last-In First-Out**. You can think of this like a stack of pancakes... as you make them, you add them to the top of stack. And when you want to eat one, you take them from the top of the stack. Delicious! Instead of enqueueing and dequeuing, data is pushed onto the stack and popped from the stacks. Yep, those are the official terms! 

> 只需做一个小小的改变，我们就可以把链接列表当作**栈**来使用，它是后进先出的......**后进先出的**。你可以把它想象成一叠煎饼......当你做煎饼时，你把它们加到栈顶。当你想吃的时候，你就把它们从堆栈的顶部拿出来。味道很好! 代替enqueueing和dequeuing的是，数据被推到堆栈上并从堆栈中弹出。是的，这些都是正式的术语! 

If we update our node struct to contain not just one, but two pointers, we can build **trees**, another data structure that’s used in many algorithms. Again, programmers rarely look at the values of these pointers, and instead conceptualize trees like this: The top most node is called the root. And any nodes that hang from other nodes are called children nodes. As you might expect, nodes above children are called parent nodes. Does this example imply that Thomas Jefferson is the parent of Aaron Burr? I’ll leave that to your fanfiction to decide. And finally, any nodes that have no children -- where the tree ends -- are called Leaf Nodes. In our example, nodes can have up to two children, and for that reason, this particular data structure is called a **binary tree**. But you could just as easily have trees with three, four or any number of children by modifying the data structure accordingly. You can even have tree nodes that use linked lists to store all the nodes they point to. An important property of trees – both in real life and in data structures – is that there’s a one-way path from roots to leaves. It’d be weird if roots connected to leaves, that connected to roots. 

> 如果我们更新我们的节点结构，使其不仅仅包含一个，而是两个指针，我们就可以建立**树**，这是在许多算法中使用的另一种数据结构。同样，程序员很少看这些指针的值，而是像这样对树进行概念化。最上面的节点被称为根。而任何挂在其他节点上的节点都被称为子节点。正如你所期望的那样，子节点上面的节点被称为父节点。这个例子是否意味着托马斯-杰斐逊是亚伦-伯尔的父母？我把这个问题留给你的粉丝小说来决定。最后，任何没有子节点的节点--树的末端--被称为叶节点。在我们的例子中，节点最多可以有两个孩子，由于这个原因，这种特殊的数据结构被称为**二叉树**。但你也可以通过对数据结构进行相应的修改，轻松地拥有拥有三个、四个或任何数量的子节点的树。你甚至可以拥有使用链表来存储它们指向的所有节点的树节点。无论是在现实生活中还是在数据结构中，树的一个重要属性是，从根到叶有一条单向的路径。如果根连接到叶，叶连接到根，那就很奇怪了。

Trees: 

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\treee.png)

For data that links arbitrarily, that include things like loops, we can use a **graph data structure** instead. Remember our graph from last episode of cities connected by roads? This can be stored as nodes with many pointers, very much like a tree, but there is no notion of roots and leaves, and children and parents… Anything can point to anything! So that’s a whirlwind overview of pretty much all of the fundamental data structures used in computer science. 

> 对于那些任意链接的数据，包括像循环这样的东西，我们可以使用**图数据结构**来代替。还记得我们上一集的由道路连接的城市图吗？这可以被存储为具有许多指针的节点，非常像一棵树，但没有根和叶的概念，也没有子和父的概念......任何东西都可以指向任何东西！这就是我们的数据结构。这就是对计算机科学中使用的几乎所有基本数据结构的旋风式概述。

On top of these basic building blocks, programmers have built all sorts of clever variants, with slightly different properties -- data structures like **red-black trees** and **heaps**, which we don’t have time to cover. These different data structures have properties that are useful for particular computations. The right choice of data structure can make your job a lot easier, so it pays off to think about how you want to structure your data before you jump in. 

> 在这些基本构件的基础上，程序员们建立了各种巧妙的变体，其属性略有不同--数据结构如**红黑树**和**堆**，我们没有时间来介绍这些。这些不同的数据结构具有对特定计算有用的属性。正确的数据结构选择可以使你的工作变得更容易，所以在你跳入数据结构之前，考虑一下你想如何结构你的数据是有好处的。

Fortunately, most programming languages come with libraries packed full of ready-made data structures. For example, C++ has its Standard Template Library, and Java has the Java Class Library. These mean programmers don’t have to waste time implementing things from scratch, and can instead wield the power of data structures to do more interesting things, once again allowing us to operate at a new level of abstraction! I’ll see you next week. 

> 幸运的是，大多数编程语言的库中都充满了现成的数据结构。例如，C++有其标准模板库，Java有Java类库。这些意味着程序员不必浪费时间从头开始实现，而是可以挥舞数据结构的力量来做更多有趣的事情，再次让我们在一个新的抽象水平上进行操作 下周见。



## #15 Alan Turing

<iframe width="560" height="315" src="https://www.youtube.com/embed/7TycxwFmdB0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I'm Kerry Ann and welcome to Crash Course computer science. Over the past few episodes, we've been building up our understanding of computer science fundamentals, such as functions, algorithms and data structures. Today, we're going to take a step back and look at the person who formulated many of the theoretical concepts that underline modern computation. The father of computer science and not quite Benedict Cumberbatch lookalike [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing). 

> 大家好，我是Kerry Ann，欢迎来到计算机科学速成班。在过去的几期节目中，我们一直在积累对计算机科学基础知识的理解，比如函数、算法和数据结构。今天，我们要回过头来看看那个制定了许多支撑现代计算的理论概念的人。计算机科学之父和不太像本尼迪克特-康伯巴奇的人[艾伦-图灵](https://en.wikipedia.org/wiki/Alan_Turing)。

Alan Mathison Turing was born in London in 1912 and showed an incredible aptitude for maths and science throughout his early education. His first brush of what we now call computer science came in 1935 while he was a master's student at King's College in Cambridge. He set out to solve a problem posed by German Mathematician David Hilbert known as the **Entscheidungsproblem** or [decision problem](https://en.wikipedia.org/wiki/Decision_problem), which asked the following: is there an algorithm that takes as input a statement written in formal logic, and produces a "yes" or "no" answer that's always accurate? If such an algorithm existed, we could use it to answer questions like, "Is there a number bigger than all numbers?" No, there's not. We know the answer to that one, but there are many other questions in mathematics that we'd like to know the answer to. So if this algorithm existed, we'd want to know it. 

> 阿兰-麦吉森-图灵于1912年出生于伦敦，在他的早期教育中表现出令人难以置信的数学和科学天赋。1935年，当他还是剑桥大学国王学院的一名硕士生时，他第一次接触了我们现在所说的计算机科学。他着手解决一个由德国数学家大卫-希尔伯特（David Hilbert）提出的问题，该问题被称为 "决定问题"（**Entscheidungsproblem**），即“可判定性问题（[decision problem](https://en.wikipedia.org/wiki/Decision_problem)）”，它提出了以下问题：是否有一种算法可以将一个用形式逻辑编写的语句作为输入，并产生一个总是准确的 "是 "或 "否 "的答案？如果这样的算法存在，我们可以用它来回答诸如 "是否有一个数字比所有数字都大 "这样的问题。不，没有。我们知道这个问题的答案，但在数学中还有许多其他问题我们也想知道答案。因此，如果这个算法存在，我们会想知道它。

The American mathematician Alonzo Church first presented a solution to this problem in 1935. He developed a system of mathematical expressions called **Lambda Calculus** and demonstrated that no such universal algorithm could exist. Although Lambda Calculus was capable of representing any computation, the mathematical technique was difficult to apply and understand. At pretty much the same time on the other side of the Atlantic, Alan Turing came up with his own approach to solve the decision problem. He proposed a hypothetical computing machine, which we now call a **Turing Machine**. Turing Machines provided a simple, yet powerful mathematical model of computation. Although using totally different mathematics, they were functionally equivalent to lambda calculus in terms of their computational power. However their relative simplicity made them much more popular in the burgeoning field of computer science. In fact, they're simple enough that I'm going to explain it right now. A Turing Machine is a theoretical computing device equipped with an infinitely long memory tape which stores symbols and a device called a read/write head which can read and write, or modify, symbols on that tape. There's also a state variable in which we can hold a piece of information about the current state of the machine. And a set of rules that describes what the machine does. Given a state and the current symbol the head is reading, the rule can be to write a symbol on the tape change the state of the machine move the read/write head to the left or right by one spot or any combination of these actions. 

> 美国数学家Alonzo Church在1935年首次提出了这个问题的解决方案。他开发了一个称为**兰姆达微积分**的数学表达系统，并证明不可能存在这样的通用算法。尽管兰姆达微积分能够代表任何计算，但这种数学技术却很难应用和理解。几乎在同一时间，在大西洋的另一边，艾伦-图灵想出了他自己的方法来解决“可判定性问题”。他提出了一个假想的计算机，我们现在称之为**图灵机**。图灵机提供了一个简单而又强大的计算数学模型。虽然使用的是完全不同的数学，但就其计算能力而言，它们在功能上等同于**λ微积分**。然而，它们的相对简单性使它们在新兴的计算机科学领域更受欢迎。事实上，它们简单到我现在就来解释一下。图灵机是一种理论上的计算设备，它配备了一个存储符号的无限长的记忆带和一个被称为读/写头的设备，可以读写或修改该记忆带上的符号。还有一个状态变量，我们可以在其中保存一段关于机器当前状态的信息。还有一套规则，描述机器的工作。给定一个状态和当前读写头正在读取的符号，规则可以是在磁带上写一个符号，改变机器的状态，将读写头向左或向右移动一个位置，或这些动作的任何组合。

To make this concrete let's work through a simple example: a Turing Machine that reads a string of ones ending in a zero and computes whether there is an even number of ones. If that's true The machine will write a one to the tape and if it's false, it'll write a zero. First We need to define our Turing machine rules. If the state is even and the current symbol of the tape is one, then we update the machine state to odd and move the head to the right. On the other hand if the state is even and The current symbol is zero, which means we've reached the end of the string of ones, then we write one to the tape and change the state to halt, as in we're finished and the Turing machine has completed the computation. We also need rules for when the Turing machine is in an odd state, one rule for the symbol on the tape is a zero and another for when it is one. Lastly we need to define a Starting state, which we'll set to be even. Now we've defined the rules in the starting state of our Turing machine, which is comparable to a computer program, we can run it on some example input. 

> 为了使之具体化，让我们通过一个简单的例子：一台图灵机读取一串以0结尾的1，并计算是否有双数的1。如果是真的，机器就会在磁带上写一个1，如果是假的，就会写一个0。首先我们需要定义我们的图灵机规则。如果状态是偶数，并且磁带的当前符号是1，那么我们将机器状态更新为奇数，并将磁头向右移动。另一方面，如果状态是偶数，并且当前符号是0，这意味着我们已经到达了一串1的终点，那么我们将1写到磁带上，并将状态改为停止，因为我们已经完成了，图灵机完成了计算。我们还需要为图灵机处于奇数状态时制定规则，当磁带上的符号是0时，有一条规则，当它是1时，有一条规则。最后，我们需要定义一个起始状态，我们将其设置为偶数。现在我们已经定义了图灵机起始状态下的规则，它相当于一个计算机程序，我们可以在一些示例输入上运行它。

Let's say we store 1 1 0 onto tape. That's two ones which means there is an even number of ones, and if that's news to you, We should probably get working on crash course Math. Notice that our rules only ever move their head to the right so the rest of the tape is irrelevant. We'll leave it blank for simplicity. Our Turing machine is all ready to go. so let's start it. Our state is even and the first number we see is a one. That matches our topmost rule and so we execute the effect, which is to update the state to odd and move the read/write head to the right by one spot. Okay, now we see another one on the tape But this time our state is odd and so we execute our third rule which sets the state back to even and moves the head to the right. Now we see a 0 and our current state is even so we execute our second rule which is to write a 1 to the tape signifying that yes, it's true, there is an even number of ones, and finally the machine halts. That's how turing machines work pretty simple right so you might be wondering why there's such a big deal. 

> 比方说，我们在磁带上存储11个0。这是两个1，这意味着有一个偶数的1，如果这对你来说是新闻，我们也许应该开始学习数学速成课程。请注意，我们的规则只向右移动它们的头，所以磁带的其他部分是不相关的。为了简单起见，我们将留下空白。我们的图灵机已经准备好了，所以我们开始吧。我们的状态是偶数，我们看到的第一个数字是1。这符合我们最上面的规则，所以我们执行这个效果，就是把状态更新为奇数，并把读写头向右移动一个位置。好的，现在我们在磁带上又看到了一个1，但这次我们的状态是奇数，所以我们执行第三条规则，将状态设为偶数，并将读写头向右移动。现在我们看到了一个0，我们当前的状态是偶数，所以我们执行第二条规则，即在磁带上写一个1，表示是的，这是真的，有一个偶数的1，最后机器停止了。这就是图灵机的工作原理，非常简单，所以你可能想知道为什么会有这么大的问题。

Turing machine example:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\turing machine.jpg)

Well Turing shows that this simple hypothetical machine can perform any computation if given enough time and memory It's a general-purpose computer, our program was a simple example. But with enough Rules states and tape you could build anything - a web browser, world of warcraft - whatever! Of course it would be ridiculously inefficient, but it is theoretically possible. And that's why, as a model of computing, it's such a powerful idea. In fact in terms of what it can and cannot compute there's no computer more powerful than a turing machine. A computer that is as powerful is called **Turing complete**. Every modern computing system your laptop your smartphone and even the little computer inside your microwave and thermostat are all Turing Complete. 

> 那么图灵表明，如果有足够的时间和内存，这个简单的假设机器可以进行任何计算。 这是一个通用的计算机，我们的程序就是一个简单的例子。但如果有足够的规则状态和磁带，你可以建立任何东西--网络浏览器、魔兽世界--任何东西！当然，这将是可笑的低效率，但理论上是可能的。这就是为什么，作为一个计算模型，它是一个如此强大的想法。事实上，就其能够和不能计算的内容而言，没有任何计算机比图灵机更强大。一台同样强大的计算机被称为**图灵完备**。每一个现代计算系统，你的笔记本电脑，你的智能手机，甚至你的微波炉和恒温器里面的小计算机，都是图灵完备。

To answer Hilbert's decision problem, Turing applied these new Turing machines to an intriguing computational puzzle: **the halting problem**. Put simply this asks "Is there an algorithm that can determine, given a description of a turing machine and the input from its tape, whether the Machine will run forever or halt?" For example we know our Turing machine will halt when given the input 1 1 0. Because we literally walk through the example until it halted, but what about a more complex problem? Is there a way to figure out if the program will halt without executing it? Some programs might take years to run so it would be useful to know before we run it and wait and wait and wait and then start getting worried and wonder. and then decades later when you're old and gray control-alt-delete so much sadness. Unfortunately Turing came up with a proof that shows the halting problem was in fact unsolvable, through a clever logical contradiction. 

> 为了回答希尔伯特的“可判定性问题”，图灵将这些新的图灵机应用于一个有趣的计算难题。**停顿问题**。简单地说，这个问题问的是："是否有一种算法，可以在给定图灵机的描述和它的磁带输入时，确定该机器是永远运行还是停止？" 例如，我们知道我们的图灵机在给定输入1 1 0时将停止运行。因为我们从字面上走过这个例子，直到它停止，但对于一个更复杂的问题呢？有没有一种方法可以在不执行程序的情况下弄清程序是否会停止？有些程序可能需要几年的时间才能运行，所以在我们运行它之前就知道，然后等待，等待，等待，然后开始担心和疑惑。然后几十年后，当你老了，灰色的控制-停止-删除的时候，就会有很多悲伤。不幸的是，图灵想出了一个证明，表明停止问题实际上是无法解决的，通过一个巧妙的逻辑矛盾。
>

Let's Follow his reasoning. Imagine we have a hypothetical Turing machine that takes a description of a program and some input for his tape and always outputs either Yes, it halts or no, it doesn't and I'm going to give this machine a fun name H for Haltz. Don't worry about how it works. Let's just assume such a machine exists We're talking theory here. Turing reasoned if there existed a program Whose halting behavior was not decidable by age it would mean the halting problem is Unsolvable to find one Turing designed another Turing machine that built on top of H. If H says the program holds. Then we'll make our new machine loop forever. If the answer is no It doesn't halt, we'll have the new machine output a no and halt. In essence We're building a machine that does the opposite of what H says: halt if the program doesn't halt and run forever if the program halts. For this argument we'll also need to add a splitter to the front of our new machine, So that it accepts only one input and passes that as both the program and input into H. Let's call this new machine "Bizzaro". So far this seems like a plausible machine, right? 

> 让我们跟随他的推理。想象一下，我们有一台假设的图灵机，它接受一个程序的描述和他的磁带的一些输入，并且总是输出是的，它停止了，或者是不，它没有，我将给这台机器起一个有趣的名字H，代表停止。不要担心它是如何工作的。我们只是假设这样的机器存在，我们在这里谈论理论。图灵推断，如果存在一个程序，其停止行为在年龄上是不可解的，那就意味着停止问题是不可解的，要找到一个图灵设计了另一个图灵机，建立在H的基础上。那么我们就使我们的新机器永远循环下去。如果答案是否定的 它不停止，我们就会让新机器输出一个否定的答案并停止。本质上，我们正在建立一个与H所说的相反的机器：如果程序不停止，就停止，如果程序停止，就永远运行。为了这个论点，我们还需要在新机器的前面添加一个分割器，以便它只接受一个输入，并将其作为程序和输入传给H。到目前为止，这似乎是一个合理的机器，对吗？

Now it's going to get pretty complicated. But bear with me for a second. Look what happens when you pass Bizzaro a description of itself as the input. This means we're asking H what Bizzaro will do when asked to evaluate itself. But if H says Bizzaro halts then Bizzaro enters its infinite loop and thus doesn't halt. And if H says Bizarro doesn't halt then Bizzaro outputs a no and halts so H can't possibly decide the halting problem correctly because there is no answer. It's a paradox and this paradox means that the halting problem cannot be solved with Turing machines. Remember Turing proved that Turing machines could implement any computation, So this solution to the halting problem proves that not all problems can be solved by computation. Wow, that's some heavy stuff I might have to watch that again myself. Long story short Church and Turing showed there were limits to the ability of computers no matter how much time or memory you have there are just some problems that cannot be solved ever. 

> 现在情况会变得很复杂。但请先听我说一下。看看当你把对自己的描述传递给Bizzaro作为输入时会发生什么。这意味着我们在问H，当要求Bizzaro评估自己时，Bizzaro会做什么。但是如果H说比扎罗停止了，那么比扎罗就进入了它的无限循环，因此不会停止。而如果H说Bizarro不停止，那么Bizzaro就会输出 "不 "并停止，所以H不可能正确决定停止问题，因为没有答案。这是一个悖论，这个悖论意味着停止问题不能用图灵机解决。记得图灵证明了图灵机可以实现任何计算，所以这个对停止问题的解决方案证明了并非所有问题都可以通过计算来解决。哇，那是一些沉重的东西，我可能得自己再看一遍。长话短说，丘吉尔和图灵证明了计算机的能力是有限的，无论你有多少时间或内存，有些问题是永远无法解决的。

The concurrent efforts by Church and Turing to determine the limits of computation, and in general, formalize computability, are now called the **Church-Turing Thesis**. At this point in 1936 Turing was only 24 years old and really only just beginning his career. From 1936 through 1938 he completed a PhD at Princeton University under the guidance of Church then after graduating he returned to Cambridge. Shortly after in 1939 Britain became embroiled in World War II. Turing's genius was quickly applied to the war effort. In fact a year before the war Started he was already working part-time at the UK's government code and Cypher school which was the British code breaking group based out of Bletchley Park. One of his main efforts was figuring out how to decrypt German communications, especially those that use the **enigma machine**. In short these machines scrambled text. Like you'd type the letters H-E-L-L-O and the letters XWDBJ Would come out. This process is called **encryption**. The scrambling wasn't random. The behavior was defined by a series of re-orderable rotors on the top of the enigma machine each with 26 possible rotational positions. There was also a plug board at the front of the machine that allowed pairs of letters to be swapped in total, there were billions of possible settings. If you had your own enigma machine and you knew the correct rotor and plug board settings You could type in XWDBJ and hello would come out. In other words, you **decrypted** the message.

> 丘奇和图灵同时为确定计算的极限，以及一般而言，将可计算性形式化所做的努力，现在被称为**丘奇-图灵论文**。在1936年的这个时候，图灵只有24岁，实际上只是刚刚开始他的职业生涯。从1936年到1938年，他在丘奇的指导下完成了普林斯顿大学的博士学位，然后在毕业后回到了剑桥。不久后的1939年，英国卷入了第二次世界大战。图灵的天才很快被应用到战争中。事实上，在战争开始的前一年，他已经在英国政府的密码和赛普斯学校兼职工作，这是英国的密码破解小组，总部设在布莱切利公园。他的主要工作之一是弄清如何解密德国通信，特别是那些使用**英格玛机器**的通信。简而言之，这些机器对文本进行扰乱。比如你输入字母H-E-L-L-O，就会出现字母XWBJ。这个过程被称为**加密**，加扰并不是随机的。这种行为是由英格玛机器顶部的一系列可重新排序的转子定义的，每个转子有26个可能的旋转位置。在机器的前面还有一个插板，允许成对的字母被交换，总共有数十亿种可能的设置。如果你有自己的英格玛机器，并且你知道正确的转子和插板设置，你可以输入XWDBJ，Hello会出来。换句话说，你**解密**了这个信息。

Of course the German military wasn't sharing their enigma settings on Social Media So the allies had to break the code with billions of Rotor and plug board combinations. There was no way to check them all by hand. Fortunately for Turing, enigma machines and the people who operated them were not perfect. Like one key flaw was that a letter would never be encoded as itself as in an h was never encrypted as an h. Turing, building on earlier work by Polish code breakers, designed a special-Purpose electromechanical Computer called the **Bombe** that took advantage of this flaw. It tried lots and lots of combinations of enigma settings for a given encrypted message if the Bombe found a setting that led to a letter being encoded as itself which we know the real Enigma machine couldn't do, that combination was discarded then the machine moved on to try another combination. So Bombes were used to greatly narrow the number of Possible enigma settings. This allowed human code breakers to hone their efforts on the most probable solutions. Looking for things like common German words in fragments of decoded text. Periodically the Germans would suspect someone was decoding their communications and upgrade the enigma machine like they'd add another rotor creating many more combinations they even built entirely new encryption machines. Throughout the war Turing and his colleagues at bletchley park worked tirelessly to defeat these mechanisms and overall the intelligence gained from Decrypted German communications gave the allies an Edge in many theatres with some historians arguing it shortened the war by years. 

> 当然，德国军方没有在社交媒体上分享他们的英格玛设置，所以盟军不得不用数十亿的转子和插板组合来破解密码。没有办法用手去检查它们。对图灵来说，幸运的是，英格玛机器和操作它们的人并不完美。像一个关键的缺陷是，一个字母永远不会被编码为其本身，如h永远不会被加密为h。图灵在波兰密码破译者早期工作的基础上，设计了一个特殊用途的机电计算机，称为**Bombe**，利用这一缺陷。它为一个给定的加密信息尝试了很多很多的英格玛设置组合，如果Bombe发现一个设置导致一个字母被编码为自己，而我们知道真正的英格玛机器是做不到的，这个组合被丢弃，然后机器继续尝试另一个组合。因此，炸弹被用来大大缩小可能的英格玛设置的数量。这使得人类破译者能够将他们的努力集中在最可能的解决方案上。寻找诸如解码文本片段中的常见德语单词。德国人会定期怀疑有人在破译他们的通信，并升级英格玛机器，比如他们会增加一个转子，创造更多的组合，他们甚至建造了全新的加密机器。在整个战争期间，图灵和他在布莱切利公园的同事们不知疲倦地工作，以击败这些机制，总的来说，从解密的德国通信中获得的情报使盟军在许多战场上获得了优势，一些历史学家认为它缩短了战争的时间。

After the war turing returned to Academia and Contributed to many early electronic computing efforts like the **Manchester Mark 1** which was an early and influential stored-Program computer. But his most famous post-war contribution was to **artificial intelligence**, a field so new that it didn't even get that name until 1956. It's a huge topic So we'll get to it again in future episodes. In 1950 Turing could envision a future where computers were powerful enough to exhibit intelligence equivalent to or at least indistinguishable from that of a human. Turing postulated that a computer would deserve to be called intelligent if it could deceive a human into believing that it was human. This became the basis of a simple test now called the **Turing test**. Imagine that you are having a conversation with two different people not by voice or in person but by sending typed notes back and forth. You can ask any questions you want and you get replies. But one of those two people is actually a computer. If you can't tell which one is human and which one is a computer then the computer passes the test. There's a modern version of this test called a completely automated public Turing test to tell computers and humans apart or captcha for short. These are frequently used on the internet to prevent automated systems from doing things like posting spam on Websites. I'll admit sometimes I can't read what those squiggly things say. Does that mean I'm a computer? 

> 战后，图灵回到了学术界，为许多早期的电子计算工作做出了贡献，如**曼彻斯特马克1**，这是一台早期的、有影响力的存储程序计算机。但他战后最著名的贡献是对**人工智能**的贡献，这个领域非常新，甚至直到1956年才有这个名字。这是一个巨大的话题，所以我们将在未来的节目中再次讨论。在1950年，图灵可以设想一个未来，即计算机强大到足以表现出与人类相当或至少无法区分的智能。图灵推测，如果一台计算机能够欺骗人类，使其相信自己是人类，那么它就值得被称为智能。这成为现在被称为**图灵测试**的简单测试的基础。想象一下，你正在与两个不同的人进行对话，不是通过语音或面谈，而是通过来回发送打字的笔记。你可以问任何你想问的问题，你会得到答复。但这两个人中的一个实际上是一台电脑。如果你无法分辨哪个是人，哪个是电脑，那么电脑就能通过测试。这个测试有一个现代版本，叫做完全自动化的公共图灵测试，以区分计算机和人类，或简称为captcha。这在互联网上经常使用，以防止自动系统做一些事情，如在网站上发布垃圾邮件。我承认，有时我看不懂这些方块字的内容。这是否意味着我是一台电脑？

Normally in this series We don't delve into the personal lives of these historical figures. But in Turing's case his name has been inextricably tied to tragedy so his story is worth mentioning. Turing was gay in a time when homosexuality was illegal in the united Kingdom and much of the world. An investigation into a 1952 Burglary at his home revealed his sexual orientation to the authorities who charged him with gross indecency. Turing was convicted and given a choice between Imprisonment or probation with hormonal treatments to suppress his sexuality. He chose the latter in part to continue his academic work, but it altered his mood and personality. Although the exact circumstances will never be known, it's most widely accepted that Alan Turing took his own life by poison in 1954 He was only 41. Many things have been named in recognition of Turing's contributions to theoretical computer science. But perhaps the most prestigious among them is the **Turing award** the highest distinction in the field of computer science, equivalent to a nobel prize in Physics, chemistry or other sciences. Despite a life cut short Alan inspired the first generation of computer scientists and laid key groundwork that enabled a digital era we get to enjoy today I'll see you next week.

> 通常在这个系列中，我们不会深入研究这些历史人物的个人生活。但在图灵的案例中，他的名字已经与悲剧密不可分，所以他的故事值得一提。图灵是同性恋者，当时同性恋在英国和世界大部分地区都是非法的。在1952年对他家的一次盗窃案的调查中，当局发现了他的性取向，并以严重猥亵罪指控他。图灵被定罪，并在监禁或缓刑与荷尔蒙治疗之间做出选择，以抑制他的性行为。他选择了后者，部分是为了继续他的学术工作，但这改变了他的情绪和个性。虽然确切的情况永远不会被知道，但最被广泛接受的是，艾伦-图灵在1954年服毒自杀，当时他只有41岁。为了表彰图灵对理论计算机科学的贡献，许多东西都被命名了。但其中最负盛名的也许是**图灵奖**，这是计算机科学领域的最高荣誉，相当于物理学、化学或其他科学领域的诺贝尔奖。尽管艾伦的生命被缩短了，但他激励了第一代计算机科学家，并为我们今天能够享受的数字时代奠定了关键基础。



## #16 Software engineering

<iframe width="560" height="315" src="https://www.youtube.com/embed/O753uuutqH8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science! So we’ve talked a lot about sorting in this series and often code to sort a list of numbers might only be ten lines long, which is easy enough for a single programmer to write. Plus, it’s short enough that you don’t need any special tools – you could do it in Notepad. Really! But, a sorting algorithm isn’t a program; it’s likely only a small part of a much larger program. For example, Microsoft Office has roughly 40 millions lines of code. 40 MILLION! That’s way too big for any one person to figure out and write! To build huge programs like this, programmers use a set of tools and practices. Taken together, these form the discipline of **Software Engineering** – a term coined by engineer Margaret Hamilton, who helped NASA prevent serious problems during the Apollo missions to the moon. She once explained it this way: “It’s kind of like a root canal: you waited till the end, [but] there are things you could have done beforehand. It’s like preventative healthcare, but it’s preventative software.” 

> 在这个系列中，我们已经谈了很多关于排序的问题，通常对一个数字列表进行排序的代码可能只有十行，这对一个程序员来说是很容易写的。另外，它足够短，你不需要任何特殊的工具--你可以在记事本中完成它。真的! 但是，一个排序算法并不是一个程序；它可能只是一个更大的程序中的一小部分。例如，微软办公室大约有4000万行代码。4000万! 这对任何一个人来说都太大了，根本无法计算和编写! 为了建立像这样的庞大程序，程序员们使用了一系列的工具和实践。总的来说，这些构成了**软件工程**这个学科--这个术语是由工程师玛格丽特-汉密尔顿（Margaret Hamilton）创造的，她帮助美国国家航空航天局（NASA）在阿波罗登月任务中防止了严重的问题。她曾经这样解释。"这有点像根管治疗：你一直等到最后，[但]有些事情你本来可以事先做。这就像预防性保健，但它是预防性的软件"。

INTRO As I mentioned in episode 12, breaking big programs into smaller functions allows many people to work simultaneously. They don’t have to worry about the whole thing, just the function they’re working on. So, if you’re tasked with writing a sort algorithm, you only need to make sure it sorts properly and efficiently. However, even packing code up into functions isn’t enough. Microsoft Office probably contains hundreds of thousands of them. That’s better than dealing with 40 million lines of code, but it’s still way too many “things” for one person or team to manage. The solution is to package functions into **hierarchies**, pulling related code together into “**objects**”. For example, car’s software might have several functions related to cruise control, like setting speed, nudging speed up or down, and stopping cruise control altogether. Since they’re all related, we can wrap them up into a unified cruise control object. But, we don’t have to stop there, cruise control is just one part of the engine’s software. There might also be sets of functions that control spark plug ignition, fuel pumps, and the radiator. So we can create a “parent” Engine Object that contains all of these “children” objects. In addition to children *objects*, the engine itself might have its *own* functions. You want to be able to stop and start it, for example. It’ll also have its own variables, like how many miles the car has traveled. 

> 正如我在第12集提到的，把大程序分解成小功能，可以让很多人同时工作。他们不必担心整个事情，只需担心他们正在工作的函数。因此，如果你的任务是编写一个排序算法，你只需要确保它正确有效地排序。然而，即使把代码打包成函数也是不够的。微软办公室可能包含数十万个。这比处理4000万行代码要好得多，但对于一个人或一个团队来说，这仍然是太多的 "东西"。解决方案是将功能打包成**层次结构**，将相关的代码拉到一起成为 "**对象**"。例如，汽车软件可能有几个与巡航控制有关的功能，如设置速度，提高或降低速度，以及完全停止巡航控制。由于它们都是相关的，我们可以把它们打包成一个统一的巡航控制对象。但是，我们不必止步于此，巡航控制只是发动机软件的一个部分。可能还有一些控制火花塞点火、燃油泵和散热器的功能集。所以我们可以创建一个 "父 "引擎对象，它包含所有这些 "子 "对象。除了子*对象*之外，发动机本身也可能有它*自己的*功能。例如，你希望能够停止和启动它。它也会有自己的变量，比如汽车已经行驶了多少英里。

In general, objects can contain other objects, functions and variables. And of course, the engine is just one part of a Car Object. There’s also the transmission, wheels, doors, windows, and so on. Now, as a programmer, if I want to set the cruise control, I navigate down the object hierarchy, from the outermost objects to more and more deeply nested ones. Eventually, I reach the function I want to trigger: “Car, then engine, then cruise control, then set cruise speed to 55”. Programming languages often use something equivalent to the syntax shown here. 

> 一般来说，对象可以包含其他对象、函数和变量。当然，发动机只是汽车对象的一个部分。还有变速器、车轮、车门、车窗，等等。现在，作为一个程序员，如果我想设置巡航控制，我就沿着对象的层次结构向下导航，从最外层的对象到越来越深的嵌套对象。最终，我达到了我想要触发的功能。"汽车，然后是发动机，然后是巡航控制，然后将巡航速度设置为55"。编程语言经常使用相当于这里显示的语法的东西。

The idea of packing up functional units into nested objects is called **[Object Oriented Programming](https://en.wikipedia.org/wiki/Object-oriented_programming)**. This is very similar to what we’ve done all series long: hide complexity by encapsulating low-level details in higher-order components. **Before we packed up things like transistor circuits into higher-level boolean gates**. **Now we’re doing the same thing with software**. Yet again, it’s a way to move up a new level of abstraction! 

> 将功能单元打包成嵌套对象的想法被称为**[面向对象的编程](https://en.wikipedia.org/wiki/Object-oriented_programming)**。这与我们在整个系列中所做的非常相似：通过将低层次的细节封装在高阶组件中来隐藏复杂性。**以前我们把晶体管电路等东西打包成高阶布尔门**。**现在我们也在对软件做同样的事情**。然而，这又是一种提升新的抽象层次的方法 ！
>

Breaking up a big program, like a car’s software, into functional units is perfect for teams. One team might be responsible for the cruise control system, and a single programmer on that team tackles a handful of functions. This is similar to how big, physical things are built, like skyscrapers. You’ll have electricians running wires, plumbers fitting pipes, welders welding, painters painting, and hundreds of other people teeming all over the hull. They work together on different parts simultaneously, leveraging their different skills. Until one day, you’ve got a whole working building! 

> 将一个大的程序，如汽车的软件，分解成不同的功能单元，对团队来说是完美的。一个团队可能负责巡航控制系统，该团队的一个程序员负责处理少数几个功能。这类似于大型实体事物的建造方式，如摩天大楼。你会有电工铺设电线，水管工安装管道，电焊工焊接，油漆工喷漆，以及数以百计的其他人员充斥在整个船体上。他们同时在不同的部位工作，利用他们不同的技能。直到有一天，你已经有了一个完整的工作建筑! 

But, returning to our cruise control example… its code is going to have to make use of functions in other parts of the engine’s software, to, you know, keep the car at a constant speed. That code isn’t part of the cruise control team’s responsibility. It’s another team’s code. Because the cruise control team didn’t write that, they’re going to need good **documentation** about what each function in the code does, and a well-defined **[Application Programming Interface](https://en.wikipedia.org/wiki/API#Libraries_and_frameworks)** -- or API for short. You can think of an API as the way that collaborating programmers interact across various parts of the code. For example, in the IgnitionControl object, there might be functions to set the RPM of the engine, check the spark plug voltage, as well as fire the individual spark plugs. Being able to set the motor’s RPM is really useful, the cruise control team is going to need to call that function. But, they don’t know much about how the ignition system works. It’s not a good idea to let them call functions that fire the individual spark plugs. Or the engine might explode! Maybe. The API allows the right people access to the right functions and data. Object Oriented Programming languages do this by letting you specify whether functions are **public or private**. If a function is marked as “private”, it means only functions inside that object can call it. So, in this example, only other functions inside of IgnitionControl, like the setRPM function, can fire the sparkplugs. On the other hand, because the setRPM function is marked as public, other objects can call it, like cruise control. This ability to hide complexity, and selectively reveal it, is the essence of Object Oriented Programming, and it’s a powerful and popular way to tackle building large and complex programs. 

> 但是，回到我们的巡航控制的例子......它的代码将不得不利用发动机软件的其他部分的功能，你知道，使汽车保持恒定的速度。这些代码不属于巡航控制团队的责任。它是另一个团队的代码。因为巡航控制团队没有写这些代码，他们将需要关于代码中每个功能的良好**文档**，以及一个定义明确的**[应用编程接口](https://en.wikipedia.org/wiki/API#Libraries_and_frameworks)**--或简称API。你可以把API看作是合作的程序员在代码的各个部分进行交互的方式。例如，在IgnitionControl对象中，可能有一些函数来设置发动机的RPM，检查火花塞电压，以及点燃各个火花塞。能够设置发动机的转速确实很有用，巡航控制小组将需要调用这个函数。但是，他们对点火系统的工作原理不甚了解。让他们调用点燃各个火花塞的功能并不是一个好主意。否则发动机可能会爆炸! 也许吧。API允许正确的人访问正确的函数和数据。面向对象的编程语言通过让你指定函数是**公共的还是私有的**来做到这一点。如果一个函数被标记为 "私有"，这意味着只有该对象内部的函数可以调用它。因此，在这个例子中，只有IgnitionControl内部的其他函数，如setRPM函数，可以启动火花塞。另一方面，由于setRPM函数被标记为公共函数，其他对象可以调用它，比如巡航控制。这种隐藏复杂性和有选择地揭示复杂性的能力是面向对象编程的精髓，它是解决构建大型复杂程序的一种强大而流行的方式。

Pretty much every piece of software on your computer, or game running on your console, was built using an Object Oriented Programming Language, like C++, C# or Objective-C. Other popular “OO” languages you may have heard of are Python and Java. It’s important to remember that code, before being compiled, is just text. As I mentioned earlier, you could write code in Notepad or any old word processor. Some people do. But generally, today’s software developers use special-purpose applications for writing programs, ones that integrate many useful tools for writing, organizing, compiling and testing code. Because they put everything you need in one place, they’re called **Integrated Development Environments,** or **IDEs** for short. All IDEs provide a text editor for writing code, often with useful features like automatic color-coding to improve readability. Many even check for syntax errors as you type, like spell check for code. Big programs contain lots of individual source files, so IDEs allow programmers to organize and efficiently navigate everything. Also built right into the IDE is the ability to compile and run code. And if your program crashes, because it’s still a work in progress, the IDE can take you back to the line of code where it happened, and often provide additional information to help you track down and fix the bug, which is a process called **debugging**. This is important because most programmers spend 70 to 80% of their time testing and debugging, not writing new code. 

> 几乎你电脑上的每一个软件，或者你游戏机上运行的游戏，都是用面向对象的编程语言，如C++、C#或Objective-C构建的。你可能听说过的其他流行的 "OO "语言是Python和Java。重要的是要记住，代码在被编译之前，只是文本而已。正如我前面提到的，你可以在记事本或任何旧的文字处理器中写代码。有些人确实如此。但一般来说，今天的软件开发者使用特殊用途的应用程序来编写程序，这些程序集成了许多有用的工具来编写、组织、编译和测试代码。因为它们把你需要的一切都放在一个地方，所以被称为**集成开发环境**，简称**IDEs**。所有的集成开发环境都提供了一个用于编写代码的文本编辑器，通常具有一些有用的功能，如自动彩色编码以提高可读性。许多IDE甚至在你输入时检查语法错误，就像代码的拼写检查。大的程序包含很多单独的源文件，所以集成开发环境允许程序员组织和有效地浏览所有东西。集成开发环境中还直接内置了编译和运行代码的功能。而且，如果你的程序崩溃了，因为它仍然是一个正在进行的工作，IDE可以带你回到发生故障的那一行代码，并经常提供额外的信息来帮助你追踪和修复错误，这是一个叫做**调试**的过程。这一点很重要，因为大多数程序员把70-80%的时间花在测试和调试上，而不是写新代码。

Good tools, contained in IDEs, can go a long way when it comes to helping programmers prevent and find errors. Many computer programmers can be pretty loyal to their IDEs though - but let’s be honest. VIM is where it’s at. Providing you know how to quit. In addition to coding and debugging, another important part of a programmer's job is **documenting their code**. This can be done in standalone files called “read-me’s” which tell other programmers to read that help file before diving in. It can also happen right in the code itself with **comments**. These are specially-marked statements that the program knows to ignore when the code is compiled. They exist only to help programmers figure out what’s what in the source code. Good documentation helps programmers when they revisit code they haven’t seen for awhile, but it’s also crucial for programmers who are totally new to it. I just want to take a second here and reiterate that it’s THE WORST when someone parachutes a load of uncommented and undocumented code into your lap, and you literally have to go line by line to understand what the code is doing. Seriously. Don’t be that person. Documentation also promotes **code reuse**. So, instead of having programmers constantly write the same things over and over, they can track down someone else’s code that does what they need. Then, thanks to documentation, they can put it to work in their program, without ever having to read through the code. “Read the docs” as they say. 

> 当涉及到帮助程序员预防和发现错误时，包含在IDE中的好工具可以发挥很大的作用。虽然许多计算机程序员对他们的集成开发环境相当忠诚--但说实话。VIM是它的优势所在。只要你知道如何退出。除了编码和调试之外，程序员工作的另一个重要部分是**记录他们的代码**。这可以在称为 "read-me's "的独立文件中完成，该文件告诉其他程序员在深入研究之前要阅读该帮助文件。它也可以发生在代码本身的**注释**中。这些都是特别标记的语句，程序在编译代码时知道要忽略它们。它们的存在只是为了帮助程序员弄清源代码中的内容。好的文档可以帮助程序员重温他们很久没有看到的代码，但对于完全陌生的程序员来说，它也是至关重要的。我只想在这里花点时间重申，当有人把一大堆没有注释的、没有文档的代码空降到你的腿上时，那是最糟糕的，你真的要逐行去理解代码在做什么。我是认真的。不要成为那个人。文档也促进了**代码的重复使用**。因此，与其让程序员不断地重复写同样的东西，不如让他们跟踪别人的代码，做他们需要的事情。然后，由于有了文档，他们可以在自己的程序中使用它，而不需要通读代码。正如他们所说的 "阅读文档"。
>

In addition to IDEs, another important piece of software that helps big teams work collaboratively on big coding projects is called **Source Control**, also known as **version control** or **revision control**. Most often, at a big software company like Apple or Microsoft, code for projects is stored on centralized servers, called a **code repository**. When a programmer wants to work on a piece of code, they can **check it out**, sort of like checking out a book out from a library. Often, this can be done right in an IDE. Then, they can edit this code all they want on their personal computer, adding new features and testing if they work. When the programmer is confident their changes are working and there are no loose ends, they can check the code back into the repository, known as **committing code**, for everyone else to use. While a piece of code is checked out, and presumably getting updated or modified, other programmers leave it alone. This prevents weird conflicts and duplicated work. In this way, hundreds of programmers can be simultaneously checking in and out pieces of code, iteratively building up huge systems. Critically, you don’t want someone committing buggy code, because other people and teams may rely on it. Their code could crash, creating confusion and lost time. The master version of the code, stored on the server, should always compile without errors and run with minimal bugs. 

> 除了集成开发环境，另一个帮助大团队在大型编码项目上协同工作的重要软件叫做**源码控制**，也被称为**版本控制**或**修订控制**。大多数情况下，在像苹果或微软这样的大型软件公司，项目的代码被储存在集中的服务器上，称为**代码库**。当一个程序员想要处理一段代码时，他们可以**检查它**，有点像从图书馆借出一本书。通常，这可以在IDE中完成。然后，他们可以在自己的个人电脑上随意编辑这段代码，增加新的功能并测试它们是否有效。当程序员确信他们的修改是有效的，并且没有任何漏洞时，他们就可以把代码检回仓库，称为**提交代码**，供其他人使用。当一段代码被检出，并且可能被更新或修改时，其他程序员就不会再管它。这可以防止奇怪的冲突和重复的工作。这样，数以百计的程序员可以同时检入和检出代码片断，迭代地建立起庞大的系统。最关键的是，你不希望有人提交有缺陷的代码，因为其他人和团队可能会依赖它。他们的代码可能会崩溃，造成混乱和时间损失。存储在服务器上的主版本的代码，应该总是无错误地编译，并以最小的错误运行。

But sometimes bugs creep in. Fortunately, source control software keeps track of all changes, and if a bug is found, the whole code, or just a piece, can be rolled back to an earlier, **stable version**. It also keeps track of who made each change, so coworkers can send nasty, I mean, helpful and encouraging emails to the offending person. Debugging goes hand in hand with writing code, and it’s most often done by an individual or small team. The big picture version of debugging is **Quality Assurance testing**, or QA. This is where a team rigorously tests out a piece of software, attempting to create unforeseen conditions that might trip it up. Basically, they elicit bugs. Getting all the wrinkles out is a huge effort, but vital in making sure the software works as intended for as many users in as many situations as imaginable before it ships. You’ve probably heard of **beta software**. This is a version of software that’s mostly complete, but not 100% fully tested. Companies will sometimes release beta versions to the public to help them identify issues, it’s essentially like getting a free QA team. 

> 但有时错误会悄悄出现。幸运的是，源码控制软件可以跟踪所有的变化，如果发现了一个错误，整个代码，或者只是一个片段，都可以回滚到较早的、**稳定的版本**。它还会记录下谁做了每一个改动，所以同事们可以给犯错的人发送讨厌的，我是说，有用的和鼓励的电子邮件。调试与写代码是相辅相成的，它通常由个人或小团队完成。调试的重要版本是**质量保证测试**，或称QA。这是一个团队对软件进行严格测试的地方，试图创造出可能绊倒软件的不可预见的条件。基本上，他们激发出错误。把所有的问题都解决掉是一项巨大的努力，但对于确保软件在发货前在尽可能多的情况下为尽可能多的用户工作是至关重要的。你可能听说过**beta软件**。这是软件的一个版本，大部分是完整的，但不是100%完全测试。公司有时会向公众发布测试版，以帮助他们发现问题，这本质上就像获得一个免费的QA团队。
>

What you don’t hear about as much is the version that comes before the beta: the **alpha version**. This is usually so rough and buggy, it’s only tested internally. So, that’s the tip of the iceberg in terms of the tools, tricks and techniques that allow software engineers to construct the huge pieces of software that we know and love today, like YouTube, Grand Theft Auto 5, and Powerpoint. As you might expect, all those millions of lines of code needs some serious processing power to run at useful speeds, so next episode we’ll be talking about how computers got so incredibly fast. See you then. 

> 你很少听到的是在测试版之前的版本：**alpha版本**。这通常是非常粗糙和有缺陷的，它只在内部测试。所以，这就是工具、技巧和技术方面的冰山一角，这些工具、技巧和技术使软件工程师能够构建我们今天所知道和喜爱的巨大软件，如YouTube、《侠盗猎车手5》和Powerpoint。正如你所期望的那样，所有这些数百万行的代码需要一些重要的处理能力才能以有用的速度运行，所以下一集我们将讨论计算机如何变得如此难以置信的快。到时见。



## #17 Integrated circuit & Moore's Law

<iframe width="560" height="315" src="https://www.youtube.com/embed/6-tKOHICqrI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science! Over the past six episodes, we delved into software, from early programming efforts to modern software engineering practices. Within about 50 years, software grew in complexity from machine code punched by hand onto paper tape, to object oriented programming languages, compiled in integrated development environments. But this growth in sophistication would not have been possible without improvements in **hardware**. 

> 在过去的六期节目中，我们深入研究了软件，从早期的编程工作到现代软件工程实践。在大约50年的时间里，软件的复杂性从手工打在纸带上的机器代码，发展到面向对象的编程语言，并在集成开发环境中进行编译。但是，如果没有**硬件**的改进，这种复杂先进的增长是不可能的。

INTRO To appreciate computing hardware’s explosive growth in power and sophistication, we need to go back to the birth of electronic computing. From roughly the 1940’s through the mid-1960s, every computer was built from individual parts, called discrete components, which were all wired together. For example, the ENIAC, consisted of more than 17,000 vacuum tubes, 70,000 resistors, 10,000 capacitors, and 7,000 diodes, all of which required 5 million hand-soldered connections. Adding more components to increase performance meant more connections, more wires, and just more complexity, what was dubbed the Tyranny of Numbers. By the mid 1950s, transistors were becoming commercially available and being incorporated into computers. These were much smaller, faster and more reliable than vacuum tubes, but each transistor was still one discrete component. In 1959, IBM upgraded their vacuum-tube-based “709” computers to transistors by replacing all the discrete vacuum tubes with discrete transistors. The new machine, the IBM 7090, was six times faster and half the cost. These **transistorized computers** marked the **second generation** of electronic computing. 

> 要了解计算硬件在功率和复杂先进性方面的爆炸性增长，我们需要回顾一下电子计算的诞生。大约从20世纪40年代到60年代中期，每台计算机都是由单独的部件（称为分立元件）构建而成，这些部件都是通过线路连接在一起的。例如，ENIAC由17,000多个真空管、70,000个电阻、10,000个电容和7,000个二极管组成，所有这些都需要500万个手工焊接的连接。添加更多的元件以提高性能意味着更多的连接，更多的电线，以及更多的复杂性，这被称为 "数字暴政"。到20世纪50年代中期，晶体管开始商业化，并被纳入计算机中。这些晶体管比真空管更小、更快、更可靠，但每个晶体管仍然是一个分立的部件。1959年，IBM将其基于真空管的 "709 "计算机升级为晶体管，用分立的晶体管取代了所有分立的真空管。新机器，即IBM 7090，速度提高了六倍，成本降低了一半。这些**晶体管计算机**标志着**第二代**电子计算机的开始。

However, although faster and smaller, discrete transistors didn’t solve the Tyranny of Numbers. It was getting unwieldy to design, let alone physically manufacture computers with hundreds of thousands of individual components. By the the 1960s, this was reaching a breaking point. The insides of computers were often just huge tangles of wires. Just look at what the inside of a PDP-8 from 1965 looked like! The answer was to bump up a new level of abstraction, and package up underlying complexity! The breakthrough came in 1958, when Jack Kilby, working at Texas Instruments, demonstrated such an electronic part, “wherein all the components of the electronic circuit are completely integrated." Put simply: instead of building computer parts out of many discrete components and wiring them all together, you put many components together, inside of a new, single component. These are called **Integrated Circuits**, or ICs. A few months later in 1959, Fairchild Semiconductor, lead by Robert Noyce, made ICs practical. Kilby built his ICs out of germanium, a rare and unstable material. But, Fairchild used the abundant **silicon**, which makes up about a quarter of the earth's crust! It’s also more stable, therefore more reliable. For this reason, Noyce is widely regarded as the father of modern ICs, ushering in the electronics era... and also Silicon Valley, where Fairchild was based and where many other semiconductor companies would soon pop up. 

> 然而，尽管速度更快、体积更小，但分立晶体管并没有解决 "数字暴政 "问题。设计越来越困难，更不用说实际制造有几十万个独立元件的计算机了。到20世纪60年代，这已经达到了一个突破点。计算机的内部往往只是巨大的电线纠结在一起。看看1965年的PDP-8的内部是什么样子的吧! 答案是提高一个新的抽象层次，并将底层的复杂性打包！在1958年取得了突破。突破出现在1958年，当时在德州仪器工作的杰克-基尔比展示了这样一种电子部件，"其中电子电路的所有部件都是完全集成的"。简单地说：你不是用许多分立的元件来制造计算机零件，然后把它们全部连在一起，而是把许多元件放在一起，放在一个新的、单一的元件里面。这些被称为**集成电路**，或IC。几个月后的1959年，由罗伯特-诺伊斯领导的飞兆半导体公司将集成电路变为现实。基尔比用锗制造了他的集成电路，这是一种稀有和不稳定的材料。但是，飞兆半导体公司使用了丰富的**硅**，它占了地壳的四分之一！它也更稳定，因此，它的价格也更低。它也更稳定，因此更可靠。由于这个原因，诺伊斯被广泛认为是现代集成电路之父，开创了电子时代......也是硅谷，飞兆公司的总部所在地，许多其他半导体公司很快就会在这里出现。

In the early days, an IC might only contain a simple circuit with just a few transistors, like this early Westinghouse example. But even this allowed simple circuits, like the logic gates from Episode 3, to be packaged up into a single component. ICs are sort of like lego for computer engineers “building blocks” that can be arranged into an infinite array of possible designs. However, they still have to be wired together at some point to create even bigger and more complex circuits, like a whole computer. For this, engineers had another innovation: **printed circuit boards**, or PCBs. Instead of soldering and bundling up bazillions of wires, PCBs, which could be mass manufactured, have all the metal wires etched right into them to connect components together. By using PCBs and ICs together, one could achieve exactly the same functional circuit as that made from discrete components, but with far fewer individual components and tangled wires. Plus, it’s smaller, cheaper and more reliable. Triple win! Many early ICs were manufactured using teeny tiny discrete components packaged up as a single unit, like this IBM example from 1964. 

> 在早期，一个集成电路可能只包含一个只有几个晶体管的简单电路，就像这个早期的西屋公司的例子。但即使是这样，简单的电路，如第3集的逻辑门，也可以被包装成一个单一的组件。集成电路有点像计算机工程师的乐高 "积木"，可以排列成无限的可能设计。然而，它们仍然必须在某些时候被连接在一起，以创建更大、更复杂的电路，如整台计算机。为此，工程师们有了另一项创新。**印制电路板**，或称PCB。与其焊接和捆绑数以百万计的电线，PCBs可以大规模生产，所有的金属线都直接蚀刻在上面，将元件连接在一起。通过将PCBs和ICs一起使用，人们可以实现与分立元件制作的电路完全相同的功能，但单个元件和纠缠的电线要少得多。此外，它还更小、更便宜、更可靠。三赢! 许多早期的集成电路都是用极小的分立元件包装成一个单元来制造的，比如这个1964年的IBM例子。

However, even when using really really itty-bitty components, it was hard to get much more than around five transistors onto a single IC. To achieve more complex designs, a radically different fabrication process was needed that changed everything: **Photolithography**! In short, it’s a way to use light to transfer complex patterns to a material, like a semiconductor. It only has a few basic operations, but these can be used to create incredibly complex circuits. Let’s walk through a simple, although extensive example, to make one of these! We start with a slice of silicon, which, like a thin cookie, is called a **wafer**. Delicious! Silicon, as we discussed briefly in episode 2, is special because it’s a semiconductor, that is, a material that can sometimes conduct electricity and other times does not. We can control where and when this happens, making Silicon the perfect raw material for making transistors. We can also use a wafer as a base to lay down complex metal circuits, so everything is integrated, perfect for... integrated circuits! The next step is to add a thin **oxide layer** on top of the silicon, which acts as a protective coating. Then, we apply a special chemical called a **photoresist**. When exposed to light, the chemical changes, and becomes soluble, so it can be washed away with a different special chemical. Photoresists aren’t very useful by themselves, but are super powerful when used in conjunction with a **photomask**. This is just like a piece of photographic film, but instead of a photo of a hamster eating a tiny burrito, it contains a pattern to be transferred onto the wafer. 

> 然而，即使使用非常非常小的元件，也很难在单个集成电路上获得超过约五个晶体管。为了实现更复杂的设计，需要一种完全不同的制造工艺来改变一切。**光刻技术**! 简而言之，它是一种利用光将复杂的图案转移到材料上的方法，如半导体。它只有几个基本操作，但这些操作可以用来创建令人难以置信的复杂电路。让我们通过一个简单的，尽管是广泛的例子，来制作一个这样的电路 我们从一片硅片开始，它就像一块薄薄的饼干，被称为**晶圆**。味道不错! 正如我们在第2集简要讨论的那样，硅是特殊的，因为它是一种半导体，也就是说，这种材料有时可以导电，有时不导电。我们可以控制这种情况发生的地点和时间，使硅成为制造晶体管的完美原材料。我们还可以用硅片作为基础来铺设复杂的金属电路，因此一切都被整合了，非常适合......集成电路！"。下一步是在硅的顶部添加一个薄的**氧化层**，作为保护层。然后，我们应用一种叫做**光刻胶**的特殊化学品。当暴露在光线下时，这种化学物质会发生变化，并变得可溶解，因此可以用不同的特殊化学品将其洗掉。光阻剂本身并不十分有用，但如果与**光罩**一起使用，则具有超强的作用。这就像一块照相胶片，但不是仓鼠吃小卷饼的照片，而是包含一个要转移到晶圆上的图案。

silicon wafer:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\silicon wafer.jpg)

We do this by putting a photomask over the wafer, and turning on a powerful light. Where the mask blocks the light, the photoresist is unchanged. But where the light does hit the photoresist it changes chemically which lets us wash away only the photoresist that was exposed to light, selectively revealing areas of our oxide layer. Now, by using another special chemical, often an acid, we can remove any exposed oxide, and etch a little hole the entire way down to the raw silicon. Note that the oxide layer under the photoresist is protected. To clean up, we use yet another special chemical that washes away any remaining photoresist. Yep, there are a lot of special chemicals in photolithography, each with a very specific function! So now we can see the silicon again, we want to modify only the exposed areas to better conduct electricity. To do that, we need to change it chemically through a process called: **doping**. I’m not even going to make a joke. Let’s move on. Most often this is done with a high temperature gas, something like Phosphorus, which penetrates into the exposed area of silicon. This alters its electrical properties. We’re not going to wade into the physics and chemistry of semiconductors, but if you’re interested, there’s a link in the description to [an excellent video by our friend Derek Muller from Veritasium](https://www.youtube.com/watch?v=IcrBqCFLHIY). 

> 我们通过把光罩放在晶圆上，然后打开一束强光来做到这一点。在掩膜阻挡光线的地方，光刻胶是不变的。但是，在光照到光刻胶的地方，它发生了化学变化，这让我们只能洗去暴露在光线下的光刻胶，有选择地显示出我们的氧化层的区域。现在，通过使用另一种特殊的化学品，通常是一种酸，我们可以去除任何暴露的氧化物，并在整个原始硅上蚀刻一个小孔。请注意，光刻胶下面的氧化层是受到保护的。为了清理，我们使用另一种特殊的化学品来洗掉任何剩余的光刻胶。是的，在光刻技术中，有很多特殊的化学品，每一种都有非常具体的功能 因此，现在我们可以再次看到硅，我们想只修改暴露的区域以更好地导电。要做到这一点，我们需要通过一个叫做：**掺杂**的过程来改变它的化学性质。我甚至不打算开个玩笑。让我们继续前进。大多数情况下，这是用一种高温气体完成的，比如磷，它渗透到硅的暴露区域。这改变了它的电性能。我们不打算涉足半导体的物理和化学，但如果你有兴趣，描述中有一个链接，是[我们来自Veritasium的朋友Derek Muller的一个优秀视频](https://www.youtube.com/watch?v=IcrBqCFLHIY)。

But, we still need a few more rounds of photolithography to build a transistor. The process essentially starts again, first by building up a fresh oxide layer ...which we coat in photoresist. Now, we use a photomask with a new and different pattern, allowing us to open a small window above the doped area. Once again, we wash away remaining photoresist. Now we dope, and avoid telling a hilarious joke, again, but with a different gas that converts part of the silicon into yet a different form. Timing is super important in photolithography in order to control things like **doping diffusion** and **etch depth**. In this case, we only want to dope a little region nested inside the other. Now we have all the pieces we need to create our transistor!

> 但是，我们仍然需要再进行几轮光刻技术来建立一个晶体管。这个过程基本上是重新开始的，首先是建立一个新的氧化层......我们将其涂上光刻胶。现在，我们使用具有新的和不同的图案的光罩，使我们能够在掺杂区域上方打开一个小窗口。再一次，我们洗掉剩余的光阻。现在，我们再一次进行掺杂，并避免讲一个搞笑的笑话，但要用不同的气体将部分硅转化为不同的形式。在光刻技术中，时间安排是超级重要的，以便控制诸如**掺杂扩散**和**蚀刻深度**的事情。在这种情况下，我们只想在一个小区域内嵌套掺杂。现在我们有了创建我们的晶体管所需的所有部件!

The final step is to make channels in the oxide layer so that we can run little metal wires to different parts of our transistor. Once more, we apply a photoresist, and use a new photomask to etch little channels. Now, we use a new process, called **metalization**, that allows us to deposit a thin layer of metal, like aluminium or copper. But we don’t want to cover everything in metal. We want to etch a very specific circuit design. So, very similar to before, we apply a photoresist, use a photomask, dissolve the exposed resist, and use a chemical to remove any exposed metal. Whew! Our transistor is finally complete! It has three little wires that connect to three different parts of the silicon, each doped a particular way to create, in this example, what’s called a **bipolar junction transistor**. 

> 最后一步是在氧化层上制作通道，这样我们就可以把小金属线接到晶体管的不同部位。再一次，我们应用光刻胶，并使用一个新的光罩来蚀刻小通道。现在，我们使用一种新的工艺，称为**金属化**，它允许我们沉积一层薄薄的金属，如铝或铜。但我们并不想用金属覆盖所有的东西。我们想蚀刻一个非常具体的电路设计。因此，与之前非常相似，我们应用光刻胶，使用光掩膜，溶解暴露的抗蚀剂，并使用化学品来去除任何暴露的金属。呜! 我们的晶体管终于完成了! 它有三条小线，连接到硅的三个不同部分，每个部分都以一种特殊的方式掺杂，以创造，在这个例子中，被称为**双极结晶体管**。

Here’s the actual patent from 1962, an invention that changed our world forever! Using similar steps, photolithography can create other useful electronic elements, like resistors and capacitors, all on a single piece of silicon (plus all the wires needed to hook them up into circuits). Goodbye discrete components! In our example, we made one transistor, but in the real world, photomasks lay down millions of little details all at once. Here is what an **IC** might look like from above, with wires crisscrossing above and below each other, interconnecting all the individual elements together into complex circuits. 

> 这是1962年的实际专利，这项发明永远地改变了我们的世界! 利用类似的步骤，光刻技术可以在一块硅片上创造出其他有用的电子元件，如电阻器和电容器（加上将它们连接到电路所需的所有导线）。再见了，分立元件! 在我们的例子中，我们制造了一个晶体管，但在现实世界中，光刻机一次就能制造出数百万个小细节。下面是一个**集成电路**从上面看的样子，上面和下面的电线纵横交错，将所有的单个元件相互连接成复杂的电路。

Although we could create a photomask for an entire wafer, we can take advantage of the fact that light can be focused and projected to any size we want. In the same way that a film can be projected to fill an entire movie screen, we can focus a photomask onto a very small patch of silicon, creating incredibly fine details. A single silicon wafer is generally used to create dozens of ICs. Then, once you’ve got a whole wafer full, you cut them up and package them into **microchips**, those little black rectangles you see in electronics all the time. Just remember: at the heart of each of those chips is one of these small pieces of silicon. 

> 尽管我们可以为整个晶圆创建一个光掩模，但我们可以利用光可以被聚焦并投射到我们想要的任何尺寸的事实。就像电影可以投射到整个电影屏幕上一样，我们可以将光罩聚焦到一个非常小的硅片上，创造出令人难以置信的精细细节。一块硅片通常被用来制造几十个集成电路。然后，一旦你有了一整块晶圆，你就把它们切开并封装成**微芯片**，就是那些你在电子产品中经常看到的黑色小长方体。请记住：这些芯片的核心是这些小硅片中的一块。

As photolithography techniques improved, the size of transistors shrunk, allowing for greater densities. At the start of the 1960s, an IC rarely contained more than 5 transistors, they just couldn’t possibly fit. But, by the mid 1960s, we were starting to see ICs with over 100 transistors on the market. In 1965, Gordon Moore could see the trend: that approximately every two years, thanks to advances in materials and manufacturing, you could fit twice the number of transistors into the same amount of space. This is  called **Moore’s Law**. The term is a bit of a misnomer though. It’s not really a law at all, more of a trend. But it’s a good one. IC prices also fell dramatically, from an average of \$50 in 1962 to around $2 in 1968. Today, you can buy ICs for cents. 

> 随着光刻技术的改进，晶体管的尺寸缩小了，允许更大的密度。在20世纪60年代初，一块集成电路很少包含5个以上的晶体管，它们根本不可能装下。但是，到了60年代中期，我们开始看到市场上有超过100个晶体管的集成电路。1965年，戈登-摩尔看到了这一趋势：大约每两年，由于材料和制造方面的进步，你可以将两倍的晶体管装入相同的空间。这被称为**摩尔定律**。但这个术语有点名不副实。它根本就不是一个真正的定律，更像是一种**趋势**。但这是一个很好的例子。集成电路的价格也急剧下降，从1962年的平均50美元到1968年的2美元左右。今天，你可以用几分钱买到集成电路。
>

Smaller transistors and higher densities had other benefits too. The smaller the transistor, the less **charge** you have to move around, allowing it to switch states faster and consume less power. Plus, more compact circuits meant less delay in signals resulting in faster clock speeds. In 1968, Robert Noyce and Gordon Moore teamed up and founded a new company, combining the words Integrated and Electronics... **Intel**... the largest **chip maker** today. **The Intel 4004 CPU, from Episodes 7 and 8, was a major milestone**. Released in 1971, it was the first processor that shipped as an IC, what’s called a **microprocessor**, because it was so beautifully small! It contained 2,300 transistors. People marveled at the level of integration, an entire CPU in one chip, which just two decades earlier would have filled an entire room using discrete components. This era of integrated circuits, especially microprocessors, **ushered in the third generation of computing**. 

> 更小的晶体管和更高的密度也有其他好处。晶体管越小，你必须移动的**电荷**就越少，使其能够更快地切换状态，并消耗更少的电力。此外，更紧凑的电路意味着更少的信号延迟，导致更快的时钟速度。1968年，罗伯特-诺伊斯和戈登-摩尔联手成立了一家新公司，将 "集成 "和 "电子 "这两个词结合在一起...... **英特尔**...今天最大的**芯片制造商**。**第7集和第8集的英特尔4004 CPU是一个重要的里程碑**。它于1971年发布，是第一个以集成电路形式出货的处理器，也就是所谓的**微处理器**，因为它是如此美丽的小东西！它包含2300个晶体管。人们惊叹于它的集成度，整个CPU都在一个芯片中，而就在20年前，如果使用分立元件，整个房间都会被塞满。这个集成电路的时代，特别是微处理器，**开创了第三代计算机**。
>

And the Intel 4004 was just the start. CPU **transistor count** exploded! By 1980, CPUs contained 30 thousand transistors. By 1990, CPUs breached the 1 million transistor count. By 2000, 30 million transistors, and by 2010, ONE. BILLION. TRANSISTORS. IN ONE. IC. OMG! To achieve this density, the finest resolution possible with photolithography has improved from roughly 10 thousand nanometers, that’s about 1/10th the thickness of a human hair, to around 14 nanometers today. That’s over 400 times smaller than a red blood cell! And of course, CPU’s weren’t the only components to benefit. Most electronics advanced essentially exponentially: **RAM**, **graphics cards**, **solid state hard drives**, **camera sensors**, you name it. Today’s processors, like the A10 CPU inside Of an iPhone 7, contains a mind melting 3.3 BILLION transistors in an IC roughly 1cm by 1cm. That’s smaller than a postage stamp! 

> 而英特尔4004只是一个开始。CPU的**晶体管数量**呈爆炸式增长! 到1980年，CPU包含3万个晶体管。到1990年，CPU突破了100万个晶体管数量。到2000年，3000万个晶体管，到2010年，十亿。晶体管。在一个。IC。OMG! 为了达到这种密度，光刻技术可能达到的最佳分辨率已经从大约1万纳米，也就是人类头发的1/10的厚度，提高到今天的14纳米左右（7纳米开始商用）。这比一个红细胞小400多倍! 当然，CPU并不是唯一受益的部件。大多数电子产品基本上都是以指数形式发展的。**RAM**，**显卡**，**固态硬盘**，**相机传感器**，你能想到的都有。今天的处理器，如iPhone 7内的A10 CPU，在一个大约1厘米乘1厘米的集成电路中包含了令人心动的33亿个晶体管。这比一张邮票还小！

And modern engineers aren’t laying out these designs by hand, one transistor at a time - it’s not humanly possible. Starting in the 1970’s, very-large-scale integration, or **[VLSI](https://en.wikipedia.org/wiki/Very_Large_Scale_Integration) software**, has been used to automatically generate chip designs instead. Using techniques like logic synthesis, where whole, high-level components can be laid down, like a **memory cache**, the software generates the circuit in the most efficient way possible. Many consider this to be the start of **fourth generation computers**. 

> 而现代的工程师们并不是用手来布置这些设计，一次一个晶体管--这在人力上是不可能的。从20世纪70年代开始，超大规模集成，或**VLSI软件**，被用来自动生成芯片设计。使用像逻辑综合这样的技术，可以铺设整个高层次的组件，如**内存缓存**，软件以最有效的方式生成电路。许多人认为这是**第四代计算机**的开始。
>

Unfortunately, experts have been predicting the end of Moore’s Law for decades, and we might finally be getting close to it. There are two significant issues holding us back from further **miniaturization**. First, we’re bumping into limits on how fine we can make features on a photomask and it’s resultant wafer due to the wavelengths of light used in photolithography. In response, scientists have been developing light sources with smaller and smaller wavelengths that can project smaller and smaller features. The second issue is that when transistors get really really small, where electrodes might be separated by only a few dozen atoms, electrons can jump the gap, a phenomenon called **[quantum tunneling](https://en.wikipedia.org/wiki/Quantum_tunnelling)**. If transistors leak current, they don’t make very good switches. Nonetheless, scientists and engineers are hard at work figuring out ways around these problems. Transistors as small as 1 nanometer have been demonstrated in research labs. Whether this will ever be commercially feasible remains MASKED in mystery. But maybe we’ll be able to RESOLVE it in the future. I’m DIEING to know. See you next week. 

> 不幸的是，几十年来，专家们一直在预测摩尔定律的终结，而我们可能最终会接近它。有两个重大问题阻碍了我们进一步**微型化**。首先，由于光刻技术中使用的光的波长，我们在光掩膜和它所产生的晶圆上制作特征的精细程度遇到了限制。作为回应，科学家们一直在开发波长越来越小的光源，可以投射越来越小的特征。第二个问题是，当晶体管变得非常非常小的时候，电极可能只隔着几十个原子，电子可以跳过这个间隙，这种现象称为**量子隧穿效应**。如果晶体管泄漏电流，它们就不能成为非常好的开关。尽管如此，科学家和工程师们正在努力工作，找出解决这些问题的方法。小至1纳米的晶体管已经在研究实验室中得到证明。这在商业上是否可行仍然是个谜。但也许我们在未来能够解决这个问题。我很想知道。下周见。



## #18 Opereating systems

<iframe width="560" height="315" src="https://www.youtube.com/embed/26QPDBe-NB8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I'm Carrie Anne, and welcome to Crash Course Computer Science! Computers in the 1940s and early 50s ran one program at a time. A programmer would write one at their desk, for example, on punch cards. Then, they’d carry it to a room containing a room-sized computer, and hand it to a dedicated computer operator. That person would then feed the program into the computer when it was next available. The computer would run it, spit out some output, and halt. This very manual process worked OK back when computers were slow, and running a program often took hours, days or even weeks. But, as we discussed last episode, computers became faster... and faster... and faster – exponentially so! Pretty soon, having humans run around and inserting programs into readers was taking longer than running the actual programs themselves. We needed a way for computers to operate themselves, and so, **[operating systems](https://en.wikipedia.org/wiki/Operating_system)** were born. 

> 嗨，我是卡丽-安，欢迎来到《计算机科学速成班》! 20世纪40年代和50年代初的计算机是一次运行一个程序。一个程序员会在他们的桌子上写一个程序，例如，在打卡机上。然后，他们会把它带到一个装有房间大小的计算机的房间，并把它交给一个专门的计算机操作员。然后，这个人在下次有时间时将程序输入电脑。计算机将运行它，吐出一些输出，然后停止。这种非常手动的过程在计算机速度很慢的时候是可行的，运行一个程序往往需要几个小时、几天甚至几个星期。但是，正如我们在上一集所讨论的，计算机变得越来越快......越来越快......越来越快--呈指数级增长！很快，让人跑来跑去的程序就会被淘汰。很快，让人类跑来跑去，把程序插入读者，比运行实际的程序本身还要费时。我们需要一种方法让计算机自己操作，于是，**[操作系统](https://en.wikipedia.org/wiki/Operating_system)**诞生了。

INTRO Operating systems, or OS’es for short, are just programs. But, special privileges on the hardware let them run and manage other programs. They’re typically the first one to start when a computer is turned on, and all subsequent programs are launched by the OS. They got their start in the 1950s, as computers became more widespread and more powerful. The very first OSes augmented the mundane, manual task of loading programs by hand. Instead of being given one program at a time, computers could be given batches. When the computer was done with one, it would automatically and near-instantly start the next. There was no downtime while someone scurried around an office to find the next program to run. This was called **batch processing**. While computers got faster, they also got cheaper. So, they were popping up all over the world, especially in universities and government offices. Soon, people started sharing software. 

> 操作系统，简称OS，只是程序。但是，硬件上的特殊权限使它们能够运行和管理其他程序。当计算机被打开时，它们通常是第一个被启动的，所有后续程序都由操作系统启动。它们开始于20世纪50年代，因为计算机变得越来越普遍，功能越来越强大。最早的操作系统增强了手动加载程序这一平凡的手工任务。计算机不再是一次给一个程序，而是可以给成批的程序。当计算机用完一个程序后，它会自动地、近乎即时地启动下一个程序。当有人在办公室里匆匆忙忙地寻找下一个要运行的程序时，不会有停工期。这就是所谓的**批处理**。当计算机变得更快，它们也变得更便宜。因此，它们在世界各地不断涌现，尤其是在大学和政府办公室。很快，人们开始分享软件。
>

But there was a problem… In the era of one-off computers, like the Harvard Mark 1 or ENIAC, programmers only had to write code for that one single machine. The processor, punch card readers, and printers were known and unchanging. But as computers became more widespread, their configurations were not always identical, like computers might have the same CPU, but not the same printer. This was a huge pain for programmers. Not only did they have to worry about writing their program, but also how to interface with each and every model of printer, and all devices connected to a computer, what are called **peripherals**. Interfacing with early peripherals was very low level, requiring programmers to know intimate hardware details about each device. On top of that, programmers rarely had access to every model of a peripheral to test their code on. So, they had to write code as best they could, often just by reading manuals, and hope it worked when shared. Things weren’t exactly plug-and-play back then… more plug-and-pray. This was clearly terrible, so to make it easier for programmers, Operating Systems stepped in as intermediaries between software programs and hardware peripherals. More specifically, they provided a software abstraction, through APIs, called **device drivers**. These allow programmers to talk to common input and output hardware, or **I/O** for short, using standardized mechanisms. 

> 但有一个问题......在一次性计算机的时代，如哈佛Mark 1或ENIAC，程序员只需要为这一台机器编写代码。处理器、打卡器和打印机都是已知的、不变的。但随着计算机的普及，它们的配置并不总是相同的，比如计算机可能有相同的CPU，但没有相同的打印机。这对程序员来说是一个巨大的痛苦。他们不仅要担心编写程序，还要担心如何与每一种型号的打印机以及与计算机相连的所有设备（即所谓的**外部设备**）进行接口。与早期外设的接口是非常低级的，需要程序员了解每个设备的具体硬件细节。此外，程序员很少有机会接触到每个型号的外设来测试他们的代码。因此，他们不得不尽其所能地编写代码，通常只是通过阅读手册，并希望它在共享时能发挥作用。当时的情况并不是即插即用......而是“祈祷能用”。这显然是很糟糕的，所以为了让程序员更轻松，操作系统作为软件程序和硬件外围设备之间的中介介入。更具体地说，他们通过API提供了一个软件抽象，称为**设备驱动**。这些允许程序员使用标准化的机制与普通的输入和输出硬件（简称**I/O**）进行对话。
>

For example, programmers could call a function like “print highscore”, and the OS would do the heavy lifting to get it onto paper. By the end of the 1950s, computers had gotten so fast, they were often idle waiting for slow mechanical things, like printers and punch card readers. While programs were blocked on I/O, the expensive processor was just chillin’... not like a villain… you know, just relaxing. In the late 50’s, the University of Manchester, in the UK, started work on a supercomputer called Atlas, one of the first in the world. They knew it was going to be wicked fast, so they needed a way to make maximal use of the expensive machine. Their solution was a program called the Atlas Supervisor, finished in 1962. This operating system not only loaded programs automatically, like earlier batch systems, but could also run several at the same time on its single CPU. It did this through clever scheduling. Let’s say we have a game program running on Atlas, and we call the function “print highscore” which instructs Atlas to print the value of a variable named “highscore” onto paper to show our friends that we’re the ultimate champion of virtual tiddlywinks. That function call is going to take a while, the equivalent of thousands of clock cycles, because mechanical printers are slow in comparison to electronic CPUs. So instead of waiting for the I/O to finish, Atlas instead puts our program to sleep, then selects and runs another program that’s waiting and ready to run. Eventually, the printer will report back to Atlas that it finished printing the value of “highscore”. Atlas then marks our program as ready to go, and at some point, it will be scheduled to run again on the CPU, and continue onto the next line of code following the print statement. In this way, Atlas could have one program running calculations on the CPU, while another was printing out data, and yet another reading in data from a punch tape. Atlas’ engineers doubled down on this idea, and outfitted their computer with 4 paper tape readers, 4 paper tape punches, and up to 8 magnetic tape drives. This allowed many programs to be in progress all at once, sharing time on a single CPU. This ability, enabled by the Operating System, is called **multitasking**. 

> 例如，程序员可以调用 "打印高分 "这样的函数，而操作系统会完成繁重的工作，将其打印在纸上。到20世纪50年代末，计算机已经变得如此之快，它们经常闲置，等待缓慢的机械事物，如打印机和打卡器。当程序被封锁在I/O上时，昂贵的处理器只是在发呆......不像是一个恶棍......你知道，只是在放松。50年代末，英国曼彻斯特大学开始研制一台名为Atlas的超级计算机，这是世界上第一台超级计算机之一。他们知道这台计算机会非常快，所以他们需要一种方法来最大限度地利用这台昂贵的机器。他们的解决方案是一个叫做Atlas监督器的程序，于1962年完成。这个操作系统不仅像早期的批处理系统一样自动加载程序，而且还可以在其单个CPU上同时运行几个程序。它通过巧妙的调度做到了这一点。假设我们有一个在Atlas上运行的游戏程序，我们调用函数 "print highscore"，指示Atlas将一个名为 "highscore "的变量的值打印到纸上，向我们的朋友展示我们是虚拟提溜球的最终冠军。这个函数调用需要一段时间，相当于数千个时钟周期，因为与电子CPU相比，机械打印机很慢。因此，Atlas没有等待I/O完成，而是让我们的程序进入睡眠状态，然后选择并运行另一个正在等待并准备运行的程序。最终，打印机将向Atlas报告，它完成了对 "高分 "值的打印。然后，Atlas将我们的程序标记为准备就绪，在某个时间点，它将被安排在CPU上再次运行，并继续进入打印语句后的下一行代码。通过这种方式，Atlas可以让一个程序在CPU上运行计算，而另一个程序正在打印数据，还有一个程序从打孔磁带上读入数据。阿特拉斯的工程师们在这个想法上加倍努力，为他们的计算机配备了4个纸带阅读器、4个纸带打孔器和多达8个磁带驱动器。这使得许多程序可以同时进行，在一个CPU上共享时间。这种由操作系统实现的能力被称为**多任务处理**。

There’s one big catch to having many programs running simultaneously on a single computer, though. Each one is going to need some memory, and we can’t lose that program’s data when we switch to another program. The solution is to allocate each program its own block of memory. So, for example, let’s say a computer has 10,000 memory locations in total. Program A might get allocated memory addresses 0 through 999, and Program B might get 1000 through 1999, and so on. If a program asks for more memory, the operating system decides if it can grant that request, and if so, what memory block to allocate next. This flexibility is great, but introduces a quirk. It means that Program A could end up being allocated non-sequential blocks of memory, in say addresses 0 through 999, and 2000 through 2999. And this is just a simple example - a real program might be allocated dozens of blocks scattered all over memory. As you might imagine, this would get really confusing for programmers to keep track of. Maybe there’s a long list of sales data in memory that a program has to total up at the end of the day, but this list is stored across a bunch of different blocks of memory. To hide this complexity, Operating Systems virtualize memory locations. With **Virtual Memory**, programs can assume their memory always starts at address 0, keeping things simple and consistent. However, the actual, physical location in computer memory is hidden and abstracted by the operating system. Just a new level of abstraction. 

> 不过，在一台电脑上同时运行许多程序有一个大问题。每个程序都需要一些内存，而且当我们切换到另一个程序时，我们不能丢失该程序的数据。解决办法是为每个程序分配自己的内存块。因此，举例来说，假设一台计算机总共有10,000个内存位置。程序A可能获得分配的内存地址0到999，程序B可能获得1000到1999，以此类推。如果一个程序要求获得更多的内存，操作系统就会决定是否能满足这个要求，如果能满足，就决定下一步分配什么内存块。这种灵活性很好，但也带来了一个怪癖。这意味着程序A可能最终被分配到非连续的内存块，比如地址为0到999，以及2000到2999。这只是一个简单的例子--一个真正的程序可能会被分配几十个散落在内存中的块。正如你所想象的那样，这对于程序员来说会变得非常混乱，难以掌握。也许内存中有一个长长的销售数据清单，程序必须在一天结束时进行汇总，但这个清单被存储在一堆不同的内存块中。为了隐藏这种复杂性，操作系统将内存位置虚拟化。有了**虚拟内存**，程序可以假设他们的内存总是从地址0开始，保持事情的简单和一致。然而，计算机内存中的实际物理位置被操作系统隐藏和抽象化了。只是一个新的抽象层次。

Let’s take our example Program B, which has been allocated a block of memory from address 1000 to 1999. As far as Program B can tell, this appears to be a block from 0 to 999. The OS and CPU handle the virtual-to-physical memory remapping automatically. So, if Program B requests memory location 42, it really ends up reading address 1042. This virtualization of memory addresses is even more useful for Program A, which in our example, has been allocated two blocks of memory that are separated from one another. This too is invisible to Program A. As far as it can tell, it’s been allocated a continuous block of 2000 addresses. When Program A reads memory address 999, that does coincidentally map to physical memory address 999. But if Program A reads the very next value in memory, at address 1000, that gets mapped behind the scenes to physical memory address 2000. This mechanism allows programs to have flexible memory sizes, called **dynamic memory allocation**, that appear to be continuous to them. It simplifies everything and offers tremendous flexibility to the Operating System in running multiple programs simultaneously. 

> 让我们以程序B为例，它被分配了一个从地址1000到1999的内存块。据程序B所知，这似乎是一个从0到999的块。操作系统和CPU自动处理虚拟到物理的内存重映射。因此，如果程序B请求内存位置42，它最终会读取地址1042。这种内存地址的虚拟化对程序A更加有用，在我们的例子中，它被分配了两个相互分离的内存块。这对程序A来说也是不可见的，就它所知，它被分配了一个连续的2000个地址的块。当程序A读取内存地址999时，它恰好映射到物理内存地址999。但是，如果程序A读取内存中的下一个值，即地址1000，它就会在幕后被映射到物理内存地址2000。这种机制允许程序拥有灵活的内存大小，称为**动态内存分配**，对它们来说似乎是连续的。它简化了一切，为操作系统同时运行多个程序提供了巨大的灵活性。

Another upside of allocating each program its own memory, is that they’re better isolated from one another. So, if a buggy program goes awry, and starts writing gobbledygook, it can only trash its own memory, not that of other programs. This feature is called **Memory Protection**. This is also really useful in protecting against malicious software, like viruses. For example, we generally don’t want other programs to have the ability to read or modify the memory of, let say, our email, with that kind of access, malware could send emails on your behalf and maybe steal personal information. Not good! Atlas had both virtual and protected memory. It was the first computer and OS to support these features! 

> 给每个程序分配自己的内存的另一个好处是，它们可以更好地相互隔离。因此，如果一个有问题的程序出了问题，开始写胡言乱语，它只能破坏自己的内存，而不是其他程序的内存。这个功能被称为**内存保护**。这在防止恶意软件，如病毒方面也非常有用。例如，我们一般不希望其他程序有能力读取或修改内存，比方说，我们的电子邮件，有了这种权限，恶意软件就可以代表你发送电子邮件，也许还可以窃取个人信息。这可不好! Atlas有虚拟内存和受保护的内存。它是第一个支持这些功能的计算机和操作系统。

By the 1970s, computers were sufficiently fast and cheap. Institutions like a university could buy a computer and let students use it. It was not only fast enough to run several programs at once, but also give several users simultaneous, interactive access. This was done through a **terminal**, which is a keyboard and screen that connects to a big computer, but doesn’t contain any processing power itself. A refrigerator-sized computer might have 50 terminals connected to it, allowing up to 50 users. Now operating systems had to handle not just multiple programs, but also multiple users. So that no one person could gobble up all of a computer's resources, operating systems were developed that offered **time-sharing**. With time-sharing each individual user was only allowed to utilize a small fraction of the computer’s processor, memory, and so on. Because computers are so fast, even getting just 1/50th of its resources was enough for individuals to complete many tasks. The most influential of early time-sharing Operating Systems was Multics, or Multiplexed Information and Computing Service, released in 1969. Multics was the first major operating system designed to be secure from the outset. Developers didn’t want mischievous users accessing data they shouldn't, like students attempting to access the final exam on their professor’s account. Features like this meant Multics was really complicated for its time, using around 1 Megabit of memory, which was a lot back then! That might be half of a computer's memory, just to run the OS! 

> 到20世纪70年代，计算机已经足够快和便宜。像大学这样的机构可以购买一台电脑，并让学生使用它。它不仅足够快，可以同时运行几个程序，而且还可以让几个用户同时进行互动访问。这是通过**终端**完成的，终端是一个连接到大型计算机的键盘和屏幕，但本身并不包含任何处理能力。一台冰箱大小的计算机可能有50个终端连接到它，允许多达50个用户。现在，操作系统不仅要处理多个程序，而且还要处理多个用户。为了不让一个人吞噬计算机的所有资源，开发了提供**分时**操作系统。通过分时，每个用户只允许使用计算机处理器、内存等的一小部分。由于计算机的速度非常快，即使只得到其资源的1/50也足以让个人完成许多任务。早期分时操作系统中最有影响力的是Multics，即多路信息和计算服务，于1969年发布。Multics是第一个从一开始就被设计为安全的主要操作系统。开发人员不希望恶作剧的用户访问他们不应该访问的数据，比如学生试图访问他们教授账户上的期末考试。像这样的功能意味着Multics在当时是非常复杂的，它使用了大约1兆的内存，这在当时是非常多的！这可能是计算机的一半。这可能是一台计算机的一半内存，只是为了运行操作系统。

Dennis Ritchie, one of the researchers working on Multics, once said: “One of the obvious things that went wrong with Multics as a commercial success was just that it was sort of over-engineered in a sense. There was just too much in it.” T his lead Dennis, and another Multics researcher, Ken Thompson, to strike out on their own and build a new, lean operating system… called **Unix**. They wanted to separate the OS into two parts: First was the core functionality of the OS, things like memory management, multitasking, and dealing with I/O, which is called the **kernel**. The second part was a wide array of useful tools that came bundled with, but not part of the kernel, things like programs and libraries. Building a compact, lean kernel meant intentionally leaving some functionality out. Tom Van Vleck, another Multics developer, recalled: “I remarked to Dennis that easily half the code I was writing in Multics was error recovery code." He said, "We left all that stuff out of Unix. If there's an error, we have this routine called panic, and when it is called, the machine crashes, and you holler down the hall, 'Hey, reboot it.'" ” You might have heard of **kernel panics**, This is where the term came from. It’s literally when the **kernel crashes**, has no recourse to recover, and so calls a function called “panic”. Originally, all it did was print the word “panic” and then enter an infinite loop. This simplicity meant that Unix could be run on cheaper and more diverse hardware, making it popular inside Bell Labs, where Dennis and Ken worked. 

> 从事Multics工作的研究人员之一Dennis Ritchie曾说。"Multics在商业上的成功，其中一个明显的问题是，它在某种意义上被过度设计了。它的内容太多了。" 这导致丹尼斯和另一位Multics研究人员肯-汤普森（Ken Thompson）开始自己动手，建立一个新的、精简的操作系统......称为**Unix**。他们想把操作系统分成两部分。首先是操作系统的核心功能，如内存管理、多任务处理和处理I/O，这被称为**内核**。第二部分是一系列有用的工具，它们与内核捆绑在一起，但不是内核的一部分，如程序和库。构建一个紧凑、精简的内核意味着有意将一些功能排除在外。另一位Multics开发者Tom Van Vleck回忆说。"我对Dennis说，我在Multics中写的代码有一半是错误恢复代码。他说："我们把所有这些东西都从Unix中剔除了。如果出现了错误，我们有一个叫做panic的例程，当它被调用时，机器就会崩溃，你就会在大厅里大喊：'嘿，重新启动它'。" 你可能听说过**内核恐慌**，这是这个术语的来源。从字面上看，它是指**内核崩溃**，没有办法恢复，因此调用一个叫做 "恐慌 "的函数。最初，它所做的只是打印 "恐慌 "一词，然后进入一个无限循环。这种简单性意味着Unix可以在更便宜和更多样化的硬件上运行，这使得它在贝尔实验室内很受欢迎，Dennis和Ken在那里工作。
>

As more developers started using Unix to build and run their own programs, the number of **contributed tools** grew. Soon after its release in 1971, it gained compilers for different programming languages and even a word processor, quickly making it one of the most popular OSes of the 1970s and 80s. At the same time, by the early 1980s, the cost of a basic computer had fallen to the point where individual people could afford one, called a **personal or home computer**. These were much simpler than the big mainframes found at universities, corporations, and governments. So, their operating systems had to be equally simple. For example, **Microsoft’s Disk Operating System**, or MS-DOS, was just 160 kilobytes, allowing it to fit, as the name suggests, onto a single disk. First released in 1981, it became the most popular OS for early home computers, even though it lacked multitasking and protected memory. This meant that programs could, and would, regularly crash the system. While annoying, it was an acceptable tradeoff, as users could just turn their own computers off and on again! 

> 随着越来越多的开发者开始使用Unix来构建和运行他们自己的程序，**贡献的工具**数量不断增加。1971年发布后不久，它就获得了不同编程语言的编译器，甚至还有一个文字处理器，迅速成为70年代和80年代最流行的操作系统之一。同时，到80年代初，基本计算机的成本已经下降到个人可以负担得起的地步，称为**个人或家庭计算机**。这些电脑比大学、公司和政府的大型主机简单得多。因此，他们的操作系统必须同样简单。例如，**微软的磁盘操作系统**，即MS-DOS，只有160千字节，顾名思义，它可以装在一张磁盘上。它于1981年首次发布，成为早期家用电脑最受欢迎的操作系统，尽管它缺乏多任务和受保护的内存。这意味着程序可能而且会经常性地崩溃系统。虽然很烦人，但这是一个可以接受的折衷办法，因为用户可以把自己的电脑关掉，然后再打开。

Even early versions of Windows, first released by Microsoft in 1985 and which dominated the OS scene throughout the 1990s, lacked strong memory protection. When programs misbehaved, you could get the blue screen of death, a sign that a program had crashed so badly that it took down the whole operating system. Luckily, newer versions of Windows have better protections and usually don't crash that often. Today, computers run modern operating systems, like **Mac OS X**, **Windows 10**, **Linux**, **iOS** and **Android**. Even though the computers we own are most often used by just a single person, you! their OSes all have multitasking and virtual and protected memory. So, they can run many programs at once: you can watch **YouTube** in your web browser, edit a photo in **Photoshop,** play music in **Spotify** and sync **Dropbox** all at the same time. This wouldn’t be possible without those decades of research and development on Operating Systems, and of course the proper memory to store those programs. Which we’ll get to next week.

> 即使是微软在1985年首次发布的、在整个20世纪90年代主导操作系统领域的早期版本的Windows，也缺乏强大的内存保护。当程序出现问题时，你可能会出现蓝屏死亡，这是一个程序严重崩溃的迹象，以至于整个操作系统都瘫痪了。幸运的是，较新版本的Windows有更好的保护措施，通常不会经常崩溃。今天，计算机运行现代操作系统，如**Mac OS X**，**Windows 10**，**Linux**，**iOS**和**Android**。尽管我们拥有的电脑最常只被一个人使用，但你！他们的操作系统都有多任务和虚拟及保护内存。因此，它们可以同时运行许多程序：你可以在网络浏览器中观看**YouTube**，在**Photoshop**中编辑照片，在**Spotify**中播放音乐，同时同步**Dropbox**。如果没有几十年来对操作系统的研究和开发，当然还有适当的内存来存储这些程序，这是不可能的。我们下周将讨论这个问题。



## #19 Memory & storage

<iframe width="560" height="315" src="https://www.youtube.com/embed/TQCr9RV7twk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I'm Carrie Anne, and welcome to Crash Course Computer Science! We’ve talked about computer memory several times in this series, and we even designed some in Episode 6. In general, computer memory is **non-permanent**. If your xbox accidently gets unplugged and turns off, any data saved in memory is lost. For this reason, it’s called **volatile memory**. What we haven’t talked so much about this series is **storage**, which is a tad different. Any data written to storage, like your **hard drive**, will stay there until it’s over-written or deleted, even if the power goes out. It’s **non-volatile**. It used to be that volatile memory was fast and non-volatile storage was slow, but as computing technologies have improved, this distinction is becoming less true, and the terms have started to blend together. Nowadays, we take for granted technologies like this little **USB stick**, which offers gigabytes of memory, reliable over long periods of time, all at low cost, but this wasn’t always true. 

> 大家好，我是Carrie Anne，欢迎来到《计算机科学速成班》! 在这个系列中，我们已经多次谈到了计算机内存，我们甚至在第6集设计了一些。一般来说，计算机内存是**非永久性的**。如果你的Xbox不小心被拔掉插头并关闭，保存在内存中的任何数据都会丢失。由于这个原因，它被称为**易失性内存**。我们在这个系列中没有谈及的是**存储器**，这有一点不同。任何写入存储器的数据，如你的**硬盘**，将留在那里，直到它被覆盖或删除，即使停电。它是**非易失性**。过去，易失性存储器速度快，非易失性存储速度慢，但随着计算技术的改进，这种区别越来越不真实，这两个术语已经开始融合在一起。如今，我们认为像这个小小的**U盘**这样的技术是理所当然的，它提供了数千兆字节的内存，在很长一段时间内都很可靠，而且成本很低。但以前不是这样的。
>

INTRO The earliest computer storage was **paper punch cards**, and its close cousin, **punched paper tape**. By the 1940s, punch cards had largely standardized into a grid of 80 columns and 12 rows, allowing for a maximum of 960 bits of data to be stored on a single card. The largest program ever punched onto cards, that we know of, was the US Military’s Semi-Automatic Ground Environment, or SAGE, an Air Defense System that became operational in 1958. The main program was stored on 62,500 punchcards, roughly equivalent to 5 megabytes of data, that’s the size of an average smartphone photo today. Punch cards were a useful and popular form of storage for decades, they didn’t need power, plus paper was cheap and reasonably durable. However, punchcards were slow and write-once, you can’t easily un-punch a hole. So they were a less useful form of memory, where a value might only be needed for a fraction of a second during a program's execution, and then discarded. A faster, larger and more flexible form of computer memory was needed. 

> 简介：最早的计算机存储是**纸质打孔卡**，以及它的近亲，**打孔纸带**。到了20世纪40年代，打孔卡已基本标准化为80列12行的网格，允许在一张卡上最多存储960比特的数据。据我们所知，有史以来最大的打卡程序是美国军方的半自动地面环境，或称SAGE，是一种防空系统，于1958年开始运作。主程序存储在62500张打孔卡上，大约相当于5兆字节的数据，这相当于今天普通智能手机照片的大小。打孔卡是一种有用的、流行了几十年的存储形式，它们不需要电源，加上纸张便宜且相当耐用。然而，打孔卡的速度很慢，而且一旦写入，你就不能轻易取消打孔。因此，它们是一种不太有用的内存形式，在程序执行过程中，一个值可能只需要几分之一秒，然后就被丢弃。我们需要一种更快、更大、更灵活的计算机内存形式。

An early and practical approach was developed by J. Presper Eckert, as he was finishing work on ENIAC in 1944. His invention was called **Delay Line Memory**, and it worked like this. You take a tube and fill it with a liquid, like mercury. Then, you put a speaker at one end and microphone at the other. When you pulse the speaker, it creates a pressure wave. This takes time to propagate to the other end of the tube, where it hits the microphone, converting it back into an electrical signal. And we can use this propagation delay to store data! Imagine that the presence of a pressure wave is a 1 and the absence of a pressure wave is a 0. Our speaker can output a binary sequence like 1010 0111. The corresponding waves will travel down the tube, in order, and a little while later, hit the microphone, which converts the signal back into 1’s and 0’s. If we create a circuit that connects the microphone to the speaker, plus a little amplifier to compensate for any loss, we can create a loop that stores data. The signal traveling along the wire is near instantaneous, so there’s only ever one bit of data showing at any moment in time. But in the tube, you can store many bits! After working on ENIAC, Eckert and his colleague John Mauchly, set out to build a bigger and better computer called EDVAC, incorporating Delay Line Memory. In total, the computer had 128 Delay Lines, each capable of storing 352 bits. That’s a grand total of 45 thousands bits of memory, not too shabby for 1949! This allowed EDVAC to be one of the very earliest **Stored-Program Computers**, which we talked about in Episode 10. However, a big drawback with delay line memory is that you could only read one bit of data from a tube at any given instant. If you wanted to access a specific bit, like bit 112, you’d have to wait for it to come around in the loop, what’s called sequential or **cyclic-access memory**, whereas we really want **random access memory**, where we can access any bit at any time. 

> 1944年，J. Presper Eckert在完成ENIAC的工作时，开发了一种早期的实用方法。他的发明被称为**延迟线内存**，其工作原理是这样的。你拿一个管子，并在里面装上液体，如水银。然后，你在一端放一个扬声器，另一端放麦克风。当你向扬声器发出脉冲时，它会产生一个压力波。这需要时间传播到管子的另一端，在那里它碰到麦克风，将其转换为电信号。而我们可以利用这种传播延迟来存储数据! 想象一下，压力波的存在是1，没有压力波是0。相应的波会按顺序沿管子向下传播，过一会儿就会碰到麦克风，麦克风会将信号转换回1和0。如果我们创建一个连接麦克风和扬声器的电路，再加上一个小放大器来补偿任何损失，我们就可以创建一个存储数据的回路。沿着电线传播的信号几乎是瞬时的，所以在任何时候都只有一个比特的数据显示。但在电子管中，你可以存储许多比特！在ENIAC的工作中，E.S.E.E.E.E.E. 在完成ENIAC的工作后，埃克特和他的同事约翰-莫奇利着手建造一个更大更好的计算机，名为EDVAC，其中包含延迟线存储器。这台计算机总共有128条延迟线，每条能够存储352位。这意味着总共有4.5万比特的内存，对于1949年来说并不算太差。这使得EDVAC成为最早的**存储程序计算机**之一，我们在第10集谈到了这一点。然而，延迟线存储器的一个很大的缺点是，你在任何时候都只能从一个管子中读取一个比特的数据。如果你想访问一个特定的位，比如第112位，你必须等待它在循环中出现，这就是所谓的顺序或**循环访问存储器**，而我们真正想要的是**随机访问存储器**，我们可以在任何时间访问任何位。

It also proved challenging to increase the density of the memory, packing waves closer together meant they were more easily mixed up. In response, new forms of delay line memory were invented, such as magnetostrictive delay lines. These delay lines use a metal wire that could be twisted, creating little torsional waves that represented data. By forming the wire into a coil, you could store around 1000 bits in a 1 foot by 1 foot square. However, delay line memory was largely obsolete by the mid 1950s, surpassed in performance, reliability and cost by a new kid on the block: **magnetic core memory** which was constructed out of little magnetic donuts, called cores. If you loop a wire around this core…. and run an electrical current through the wire, we can magnetize the core in a certain direction. If we turn the current off, the core will stay magnetized. If we pass current through the wire in the opposite direction, the magnetization direction, called polarity, flips the other way. In this way, we can store 1’s and 0’s! 1 bit of memory isn’t very useful, so these little donuts were arranged into grids. There were wires for selecting the right row and column, and a wire that ran through every core, which could be used to read or write a bit. Here is an actual piece of core memory! In each of these little yellow squares, there are 32 rows and 32 columns of tiny cores, each one holding 1 bit of data. So, each of these yellow squares could hold 1024 bits. In total, there are 9 of these, so this memory board could hold a maximum of 9216 bits, which is around 9 kilobytes. The first big use of core memory was MIT’s Whirlwind 1 computer, in 1953, which used a 32 by 32 core arrangement. And, instead of just a single plane of cores, like this, it was 16 boards deep, providing roughly 16 thousand bits of storage. Importantly, unlike delay line memory, any bit could be accessed at any time. This was a killer feature, and magnetic core memory became the predominant Random Access Memory technology for two decades, beginning in the mid 1950s even though it was typically woven by hand! 

> 事实证明，提高存储器的密度也是一种挑战，把波放在一起意味着它们更容易被混淆。作为回应，人们发明了新形式的延迟线存储器，如磁致伸缩延迟线。这些延迟线使用可以被扭曲的金属线，产生代表数据的小的扭转波。通过将金属线形成一个线圈，你可以在一个1英尺乘1英尺的正方形中存储大约1000个比特。然而，延迟线存储器在20世纪50年代中期已基本被淘汰，在性能、可靠性和成本方面被一个新的孩子超越。**磁芯存储器**，它是由被称为磁芯的小磁环构成的。如果你在这个磁芯....，并通过电线运行电流，我们可以将磁芯按一定方向磁化。如果我们把电流关掉，磁芯将保持磁化。如果我们以相反的方向通过电线，磁化方向（称为极性）会翻转到另一个方向。通过这种方式，我们可以存储1和0! 1比特的记忆不是很有用，所以这些小甜甜圈被排列成网格。有一些导线用于选择正确的行和列，还有一条贯穿每个核心的导线，可以用来读取或写入一个位。这里是一块实际的核心存储器 在这些黄色的小方块中，有32行和32列的小核，每一个都有1比特的数据。因此，这些黄色方块中的每一个都可以容纳1024位。总共有9个，所以这个内存板最多可以容纳9216位，大约是9千字节。核心存储器的第一次大规模使用是麻省理工学院的旋风1号计算机，在1953年，它采用了32乘32的核心排列。而且，它不是像这样只有一个核心平面，而是有16块板，提供大约16000比特的存储空间。重要的是，与延迟线存储器不同，任何位都可以在任何时间被访问。这是一个致命的特点，磁芯存储器在20世纪50年代中期开始成为主要的随机存取存储器技术，尽管它通常是由手工编织的。

Although starting at roughly 1 dollar per bit, the cost fell to around 1 cent per bit by the 1970s. Unfortunately, even 1 cent per bit isn’t cheap enough for storage. As previously mentioned, an average smartphone photo is around 5 megabytes in size, that’s roughly 40 million bits. Would you pay 4 hundred thousand dollars to store a photo on core memory? If you have that kind of money to drop, did you know that Crash Course is on Patreon? Right? Wink wink. Anyway, there was tremendous research into storage technologies happening at this time. By 1951, Eckert and Mauchly had started their own company, and designed a new computer called UNIVAC, one of the earliest commercially sold computers. It debuted with a new form of computer storage: **magnetic tape**. This was a long, thin and flexible strip of magnetic material, stored in reels. The tape could be moved forwards or backwards inside of a machine called a tape drive. Inside is a write head, which passes current through a wound wire to generate a magnetic field, causing a small section of the tape to become magnetized. The direction of the current sets the polarity, again, perfect for storing 1’s and 0’s. There was also a separate read head could detect the polarity non-destructively. The UNIVAC used half-inch-wide tape with 8 parallel data tracks, each able to store 128 bits of data per inch. With each reel containing 1200 feet of tape, it meant you could store roughly 15 million bits – that’s almost 2 megabytes! Although tape drives were expensive, the magnetic tape itself was cheap and compact, and for this reason, they’re still used today for archiving data. The main drawback is access speed. Tape is inherently sequential, you have to rewind or fast-forward to get to data you want. This might mean traversing hundreds of feet of tape to retrieve a single byte, which is slow. 

> 虽然开始时每比特大约为1美元，但到20世纪70年代，成本下降到每比特1美分左右。不幸的是，即使是每比特1美分，对于存储来说也不够便宜。如前所述，一张普通的智能手机照片的大小约为5兆字节，这大约是4000万比特。你会花四十万美金在核心存储器上存储一张照片吗？如果你有那么多钱可以砸，你知道Crash Course在Patreon上有赞助页吗？对吗？眨眨眼。无论如何，在这个时候，对存储技术的研究是巨大的。到1951年，埃克特和莫奇利成立了自己的公司，并设计了一台名为UNIVAC的新计算机，这是最早的商业化销售的计算机之一。它以一种新的计算机存储形式首次亮相。**磁带**。这是一条长长的、薄薄的、灵活的磁性材料条，储存在卷轴上。磁带可以在一个叫做磁带机的机器里向前或向后移动。里面有一个写头，它通过绕线的电流产生一个磁场，使磁带的一小部分被磁化。电流的方向设定了极性，同样，对于存储1和0来说是完美的。还有一个单独的读头可以非破坏性地检测极性。UNIVAC使用半英寸宽的磁带，有8条平行的数据轨道，每英寸能存储128比特的数据。每卷磁带包含1200英尺的磁带，这意味着你可以存储大约1500万比特--这几乎是2兆字节! 虽然磁带机很贵，但磁带本身却很便宜，而且结构紧凑，由于这个原因，它们今天仍然被用来归档数据。主要的缺点是访问速度。磁带本身是有顺序的，你必须倒退或快进才能得到你想要的数据。这可能意味着穿越数百英尺的磁带来检索一个字节，这很慢。

magnetic tape

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\magnetic tape.jpg)

A related popular technology in the 1950s and 60s was **Magnetic Drum Memory**. This was a metal cylinder – called a drum – coated in a magnetic material for recording data. The drum was rotated continuously, and positioned along its length were dozens of read and write heads. These would wait for the right spot to rotate underneath them to read or write a bit of data. To keep this delay as short as possible, drums were rotated thousand of revolutions per minute! By 1953, when the technology started to take off, you could buy units able to record 80,000 bits of data – that’s 10 kilobytes, but the manufacture of drums ceased in the 1970s. 

> 20世纪50年代和60年代的一项相关流行技术是**磁鼓存储器**。这是一个金属圆柱体--称为鼓--涂有磁性材料以记录数据。磁鼓是连续旋转的，沿着它的长度放置了几十个读写头。这些读写头将等待合适的位置在它们下面旋转，以读取或写入数据。为了使这一延迟尽可能短，滚筒每分钟要旋转数千圈 到1953年，当这项技术开始起飞时，你可以买到能够记录80,000比特数据的装置--也就是10千字节，但在20世纪70年代鼓的生产停止了。

However, Magnetic Drums did directly lead to the development of **Hard Disk Drives**, which are very similar, but use a different geometric configuration. Instead of large cylinder, **hard disks** use, well… disks… that are hard. Hence the name! The storage principle is the same, the surface of a disk is magnetic, allowing write and read heads to store and retrieve 1’s and 0’s. The great thing about disks is that they are thin, so you can stack many of them together, providing a lot of surface area for data storage. That’s exactly what IBM did for the world's first computer with a disk drive: the RAMAC 305. Sweet name BTW. It contained fifty, 24-inch diameter disks, offering a total storage capacity of roughly 5 megabytes.Yess!! We’ve finally gotten to a technology that can store a single smartphone photo! The year was 1956. To access any bit of data, a read/write head would travel up or down the stack to the right disk, and then slide in between them. Like drum memory, the disks are spinning, so the head has to wait for the right section to come around. The RAMAC 305 could access any block of data, on average, in around 6/10ths of a second, what’s called the **seek time**. While great for storage, this was not nearly fast enough for memory, so the RAMAC 305 also had drum memory and magnetic core memory. 

> 然而，磁鼓确实直接导致了**硬盘驱动器**的发展，它非常相似，但使用不同的几何配置。**硬盘**使用的不是大圆柱体，而是......盘子......很硬。因此而得名。存储原理是一样的，磁盘的表面有磁性，允许写头和读头存储和检索1和0。磁盘的好处是它们很薄，所以你可以把许多磁盘堆在一起，为数据存储提供大量的表面积。这正是IBM为世界上第一台带有磁盘驱动器的计算机所做的：RAMAC 305。名字很好听。它包含50个直径为24英寸的磁盘，提供了大约5兆字节的总存储容量。我们终于得到了一种可以存储一张智能手机照片的技术! 这一年是1956年。要访问任何一点数据，读写头会在堆栈中向上或向下移动到合适的磁盘，然后在它们之间滑动。像鼓式存储器一样，磁盘在旋转，所以读写头必须等待正确的部分出现。RAMAC 305可以访问任何数据块，平均约为10分之6秒，这就是所谓的**寻道时间**。虽然对存储来说很好，但这对内存来说还不够快，所以RAMAC 305也有鼓式内存和磁芯内存。

This is an example of a memory hierarchy, where you have a little bit of fast memory, which is expensive, slightly more medium-speed memory, which is less expensive, and then a lot of slowish memory, which is cheap. This mixed approach strikes a balance between cost and speed. 

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\memory hierarchy.jpg)

> 这是一个内存层次结构的例子，你有一点快速的内存，这很昂贵，稍微多一点的中速内存，这不那么昂贵，然后有很多慢速的内存，这很便宜。这种混合方法在成本和速度之间取得了平衡。

Hard disk drives rapidly improved and became commonplace by the 1970s. A hard disk like this can easily hold 1 terabyte of data today – that’s a trillion bytes – or roughly 200,000 five megabyte photos! And these types of drives can be bought online for as little as 40 US dollars. That’s 0.0000000005 cents per bit. A huge improvement over core memory’s 1 cent per bit! Also, modern drives have an average seek time of under 1/100th of a second. I should also briefly mention a close cousin of hard disks, the **floppy disk**, which is basically the same thing, but uses a magnetic medium that’s, floppy. You might recognise it as the save icon on some of your applications, but it was once a real physical object! It was most commonly used for portable storage, and became near ubiquitous from the mid 1970s up to the mid 90s. And today it makes a pretty good coaster. Higher density floppy disks, like Zip Disks, became popular in the mid 1990s, but fell out of favor within a decade. Optical storage came onto the scene in 1972, in the form of a 12-inch “laser disc.” However, you are probably more familiar with its later, smaller, are more popular cousin, the **Compact Disk**, or **CD**,  as well as the DVD which took off in the 90s. Functionally, these technologies are pretty similar to hard disks and floppy disks, but instead of storing data magnetically, optical disks have little physical divots in their surface that cause light to be reflected differently, which is captured by an optical sensor, and decoded into 1’s and 0’s. 

> 硬盘驱动器迅速改进，并在20世纪70年代成为普遍现象。今天，这样的硬盘可以轻松容纳1TB的数据--也就是一万亿字节--或者大约20万张5兆字节的照片！而这种硬盘在网上只需40美元就可以买到。而且这些类型的硬盘可以在网上买到，价格低至40美元。这相当于每比特0.0000000005美分。比起核心存储器的每比特1美分，这是一个巨大的进步! 另外，现代硬盘的平均寻道时间低于1/100秒。我还应该简单提一下硬盘的一个近亲，即**软盘**，它基本上是一样的，但使用的是磁性介质，即软盘。你可能会认识到它是你一些应用程序上的保存图标，但它曾经是一个真正的实物！它最常用于便携式存储。它最常用于便携式存储，从70年代中期到90年代中期几乎无处不在。今天，它是一个相当不错的杯垫。更高密度的软盘，如Zip Disks，在20世纪90年代中期开始流行，但在十年内就不再受欢迎了。1972年，光学存储以12英寸 "激光盘 "的形式出现在舞台上。然而，你可能更熟悉它后来的、更小的、更受欢迎的表亲--**紧凑型磁盘**，或称**CD**，以及90年代兴起的DVD（光盘）。从功能上讲，这些技术与硬盘和软盘非常相似，但光盘不是用磁力存储数据，而是在其表面有一些物理上的小凹点，导致光线的不同反射，这些光线被光学传感器捕获，并被解码为1和0。

However, today, things are moving to solid state technologies, with no moving parts, like this hard drive and also this USB stick. Inside are **Integrated Circuits**, which we talked about in Episode 15. The first RAM integrated circuits became available in 1972 at 1 cent per bit, quickly making magnetic core memory obsolete. Today, costs have fallen so far, that hard disk drives are being replaced with non-volatile, **Solid State Drives**, or **SSDs**, as the cool kids say. Because they contain no moving parts, they don’t really have to seek anywhere, so SSD access times are typically under 1/1000th of a second. That’s fast! But it’s still many times slower than your computer’s RAM. For this reason, computers today still use **memory hierarchies**. So, we’ve come along way since the 1940s. Much like transistor count and Moore’s law, which we talked about in Episode 14, memory and storage technologies have followed a similar exponential trend. From early core memory costing millions of dollars per megabyte, we’re steadily fallen, to mere cents by 2000, and only fractions of a cent today. Plus, there’s WAY less punch cards to keep track of. Seriously, can you imagine if there was a slight breeze in that room containing the SAGE program? 62,500 punch cards. I don’t even want to think about it. I'll see you next week.

> 然而，今天，事情正在向固态技术发展，没有移动部件，比如这个硬盘和这个U盘。里面是**集成电路**，我们在第15集里谈到过。第一批RAM集成电路于1972年问世，每比特1美分，很快就使磁芯存储器被淘汰。今天，成本已经下降到如此地步，以至于硬盘驱动器正在被非易失性的**固态驱动器**，或称**SSDs**，正如酷儿们所说。因为它们不包含任何移动部件，所以它们不需要寻找任何地方，所以固态硬盘的访问时间通常低于1000分之一秒。这就是快! 但它仍然比你的电脑的RAM慢很多倍。由于这个原因，今天的计算机仍然使用**内存层次结构**。因此，自1940年代以来，我们已经走过了一段路。与我们在第14集谈到的晶体管数量和摩尔定律一样，内存和存储技术也遵循类似的指数趋势。从早期每兆字节数百万美元的核心存储器，我们正在稳步下降，到2000年仅有几分钱，而今天只有几分钱。此外，需要追踪的打卡机也少了很多。说真的，你能想象在那个装有SAGE程序的房间里有一丝微风吗？62,500张打孔卡。我甚至都不愿意去想它。下周见。



## #20 Files & file systems

<iframe width="560" height="315" src="https://www.youtube.com/embed/KN8YgJnShPM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I'm Carrie Anne, and welcome to Crash Course Computer Science! Last episode we talked about data storage, how technologies like magnetic tape and hard disks can store millions, billions and trillions of bits of data, for long durations, even without power. Which is perfect for recording “big blobs” of related data, what are more commonly called computer files. You’ve no doubt encountered many types, like **text files**, **music files**, **photos** and **videos**. Today, we’re going to talk about how files work, and how computers keep them all organized with File Systems. 

> 嗨，我是卡丽-安，欢迎来到《计算机科学速成班》! 上一集我们谈到了数据存储，像磁带和硬盘这样的技术如何在长时间内存储数百万、数十亿和数万亿的数据，甚至在没有电源的情况下。这对于记录相关数据的 "大块"，也就是通常所说的计算机文件来说是最完美的。毫无疑问，你已经遇到了许多类型，如**文本文件**、**音乐文件**、**照片**和**视频**。今天，我们将讨论文件如何工作，以及计算机如何通过文件系统将它们组织起来。
>

INTRO It’s perfectly legal for a file to contain arbitrary, unformatted data, but it’s most useful and practical if the data inside the file is **organized somehow**. This is called a **file format**. You can invent your own, and programmers do that from time to time, but it’s usually best and easiest to use an **existing standard**, like JPEG and MP3. Let’s look at some simple file formats. The most straightforward are T-X-T files, which contain, surprise, text. Like all computer files, this is just a huge list of numbers, stored as binary. If we look at the raw values of a T-X-T file in storage, it would look something like this: We can view this as decimal numbers instead of binary, but that still doesn’t help us read the text. The key to interpreting this data is knowing that T-X-T files use ASCII, a character encoding standard we discussed way back in Episode 4. So, in ASCII, our first value, 72, maps to the capital letter H. And in this way, we decode the whole file. 

> 一个文件包含任意的、没有格式的数据是完全合法的，但如果文件内的数据**以某种方式被组织起来**，则是最有用和实用的。这被称为**文件格式**。你可以发明你自己的格式，程序员也时常这样做，但通常最好和最容易的是使用**存在的标准**，如JPEG和MP3。让我们看一下一些简单的文件格式。最直接的是T-X-T文件，它包含了，令人惊讶的，文本。像所有的计算机文件一样，这只是一个巨大的数字列表，以二进制形式存储。如果我们看一下存储中的T-X-T文件的原始值，它看起来会是这样的。我们可以把它看成十进制数字而不是二进制，但这仍然不能帮助我们阅读文本。解释这些数据的关键是知道T-X-T文件使用ASCII，这是我们在第4集讨论过的字符编码标准。因此，在ASCII中，我们的第一个值，72，对应于大写字母H，以这种方式，我们对整个文件进行解码。

Let’s look at a more complicated example: a **WAVE File** – also called a WAV – which stores **audio**. Before we can correctly read the data, we need to know some information, like the **bit rate** and whether it’s a **single track** or **stereo**. **Data, about data, is called meta data**. This metadata is **stored at the front of the file**, ahead of any actual data, in what’s known as a **Header**. Here’s what the first 44 bytes of a WAV file looks like. Some parts are always the same, like where it spells out W-A-V-E. Other parts contain numbers that change depending on the data contained within. The audio data comes right behind the metadata, and it’s stored as a long list of numbers. These values represent the amplitude of sound captured many times per second, and if you want a primer on sound, check out our video all about it in Crash Course Physics. Link in the dobblydoo. As an example, let’s look at a waveform of me saying: "hello!" Hello! Now that we’ve captured some sound, let’s zoom into a little snippet. A digital microphone, like the one in your computer or smartphone, samples the sound pressure thousands of times. Each sample can be represented as a number. Larger numbers mean higher sound pressure, what’s called **amplitude**. And these numbers are exactly what gets stored in a WAVE file! Thousands of amplitudes for every single second of audio! When it’s time to play this file, an audio program needs to actuate the computer's speakers such that the original waveform is emitted. “Hello!” So, now that you’re getting the hang of file formats. 

> 让我们看一个更复杂的例子：一个**WAVE文件**--也叫WAV--它存储着**音频**。在我们能够正确地读取数据之前，我们需要知道一些信息，比如**码率**和它是**单轨**还是**立体声**。**关于数据的数据，被称为元数据**。这些元数据被**存储在文件的前面**，在任何实际数据之前，被称为**文件头**。下面是一个WAV文件的前44个字节的样子。有些部分总是相同的，比如说它写着W-A-V-E的地方。其他部分包含数字，这些数字的变化取决于其中包含的数据。音频数据就在元数据的后面，它被存储为一长串数字。这些数值代表了每秒捕获的声音的振幅，如果你想了解关于声音的入门知识，可以看看我们在《物理学速成班》中关于声音的视频。链接在dobblydoo。作为一个例子，让我们看一下我说的波形图。"你好！" 你好！"。现在我们已经捕捉到了一些声音，让我们放大到一个小片段。一个数字麦克风，就像你的电脑或智能手机里的那个，对声压进行成千上万次的采样。每个样本都可以表示为一个数字。数字越大意味着声压越高，也就是所谓的**振幅**。而这些数字正是被储存在WAVE文件中的东西! 每一秒钟的音频都有数千种振幅! 当播放这个文件的时候，一个音频程序需要驱动计算机的扬声器，使其发出原始波形。"你好！" 所以，现在你已经掌握了文件格式的窍门。

matedata of wav file:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\metadata of wav.jpg)

Let’s talk about **bitmaps** or B-M-Ps, which **store pictures**. On a computer, pictures are made up of little tiny square elements called **pixels**. **Each pixel is a combination of three colors: red, green and blue.** These are called **additive primary colors**, and they can be mixed together to create any other color on our electronic displays. Now, just like WAV files, BMPs start with metadata, **including key values like image width, image height, and color depth**. As an example, let’s say the metadata specified an image 4 pixels wide, by 4 pixels tall, with a 24-bit color depth - that’s 8-bits for red, 8-bits for green, and 8-bits for blue. As a reminder, 8 bits is the same as one byte. The smallest number a byte can store is 0, and the largest is 255. Our image data is going to look something like this: Let’s look at the color of our first pixel. It has 255 for its red value, 255 for green and 255 for blue. This equates to full intensity red, full intensity green and full intensity blue. These colors blend together on your computer monitor to become white. So our first pixel is white! The next pixel has a Red-Green-Blue, or **RGB value** of 255, 255, 0. That’s the color yellow! The pixel after that has a RGB value of 0,0,0 - that’s zero intensity everything, which is black. And the next one is yellow. Because the metadata specified this was a 4 by 4 image, we know that we’ve reached the end of our first row of pixels. So, we need to drop down a row. The next RGB value is 255,255,0 – yellow again. Okay, let’s go ahead and read all the pixels in our 4x4 image… tada! A very low resolution pac-man! Obviously this is a simple example of a small image, but we could just as easily store this image in a BMP. 

> 让我们来谈谈**位图**或B-M-Ps，它们**存储图片**。在计算机上，图片是由被称为**像素**的小方块元素组成的。**每个像素都是三种颜色的组合：红色、绿色和蓝色。**这些被称为**加色原色**，它们可以混合在一起，在我们的电子显示屏上形成任何其他颜色。现在，就像WAV文件一样，BMP开始有元数据，即**包括像图像宽度、图像高度和颜色深度这样的关键值**。举个例子，假设元数据指定了一个宽4像素、高4像素的图像，颜色深度为24位--即8位红色、8位绿色和8位蓝色。作为提醒，8比特相当于一个字节。一个字节可以存储的最小数字是0，最大是255。我们的图像数据将看起来像这样。让我们看看我们的第一个像素的颜色。它的红色值为255，绿色为255，蓝色为255。这相当于全强度的红色、全强度的绿色和全强度的蓝色。这些颜色在你的电脑显示器上混合在一起，成为白色。所以我们的第一个像素是白色的! 下一个像素的红绿蓝，或**RGB值**为255，255，0。之后的像素的RGB值为0,0,0--就是零强度的一切，也就是黑色。而下一个则是黄色。因为元数据指定这是一张4乘4的图像，我们知道我们已经到达了第一行像素的终点。所以，我们需要往下掉一行。下一个RGB值是255,255,0 - 又是黄色。好了，让我们继续读取我们的4x4图像中的所有像素......嗒嗒! 一个非常低分辨率的帕克人! 显然，这是一个简单的小图像的例子，但我们也可以很容易地将这个图像存储在一个BMP中。

metadata of picture file:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\matadata of bm.jpg)

I want to emphasize again that it doesn’t matter if it’s a text file, WAV, BMP, or fancier formats we don’t have time to discuss, like ZIPs and PPTs. Under the hood, they’re all the same: **long lists of numbers, stored as binary, on a storage device**. **File formats are the key to reading and understanding the data inside**. Now that you understand files a little better, let’s move on to how computers go about storing them. Even though the underlying storage medium might be a strip of tape, a drum, a disk, or integrated circuits... hardware and software abstractions let us think of storage as a long line of little buckets that store values. In the early days, when computers only performed one computation like calculating artillery range tables – the entire storage operated like one big file. Data started at the beginning of storage, and then filled it up in order as output was produced, up to the storage capacity. However, as computational power and storage capacity improved, it became possible, and useful, to store more than one file at a time. 

> 我想再次强调，无论它是文本文件、WAV、BMP，还是我们没有时间讨论的更高级的格式，如ZIP和PPT，都没有关系。在引擎盖下，它们都是一样的。**长长的数字列表，以二进制的形式存储在一个存储设备上**。**文件格式是阅读和理解里面的数据的关键**。现在你对文件有了一些了解，让我们来看看计算机是如何存储它们的。尽管底层的存储介质可能是一条磁带、一个鼓、一个磁盘或集成电路......硬件和软件的抽象让我们认为存储是一长串储存数值的小桶。在早期，当计算机只进行一种计算，如计算火炮射程表时--整个存储的操作就像一个大文件。数据从存储的开头开始，然后随着输出的产生按顺序填满，直到存储容量。然而，随着计算能力和存储容量的提高，一次存储多个文件成为可能，而且很有用。

The simplest option is to store files back-to-back. This can work... but how does the computer know where files begin and end? **Storage devices** have no notion of files – they’re just a mechanism for storing lots of bits. So, for this to work, we need to have a special file that records where other ones are located. This goes by many names, but a good general term is **Directory File**. Most often, it’s kept right at the front of storage, so we always know where to access it. Location zero! Inside the Directory File are the names of all the other files in storage. In our example, they each have a name, followed by a period, and end with what’s called a **File Extension**, like “BMP” or “WAV”. Those further assist programs in identifying file types. The Directory File also stores metadata about these files, like when they were created and last modified, who the owner is, and if it can be read, written or both. But most importantly, the directory file contains where these files begin in storage, and how long they are. If we want to add a file, remove a file, change a filename, or similar, we have to update the information in the Directory File. It’s like the Table of Contents in a book, if you make a chapter shorter, or move it somewhere else, you have to update the table of contents, otherwise the page numbers won’t match! 

> 最简单的选择是背对背（连续）地存储文件。这也是可行的......但计算机如何知道文件的开始和结束？**存储设备**没有文件的概念--它们只是一种存储大量比特的机制。因此，为了使其发挥作用，我们需要有一个特殊的文件来记录其他文件的位置。这有很多名字，但一个好的通用术语是**目录文件**。大多数情况下，它被保存在存储空间的正前方，所以我们总是知道在哪里可以访问它。位置0! 在目录文件内，是存储中所有其他文件的名称。在我们的例子中，它们都有一个名字，后面是一个句号，最后是所谓的**文件扩展名**，如 "BMP "或 "WAV"。这些进一步帮助程序识别文件类型。目录文件还存储了关于这些文件的元数据，如它们何时被创建和最后一次修改，谁是所有者，以及它是否可以被读取、写入或两者都可以。但最重要的是，目录文件包含这些文件在存储中的起始位置，以及它们的长度。如果我们想添加一个文件，删除一个文件，改变一个文件名，或类似的，我们必须更新目录文件中的信息。这就像一本书的目录，如果你把一个章节变短了，或者把它移到别的地方，你必须更新目录，否则页码就不匹配了！"。

location of directory file:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\location of diretory file.jpg)

Inside the directory file:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\inside the directory file.jpg)

The Directory File, and the maintenance of it, is an example of a very basic **File System**, the part of an Operating System that manages and keep track of stored files. This particular example is a called a **Flat File System**, because they’re all stored at one level. It’s flat! Of course, packing files together, back-to-back, is a bit of a problem, because if we want to add some data to let’s say “todo.txt”, there’s no room to do it without overwriting part of “carrie.bmp”. So modern File Systems do two things. First, they store files in blocks. This leaves a little extra space for changes, called **slack space**. It also means that all file data is aligned to a common size, which simplifies management. In a scheme like this, our Directory File needs to keep track of what block each one is stored in. The second thing File Systems do, is allow files to be broken up into chunks and stored across many blocks. So let’s say we open “todo.txt”, and we add a few more items then the file becomes too big to be saved in its one block. We don’t want to overwrite the neighboring one, so instead, the File System allocates an unused block, which can accommodate extra data. With a File System scheme like this, the Directory File needs to store not just one block per file, but rather a list of blocks per file. In this way, we can have files of variable sizes that can be easily expanded and shrunk, simply by allocating and deallocating blocks. If you watched our episode on Operating Systems, this should sound a lot like Virtual Memory. Conceptually it’s very similar! 

> 目录文件，以及对它的维护，是一个非常基本的**文件系统**的例子，它是操作系统中管理和跟踪存储文件的部分。这个特殊的例子被称为**扁平文件系统**，因为它们都存储在一个层次。它是扁平的! 当然，把文件打包在一起，背对背（连续地），是个问题，因为如果我们想在 "todo.txt "中添加一些数据，如果不覆盖 "carrie.bmp "的一部分，就没有地方可以做。因此，现代文件系统做了两件事。首先，他们将文件存储在块中。这为变化留下了一点额外的空间，称为**松弛空间**。这也意味着，所有的文件数据都被调整到一个共同的大小，这就简化了管理。在这样的方案中，我们的目录文件需要跟踪每个文件被存储在哪个块中。文件系统所做的第二件事，是允许将文件分解成几块，并存储在许多块中。因此，假设我们打开 "todo.txt"，并增加了一些项目，然后文件变得太大，无法保存在一个块中。我们不想覆盖邻近的块，所以文件系统分配了一个未使用的块，它可以容纳额外的数据。有了这样的文件系统方案，目录文件需要存储的不仅仅是每个文件的一个块，而是每个文件的块列表。通过这种方式，我们可以拥有大小不一的文件，只需通过分配和删除块，就可以轻松地扩大和缩小。如果你看了我们关于操作系统的一集，这听起来应该很像虚拟内存。从概念上讲，它是非常相似的。

modern file system storage:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\actual storage.jpg)

Now let’s say we want to delete “carrie.bmp”. To do that, we can simply remove the entry from the Directory File. This, in turn, causes one block to become free. Note that we didn’t actually erase the file’s data in storage, we just deleted the record of it. At some point, that block will be overwritten with new data, but until then, it just sits there. This is one way that computer forensic teams can “**recover**” data from computers even though people think it has been deleted. Crafty! Ok, let’s say we add even more items to our todo list, which causes the File System to allocate yet another block to the file, in this case, recycling the block freed from carrie.bmp. Now our “todo.txt” is stored across 3 blocks, spaced apart, and also out of order. Files getting broken up across storage like this is called **fragmentation**. It’s the inevitable byproduct of files being created, deleted and modified. For many storage technologies, this is bad news. 

> 现在让我们说，我们想删除 "carrie.bmp"。要做到这一点，我们可以简单地从目录文件中删除该条目。这反过来又使一个块成为空闲。请注意，我们实际上并没有删除该文件在存储中的数据，我们只是删除了它的记录。在某些时候，该块会被新的数据覆盖，但在那之前，它只是坐在那里。这是计算机取证团队从计算机中 "**恢复**"数据的一种方式，即使人们认为它已经被删除。狡猾! 好吧，让我们说，我们在我们的待办事项列表中添加了更多的项目，这导致文件系统为文件分配了另一个块，在这种情况下，回收从carrie.bmp释放出来的块。现在，我们的 "todo.txt "被存储在3个区块中，间隔开来，而且也没有顺序。像这样的文件在存储中被分割开来，叫做**碎片化**。这是文件被创建、删除和修改的不可避免的副产品。对于许多存储技术来说，这是个坏消息。

On magnetic tape, reading todo.txt into memory would require seeking to block 1, then fast forwarding to block 5, and then rewinding to block 3 – that’s a lot of back and forth! In real world File Systems, large files might be stored across hundreds of blocks, and you don’t want to have to wait five minutes for your files to open. The answer is **defragmentation**! That might sound like technobabble, but the process is really simple, and once upon a time it was really fun to watch! The computer copies around data so that files have blocks located together in storage and in the right order. After we’ve **defragged**, we can read our todo file, now located in blocks 1 through 3, in a single, quick read pass. So far, we’ve only been talking about Flat File Systems, where they’re all stored in one directory. This worked ok when computers only had a little bit of storage, and you might only have a dozen or so files. 

> 在磁带上，将todo.txt读入内存需要寻找到第1块，然后快进到第5块，再倒退到第3块--这是一个很大的来回过程。在现实世界的文件系统中，大文件可能存储在数百个块中，你不希望为打开文件而等待5分钟。答案是**碎片整理**! 这可能听起来像技术废话，但这个过程真的很简单，而且从前它真的很有趣，让人看得津津有味! 计算机在数据周围进行复制，使文件的块在存储中位于一起，并以正确的顺序排列。在我们进行了**碎片整理**之后，我们可以通过一次快速读取的方式来读取我们的todo文件，现在它位于1到3块中。到目前为止，我们只讨论了扁平文件系统，即它们都存储在一个目录中。这在计算机只有少量存储空间，而你可能只有十几个文件的情况下是可行的。

defragmentation:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\defragement.png)

But as storage capacity exploded, like we discussed last episode, so did the number of files on computers. Very quickly, it became impractical to store all files together at one level. Just like documents in the real world, it’s handy to store related files together in folders. Then we can put connected folders into folders, and so on. This is a **Hierarchical File System**, and its what your computer uses.There are a variety of ways to implement this, but let’s stick with the File System example we’ve been using to convey the main idea. The biggest change is that our Directory File needs to be able to point not just to files, but also other directories. To keep track of what’s a file and what’s a directory, we need some extra metadata. This Directory File is the top-most one, known as the **Root Directory**. All other files and folders lie beneath this directory along various file paths. We can see inside of our “Root” Directory File that we have 3 files and 2 subdirectories: music and photos. If we want to see what’s stored in our music directory, we have to go to that block and read the Directory File located there; the format is the same as our root directory. There’s a lot of great songs in there! In addition to being able to create hierarchies of unlimited depth, this method also allows us to easily move around files. 

> 但是，随着存储容量的爆炸性增长，就像我们上一集讨论的那样，计算机上的文件数量也在增加。很快，把所有的文件存储在一个层面上变得不切实际。就像现实世界中的文件一样，把相关的文件存放在一起，放在文件夹里是很方便的。然后我们可以把相连的文件夹放到文件夹中，以此类推。这就是**层次文件系统**，也是你的计算机所使用的。有多种方法来实现这一点，但让我们坚持使用我们一直在使用的文件系统的例子来表达主要思想。最大的变化是，我们的目录文件不仅需要能够指向文件，还需要指向其他目录。为了跟踪什么是文件和什么是目录，我们需要一些额外的元数据。这个目录文件是最上面的一个，被称为**根目录**。所有其他的文件和文件夹都在这个目录下面，沿着各种文件路径。我们可以看到在我们的 "根 "目录文件中，我们有3个文件和2个子目录：音乐和照片。如果我们想看看我们的音乐目录中存储了什么，我们必须去那个区块，读取位于那里的目录文件；其格式与我们的根目录相同。那里有很多很棒的歌曲! 除了能够创建无限深度的层次结构外，这种方法还允许我们轻松地移动文件。

Hierarchical File System:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\file system.jpg)

So, if we wanted to move “theme.wav” from our root directory to the music directory, we don’t have to re-arrange any blocks of data. We can simply modify the two Directory Files, removing an entry from one and adding it to another. Importantly, the theme.wav file stays in block 5. So that’s a quick overview of the key principles of File Systems. They provide yet another way to move up a new level of abstraction. 

> 所以，如果我们想把 "theme.wav "从我们的根目录移到音乐目录中，我们不必重新排列任何数据块。我们可以简单地修改这两个目录文件，从一个目录中删除一个条目，然后把它添加到另一个目录中。重要的是，theme.wav文件保持在第5块。这就是对文件系统关键原理的一个快速概述。它们提供了另一种方法来提升一个新的抽象层次。

File systems allow us to hide the raw bits stored on magnetic tape, spinning disks and the like, and they let us think of data as neatly organized and easily accessible files. We even started talking about users, not programmers, manipulating data, like opening files and organizing them, foreshadowing where the series will be going in a few episodes. I’ll see  you next week. 

> 文件系统使我们能够隐藏存储在磁带、旋转磁盘等的原始比特，它们让我们把数据看作是组织整齐、易于访问的文件。我们甚至开始谈论用户，而不是程序员，操纵数据，比如打开文件和组织它们，预示着这个系列将在几集里进行。下周见。



## #21 Compression

<iframe width="560" height="315" src="https://www.youtube.com/embed/OtDxDvCpPL4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I'm Carrie Anne, and welcome to Crash Course Computer Science! Last episode we talked about Files, bundles of data, stored on a computer, that are formatted and arranged to encode information, like text, sound or images. We even discussed some basic file formats, like text, wave, and bitmap. While these formats are perfectly fine and still used today, their simplicity also means they’re not very efficient. Ideally, we want files to be as small as possible, so we can store lots of them without filling up our hard drives, and also transmit them more quickly. Nothing is more frustrating than waiting for an email attachment to download. Ugh! The answer is **compression**, which literally squeezes data into a smaller size. To do this, we have to encode data using fewer bits than the original representation. That might sound like magic, but it’s actually computer science! 

> 上一集我们谈到了文件，即存储在计算机上的数据束，它们被格式化并被安排成信息编码，如文本、声音或图像。上一集我们谈到了文件，即存储在计算机上的数据束，它们被格式化并被安排用来编码信息，如文本、声音或图像。我们甚至讨论了一些基本的文件格式，如文本、波形和位图。虽然这些格式完全没有问题，而且今天仍然在使用，但它们的简单性也意味着它们的效率不高。理想情况下，我们希望文件越小越好，这样我们就可以存储大量的文件而不至于塞满我们的硬盘，同时也可以更快地传输它们。没有什么比等待一个电子邮件附件的下载更令人沮丧的了。唉! 答案是**压缩**，它实际上是将数据压缩到更小的尺寸。要做到这一点，我们必须使用比原始表述更少的比特对数据进行编码。这可能听起来像魔术，但实际上是计算机科学！

INTRO Lets return to our old friend from last episode, Mr. Pac-man! This image is 4 pixels by 4 pixels. As we discussed, **image data is typically stored as a list of pixel values**. To know where rows end, image files have metadata, which defines properties like dimensions. But, to keep it simple today, we’re not going to worry about it. Each pixel’s color is a combination of three additive primary colors: red, green and blue. We store each of those values in one byte, giving us a range of 0 to 255 for each color. If you mix full intensity red, green and blue - that’s 255 for all three values - you get the color white. If you mix full intensity red and green, but no blue (it’s 0), you get yellow. We have 16 pixels in our image, and each of those needs 3 bytes of color data. That means this image’s data will consume 48 bytes of storage. But, we can compress the data and pack it into a smaller number of bytes than 48! One way to compress data is to **reduce repeated or redundant information**. The most straightforward way to do this is called **Run-Length Encoding**. This takes advantage of the fact that there are often runs of identical values in files. For example, in our pac-man image, there are 7 yellow pixels in a row. Instead of encoding redundant data: yellow pixel, yellow pixel, yellow pixel, and so on, we can just say “there’s 7 yellow pixels in a row” by inserting an extra byte that specifies the length of the run, like so: And then we can eliminate the redundant data behind it. To ensure that computers don’t get confused with which bytes are run lengths and which bytes represent color, we have to be consistent in how we apply this scheme. So, we need to preface all pixels with their run-length. In some cases, this actually adds data, but on the whole, we’ve dramatically reduced the number of bytes we need to encode this image. We’re now at 24 bytes, down from 48. That’s 50% smaller! A huge saving! 

> 让我们回到上一集的老朋友，吃豆人先生! 这张图片是4像素乘4像素。正如我们所讨论的，**图像数据通常以像素值的列表形式存储**。为了知道行的终点，图像文件有元数据，它定义了尺寸等属性。但是，为了保持简单，今天我们不打算担心这个问题。每个像素的颜色都是三种加法原色的组合：红、绿、蓝。我们将这些值分别存储在一个字节中，使每种颜色的范围为0至255。如果你混合了全强度的红、绿和蓝--这三个值都是255--你会得到白色。如果你混合了全强度的红色和绿色，但没有蓝色（是0），你会得到黄色。我们的图像有16个像素，每个像素需要3个字节的颜色数据。这意味着这个图像的数据将消耗48字节的存储空间。但是，我们可以压缩数据，将其打包成比48个字节更小的字节数！一种方法是压缩数据。压缩数据的一个方法是**减少重复或多余的信息**。做到这一点的最直接的方法被称为**运行长度编码**。它利用了文件中经常有相同数值的运行这一事实。例如，在我们的pac-man图像中，一排有7个黄色像素。我们不需要对冗余数据进行编码：黄色像素、黄色像素、黄色像素，等等，我们可以直接说 "一排有7个黄色像素"，插入一个额外的字节，指定运行的长度，像这样。然后我们就可以消除后面的冗余数据。为了确保计算机不会混淆哪些字节是运行长度，哪些字节代表颜色，我们在应用这一方案时必须保持一致。因此，我们需要在所有像素前加上它们的运行长度。在某些情况下，这实际上增加了数据，但总的来说，我们已经大大减少了编码该图像所需的字节数。我们现在是24个字节，比48个字节少了。这就减少了50%! 一个巨大的节省！

run length:

![run length](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\run length.jpg)

Also note that we haven’t lost any data. We can easily expand this back to the original form without any degradation. A compression technique that has this characteristic is called **lossless compression**, because we don’t lose anything. The decompressed data is identical to the original before compression, bit for bit. Let's take a look at another type of lossless compression, where **blocks of data are replaced by more compact representations**. This is sort of like “don’t forget to be awesome” being replaced by DFTBA. To do this, we need a dictionary that stores the mapping from codes to data. Lets see how this works for our example. We can view our image as not just a string of individual pixels, but as little blocks of data. For simplicity, we’re going to use pixel pairs, which are 6 bytes long, but blocks can be any size. In our example, there are only four pairings: White-yellow, black-yellow, yellow-yellow and white-white. Those are the data blocks in our dictionary we want to generate compact codes for. What’s interesting, is that these blocks occur at different frequencies. There are 4 yellow-yellow pairs, 2 white-yellow pairs, and 1 each of black-yellow and white-white. Because yellow-yellow is the most common block, we want that to be substituted for the most compact representation. On the other hand, black-yellow and white-white, can be substituted for something longer because those blocks are infrequent. 

> 还要注意的是，我们并没有丢失任何数据。我们可以很容易地将其扩展回原来的形式，而不会出现任何退化。具有这种特性的压缩技术被称为**无损压缩**，因为我们没有损失任何东西。解压后的数据与压缩前的原始数据完全一样，逐位相同。让我们来看看另一种类型的无损压缩，即**数据块被更紧凑的表示所取代**。这有点像 "不要忘了，你真棒 "被DFTBA所取代。要做到这一点，我们需要一个字典来存储从代码到数据的映射。让我们看看这在我们的例子中是如何运作的。我们可以把我们的图像看作不仅仅是一串单独的像素，而是一个个小的数据块。为了简单起见，我们将使用像素对，其长度为6字节，但块可以是任何大小。在我们的例子中，只有四个配对。白-黄、黑-黄、黄-黄和白-白。这些是我们的字典中的数据块，我们想为其生成紧凑的代码。有趣的是，这些数据块以不同的频率出现。有4对黄-黄，2对白-黄，以及黑-黄和白-白各1对。因为黄-黄是最常见的区块，我们希望将其替换为最紧凑的表示。另一方面，黑-黄和白-白，可以被替换成更长的东西，因为这些块不常出现。

One method for generating efficient codes is building a **Huffman Tree**, invented by David Huffman while he was a student at MIT in the 1950s. His algorithm goes like this. First, you layout all the possible blocks and their frequencies. At every round, you select the two with the lowest frequencies. Here, that’s Black-Yellow and White-White, each with a frequency of 1. You combine these into a little tree... ...which have a combined frequency of 2, so we record that. And now one step of the algorithm done. Now we repeat the process. This time we have three things to choose from. Just like before, we select the two with the lowest frequency, put them into a little tree, and record the new total frequency of all the sub items. Ok, we’re almost done. This time it’s easy to select the two items with the lowest frequency because there are only two things left to pick. We combine these into a tree, and now we’re done! Our tree looks like this, and it has a very cool property: it’s arranged by frequency, with less common items lower down. So, now we have a tree, but you may be wondering how this gets us to a dictionary. Well, we use our frequency-sorted tree to generate the codes we need by labeling each branch with a 0 or a 1, like so: With this, we can write out our code dictionary. Yellow-yellow is encoded as just a single 0. White-yellow is encoded as 1 0 (“one zero”) Black-Yellow is 1 1 0 and finally white-white is 1 1 1. The really cool thing about these codewords is that there’s no way to have conflicting codes, because each path down the tree is unique. This means our codes are prefix-free, that is no code starts with another complete code. 

> 产生高效代码的一种方法是建立**哈夫曼树**，这是大卫-哈夫曼在20世纪50年代在麻省理工学院当学生时发明的。他的算法是这样的。首先，你将所有可能的区块和它们的频率进行布局。在每一轮中，你选择频率最低的两个。这里是黑-黄和白-白，每个频率都是1。你把这些组合成一个小树......它们的组合频率是2，所以我们记录下来。现在算法的一个步骤完成了。现在我们重复这个过程。这一次我们有三样东西可以选择。就像以前一样，我们选择频率最低的两样东西，把它们放到一个小树上，然后记录所有子项目的新的总频率。好了，我们快完成了。这一次很容易选择频率最低的两个项目，因为只剩下两个东西可以挑选。我们把这些东西合并成一棵树，现在我们就完成了! 我们的树看起来是这样的，它有一个非常酷的特性：它是按频率排列的，不常见的项目在下面。所以，现在我们有了一棵树，但你可能想知道这怎么能让我们得到一个字典。好吧，我们使用我们的频率排序树来生成我们需要的代码，在每个分支上标上0或1，像这样。有了这个，我们就可以写出我们的代码字典了。黄-黄被编码为一个0，白-黄被编码为1个0（"一个0"），黑-黄是1个0，最后白-白是1个1。这些代码真正酷的地方在于，不可能有冲突的代码，因为树上的每条路径都是唯一的。这意味着我们的代码是无前缀的，也就是说，没有代码是以另一个完整的代码开始的。

Huffman tree Dictionary:

![](H:\资料资源\regular-invest-on-evolution\编程课\计算机知识\reference pics\huffman tree dictionary.jpg)

Now, let’s return to our image data and compress it! Our first pixel pair, white-yellow, is substituted for the bits “1 0”. The next pair is black-yellow, which is substituted for “1 1 0”. Next is yellow-yellow with the incredibly compact substitution of just “0”. And this process repeats for the rest of the image: So instead of 48 bytes of image data ...this process has encoded it into 14 bits -- NOT BYTES -- BITS!! That’s less than 2 bytes of data! But, don’t break out the champagne quite yet! This data is meaningless unless we also save our code dictionary. So, we’ll need to append it to the front of the image data, like this. Now, including the dictionary, our image data is 30 bytes long. That’s still a significant improvement over 48 bytes. The two approaches we discussed, **removing redundancies and using more compact representations, are often combined, and underlie almost all lossless compressed file formats**, like GIF, PNG, PDF and ZIP files. Both run-length encoding and dictionary coders are lossless compression techniques. No information is lost; when you decompress, you get the original file. That’s really important for many types of files. Like, it’d be very odd if I zipped up a word document to send to you, and when you decompressed it on your computer, the text was different. 

> 现在，让我们回到我们的图像数据并对其进行压缩! 我们的第一个像素对，白色-黄色，被替换为 "1 0 "位。下一对是黑-黄，被替换为 "1 1 0"。接下来是黄-黄，仅用 "0 "进行令人难以置信的紧凑替换。这个过程在图像的其余部分重复进行。因此，48个字节的图像数据......这个过程将其编码为14个比特 -- 不是字节 -- 是比特！！这还不到2个字节。这还不到2个字节的数据! 但是，先别急着开香槟! 除非我们同时保存我们的代码字典，否则这些数据是没有意义的。所以，我们需要把它附加到图像数据的前面，像这样。现在，包括字典在内，我们的图像数据有30个字节长。这比48字节还是有很大的进步。我们讨论过的两种方法，**去除冗余和使用更紧凑的表示方法，经常结合在一起，是几乎所有无损压缩文件格式的基础**，如GIF、PNG、PDF和ZIP文件。运行长度编码和字典编码器都是无损压缩技术。没有信息丢失；当你解压时，你会得到原始文件。这对许多类型的文件来说真的很重要。比如，如果我压缩了一个word文档发给你，而当你在你的电脑上解压时，文本却不一样了，那就很奇怪了。

But, there are other types of files where we can get away with little changes, perhaps by removing unnecessary or less important information, especially information that human perception is not good at detecting. And this trick underlies most **lossy compression** techniques. These tend to be pretty complicated, so we’re going to attack this at a conceptual level. Let’s take sound as an example. Your hearing is not perfect. We can hear some frequencies of sound better than others. And there are some we can’t hear at all, like ultrasound. Unless you’re a bat. Basically, if we make a recording of music, and there’s data in the ultrasonic frequency range, we can discard it, because we know that humans can’t hear it. On the other hand, humans are very sensitive to frequencies in the vocal range, like people singing, so it’s best to preserve quality there as much as possible. Deep bass is somewhere in between. Humans can hear it, but we’re less attuned to it. We mostly sense it. Lossy audio compressors takes advantage of this, and encode different frequency bands at different precisions. Even if the result is rougher, it’s likely that users won’t perceive the difference. Or at least it doesn’t dramatically affect the experience. And here comes the hate mail from the audiophiles! You encounter this type of audio compression all the time. It’s one of the reasons you sound different on a cellphone versus in person. The audio data is being compressed, allowing more people to take calls at once. 

> 但是，在其他类型的文件中，我们可以不做什么改动，也许通过删除不必要的或不太重要的信息，特别是人类感知不擅长检测的信息。而这种技巧是大多数**有损压缩**技术的基础。这些往往是相当复杂的，所以我们要在概念层面上攻击这个问题。让我们以声音为例。你的听力并不完美。我们可以听到一些频率的声音比其他频率更好。还有一些我们根本就听不到，比如超声波。除非你是一只蝙蝠。基本上，如果我们做一个音乐录音，其中有超声波频率范围内的数据，我们可以将其丢弃，因为我们知道人类听不到它。另一方面，人类对人声范围内的频率非常敏感，比如人们唱歌，所以最好尽可能地保留那里的质量。低音是介于两者之间的。人类可以听到它，但我们对它的适应性较差。我们主要是感觉到它。有损的音频压缩器利用了这一点，以不同的精度对不同的频段进行编码。即使结果更粗糙，用户也很可能感觉不到差别。或者说，至少它不会极大地影响体验。来自发烧友的仇恨邮件来了! 你经常会遇到这种类型的音频压缩。这也是你在手机上的声音与在人前的声音不同的原因之一。音频数据被压缩，允许更多人同时接听电话。

As the signal quality or bandwidth get worse, compression algorithms remove more data, further reducing precision, which is why Skype calls sometimes sound like robots talking. Compared to an uncompressed audio format, like a WAV or FLAC (there we go, got the audiophiles back) compressed audio files, like MP3s, are often 10 times smaller. That’s a huge saving! And it’s why I’ve got a killer music collection on my retro iPod. Don’t judge. This idea of discarding or reducing precision in a manner that aligns with human perception is called **perceptual coding**, and it relies on models of human perception, which come from a field of study called **Psychophysics**. This same idea is the basis of lossy compressed image formats, most famously JPEGs. Like hearing, the human visual system is imperfect. We’re really good at detecting sharp contrasts, like the edges of objects, but our perceptual system isn’t so hot with subtle color variations. JPEG takes advantage of this by breaking images up into blocks of 8x8 pixels, then throwing away a lot of the high-frequency spatial data. For example, take this photo of our directors dog - Noodle. So cute! Let’s look at patch of 8x8 pixels. Pretty much every pixel is different from its neighbor, making it hard to compress with loss-less techniques because there’s just a lot going on. Lots of little details. But human perception doesn’t register all those details. So, we can discard a lot of that detail, and replace it with a simplified patch like this. This maintains the visual essence, but might only use 10% of the data. We can do this for all the patches in the image and get this result. You can still see it’s a dog, but the image is rougher. So, that’s an extreme example, going from a slightly compressed JPEG to a highly compressed one, one-eighth the original file size. 

> 随着信号质量或带宽越来越差，压缩算法会删除更多的数据，进一步降低精度，这就是为什么Skype电话有时听起来像机器人在说话。与未压缩的音频格式相比，如WAV或FLAC（我们走了，让发烧友们回来了），压缩的音频文件，如MP3，通常小10倍。这是一个巨大的节省! 这就是为什么我的复古iPod上有一个杀手级的音乐收藏。不要评判。这种以符合人类感知的方式抛弃或降低精度的想法被称为**感知编码**，它依赖于人类感知的模型，这些模型来自一个被称为**心理物理学**的研究领域。这个想法也是有损压缩图像格式的基础，最有名的是JPEG。像听觉一样，人类的视觉系统是不完美的。我们真的很擅长检测尖锐的对比，比如物体的边缘，但我们的感知系统对微妙的颜色变化并不那么敏感。JPEG(Joint Photographic Experts Group)利用了这一点，将图像分割成8x8的像素块，然后丢弃了大量的高频空间数据。例如，拿这张我们导演的狗--Noodle的照片来说。太可爱了! 让我们来看看8x8像素的补丁。几乎每个像素都与它的邻居不同，使得它很难用无损技术进行压缩，因为有很多事情要做。大量的小细节。但人类的感知并不能记录所有这些细节。因此，我们可以丢弃很多细节，用这样一个简化的补丁来代替它。这就保持了视觉上的本质，但可能只用了10%的数据。我们可以对图像中的所有斑块都这样做，得到这样的结果。你仍然可以看到那是一只狗，但图像更粗糙了。所以，这是一个极端的例子，从一个略微压缩的JPEG到一个高度压缩的，八分之一的原始文件大小。

Often, you can get away with a quality somewhere in between, and perceptually, it’s basically the same as the original. The one on the left is one-third the file size of the one on the right. That’s a big savings for essentially the same thing. Can you tell the difference between the two? Probably not, but I should mention that video compression plays a role in that too, since I’m literally being compressed in a video right now. Videos are really just long sequences of images, so a lot of what I said about them applies here too. But videos can do some extra clever stuff, because between frames, a lot of pixels are going to be the same. Like this whole background behind me! This is called **temporal redundancy**. We don’t need to re-transmit those pixels every frame of the video. We can just copy patches of data forward. When there are small pixel differences, like the readout on this frequency generator behind me, most video formats send data that encodes just the difference between patches, which is more efficient than re-transmitting all the pixels afresh, again taking advantage of **inter-frame similarity**. The fanciest video compression formats go one step further. They find patches that are similar between frames, and not only copy them forward, with or without differences, but also can apply simple effects to them, like a shift or rotation. They can also lighten or darken a patch between frames. So, if I move my hand side to side like this the video compressor will identify the similarity, capture my hand in one or more patches, then just move these patches around between frames. You’re actually seeing my hand from the past… kinda freaky, but it uses a lot less data. **MPEG-4 videos**, a common standard, are often 20 to 200 times smaller than the original, uncompressed file. 

> 通常情况下，你可以使用介于两者之间的质量，而且从感观上讲，它基本上与原件相同。左边这张照片的文件大小是右边这张的三分之一。对于本质上相同的东西来说，这是一个很大的节省。你能看出两者之间的区别吗？也许不能，但我应该提到，视频压缩在其中也起了作用，因为我现在正被压缩在一个视频中。视频实际上只是图像的长序列，所以我说的关于它们的很多东西在这里也适用。但视频可以做一些特别聪明的事情，因为在帧与帧之间，很多像素都是一样的。就像我身后的这整个背景。这就是所谓的**时空冗余**。我们不需要在视频的每一帧都重新传输这些像素。我们可以只是向前复制数据的补丁。当有小的像素差异时，比如我身后的这个频率发生器的读数，大多数视频格式发送的数据只是编码了补丁之间的差异，这比重新传输所有的像素更有效率，再次利用了**帧间的相似性**。最先进的视频压缩格式更进一步。它们找到帧间相似的补丁，不仅将它们向前复制，无论有无差异，而且还可以对它们应用简单的效果，如移动或旋转。它们还可以在帧之间使一个补丁变亮或变暗。因此，如果我的手像这样左右移动，视频压缩器会识别出相似性，在一个或多个补丁中捕捉我的手，然后在帧之间移动这些补丁。你实际上看到的是我过去的手......有点怪异，但它使用的数据要少很多。**MPEG-4视频**，一种常见的标准，通常比未经压缩的原始文件小20到200倍。

However, encoding frames as translations and rotations of patches from previous frames can go horribly wrong when you compress too heavily, and there isn’t enough space to update pixel data inside of the patches. The video player will forge ahead, applying the right motions, even if the patch data is wrong. And this leads to some hilarious and trippy effects, which I’m sure you’ve seen. Overall, it’s extremely useful to have compression techniques for all the types of data I discussed today. (I guess our imperfect vision and hearing are “useful,” too.) And it’s important to know about compression because **it allows users to store pictures, music, and videos in efficient ways**. Without it, streaming your favorite Carpool Karaoke videos on YouTube would be nearly impossible, **due to bandwidth and the economics of transmitting that volume of data for free**. And now when your Skype calls sound like they’re being taken over by demons, you’ll know what’s really going on. I’ll see you next week.

> 然而，如果你压缩得太厉害，没有足够的空间来更新补丁内的像素数据，那么将帧编码为前一帧补丁的平移和旋转会出现可怕的错误。视频播放器将继续前进，应用正确的动作，即使补丁数据是错误的。而这导致了一些搞笑的、迷幻的效果，我相信你已经看到了。总的来说，对于我今天讨论的所有类型的数据，拥有压缩技术是非常有用的。(我想我们不完美的视觉和听觉也是 "有用的"。)而且了解压缩技术很重要，因为**它允许用户以有效的方式存储图片、音乐和视频**。没有它，在YouTube上播放你最喜欢的Carpool Karaoke视频几乎是不可能的，**因为带宽和免费传输这种数据量的经济性**。现在，当你的Skype电话听起来像被恶魔占领时，你会知道到底发生了什么。下周见。



## #22 Keyboards & Command line interfaces

<iframe width="560" height="315" src="https://www.youtube.com/embed/4RPtJ9UyHS0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science! We’ve talked a lot about inputs and outputs in this series, but they’ve mostly been between different parts of a computer – like outputting data from RAM or inputting instructions to a CPU. We haven’t discussed much about inputs coming from humans. We also haven’t learned how people get information out of a computer, other than by printing or punching it onto paper. Of course, there’s a wide variety of **input and output devices** that allow us users to communicate with computers. They provide an interface between human and computer. And today, there’s a whole field of study called **Human-Computer Interaction**. These interfaces are so fundamental to the user experience that they’re the focus of the next few episodes.

> 在这个系列中，我们已经谈了很多关于输入和输出的问题，但它们大多是在计算机的不同部分之间--比如从RAM输出数据或向CPU输入指令。我们还没怎么讨论过来自人类的输入。我们也没有了解到人们是如何从计算机中获取信息的，除了通过打印或打到纸上。当然，有各种各样的**输入和输出设备**，使我们用户能够与计算机进行交流。它们为人类和计算机之间提供了一个接口。而今天，有一个完整的研究领域叫做**人机交互**。这些界面是用户体验的基础，所以它们是接下来几集的重点。
>

INTRO As we discussed at the very beginning of the series, the earliest mechanical and electro-mechanical computing devices used physical controls for inputs and outputs, like gears, knobs and switches, and this was pretty much the extent of the human interface. Even the first electronic computers, like Colossus and ENIAC, were configured using huge panels of mechanical controls and patch wires. It could take weeks to enter in a single program, let alone run it, and to get data out after running a program, results were most often printed to paper. Paper printers were so useful that even Babbage designed one for his Difference Engine, and that was in the 1820s! However, by the 1950s, mechanical inputs were rendered obsolete by programs and data stored entirely on mediums like punch cards and magnetic tape. Paper printouts were still used for the final output, and huge banks of indicator lights were developed to provide real time feedback while the program was in progress. It’s important to recognize that computer input of this era was designed to be as simple and robust as possible for computers. Ease and understanding for users was a secondary concern. Punch tape is a great example – this was explicitly designed to be easy for computers to read. The continuous nature of tape made it easy to handle mechanically, and the holes could be reliably detected with a mechanical or optical system, which encoded instructions and data. 

> 正如我们在这个系列的一开始所讨论的，最早的机械和电子机械计算设备使用物理控制进行输入和输出，比如齿轮、旋钮和开关，这几乎就是人类界面的范围。即使是最早的电子计算机，如Colossus和ENIAC，也是使用巨大的机械控制面板和跳线来配置的。输入一个程序可能需要数周时间，更不用说运行它了，而在运行一个程序后要想得到数据，最常见的是将结果打印在纸上。纸张打印机非常有用，甚至巴贝奇也为他的 "差分引擎 "设计了一台，而那是在19世纪20年代。然而，到了20世纪50年代，机械输入已经被完全存储在打卡和磁带等介质上的程序和数据所淘汰了。纸张打印仍被用于最终输出，巨大的指示灯组被开发出来，在程序进行中提供实时反馈。重要的是要认识到，这个时代的计算机输入被设计成对计算机来说尽可能的简单和强大。对用户来说，方便和理解是次要的。打孔磁带就是一个很好的例子--它被明确地设计为便于计算机读取。磁带的连续性质使它容易被机械地处理，而且孔可以被机械或光学系统可靠地检测到，从而对指令和数据进行编码。

But of course, humans don’t think in terms of little punched holes on strips of paper. So, the burden was on programmers. They had to spend the extra time and effort to convert their ideas and programs into a language and a format that was easy for computers of the era to understand – often with the help of additional staff and auxiliary devices. It’s also important to note that early computers, basically pre-1950, had an extremely simple notion of human input. Yes, humans input programs and data into computers, but these machines generally didn’t respond interactively to humans. Once a program was started, it typically ran until it was finished. That’s because these machines were way too expensive to be waiting around for humans to type a command or enter data. Any input needed for a computation was fed in at the same time as the program. This started to change in the late 1950s. On one hand, smaller-scale computers started to become cheap enough that it was feasible to have a **human-in-the loop**; that is, a back and forth between human and computer(HITL). And on the other hand, big fancy computers became fast and sophisticated enough to support many programs and users at once, what were called **multitasking** and **time-sharing systems**. 

> 但当然，人类不会用纸条上的小孔来思考问题。所以，负担就落在了程序员身上。他们必须花费额外的时间和精力，将他们的想法和程序转换成那个时代的计算机容易理解的语言和格式--通常需要借助额外的工作人员和辅助设备。同样重要的是要注意，早期的计算机，基本上是1950年以前的计算机，对人类的输入有一个极其简单的概念。是的，人类向计算机输入程序和数据，但这些机器一般不会对人类作出互动反应。一旦一个程序被启动，它通常会一直运行到结束。这是因为这些机器太昂贵了，不能等待人类输入命令或输入数据。计算所需的任何输入都是与程序同时输入的。这种情况在20世纪50年代末开始改变。一方面，较小规模的计算机开始变得足够便宜，以至于有一个人的循环是可行的；也就是说，人和计算机之间来回交互(HITL).。另一方面，大型花哨的计算机变得足够快和复杂，可以同时支持许多程序和用户，即所谓的**多任务**和**分时系统**。
>

But these computers needed a way to get input from users. For this, computers borrowed the ubiquitous data entry mechanism of the era: **keyboards**. At this point, typing machines had already been in use for a few centuries, but it was Christopher Latham Sholes, who invented the modern typewriter in 1868. It took until 1874 to refine the design and manufacture it, but it went on to be a commercial success. Sholes’ typewriter adopted an unusual keyboard layout that you know well – QWERTY – named for the top-left row of letter keys. There has been a lot of speculation as to why this design was used. The most prevalent theory is that it put common letter pairings in English far apart to reduce the likelihood of typebars jamming when entered in sequence. It’s a convenient explanation, but it’s also probably false, or at least not the full story. In fact, QWERTY puts many common letter pairs together, like “TH” and “ER”. And we know that Sholes and his team went through many iterations before arriving at this iconic arrangement. Regardless of the reason, the commercial success of Sholes’ typewriter meant the competitor companies that soon followed duplicated his design. Many alternative keyboard layouts have been proposed over the last century, claiming various benefits. But, once people had invested the time to learn QWERTY, they just didn't want to learn something new. This is what economists would call a switching barrier or switching cost. And it’s for this very basic human reason that we still use QWERTY keyboards almost a century and a half later! I should mention that QWERTY isn’t universal. There are many international variants, like the French AZERTY layout, or the QWERTZ layout common in central Europe. 

> 但这些计算机需要一种方法来获得用户的输入。为此，计算机借用了那个时代无处不在的数据输入机制。**键盘**。此时，打字机已经使用了几个世纪，但在1868年发明现代打字机的是克里斯托弗-拉萨姆-肖尔斯。直到1874年才完善了设计和制造，但它在商业上获得了成功。肖尔斯的打字机采用了你所熟悉的不寻常的键盘布局--QWERTY--以左上角的一排字母键命名。对于为什么采用这种设计，人们有很多猜测。最普遍的理论是，它把英语中常见的字母配对相隔很远，以减少依次输入时打字杆堵塞的可能性。这是一个方便的解释，但它也可能是错误的，或者至少不是完整的故事。事实上，QWERTY把许多常见的字母对放在一起，比如 "TH "和 "ER"。而且我们知道，肖尔斯和他的团队在达成这种标志性的安排之前经历了许多次反复。不管是什么原因，肖尔斯打字机在商业上的成功意味着不久之后的竞争公司都会复制他的设计。在过去的一个世纪里，人们提出了许多替代性的键盘布局，声称有各种好处。但是，一旦人们投入时间学习QWERTY，他们就不想再学习新东西了。这就是经济学家所说的转换障碍或转换成本。正是由于这个非常基本的人类原因，我们在将近一个半世纪之后仍然使用QWERTY键盘！我应该提到，QWERTY并不普遍。有许多国际变体，如法国的AZERTY布局，或中欧常见的QWERTZ布局。
>

Interestingly, Sholes didn’t envision that typing would ever be faster than handwriting, which is around 20 words per minute. Typewriters were introduced chiefly for legibility and standardization of documents, not speed. However, as they became standard equipment in offices, the desire for speedy typing grew, and there were two big advances that unlocked typing’s true potential. Around 1880, Elizabeth Longley, a teacher at the Cincinnati Shorthand and Type-Writer Institute, started to promote ten-finger typing. This required much less finger movement than hunt-and-peck, so it offered enhanced typing speeds. Then, a few years later, Frank Edward McGurrin, a federal court clerk in Salt Lake City, taught himself to touch-type; as in, he didn’t need to look at the keys while typing. In 1888, McGurrin won a highly publicized typing-speed contest, after which ten-finger, touch-typing began to catch on. Professional typists were soon able to achieve speeds upwards of 100 words per minute, much faster than handwriting! And nice and neat too! So, humans are pretty good with typewriters, but we can’t just plunk down a typewriter in front of a computer and have it type – they have no fingers! 

> 有趣的是，肖尔斯并没有设想打字会比手写快，手写的速度大约是每分钟20字。打字机的出现主要是为了文件的可读性和标准化，而不是速度。然而，随着打字机成为办公室的标准设备，人们对快速打字的渴望越来越强烈，有两项重大进展释放了打字的真正潜力。1880年左右，辛辛那提速记和打字机研究所的教师伊丽莎白-朗利开始推广十指打字法。这需要的手指动作比猎杀式打字法少得多，所以它提供了更高的打字速度。然后，几年后，盐湖城的联邦法院书记员弗兰克-爱德华-麦克格林自学了触摸式打字（盲打）；也就是说，他在打字时不需要看键盘。1888年，麦格林赢得了一场备受关注的打字速度比赛，此后，十指触摸打字法开始流行起来。专业打字员很快就能达到每分钟100字以上的速度，比手写快得多！而且还很整齐。而且还很整洁! 所以，人类对打字机很在行，但我们不能把打字机放在电脑前，让它打字--它们没有手指！"。

Instead, early computers adapted a special type of typewriter that was used for telegraphs, called a **teletype machine**. These were electromechanically-augmented typewriters that could send and receive text over telegraph lines. Pressing a letter on one teletype keyboard would cause a signal to be sent, over telegraph wires, to a teletype machine on the other end, which would then electromechanically type that letter. This allowed two humans to type to one another over long distances... basically a steampunk version of a chat room. Since these teletype machines already had an electronic interface, they were easily adapted for computer use, and teletype computer interfaces were common in the 1960s and 70s. Interaction was pretty straightforward. Users would type a command, hit enter, and then the computer would type back. This text “conversation” between a user and a computer went back and forth. These were called **command line interfaces**, and they remained the most prevalent form of human-computer interaction up until around the 1980s.

> 相反，早期的计算机采用了一种用于电报的特殊类型的打字机，称为**电传打字机**。这些打字机是机电增强的打字机，可以通过电报线发送和接收文本。在一台电传打字机的键盘上按下一个字母，就会通过电报线向另一端的电传打字机发送信号，然后电传打字机就会以机电方式输入该字母。这使得两个人可以远距离地互相打字......基本上是一个蒸汽朋克版的聊天室。由于这些电传打字机已经有了电子接口，它们很容易被改造成计算机使用，电传计算机接口在20世纪60年代和70年代很常见。交互是非常直接的。用户输入一个命令，点击回车，然后计算机就会回车。用户和计算机之间的这种文字 "对话 "来来回回。这些被称为**命令行界面**，直到20世纪80年代，它们仍然是最普遍的人机交互形式。

Command Line interaction on a teletype machine looks something like this. A user can type any number of possible commands. Let’s check out a few, beginning with seeing all of the files in the current directory we’re in. For this, we would type the command, “**ls**”, which is short for **list**, and the computer replies with a list of the files in our current directory. If we want to see what’s in our “secret StarTrek Discovery Cast dot t-x-t file”, we use yet another command to display the contents. In unix, we can call “**cat**” - short for **concatenate**. We need to specify which file to display, so we include that after the command, called an argument. If you’re connected to a network with other users, you can use a primitive version of a Find My Friends app to get more info on them with the command “finger”. Electromechanical teletype machines were the primary computing interface for most users up until around the 1970s. Although computer screens first emerged in the 1950s, and were used for graphics they were too expensive and low resolution for everyday use. However, mass production of televisions for the consumer market, and general improvements in processors and memory, meant that by 1970, it was economically viable to replace electromechanical teletype machines with screen-based equivalents. But, rather than build a whole new standard to interface computers with these screens, engineers simply recycled the existing text-only, teletype protocol. These machines used a screen, which simulated endless paper. It was text in and text out, nothing more. The protocol was identical, so computers couldn’t even tell if it was paper or a screen. These virtual teletype or glass teletype machines became known as **terminals**. 

> 电传打字机上的命令行交互看起来是这样的。用户可以输入任何数量的可能命令。让我们来看看几个，首先是查看我们当前目录中的所有文件。为此，我们可以输入 "**ls**"命令，这是 "**list**"的缩写，计算机会回复我们当前目录中的文件列表。如果我们想看看我们的 "秘密星际迷航Discovery Cast dot t-x-t文件 "里有什么，我们可以使用另一个命令来显示内容。在unix中，我们可以调用 "**cat**"--即**concatenate**的简称。我们需要指定要显示的文件，所以我们在命令后面加上这个，称为参数。如果你与其他用户连接在一个网络上，你可以使用原始版本的 "查找我的朋友 "应用程序，用 "finger "命令获得更多关于他们的信息。直到1970年代左右，机电式电传打字机一直是大多数用户的主要计算界面。虽然计算机屏幕在20世纪50年代首次出现，并被用于图形，但对于日常使用来说，它们太昂贵了，而且分辨率低。然而，面向消费市场的电视的大规模生产，以及处理器和内存的普遍改进，意味着到1970年，用基于屏幕的同等产品取代机电式电传打字机在经济上是可行的。但是，工程师们没有建立一个全新的标准来连接计算机和这些屏幕，而是简单地回收了现有的纯文本电传协议。这些机器使用一个屏幕，模拟无尽的纸张。它是文本输入和文本输出，仅此而已。协议是相同的，所以计算机甚至无法辨别它是纸还是屏幕。这些虚拟电传打字机或玻璃电传打字机被称为**终端**。
>

By 1971, it was estimated, in the United States, there was something on the order of 70,000 electro-mechanical teletype machines and 70,000 screen-based terminals in use. Screens were so much better, faster and more flexible, though. Like, you could delete a mistake and it would disappear. So, by the end of the 1970s, screens were standard. You might think that command line interfaces are way too primitive to do anything interesting. But even when the only interaction was through text, programmers found a way to make it fun. Early interactive, text-based computer games include famous titles like Zork, created in 1977. Players of these sorts of early games were expected to engage their limitless imaginations as they visualized the fictional world around them, like what terrifying monster confronted them when it was pitch black and you were likely to be eaten by a grue. Let’s go back to our command line, now on a fancy screen-based terminal, and play! Just like before, we can see what’s in our current directory with the “ls” command. Then, let’s go into our games directory by using the “**cd**” command, for “**change directory**”. Now, we can use our “ls” command again to see what games are installed on our computer. Sweet, we have Adventure! 

> 据估计，到1971年，在美国有大约7万台电动机械电传打字机和7万台基于屏幕的终端在使用。不过，屏幕要好得多，更快，更灵活。比如，你可以删除一个错误，它就会消失。所以，到了70年代末，屏幕成为了标准。你可能认为命令行界面太原始了，做不出什么有趣的事情。但是，即使在唯一的互动是通过文本的时候，程序员们也找到了让它变得有趣的方法。早期的交互式、基于文本的计算机游戏包括像1977年创造的Zork这样的著名作品。这类早期游戏的玩家在想象他们周围的虚构世界时，要发挥他们无限的想象力，比如在一片漆黑中，你很可能会被毒虫吃掉时，会有什么可怕的怪物来面对他们。让我们回到我们的命令行，现在是在一个花哨的基于屏幕的终端上，然后玩吧 就像以前一样，我们可以用 "ls "命令查看我们当前目录中的内容。然后，让我们使用 "**cd**"命令进入我们的游戏目录，即 "**改变目录**"。现在，我们可以再次使用 "ls "命令来查看我们的电脑上安装了哪些游戏。很好，我们有了《冒险》! 

All we have to do to run this program is type its name. Until this application halts, or we quit it, it takes over the command line. What you’re seeing here is actual interaction from “Colossal Cave Adventure”, first developed by Will Crowther in 1976. In the game, players can type in one- or two-word commands to move around, interact with objects, pickup items and so on. The program acts as the narrator, describing locations, possible actions, and the results of those actions. Certain ones resulted in death! The original version only had 66 locations to explore, but it’s widely considered to be the first example of interactive fiction. These text adventure games later became multiplayer, called MUDs or **Multi-User Dungeons**. And they’re the great-forbearers of the awesome graphical MMORPG’s (**massive, multiplayer online role playing games**) we enjoy today. And if you want to know more about the history of these and other games we’ve got a whole series on it hosted by Andre Meadows! 

> 我们要运行这个程序所要做的就是输入它的名字。直到这个程序停止，或者我们退出它，它就会接管命令行。你在这里看到的是 "巨型洞穴探险 "的实际互动，该游戏由威尔-克劳瑟于1976年首次开发。在游戏中，玩家可以输入一个或两个字的命令来移动，与物体互动，拾取物品等等。该程序充当叙述者，描述地点、可能的行动以及这些行动的结果。某些行动会导致死亡。原始版本只有66个地点可供探索，但它被广泛认为是互动小说的第一个例子。这些文字冒险游戏后来成为多人游戏，被称为MUD或**多用户地下城**。它们是我们今天享受的令人敬畏的图形化MMORPG（**大型多人在线角色扮演游戏**）的伟大祖先。如果你想了解更多关于这些和其他游戏的历史，我们有一个完整的系列，由Andre Meadows主持! 

Command line interfaces, while simple, are very powerful. Computer programming is still very much a written task, and as such, command lines are a natural interface. For this reason, even today, most programmers use command line interfaces as part of their work. And they’re also the **most common way to access computers that are far away, like a server in a different country**. If you’re running Windows, macOS or Linux, your computer has a command line interface – one you may have never used. Check it out by typing “cmd” in your Windows search bar, or search for Terminal on Mac. Then install a copy of Zork and play on! So, you can see how these early advancements still have an impact on computing today. Just imagine if your phone didn’t have a good ol’ fashioned QWERTY keyboard. It could take forever to type your Instagram captions. But, there’s still something missing from our discussion. All the sweet sweet graphics! That’s our topic for next week. See you soon. 

> 命令行界面虽然简单，但却非常强大。计算机编程在很大程度上仍然是一项书面任务，因此，命令行是一种自然的界面。由于这个原因，即使在今天，大多数程序员都使用命令行界面作为他们工作的一部分。而且它们也是**访问远处的计算机的最常见方式，比如在不同国家的服务器**。如果你运行的是Windows、macOS或Linux，你的电脑有一个命令行界面--你可能从未使用过。在你的Windows搜索栏中输入 "cmd"，或者在Mac上搜索 "终端"，就可以看到它。然后安装一个Zork的副本，继续玩吧! 因此，你可以看到这些早期的进步如何对今天的计算产生影响。试想一下，如果你的手机没有一个好的老式QWERTY键盘。它可能要花很长时间来输入你的Instagram标题。但是，我们的讨论中仍然缺少一些东西。所有甜美的图形！这就是我们下周的主题。很快就会见到你了。



## #23 Screens & 2D graphics

<iframe width="560" height="315" src="https://www.youtube.com/embed/7Jr0SFMQ4Rs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science! This 1960 PDP-1 is a great example of early computing with graphics. You can see a cabinet-sized computer on the left, an electromechanical teletype machine in the middle, and a round screen on the right. Note how they’re separated. That’s because text-based tasks and graphical tasks were often distinct back then. In fact, these early computer screens had a very hard time rendering crisp text, whereas typed paper offered much higher contrast and resolution. The most typical use for early computer screens was to keep track of a program's operation, like values in registers. It didn’t make sense to have a teletype machine print this on paper over and over and over again -- that’d waste a lot of paper, and it was slow. On the other hand, screens were dynamic and quick to update -- perfect for temporary values. Computer screens were rarely considered for program output, though. Instead, any results from a computation were typically written to paper or some other more permanent medium. But, screens were so darn useful that by the early 1960s, people started to use them for awesome things. 

> 这台1960年的PDP-1是早期图形计算的一个好例子。你可以看到左边是一台机柜大小的计算机，中间是一台机电式电传打字机，右边是一个圆形屏幕。注意它们是如何被分开的。这是因为基于文本的任务和图形任务在当时往往是截然不同的。事实上，这些早期的计算机屏幕很难呈现清晰的文字，而打字纸则提供了更高的对比度和分辨率。早期计算机屏幕最典型的用途是记录程序的运行情况，如寄存器中的值。让电传打字机一遍又一遍地在纸上打印这些内容是没有意义的--那会浪费大量的纸张，而且速度很慢。另一方面，屏幕是动态的，可以快速更新--非常适合临时数值。不过，计算机屏幕很少被考虑用于程序输出。相反，任何计算的结果通常都被写在纸上或其他更持久的媒介上。但是，屏幕是如此的有用，以至于到了1960年代早期，人们开始用它来做一些了不起的事情。

INTRO A lot of different display technologies have been created over the decades, but the most influential, and also the earliest, were Cathode Ray Tubes, or **CRTs**. These work by shooting electrons out of an emitter at a phosphor-coated screen. When electrons hit the coating, it glows for a fraction of a second. Because electrons are charged particles, their paths can be manipulated with electromagnetic fields. Plates or coils are used inside to steer electrons to a desired position, both left-right and up-down. With this control, there are two ways you can draw graphics. The first option is to direct the electron beam to trace out shapes. This is called **Vector Scanning**. Because the glow persists for a little bit, if you repeat the path quickly enough, you create a solid image. The other option is to repeatedly follow a fixed path, scanning line by line, from top left to bottom right, and looping over and over again. You only turn on the electron beam at certain points to create graphics. This is called **Raster Scanning**. With this approach, you can display shapes... and even text... all made of little line segments. 

> 几十年来创造了许多不同的显示技术，但最有影响的，也是最早的，是阴极射线管，或**CRT**。它们的工作原理是将电子从发射器中射向涂有荧光粉的屏幕。当电子击中涂层时，它就会在几分之一秒的时间内发亮。由于电子是带电粒子，它们的路径可以用电磁场来操纵。里面的板子或线圈用来引导电子到一个理想的位置，包括左右和上下。通过这种控制，有两种方式可以绘制图形。第一种方法是引导电子束描出形状。这就是所谓的**矢量扫描**。因为辉光会持续一段时间，如果你重复路径的速度足够快，你就会创造出一个实体图像。另一种方法是反复按照固定的路径，逐行扫描，从左上到右下，反复循环。你只在某些点上打开电子束来创建图形。这就是所谓的**光栅扫描**。用这种方法，你可以显示形状......甚至文字......都由小线段组成。

Eventually, as display technologies improved, it was possible to render crisp dots onto the screen, aka **pixels**. The **Liquid Crystal Displays**, or **LCDs**, that we use today are quite a different technology. But, they use raster scanning too, updating the brightness of little tiny red, green and blue pixels many times a second. Interestingly, most early computers didn’t use pixels -- not because they couldn’t physically, but because it consumed way too much memory for computers of the time. A 200 by 200 pixel image contains 40,000 pixels. Even if you use just one bit of data for each pixel, that’s black OR white -- not grayscale! -- the image would consume 40,000 bits of memory. That would have gobbled up more than half of a PDP-1’s entire RAM. So, computer scientists and engineers had to come up with clever tricks to render graphics until memory sizes caught up to our pixelicious ambitions. Instead of storing tens of thousands of pixels, early computers stored a much smaller grid of letters, most typically 80 by 25 characters. That’s 2000 characters in total. And if each is encoded in 8 bits, using something like ASCII, it would consume 16,000 bits of memory for an entire screen full of text, which is way more reasonable. To pull this off, computers needed an extra piece of hardware that could read characters out of RAM, and convert them into raster graphics to be drawn onto the screen. This was called a **character generator**, and they were basically the first **graphics cards**. Inside, they had a little piece of **Read Only Memory**, a **ROM**, that stored graphics for each character, called a **dot matrix pattern**. If the graphics card saw the 8-bit code for the letter “K”, then it would raster scan the 2D pattern for the letter K onto the screen, in the appropriate position. To do this, the character generator had special access to a portion of a computer's memory reserved for graphics, a region called the **screen buffer**. 

> 最终，随着显示技术的改进，有可能在屏幕上呈现清晰的点，又称**像素**。我们今天使用的**液晶显示器**，或**LCD**，是一种相当不同的技术。但是，它们也使用光栅扫描，每秒多次更新小的红、绿、蓝像素的亮度。有趣的是，大多数早期的计算机没有使用像素--不是因为它们在物理上不能，而是因为它对当时的计算机来说消耗了太多的内存。一个200×200像素的图像包含40,000个像素。即使你对每个像素点只使用一个比特的数据，也就是黑色或白色--而不是灰度！--图像将消耗40,000个像素。--该图像将消耗40,000比特的内存。这将吞噬掉PDP-1的全部内存的一半以上。因此，计算机科学家和工程师们不得不想出一些巧妙的技巧来渲染图形，直到内存大小赶上我们对像素的渴望。早期的计算机不是存储数以万计的像素，而是存储一个小得多的字母网格，最典型的是80×25个字符。这总共是2000个字符。如果每个字符用8比特编码，使用类似ASCII的东西，那么整个屏幕上的文字将消耗16,000比特的内存，这就更合理了。为了做到这一点，计算机需要一个额外的硬件，可以从RAM中读取字符，并将其转换为光栅图形，绘制在屏幕上。这被称为**字符生成器**，它们基本上是第一批**显卡**。它们内部有一小块**只读存储器**，即**ROM**，为每个字符存储图形，称为**点阵图案**。如果图形卡看到字母 "K "的8位代码，它就会在适当的位置将字母K的二维图案光栅扫描到屏幕上。要做到这一点，字符生成器要特别访问计算机内存中为图形保留的部分，这个区域称为**屏幕缓冲区**。

Computer programs wishing to render text to the screen simply manipulated the values stored in this region, just as they could with any other data in RAM. This scheme required much less memory, but it also meant the only thing you could draw was text. Even still, people got pretty inventive with ASCII art! People also tried to make rudimentary, pseudo-graphical interfaces out of this basic set of characters, using things like underscores and plus signs to create boxes, lines and other primitive shapes. But, the character set was really too small to do anything terribly sophisticated. So, various extensions to ASCII were made that added new **semigraphical characters**, like IBM’s CP437 character set, seen here, which was used in DOS. On some systems, the text color and background color could be defined with a few extra bits. That allowed glorious interfaces like this DOS example, which is built entirely out the character set you just saw. Character generators were a clever way to save memory. But, they didn’t provide any way to draw arbitrary shapes. And that’s important if you want to draw content like electrical circuits, architectural plans, maps, and... well… pretty much everything that isn’t text! 

> 希望将文本呈现在屏幕上的计算机程序只需操作存储在这个区域的数值，就像他们可以操作RAM中的任何其他数据一样。这种方案需要的内存要少得多，但也意味着你唯一能画的就是文字。即使如此，人们还是对ASCII艺术有了很大的创造性。人们还试图用这套基本的字符集来制作简陋的伪图形界面，用下划线和加号等东西来创建方框、线条和其他原始的形状。但是，这个字符集实在是太小了，无法做任何非常复杂的事情。因此，人们对ASCII进行了各种扩展，增加了新的**图形字符**，如IBM的CP437字符集，见此，它被用于DOS中。在一些系统中，文本颜色和背景颜色可以通过一些额外的位来定义。这使得像这个DOS的例子一样的光辉界面成为可能，它完全是由你刚才看到的字符集构建的。字符生成器是一种节省内存的聪明方法。但是，他们并没有提供任何方法来绘制任意的形状。如果你想绘制电路、建筑图、地图等内容，这就很重要了......嗯......几乎所有不是文字的东西！"。

To do this, without resorting to memory-gobbling pixels, computer scientists used the **vector mode** available on CRTs. The idea is pretty straightforward: all content to be drawn on screen is defined by a series of lines. There’s no text. If you need to draw text, you have to draw it out of lines. Don’t read between the lines here. There is only lines! Got it? Alright, no more word play. I’m drawing the line here. Let’s pretend this video is a **cartesian plane**, 200 units wide and 100 tall, with the origin – that’s the zero-zero point – in the upper left corner. We can draw a shape with the following vector commands, which we’ve borrowed from the **Vectrex**, an early vector display system. First, we reset, which clears the screen, moves the drawing point of the electron gun to zero-zero, and sets the brightness of lines to zero. Then we move the drawing point down to 50 50, and set the line intensity to 100%. With the intensity up, now we move to 100, 50, then 60, 75 and then back to 50,50. The last thing to do is set our line intensity back to 0%. Cool! We’ve got a triangle! This sequence of commands would consume on the order of 160 bits, which is way more efficient than keeping a huge matrix of pixel values! Just like how characters were stored in memory and turned into graphics by a character generator, these vector instructions were also stored in memory, and rendered to a screen using a vector graphics card. Hundreds of commands could be packed together, sequentially, in the screen buffer, and used to build up complex graphics. All made of lines! Because all these vectors are stored in memory, computer programs can update the values freely, allowing for graphics that change over time -- **Animation**! 

> 为了做到这一点，计算机科学家们使用了CRT上的**矢量模式**，而不需要借助于内存中混乱的像素。这个想法非常直接：所有要在屏幕上绘制的内容都由一系列线条来定义。这里没有文字。如果你需要绘制文本，你必须用线条来绘制它。不要在这里读懂字里行间的意思。这里只有线条! 明白了吗？好了，不要再玩文字游戏了。我在这里画线。让我们假装这段视频是一个**笛卡尔平面**，200个单位宽，100个单位高，原点--也就是零点--在左上角。我们可以用下面的矢量命令画出一个形状，这些命令是我们从**Vectrex**（一种早期的矢量显示系统）上借用的。首先，我们复位，这样可以清除屏幕，将电子枪的绘制点移到零点，并将线条的亮度设置为零。然后，我们将绘图点向下移动到50 50，并将线条强度设置为100%。随着强度的提高，现在我们移动到100，50，然后是60，75，然后再回到50，50。最后要做的是把我们的线条强度设回0%。酷！我们已经有了一个三角形。我们已经有了一个三角形! 这一连串的命令将消耗160个比特，这比保存一个巨大的像素值矩阵要有效得多! 就像字符被存储在内存中并由字符生成器转化为图形一样，这些矢量指令也被存储在内存中，并通过矢量图形卡渲染到屏幕上。数以百计的指令可以在屏幕缓冲区内按顺序打包，并用于建立复杂的图形。所有这些都是由线条组成的! 由于所有这些矢量都存储在内存中，计算机程序可以自由更新数值，允许图形随时间变化 -- **动画**! 
>

One of the very earliest video games, Spacewar!, was built on a PDP-1 in 1962 using **vector graphics**. It’s credited with inspiring many later games, like Asteroids, and even the first commercial arcade video game: Computer Space. 1962 was also a huge milestone because of **Sketchpad**, an interactive graphical interface that offered Computer-Aided Design -- called **CAD Software** today. It’s widely considered the earliest example of a complete graphical application. And its inventor, Ivan Sutherland, later won the Turing Award for this breakthrough. To interact with graphics, Sketchpad used a recently invented input device called a light pen, which was a stylus tethered to a computer with a wire. By using a light sensor in the tip, the pen detected the refresh of the computer monitor. Using the timing of the refresh, the computer could actually figure out the pen’s position on the screen! With this light pen, and various buttons on a gigantic computer, users could draw lines and other simple shapes. Sketchpad could do things like make lines perfectly parallel, the same length, straighten corners into perfect 90 degree intersections, and even scale shapes up and down dynamically. These things that were laborious on paper, a computer now did with a press of a button! Users were also able to save complex designs they created, and then paste them into later designs, and even share with other people. You could have whole libraries of shapes, like electronic components and pieces of furniture that you could just plop in and manipulate in your creations. This might all sound pretty routine from today’s perspective. But in 1962, when computers were still cabinet-sized behemoths chugging through punch cards, Sketchpad and light pens were equal parts eye opening and brain melting. They represented a key turning point in how computers could be used. They were no longer just number crunching math machines that hummed along behind closed doors. Now, they were potential assistants, interactively augmenting human tasks. 

> 最早的视频游戏之一《太空战！》是1962年在PDP-1上使用**矢量图形**制作的。它被认为启发了许多后来的游戏，如《小行星》，甚至是第一个商业街机视频游戏。计算机空间》。1962年也是一个巨大的里程碑，因为**Sketchpad**，一个提供计算机辅助设计的交互式图形界面--今天称为**CAD软件**。它被广泛认为是最早的完整图形应用程序的例子。它的发明者伊万-萨瑟兰后来因为这一突破获得了图灵奖。为了与图形互动，Sketchpad使用了一种最近发明的输入设备，称为光笔，这是一种用电线拴在电脑上的手写笔。通过使用笔尖上的光传感器，该笔可以检测到计算机显示器的刷新。利用刷新的时间，计算机实际上可以计算出笔在屏幕上的位置! 有了这支光笔和巨大的电脑上的各种按钮，用户可以画出线条和其他简单的形状。Sketchpad可以做的事情包括使线条完全平行，长度相同，将角落拉直成完美的90度交叉点，甚至可以动态地放大和缩小形状。这些在纸上很费力的事情，现在电脑只需按下一个按钮就能做到了。用户还能够保存他们创建的复杂设计，然后将其粘贴到以后的设计中，甚至与其他人分享。你可以拥有整个形状库，比如电子元件和家具碎片，你可以在你的作品中随意摆放和操作。从今天的角度来看，这一切可能听起来很平常。但在1962年，当计算机还是柜子大小的庞然大物，通过打孔卡来运转的时候，Sketchpad和光笔是开眼界和融化大脑的同等部分。它们代表了计算机使用方式的一个关键转折点。它们不再是闭门造车的数学机器。现在，它们是潜在的助手，以互动的方式增强人类的任务。

The earliest computers and displays with true pixel graphics emerged in the late 1960s. Bits in memory directly “mapped” to pixels on the screen, what are called **bitmapped displays**. With full pixel control, totally arbitrary graphics were possible. You can think of a screen’s graphics as a huge matrix of pixel values . As before, computers reserve a special region of memory for pixel data, called the **frame buffer**. In the early days, the computer’s RAM was used, but later systems used special high speed Video RAM, or **VRAM**, which was located on the graphics card itself for high speed access. This is how it’s done today. On an 8-bit grayscale screen, we can set values from 0 intensity, which is black, to 255 intensity, which is white. Well actually, it might be green... or orange, as many early displays couldn’t do white. Let’s pretend this video is a really low resolution bitmapped screen, with a resolution of 60 by 35 pixels. If we wanted to set the pixel at (10 10) to be white, we could do it with a piece of code like this. If we wanted to draw a line, let’s say from (30, 0) to (30, 35), we can use a loop, like so…. ….And this changes a whole line of pixels to white. If we want to draw something more complicated, let’s say a rectangle, we need to know four values. The X and Y coordinate of its starting corner, and its width and height. So far, we’ve drawn everything in white, so let’s specify this rectangle to be grey. Grey is halfway between 0 and 255, so that’s a color value of 127. Then, with two loops – one nested in the other, so that the inner loop runs once for every iteration of the outer loop – we can draw a rectangle. When the computer executes our code as part of its draw routine, it colors in all the pixels we specified. Let’s wrap this up into a “draw rectangle function”, like this: Now, to draw a second rectangle on the other side of the screen, maybe in black this time, we can just call our rectangle drawing function. Voila!! Just like the other graphics schemes we’ve discussed, programs can manipulate pixel data in the framebuffer, creating interactive graphics. Pong time! 

> 最早的计算机和具有真正像素图形的显示器出现在1960年代末。内存中的比特直接 "映射 "到屏幕上的像素，这就是所谓的**位图显示**。有了完全的像素控制，完全任意的图形成为可能。你可以把屏幕上的图形想象成一个巨大的像素值矩阵。和以前一样，计算机为像素数据保留了一个特殊的内存区域，称为**帧缓冲区**。在早期，使用的是计算机的RAM，但后来的系统使用了特殊的高速视频RAM，即**VRAM**，它位于显卡本身，可以高速访问。今天就是这样做的。在一个8位灰度屏幕上，我们可以设置从0强度，也就是黑色，到255强度，也就是白色的值。实际上，它可能是绿色......或橙色，因为许多早期的显示器不能做白色。让我们假设这个视频是一个非常低分辨率的位图屏幕，分辨率为60×35像素。如果我们想把（10 10）处的像素设置为白色，我们可以用这样一段代码来实现。如果我们想画一条线，比方说从（30，0）到（30，35），我们可以用一个循环，像这样.... ....，这样就把一整行的像素改为白色。如果我们想画一个更复杂的东西，比方说一个矩形，我们需要知道四个值。它的起始角的X和Y坐标，以及它的宽度和高度。到目前为止，我们已经把所有东西都画成了白色，所以让我们把这个矩形指定为灰色。灰色在0和255之间，所以颜色值是127。然后，通过两个循环--一个嵌套在另一个里面，所以内循环在外循环的每一次迭代中运行一次--我们可以画一个矩形。当计算机执行我们的代码作为其绘制程序的一部分时，它将为我们指定的所有像素上色。让我们把它包装成一个 "绘制矩形函数"，像这样。现在，要在屏幕的另一边绘制第二个矩形，也许这次是黑色的，我们可以直接调用我们的矩形绘制函数。Voila! 就像我们讨论过的其他图形方案一样，程序可以操作帧缓冲区中的像素数据，创建交互式图形。乒乓时间! 
>

Of course, programmers aren’t wasting time writing drawing functions from scratch. They use **graphics libraries** with ready-to-go functions for drawing lines, curves, shapes, text, and other cool stuff. Just a new level of abstraction! The flexibility of bitmapped graphics opened up a whole new world of possibilities for interactive computing, but it remained expensive for decades. As I mentioned last episode, by as late as 1971, it was estimated there were around 70,000 electro-mechanical teletype machines and 70,000 terminals in use, in the United States. Amazingly, there were only around 1,000 computers in the US that had interactive graphical screens. That’s not a lot! But the stage was set – helped along by pioneering efforts like Sketchpad and Spacewars – for computer displays to become ubiquitous, and with them, the dawn of graphical user interfaces, which we’ll cover in a few episodes! I’ll see you next week. 

> 当然，程序员并没有浪费时间从头开始编写绘图函数。他们使用**图形库**，其中有现成的函数用于绘制线条、曲线、形状、文本和其他很酷的东西。这只是一个新的抽象层次! 位图的灵活性为交互式计算开辟了一个全新的世界，但几十年来它仍然很昂贵。正如我在上一集提到的，到1971年，据估计在美国有大约7万台电动机械电传打字机和7万个终端在使用。令人惊讶的是，在美国只有大约1000台计算机有交互式图形屏幕。这还不算多！但舞台已经搭好了。但舞台已经搭建好了--在Sketchpad和Spacewars这样的先锋努力的帮助下--计算机显示器变得无处不在，随之而来的是图形用户界面的曙光，我们将在几集里介绍这些内容。下周见。



## #24 The cold war and consumerism

<iframe width="560" height="315" src="https://www.youtube.com/embed/m8i38Yq1wX4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I'm Carrie Anne and welcome to Crash Course Computer Science. Early in this series we covered computing history from roughly the dawn of civilization, up to the birth of electronic general purpose computers in the mid 1940s. A lot of the material we've discussed over the past 23 episodes like programming languages and compilers algorithms and integrated circuits Floppy disks and operating systems, telly types and screens all emerged over roughly a **30-year** period, from the mid 1940s up to the mid 1970s. This is the era of computing before companies like **Apple** and **Microsoft** existed and long before anyone **tweeted**, **Googled** or **Uber-d**. It was a formative period setting the stage for personal computers, **worldwide web**, **self-driving cars**, **virtual reality**, and many other topics we'll get to in the second half of this series. Today we're going to step back from circuits and algorithms and review this influential period. We'll pay special attention to the historical backdrop of the **cold war**, the space race and **the rise of globalization and consumerism**. 

> 在这个系列的早期，我们涵盖了大约从文明的黎明开始的计算历史，直到1940年代中期电子通用计算机的诞生。我们在过去23期节目中讨论的很多材料，如编程语言和编译器、算法和集成电路 软盘和操作系统、电视类型和屏幕都是在大约**30年**的期内出现的，从1940年代中期到1970年代中期。这是在像**苹果**和**微软**这样的公司出现之前的计算机时代，也是在任何人**推特**、**谷歌**或**Uber-d**之前的很长一段时间。这是一个形成期，为个人电脑、**全球网络**、**自动驾驶汽车**、**虚拟现实**以及我们将在本系列的后半部分讨论的许多其他主题创造了条件。今天，我们将从电路和算法中抽身出来，回顾这个有影响力的时期。我们将特别关注**冷战**的历史背景，**太空竞赛**以及**全球化和消费主义的崛起**。

Pretty much immediately after World War II concluded in 1945, there was tension between the world's two new superpowers the United States and the USSR. The Cold War had begun and with it, massive government spending on science and engineering. Computing which had already demonstrated its value in wartime efforts like the Manhattan Project and code breaking Nazi communications, was lavished with government funding. They enabled huge ambitious computing projects to be undertaken, like ENIAC, EDVAC, Atlas and Whirlwind all mentioned in previous episodes. This spurred rapid advances that simply weren't possible in the commercial sector alone, where projects were generally expected to recoup development costs through sales. This began to change in the early 1950s, especially with Eckert and Buckley's Univac 1, the first commercially successful computer. Unlike any Echo Atlas, this wasn't just one single computer. It was a model of computers, in total more than 40 were built. Most of these Univacs went to government offices or large companies. Which was part of the growing military industrial complex in the United States, with pockets deep enough to afford the cutting edge. Famously a Univac one built for the U.S atomic energy commission was used by CBS to predict the results of the 1952 U.S. presidential election. With just 1% of the vote the computer correctly predicted an Eisenhower landslide while pundits favored Stevenson. It was a media event that helped propel computing to the forefront of the public's imagination Computing was unlike machines of the past, which generally augmented human physical abilities. 

> 1945年二战结束后，世界上两个新的超级大国--美国和苏联之间的关系几乎立即紧张起来。冷战已经开始，随之而来的是政府在科学和工程方面的大量支出。计算技术已经在战时的工作中证明了它的价值，如曼哈顿计划和破解纳粹通信的密码，因此得到了政府的慷慨资助。他们使巨大的雄心勃勃的计算项目得以实施，如前几集提到的ENIAC、EDVAC、Atlas和Whirlwind。这刺激了快速的进步，而这些进步仅仅在商业领域是不可能实现的，在商业领域，项目通常被期望通过销售来收回开发成本。这种情况在20世纪50年代初开始改变，特别是埃克特和巴克利的Univac 1，第一台商业上成功的计算机。与任何Echo Atlas不同，这不仅仅是一台单一的计算机。它是一种型号的计算机，总共建造了40多台。这些Univacs大多被用于政府办公室或大公司。这是美国不断增长的军事工业综合体的一部分，它的口袋足够大，能够负担得起最先进的技术。著名的是，为美国原子能委员会制造的一台Univac被CBS用来预测1952年美国总统选举的结果。在只有1%的选票的情况下，计算机正确地预测了艾森豪威尔的压倒性胜利，而专家们则倾向于史蒂文森。这是一个有助于将计算机推向公众想象力前沿的媒体事件 计算机与过去的机器不同，过去的机器一般是增强人类的身体能力。

Trucks allowed us to carry, more automatic looms whoa faster Machine tools were more precise and so on for a bunch of contraptions that typify the industrial revolution. But computers on the other hand could augment human intellect. This potential wasn't lost on Vannevar Bush, who in 1945 published an article on a hypothetical computing device he envisioned called the Memex. This was a device in which an individual stores all his books, records and communications and which is mechanized, so it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory. He also predicted that **wholly new forms of encyclopedia will appear, ready-made, with a mesh of associative trails running through them**. Sound familiar? Memex directly inspired several subsequent game-changing systems, like Ivan Sutherland Sketchpad, which we discussed last episode, and Dough Engelbart's online system, which we will cover soon. Vannevar Bush was the head of the U.S. office of Scientific Research and Development, which was responsible for funding and coordinating scientific research during World War Two. With the Cold War brewing, Bush lobbied for a creation of a peace time equivalent, the National Science Foundation, formed in 1950. To this day the NSF provides federal funding to support scientific research in the United States. And it is a major reason the U.S. has continued to be a leader in the technology sector. 

> 卡车使我们能够携带，更多的自动织布机，哇，速度更快，机器工具更精确，等等，这些都是工业革命的典型装置。但另一方面，计算机可以增强人类的智力。范尼瓦尔-布什并没有忽视这种潜力，他在1945年发表了一篇关于他所设想的名为Memex的假设性计算设备的文章。这是一个个人储存其所有书籍、记录和通信的设备，它是机械化的，因此可以以超乎寻常的速度和灵活性进行查阅。它是对他的记忆的一种扩大的亲密的补充。他还预言，**全新形式的百科全书将出现，现成的，有网状的联想线索贯穿其中**。听起来很熟悉吧？Memex直接启发了后来的几个改变游戏规则的系统，比如我们上一集讨论的伊万-萨瑟兰的素描板，以及我们即将讨论的多夫-恩格尔巴特的在线系统。范尼瓦尔-布什是美国科学研究和发展办公室的负责人，该办公室在第二次世界大战期间负责资助和协调科学研究。随着冷战的酝酿，布什游说建立了一个和平时期的类似机构，即1950年成立的国家科学基金会。时至今日，国家科学基金会提供联邦资金支持美国的科学研究。这也是美国在技术领域持续领先的一个重要原因。

It was also in the 1950s that consumers started to buy transistor powered gadgets, notable among them was the **transistor radio**, which was small, durable and battery-powered. And it was portable, unlike the vacuum tube based radio sets from the 1940s and before. It was a runaway success, the furby or iphone of its day. The Japanese government looking for industrial opportunities, to bolster their post-war economy, soon got in on the action. Licensing the rights to Transistors from Bell Labs in 1952. Helping launch the Japanese semiconductor and electronics industry. In 1955, the first Sony product was released: **The TR-55 Transistor Radio**. Concentrating on quality and price, Japanese companies captured half of the U.S. Market for **portable radios** in just five years. This planted the first seeds of a major industrial rivalry in the decades to come. In 1953, there were only around 100 computers on the entire planet and at this point, the USSR was only a few years behind the West in computing technology, completing their first programmable electronic computer in 1950. But the Soviets were way ahead in the burgeoning space race. 

> 也是在20世纪50年代，消费者开始购买晶体管供电的小工具，其中值得注意的是**晶体管收音机**，它小巧、耐用、用电池供电。而且它是便携式的，与20世纪40年代及以前基于真空管的收音机不同。它获得了巨大的成功，是当时的毛绒玩具或苹果手机。日本政府寻找工业机会，以支持他们的战后经济，很快就加入了这一行动。1952年，从贝尔实验室获得了晶体管的授权。帮助启动了日本的半导体和电子工业。1955年，第一个索尼产品发布。**TR-55晶体管收音机**。由于专注于质量和价格，日本公司在短短五年内占领了美国**便携式收音机**市场的一半。这为今后几十年的主要工业竞争埋下了最初的种子。1953年，整个地球上只有大约100台计算机，在这一点上，苏联在计算技术方面只比西方落后几年，在1950年完成了他们的第一台可编程的电子计算机。但苏联在蓬勃发展的太空竞赛中却遥遥领先。
>

Let's go to the thought-bubble. The Soviets launched the world's first **satellite** into orbit, Sputnik one, in 1957 and a few years later in 1961. Soviet Cosmonaut, Yuri Gagarin became the first human in space. This didn't sit well with the American public and prompted President Kennedy, a month after Gagarin's mission, to encourage the nation to land a man on the moon within the decade. And it was expensive! Nasa's budget grew almost tenfold, peaking in 1966 at roughly 4.5 percent of the u.s. Federal budget today. It's around half a percent Nasa used this funding to tackle a huge array of enormous challenges this culminated in the apollo program Which is peak employed roughly? 400,000 people further supported by over 20,000 universities and companies one of these huge challenges was navigating in space. Nasa needed a computer to process complex trajectories and issue Guidance commands to the spacecraft. for this they built the apollo guidance computer. There were three significant requirements first the computer had to be fast no surprise there, second it has to be small and lightweight there's not a lot of room in a spacecraft and every ounce is precious when you're flying a quarter million miles to the moon and Finally it had to be really really ridiculously reliable. This is super important in a spacecraft where there's lots of vibration radiation and temperature change. And there's no running to best buy it something breaks the technology of the era of vacuum Tubes and discrete transistors Just weren't up to the task so Nasa turned to a brand-new technology integrated circuits Which we discussed a few episodes ago the **apollo guidance computer** was the first computer to use them a huge paradigm shift Nasa was also the only place that could afford them initially each chip cost around $50 and the guidance computer needed thousands of them. But by paying that price the Americans were able to beat the soviets to the moon Thanks, thought-bubble.

> 让我们去看看思想泡影。苏联在1957年发射了世界上第一颗**卫星**进入轨道，即斯普特尼克一号，几年后的1961年。苏联宇航员尤里-加加林成为第一个进入太空的人。这让美国公众很不满意，促使肯尼迪总统在加加林的任务结束后一个月，鼓励国家在十年内让人类登上月球。而这是很昂贵的! 国家航空航天局的预算几乎增长了10倍，在1966年达到顶峰，约占今天美国联邦预算的4.5%。这大约是半个百分点 国家航空航天局利用这笔资金来解决一系列巨大的挑战，这在阿波罗计划中达到了顶峰，该计划大约雇用了？40万人，并得到2万多所大学和公司的支持，其中一个巨大的挑战是在太空中导航。美国国家航空航天局需要一台计算机来处理复杂的轨迹，并向航天器发出引导指令。为此，他们建造了阿波罗引导计算机。有三个重要的要求：首先，计算机必须快速，这一点不足为奇；其次，它必须小而轻，在航天器中没有很多空间，当你向月球飞行25万英里时，每一盎司都很珍贵；最后，它必须非常非常可靠。这在有大量振动辐射和温度变化的航天器中超级重要。而且没有运行的最佳购买方式，真空管和分立晶体管时代的技术无法胜任，所以Nasa转向了一种全新的技术集成电路，我们在几期节目中讨论过，**阿波罗导航计算机**是第一台使用集成电路的计算机，这是一个巨大的范式转变，Nasa也是唯一能够负担得起它们的地方，最初每个芯片的成本约为50美元，导航计算机需要成千上万的芯片。但由于付出了这一代价，美国人得以将苏联人打败，谢谢你，思想泡泡。

Although the apollo Guidance computer is credited with spurring the development and adoption of integrated circuits It was a low volume product there are only 17 apollo missions after all it was actually military applications Especially the minuteman and polaris nuclear missile systems that allowed integrated circuits to become a mass-produced Item. this rapid Advancement was further accelerated by the u.s. Building and buying huge powerful computers often called supercomputers because they were frequently 10 times faster than any other computer on the planet. Upon their release but these machines built by companies Like CDC cray And Ibm were also super in cost and pretty much only governments could afford to buy them. in the us these machines went to government Agencies like the NSA and government research labs like Lawrence Livermore and Los Alamos National laboratories. initially the u.s. semiconductor industry boomed buoyed by High profit government contracts. However this meant that most us companies overlooked the consumer market where profit Margins were small. the Japanese Semiconductor industry came to dominate this niche by having to operate with lean profit margins in the 1950s and 60s. the Japanese had invested heavily in manufacturing capacity to achieve economies of scale in Research to improve quality and Yields. and in automation to keep manufacturing costs low in the 1970s. with the space Race and cold war subsiding previously juicy defense contracts began to dry up and Consumer conductor and electronics companies found it harder to compete. it didn't help the many computing components had been **commoditized** Be around with Dram. So why buy expensive intel memory when you could buy the same chip for less from Hitachi? 

> 尽管 "阿波罗指导 "计算机被认为促进了集成电路的发展和采用，但它是一个小批量产品，毕竟只有17次阿波罗任务，实际上是军事应用，特别是 "分钟人 "和 "北极星 "核导弹系统，使集成电路成为一个大规模生产的项目。这种快速进步被美国进一步加速，建立和购买强大的计算机，通常称为超级计算机，因为它们经常比地球上任何其他计算机快10倍。在美国，这些机器被用于政府机构，如国家安全局和政府研究实验室，如Lawrence Livermore和Los Alamos国家实验室。然而，这意味着我们大多数公司忽视了利润率较低的消费市场。日本半导体行业在20世纪50年代和60年代不得不以较低的利润率经营，从而在这个利基市场上占据了主导地位。日本人在制造能力方面进行了大量投资，以实现规模经济，研究提高质量和产量。随着太空竞赛和冷战的平息，以前多汁的国防合同开始枯竭，消费导体和电子公司发现更难竞争了，这并没有帮助许多计算组件被**商品化**。那么，如果你能以更低的价格从日立公司购买同样的芯片，为什么还要购买昂贵的英特尔内存？

Throughout the 1970s us companies began to downsize consolidate or outright fail. intel had to lay off a third of its Workforce in 1974 and even the storied Fairchild semiconductor was acquired in 1979 after near bankruptcy to survive. many of these companies began to outsource their manufacturing in a bid to reduce costs. Intel weave drew from its main product category memory Ics and decided to refocus on processes Which ultimately saved the company. this low and us? electronics industry allowed Japanese companies like **Sharp** and **Casio** to dominate the Breakout computing product of the 1970s. **Handheld electronic calculators** by using integrated circuits these could be made small and cheap, they replaced Expensive desktop adding machines you find in offices. For most people it was the first time they didn't have to do math on paper, or use a slide rule. They were an instant hit selling by the millions. This server drove down the cost of integrated circuits and led to the development and widespread use of **micro processors** like the intel 4004 we've discussed previously. This chip was built by intel in 1971 at the request of Japanese calculator company busy calm. soon Japanese electronics were everywhere from televisions of VcRs to digital wristwatches and Walkmans the availability of inexpensive microprocessor Spawned in Highly new Products like video arcades. the world got pong in 1972 and Breakout in 1976 as cost continued to plummet, soon it became possible for regular people to afford computing devices during this time we see the emergence of the first successful home computers like the 1975 Altair 8800 and also the first home gaming consoles like the Atari 2600 in 1977, home. now, I repeat that, home. that seems like a small thing today. But this was **the dawn of a whole new era in computing** in just three decades computers have evolved from machines where you could literally Walk inside of the cPU, assuming you had government clearance. to the point where a child could play with a handheld toy Containing a microprocessor many times faster, critically this dramatic evolution would have been but without two powerful forces at play governments and consumers.

> 在整个20世纪70年代，美国公司开始缩小规模，合并或彻底失败。英特尔不得不在1974年裁减了三分之一的员工，甚至传奇的飞兆半导体也在1979年濒临破产后被收购，以求得生存。英特尔公司从其主要产品类别内存ICS中抽出，并决定重新专注于工艺，这最终拯救了公司。这种低迷的电子行业让日本公司如**夏普**和**卡西奥**在20世纪70年代的突破性计算产品中占据主导地位。**手持式电子计算器**通过使用集成电路，可以做得又小又便宜，它们取代了办公室里昂贵的台式加法器。对大多数人来说，这是他们第一次不必在纸上做数学题，或使用滑尺。它们一经推出就受到欢迎，销售量达数百万。这台服务器推动了集成电路成本的下降，并导致了**微处理器**的发展和广泛使用，如我们之前讨论的英特尔4004。这个芯片是英特尔在1971年应日本计算器公司的要求而制造的。很快，日本的电子产品随处可见，从电视机、录像机到数字手表和随身听，廉价的微处理器的出现催生了像视频游戏机这样的高度新产品。1972年世界上出现了乒乓，1976年出现了Breakout，随着成本的不断下降，很快普通人就有可能买得起计算设备，在这期间我们看到了第一台成功的家用电脑的出现，如1975年的Altair 8800，还有第一台家用游戏机，如1977年的Atari 2600。家用，现在，我再说一遍，家用。今天的家用似乎是一件小事。但这是一个**全新的计算时代的黎明**，在短短三十年里，计算机已经从你可以从字面上看到的机器发展到了现在。如果你有政府许可的话，你可以走进中央处理器的内部，发展到一个孩子可以玩一个手持玩具的地步，其中包含一个快很多倍的微处理器.关键是这种戏剧性的演变，如果没有两个强大的力量在发挥作用，政府和消费者，将难以想象。

government funding like the United States provided during the cold war enabled early adoption of many nascent computing technologies. this funding helped flow entire Industries relate into computing long enough for the technology to mature and become commercially feasible, then **businesses and ultimately consumers** Provided the demand to take it mainstream. the cold war may be over, but this relationship continues today. Governments are still funding science research intelligence, agencies are still buying supercomputers, humans are still being launched into space. And you're still buying TVs xboxes playstations laptops and smartphones. and for these reasons Computing continues to Advance a lightning pace. I'll see you next week.

> 像美国在冷战期间提供的政府资金使许多新生的计算技术得以早期采用。这种资金有助于将整个工业与计算联系起来，使技术足够成熟并在商业上可行，然后**企业和最终的消费者**提供需求来把它变成主流。冷战可能已经结束，但这种关系持续到今天。政府仍在资助科学研究情报，机构仍在购买超级计算机，人类仍在被发射到太空。而你仍然在购买电视Xboxes游戏机笔记本电脑和智能手机。由于这些原因，计算技术继续以闪电般的速度前进。我们下周见。
>



## #25 The personal computer revolution

<iframe width="560" height="315" src="https://www.youtube.com/embed/M5BZou6C01w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science! As we discussed last week, the idea of having a computer all to yourself – a **personal computer** – was elusive for the first three decades of electronic computing. It was just way too expensive for a computer to be owned and used by one single person. But, by the early 1970s, all the required components had fallen into place to build a low cost, but still usefully powerful computer. Not a toy, but a tool. Most influential in this transition was the advent of **single-chip CPUs**, which were surprisingly powerful, yet small and inexpensive. Advances in **integrated circuits** also offered low-cost **solid-state memory**, both for computer **RAM** and **ROM**. Suddenly it was possible to have an entire computer on one circuit board, dramatically reducing manufacturing costs. Additionally, there was cheap and reliable computer storage, like magnetic tape cassettes and floppy disks. And finally, the last ingredient was low cost **displays**, often just repurposed televisions. If you blended these four ingredients together in the 1970s, you got, what was called a **microcomputer**, because these things were so tiny compared to “normal” computers of that era, the types you’d see in business or universities. But more important than their size was their cost. These were, for the first time, sufficiently cheap. It was practical to buy one and only have one person ever use it. No time sharing, no multi-user logins, just a single owner and user. The personal computer era had arrived. 

> 嗨，我是卡丽-安，欢迎来到《计算机科学速成班》! 正如我们上周所讨论的，在电子计算的前三十年里，拥有一台属于自己的计算机--**个人计算机**--的想法是难以实现的。对于一个人拥有和使用的计算机来说，它实在是太昂贵了。但是，到了20世纪70年代初，所有必要的组件都已到位，可以建立一个低成本但仍然有用的强大计算机。这不是一个玩具，而是一个工具。在这一转变中最具影响力的是**单芯片CPU**的出现，它的功能出奇地强大，但体积小，价格便宜。**集成电路**的进步也提供了低成本的**固态存储器**，用于计算机**RAM**和**ROM**。突然间，在一块电路板上有了整台计算机，极大地降低了制造成本。此外，还有廉价和可靠的计算机存储，如磁带盒和软盘。最后，最后一个成分是低成本的**显示器**，通常只是重新利用电视。在20世纪70年代，如果你把这四种成分混合在一起，你就得到了所谓的**微型计算机**，因为这些东西与那个时代的 "普通 "计算机，即你在商业或大学里看到的那种类型相比是如此之小。但比它们的尺寸更重要的是它们的成本。这些东西第一次足够便宜。购买一台并只让一个人使用它是很实际的。没有时间共享，没有多用户登录，只有一个所有者和使用者。个人电脑时代已经到来。
>

INTRO Computer cost and performance eventually reached the point where personal computing became viable. But, it’s hard to define exactly when that happened. There’s no one point in time. And as such, there are many contenders for the title of “first” personal computer, like the Kenback-1 and MCM/70. Less disputed, however, is the first commercially successful personal computer: The **Altair 8800**. This machine debuted on the cover of Popular Electronics in 1975, and was sold as a \$439 kit that you built yourself. **Inflation adjusted**, that’s about \$2,000 today, which isn’t chump change, but extremely cheap for a computer in 1975. Tens of thousands of kits were sold to computer hobbyists, and because of its popularity, there were soon all sorts of nifty add-ons available... things like extra memory, a paper tape reader and even a teletype interface. This allowed you, for example, to load a longer, more complicated program from punch tape, and then interact with it using a teletype terminal. However, these programs still had to be written in machine code, which was really low level and nasty, even for hardcore computer enthusiasts. This problem didn’t escape a young Bill Gates and Paul Allen, who were 19 and 22 respectively. They contacted MITS, the company making the Altair 8800, suggesting the computer would be more attractive to hobbyists if it could run programs written in **BASIC**, a popular and simple programming language. To do this, they needed to write a program that converted BASIC instructions into native machine code, what’s called an **interpreter**. This is very similar to a compiler, but **happens as the programs runs instead of beforehand**. 

> 计算机成本和性能最终达到了个人计算变得可行的地步。但是，很难准确地定义那是什么时候发生的。没有一个时间点。因此，有很多人争夺 "第一台 "个人电脑的称号，如Kenback-1和MCM/70。然而，较少争议的是第一台商业上成功的个人电脑。**Altair 8800**。这台机器于1975年首次出现在《大众电子》杂志的封面上，以439美元的套件出售，由你自己制作。**通货膨胀调整**，今天大约是2000美元，这不是小钱，但对于1975年的计算机来说是非常便宜的。数以万计的套件被卖给了计算机爱好者，由于它的流行，很快就有了各种有趣的附加组件......如额外的内存、纸带阅读器，甚至电传接口。例如，这允许你从打孔磁带上加载一个更长、更复杂的程序，然后用电传终端与之互动。然而，这些程序仍然必须用机器代码编写，即使对铁杆计算机爱好者来说，这也是非常低级和讨厌的。这个问题没有逃过年轻的比尔-盖茨和保罗-艾伦，他们当时分别是19岁和22岁。他们联系了制造Altair 8800的MITS公司，建议如果计算机能够运行用**BASIC**（一种流行的简单编程语言）编写的程序，将对业余爱好者更具吸引力。要做到这一点，他们需要编写一个程序，将BASIC指令转换成本地机器代码，即所谓的**解释器**。这与编译器非常相似，但**是在程序运行时发生的，而不是事先发生的**。
>

Let’s go to the thought bubble! MITS was interested, and agreed to meet Bill and Paul for a demonstration. Problem is, they hadn’t written the interpreter yet. So, they hacked it together in just a few weeks without even an Altair 8800 to develop on, finishing the final piece of code on the plane. The first time they knew their code worked was at MITS headquarters in Albuquerque, New Mexico, for the demo. Fortunately, it went well and MITS agreed to distribute their software. **Altair BASIC became the newly formed Microsoft’s first product**. Although computer hobbyists existed prior to 1975, the Altair 8800 really jump-started the movement. Enthusiast groups formed, sharing knowledge and software and passion about computing. Most legendary among these is the **Homebrew Computer Club**, which met for the first time in March 1975 to see a review unit of the Altair 8800, one of the first to ship to California. At that first meeting was 24-year-old Steve Wozniak, who was so inspired by the Altair 8800 that he set out to design his own computer. In May 1976, he demonstrated his prototype to the Club and shared the schematics with interested members. Unusual for the time, it was designed to connect to a TV and offered a text interface ‒ a first for a low-cost computer. Interest was high, and shortly after fellow club member and college friend Steve Jobs (his maxim: if you good at something , never do it for free)suggested that instead of just sharing the designs for free, that they should just sell an **assembled motherboard**. However, you still had to add your own keyboard, power supply, and enclosure. It went on sale in July 1976 with a price tag of $666.66. **It was called the Apple 1, and it was Apple Computer’s first product**. Thanks thought bubble! 

> 让我们去看看思想的泡沫吧! MITS很感兴趣，并同意与比尔和保罗见面进行演示。问题是，他们还没有编写解释器。所以，他们在没有Altair 8800的情况下，在短短的几周内就把它黑掉了，在飞机上完成了最后一段代码。他们第一次知道他们的代码工作是在新墨西哥州阿尔伯克基的MITS总部进行的演示。幸运的是，演示很顺利，MITS同意分发他们的软件。**Altair BASIC成为新成立的微软公司的第一个产品**。尽管计算机爱好者在1975年之前就已经存在，但Altair 8800真正启动了这个运动。发烧友团体形成，分享知识、软件和对计算机的热情。其中最具传奇色彩的是**自制计算机俱乐部**，该俱乐部于1975年3月首次聚会，观看Altair 8800的评测机，这是首批运往加利福尼亚的产品之一。在第一次会议上，24岁的史蒂夫-沃兹尼亚克受到Altair 8800的启发，开始设计自己的计算机。1976年5月，他向俱乐部展示了他的原型机，并与感兴趣的会员分享了原理图。在当时是不寻常的，它被设计成可以连接到电视上，并提供一个文本界面--这是低成本计算机的第一次。人们的兴趣很高，不久之后，俱乐部的成员和大学朋友史蒂夫-乔布斯（他的格言：如果你擅长什么，就不要免费做。）建议，与其免费分享设计，不如直接出售**组装的主板**。然而，你仍然需要添加你自己的键盘、电源和外壳。它于1976年7月上市销售，价格为666.66美元。**它被称为苹果1，是苹果电脑的第一款产品**。谢谢你的思想泡泡! 

Like the Altair 8800, the Apple 1 was sold as a kit. It appealed to hobbyists, who didn’t mind tinkering and soldering, but consumers and businesses weren’t interested. This changed in 1977, with the release of three game-changing computers, that could be used right out of the box. First was the Apple II, Apple’s earliest product that sold as a complete system that was professionally designed and manufactured. It also offered rudimentary color graphics and sound output, amazing features for a low cost machine. The Apple II series of computers sold by the millions and quickly propelled Apple to the forefront of the personal computing industry. The second computer was the TRS-80 Model I, made by the Tandy Corporation and sold by Radioshack – hence the “TRS”. Although less advanced than the Apple II, it was half the cost and sold like hot cakes. Finally, there was the Commodore PET 2001, with a unique **all-in-one design** that combined computer, monitor, keyboard and tape drive into one device, aimed to appeal to consumers. It started to blur the line between computer and appliance. These three computers became known as the 1977 Trinity. They all came bundled with BASIC interpreters, allowing non-computer-wizards to create programs. 

> 与Altair 8800一样，Apple 1也是以套件的形式出售。它吸引了那些不介意修补和焊接的业余爱好者，但消费者和企业并不感兴趣。1977年，随着三款改变游戏规则的计算机的发布，这种情况发生了改变，它们可以开箱即用。首先是苹果II，这是苹果公司最早的产品，作为一个专业设计和制造的完整系统出售。它还提供了基本的彩色图形和声音输出，对于一台低成本的机器来说，这是很了不起的功能。苹果II系列电脑销售量达数百万，并迅速将苹果公司推向了个人计算行业的前沿。第二台电脑是TRS-80 Model I，由坦迪公司制造，由Radioshack销售，因此被称为 "TRS"。虽然没有苹果II那么先进，但它的成本只有苹果II的一半，而且卖得很火。最后是Commodore PET 2001，它采用了独特的**一体式设计**，将计算机、显示器、键盘和磁带机合二为一，旨在吸引消费者。它开始模糊了计算机和电器之间的界限。这三台计算机被称为1977年的三“三剑客”。它们都捆绑了BASIC解释器，使非计算机专家也能创建程序。

The consumer software industry also took off, offering games and productivity tools for personal computers, like **calculators** and **word processors**. The killer app of the era was 1979’s **VisiCalc**, the first **spreadsheet program** – which was infinitely better than paper – and the forbearer of programs like Microsoft Excel and Google Sheets. But perhaps the biggest legacy of these computers was their marketing – they were the first to be targeted at households, and not just businesses and hobbyists. And for the first time in a substantial way, computers started to appear in homes, and also small businesses and schools. This caught the attention of the biggest computer company on the planet, IBM, who had seen its share of the overall computer market shrink from 60% in 1970 to around 30% by 1980. This was mainly because IBM had ignored the microcomputer market, which was growing at about 40% annually. As microcomputers evolved into personal computers, IBM knew it needed to get in on the action. But to do this, it would have to radically rethink its computer strategy and design. In 1980, IBM’s least-expensive computer, the 5120, cost roughly ten thousand dollars, which was never going to compete with the likes of the Apple II. This meant starting from scratch. A crack team of twelve engineers, later nicknamed the dirty dozen, were sent off to offices in Boca Raton, Florida, to be left alone and put their talents to work. Shielded from IBM internal politics, they were able to design a machine as they desired. Instead of using IBM proprietary CPUs, they chose Intel chips. Instead of using IBM’s prefered operating system, CP/M, they licenced Microsoft's Disk Operating System: DOS and so on, from the screen to the printer. For the first time, IBM divisions had to compete with outside firms to build hardware and software for the new computer. This radical break from the company tradition of in-house development kept costs low and brought partner firms into the fold. After just a year of development, the **IBM Personal Computer**, or **IBM PC** was released. It was an immediate success, especially with businesses that had long trusted the IBM brand. But, most influential to its ultimate success was that the computer featured an **open architecture**, with good documentation and **expansion slots**, allowing third parties to create new hardware and peripherals for the platform. That included things like graphics cards, **sounds cards**, external hard drives, joysticks, and countless other add-ons. This spurred innovation, and also competition, resulting in a huge ecosystem of products. This open architecture became known as “**IBM Compatible**”. 

> 消费软件行业也开始起飞，为个人电脑提供游戏和生产力工具，如**计算器和**文字处理器。这个时代的杀手级应用是1979年的**VisiCalc**，这是第一个**电子表格程序**--它比纸张好得多，也是微软Excel和谷歌表格等程序的前身。但是，这些计算机最大的遗产也许是它们的营销--它们是第一个针对家庭的计算机，而不仅仅是企业和业余爱好者。这是第一次以一种实质性的方式，计算机开始出现在家庭、小型企业和学校。这引起了地球上最大的计算机公司IBM的注意，它在整个计算机市场的份额从1970年的60%缩减到1980年的30%左右。这主要是因为IBM忽视了微型计算机市场，而该市场每年的增长速度约为40%。随着微型计算机演变成个人计算机，IBM知道它需要加入到这一行动中来。但要做到这一点，它必须从根本上重新考虑其计算机战略和设计。1980年，IBM最便宜的电脑，5120，价格大约为1万美元，这永远无法与苹果II这样的产品竞争。这意味着从头开始。一个由12名工程师组成的精锐团队，后来被戏称为 "肮脏十二人组"，被派往佛罗里达州博卡拉顿的办公室，让他们独自工作，发挥他们的才能。在IBM内部政治的保护下，他们能够按照自己的意愿设计机器。他们没有使用IBM专有的CPU，而是选择了英特尔芯片。他们没有使用IBM喜欢的操作系统CP/M，而是许可了微软的磁盘操作系统。DOS等等，从屏幕到打印机。这是第一次，IBM各部门不得不与外部公司竞争，为新的计算机建造硬件和软件。这种与公司内部开发传统的彻底决裂使成本降低，并使合作伙伴公司加入其中。经过短短一年的开发，**IBM个人电脑**（或称**IBM PC**）正式发布。它立即获得了成功，特别是在那些长期信任IBM品牌的企业中。但是，对其最终成功影响最大的是，该计算机具有**开放的架构**，有良好的文档和**扩展槽**，允许第三方为该平台创建新的硬件和外围设备。这包括诸如显卡、**声卡**、外置硬盘、操纵杆和无数其他附加设备。这刺激了创新，也刺激了竞争，形成了一个巨大的产品生态系统。这种开放的架构被称为 "**IBM兼容**"。

If you bought an “IBM Compatible” computer, it meant you could use that huge ecosystem of software and hardware. Being an open architecture also meant that competitor companies could follow the standard and create their own IBM Compatible computers. Soon, Compaq and Dell were selling their own PC clones... And Microsoft was happy to license MS-DOS to them, quickly making it the most popular **PC operating system**. IBM alone sold two million PCs in the first three years, overtaking Apple. With a large user base, software and hardware developers concentrated their efforts on IBM Compatible platforms – there were just more users to sell to. Then, people wishing to buy a computer bought the one with the most software and hardware available, and this effect snowballed. Companies producing non-IBM-compatible computers, often with superior specs, failed. Only Apple kept significant market share without IBM compatibility. Apple ultimately chose to take the opposite approach – a “**closed architecture**” – proprietary designs that typically prevent people from adding new hardware to their computers. This meant that **Apple made its own computers, with its own operating system, and often its own peripherals, like displays, keyboards, and printers**. **By controlling the full stack, from hardware to software, Apple was able to control the user experience and improve reliability**. These competing business strategies were the genesis of the “Mac” versus “PC” division that still exists today... which is a misnomer, because they’re both personal computers! 

> 如果你买了一台 "IBM兼容 "的电脑，就意味着你可以使用这个庞大的软件和硬件生态系统。作为一个开放的架构，也意味着竞争对手的公司可以遵循这个标准，创造他们自己的IBM兼容电脑。很快，康柏和戴尔就开始销售他们自己的克隆电脑...... 而微软也很乐意将MS-DOS授权给他们，迅速使其成为最受欢迎的**PC操作系统**。仅IBM在头三年就卖出了200万台PC，超过了苹果。有了庞大的用户群，软件和硬件开发商将他们的努力集中在与IBM兼容的平台上--只是有更多的用户可以销售。然后，希望购买电脑的人就会购买拥有最多软件和硬件的电脑，这种影响就像滚雪球一样。生产非IBM兼容电脑的公司，通常都有出色的规格，但都失败了。只有苹果公司在没有IBM兼容性的情况下保持了大量的市场份额。苹果最终选择了相反的方法--"**封闭式架构**"--专有设计，通常会阻止人们在电脑上添加新的硬件。这意味着**苹果制造了自己的电脑，有自己的操作系统，通常还有自己的外围设备，如显示器、键盘和打印机**。**通过控制从硬件到软件的整个堆栈，苹果能够控制用户体验并提高可靠性**。这些相互竞争的商业策略是今天仍然存在的 "Mac "与 "PC "之争的起源......这是个错误的说法，因为它们都是个人电脑 

But whatever. To survive the onslaught of low-cost PCs, Apple needed to up its game, and offer a user experience that PCs and DOS couldn’t. Their answer was the Macintosh, released in 1984. This ground breaking, reasonably-low-cost, all-in-one computer booted not a command-line text-interface, but rather a **graphical user interface**, our topic for next week. See you then. 

> 但不管怎样。为了在低成本PC的冲击下生存下来，苹果公司需要提高其游戏水平，并提供PC和DOS无法提供的用户体验。他们的答案是1984年发布的Macintosh。这款具有突破性意义的、成本低廉的一体化电脑所启动的不是命令行文本界面，而是**图形用户界面**，也就是我们下周的主题。到时见。



## #26 Graphical user interfaces

<iframe width="560" height="315" src="https://www.youtube.com/embed/XIGSJshYb90" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science! We ended last episode with the 1984 release of **Apple’s Macintosh personal computer**. **It was the first computer a regular person could buy with a graphical user interface and a mouse to interact with it**. This was a radical evolution from the command line interfaces found on all other personal computers of the era. Instead of having to remember... or guess... the right commands to type in, a graphical user interface shows you what functions are possible. You just have to look around the screen for what you want to do. It’s a “point and click” interface. All of a sudden, computers were much more intuitive. Anybody, not just hobbyists or computer scientists, could figure things out all by themselves. 

> 我们以1984年**苹果公司的Macintosh个人电脑的发布结束了上一集**。**这是第一台普通人可以买到的带有图形用户界面和鼠标互动的电脑**。这是对当时所有其他个人电脑上的命令行界面的一次彻底的进化。你不需要记住......或猜测......正确的命令来输入，图形用户界面会显示哪些功能是可能的。你只需要在屏幕上寻找你想做的事情。这是一个 "点和点击 "的界面。突然间，计算机变得更加直观了。任何人，不仅仅是业余爱好者或计算机科学家，都可以自己把事情弄明白。

INTRO The Macintosh is credited with taking **Graphical User Interfaces**, or **GUIs**, mainstream, but in reality they were the result of many decades of research. In previous episodes, we discussed some early interactive graphical applications, like Sketchpad and Spacewar!, both made in 1962. But these were one-off programs, and not whole integrated computing experiences. Arguably, the true forefather of modern GUIs was Douglas Engelbart. 

> Macintosh被认为是将**图形用户界面（Graphical User Interfaces）**或（**GUI**）推向了主流，但实际上它们是几十年来研究的结果。在以前的节目中，我们讨论了一些早期的交互式图形应用程序，如Sketchpad和Spacewar！，都是在1962年制作的。但这些都是一次性的程序，而不是整个综合计算体验。可以说，现代图形用户界面的真正先驱是Douglas Engelbart。

Let’s go to the thought bubble! During World War 2, while Engelbart was stationed in the Philippines as a radar operator, he read Vannevar Bush’s article on the Memex. These ideas inspired him, and when his Navy service ended, he returned to school, completing a Ph.D. in 1955 at U.C. Berkeley. Heavily involved in the emerging computing scene, he collected his thoughts in a seminal 1962 report, titled: “Augmenting Human Intellect”. Engelbart “believed that **the complexity of the problems facing mankind [was] growing faster than our ability to solve them**. Therefore, finding ways to augment our intellect would seem to be both a necessary and a desirable goal." He saw that computers could be useful beyond just automation, and be essential interactive tools for future knowledge workers to tackle complex problems. Further inspired by Ivan Sutherland’s recently demonstrated Sketchpad, Engelbart set out to make his vision a reality, recruiting a team to build the oN-Line System. He recognized that a keyboard alone was insufficient for the type of applications he was hoping to enable. In his words: "We envisioned problem-solvers using computer-aided working stations to augment their efforts. They required the ability to interact with information displays using some sort of device to move [a cursor] around the screen." And in 1964, working with colleague Bill English, he created the very first computer mouse. The wire came from the bottom of the device and looked very much like a rodent and the nickname stuck. Thanks thought bubble! 

> 让我们去看看思想泡影吧! 在第二次世界大战期间，当恩格尔巴特作为雷达操作员驻扎在菲律宾时，他读了范尼瓦尔-布什关于Memex的文章。这些想法启发了他，当他的海军服役结束后，他回到了学校，于1955年在加州大学伯克利分校完成了博士学位。他大量参与了新兴的计算领域，在1962年的一份开创性报告中收集了他的想法，题目是："增强人类智力"。恩格尔巴特 "相信，**人类面临的问题的复杂性[正在]比我们解决它们的能力增长得更快**。因此，找到增强我们智力的方法似乎既是一个必要的也是一个理想的目标"。他看到，计算机的作用不仅仅是自动化，还可以成为未来知识工作者解决复杂问题的重要互动工具。受到伊万-萨瑟兰最近展示的Sketchpad的进一步启发，恩格尔巴特开始将他的设想变成现实，招募了一个团队来建造oN-Line系统。他认识到，对于他希望实现的那种应用，仅靠键盘是不够的。用他的话说。"我们设想问题解决者使用计算机辅助工作站来增强他们的工作。他们需要有能力与信息显示互动，使用某种设备在屏幕上移动[光标]"。1964年，他与同事比尔-英格利希合作，创造了第一个计算机鼠标。电线来自设备的底部，看起来非常像一只啮齿动物，这个绰号就这样传开了。谢谢思想泡泡 

In 1968, Engelbart demonstrated his whole system at the Fall Joint Computer Conference, in what’s often referred to as “the mother of all demos”. The demo was 90 minutes long and demonstrated many features of modern computing: bitmapped graphics, video conferencing, word processing, and collaborative real-time editing of documents. There were also precursors to modern GUIs, like the mouse and multiple windows – although they couldn’t overlap. It was way ahead of its time, and like many products with that label, it ultimately failed, at least commercially. But its influence on computer researchers of the day was huge. Engelbart was recognized for this watershed moment in computing with a Turing Award in 1997. Federal funding started to reduce in the early 1970s, which we discussed two episodes ago. At that point, many of Engelbart’s team, including Bill English, left and went to Xerox's newly formed Palo Alto Research Centre, more commonly known as Xerox PARC. It was here that the first true GUI computer was developed: the **Xerox Alto**, finished in 1973. For the computer to be easy to use, it needed more than just fancy graphics. It needed to be built around a concept that people were already familiar with, so they could immediately recognize how to use the interface with little or no training. Xerox’s answer was to treat the 2D screen like the top of a desk… or desktop. Just like how you can have many papers laid out on a desk, a user could have several computer programs open at once. Each was contained in their own frame, which offered a view onto the application – called a **window**. Also like papers on a desk, these windows could overlap, blocking the items behind them. And there were desk accessories, like a calculator and clock, that the user could place on the screen and move around. It wasn’t an exact copy of a desktop though. Instead, it was a metaphor of a desktop. For this reason, surprisingly, it’s called the **Desktop Metaphor**. 

> 1968年，恩格尔巴特在秋季联合计算机会议上演示了他的整个系统，这通常被称为 "所有演示之母"。该演示长达90分钟，展示了现代计算的许多功能：位图、视频会议、文字处理和文档的实时协作编辑。还有现代图形用户界面的前兆，如鼠标和多窗口--尽管它们不能重叠在一起。它远远领先于它的时代，就像许多带有这种标签的产品一样，它最终失败了，至少在商业上是这样。但它对当时的计算机研究人员的影响是巨大的。恩格尔巴特因这一计算领域的分水岭时刻而在1997年被授予图灵奖。联邦资金在20世纪70年代初开始减少，我们在两集前讨论过这个问题。那时，包括比尔-英格利希在内的许多恩格尔巴特的团队离开了，去了施乐公司新成立的帕洛阿尔托研究中心，更常见的是施乐PARC。正是在这里，第一台真正的GUI计算机被开发出来：**施乐公司的Alto**，于1973年完成。为了使计算机易于使用，它需要的不仅仅是华丽的图形。它需要围绕着一个人们已经熟悉的概念，所以他们可以立即认识到如何使用界面，只需很少或没有培训。施乐公司的答案是把二维屏幕当作桌子的顶部......或桌面。就像你可以在桌子上摆放许多纸张一样，用户可以同时打开几个计算机程序。每个程序都包含在他们自己的框架中，提供了对应用程序的视图--称为**窗口**。就像桌子上的文件一样，这些窗口可以重叠起来，挡住后面的项目。还有一些办公桌上的附件，如计算器和时钟，用户可以放在屏幕上并移动。但这并不是桌面的完全复制。相反，它是一个桌面的隐喻。由于这个原因，令人惊讶的是，它被称为**桌面隐喻**。

There are many ways to design an interface like this, but the Alto team did it with windows, icons, menus, and a pointer – what’s called a **WIMP interface**. It’s what most desktop GUIs use today. It also offered a basic set of widgets, reusable graphical building blocks...things like buttons, checkboxes, sliders, and tabs which were also drawn from real world objects to make them familiar. GUI applications are constructed from these widgets, so let’s try coding a simple example using this new programming paradigm. First, we have to tell the operating system that we need a new window to be created for our app. We do this through a GUI API. We need to specify the name of the window and also its size. Let’s say 500 by 500 pixels. Now, let’s add some widgets – a text box and a button. These require a few parameters to create. First, we need to specify what window they should appear in, because apps can have multiple windows. We also need to specify the default text, the X and Y location in the window, and a width and height. Ok, so now we’ve got something that looks like a GUI app, but has no functionality. If you click the “roll” button, nothing happens. In previous examples we’ve discussed, the code pretty much executes from top to bottom. GUIs, on the other hand, use what’s called **event-driven programming**; code can fire at any btime, and in different orders, in response to events. In this case, it’s user driven events, like clicking on a button, selecting a menu item, or scrolling a window. Or if a cat runs across your keyboard, it’s a bunch of events all at once! Let’s say that when the user clicks the “roll” button, we want to randomly generate a number between 1 and 20, and then show that value in our text box. We can write a function that does just that. We can even get a little fancy and say if we get the number 20, set the background color of the window to blood red! The last thing we need to do is hook this code up so that it’s triggered each time our button is clicked. To do this, we need to specify that our function “handles” this event for our button, by adding a line to our initialize function. The type of event, in this case, is a click event, and our function is the event handler for that event. Now we’re done. We can click that button all day long, and each time, our “roll D20” function gets dispatched and executed. This is exactly what’s happening behind the scenes when you press the little bold button in a text editor, or select shutdown from a dropdown menu – a function linked to that event is firing. Hope I don’t roll a 20. Ahhhh! 

> 有很多方法可以设计这样的界面，但Alto团队用窗口、图标、菜单和指针来做，这就是所谓的**WIMP界面**。这也是今天大多数桌面GUI使用的方式。它还提供了一套基本的部件，可重复使用的图形构件......像按钮、复选框、滑块和标签，这些东西也是从现实世界的物体中提取的，使它们变得熟悉。GUI应用程序是由这些小部件构成的，所以让我们试着用这种新的编程范式来编码一个简单的例子。首先，我们必须告诉操作系统，我们需要为我们的应用程序创建一个新窗口。我们通过GUI的API来做这件事。我们需要指定窗口的名称和它的大小。比方说500×500像素。现在，让我们添加一些小部件--一个文本框和一个按钮。这些需要一些参数来创建。首先，我们需要指定它们应该出现在哪个窗口，因为应用程序可以有多个窗口。我们还需要指定默认文本、窗口中的X和Y位置，以及宽度和高度。好了，现在我们已经有了一个看起来像GUI应用程序的东西，但没有任何功能。如果你点击 "滚动 "按钮，什么也不会发生。在我们之前讨论的例子中，代码几乎是从上到下执行的。另一方面，GUI使用所谓的**事件驱动编程**；代码可以在任何时间，以不同的顺序，响应事件而启动。在这种情况下，它是用户驱动的事件，如点击一个按钮，选择一个菜单项，或滚动一个窗口。或者，如果一只猫从你的键盘上跑过，那就是一堆事件同时发生了 比方说，当用户点击 "滚动 "按钮时，我们想随机生成一个1到20之间的数字，然后在我们的文本框中显示这个值。我们可以写一个函数来做这件事。我们甚至可以搞点花样，说如果我们得到的数字是20，就把窗口的背景颜色设为血红色 我们需要做的最后一件事是把这段代码连接起来，以便每次点击按钮时都能触发它。要做到这一点，我们需要在初始化函数中添加一行，指定我们的函数为我们的按钮 "处理 "这个事件。在这种情况下，事件的类型是一个点击事件，而我们的函数是该事件的处理者。现在我们已经完成了。我们可以整天点击那个按钮，而每次，我们的 "roll D20 "函数都会被分派和执行。这正是当你在文本编辑器中按下小的粗体按钮，或从下拉菜单中选择关闭时在幕后发生的事情--一个与该事件相关的函数正在启动。希望我没有得到20分。啊啊啊! 

Ok, back to the Xerox Alto! Roughly 2000 Altos were made, and used at Xerox and given to University labs. They were never sold commercially. Instead, the PARC team kept refining the hardware and software, culminating in the **Xerox Star system**, released in 1981. The Xerox Star extended the desktop metaphor. Now, **files looked like pieces of paper, and they could be stored in little folders, all of which could sit on your desktop, or be put away into digital filing cabinets. It’s a metaphor that sits ontop of the underlying file system**. From a user’s perspective, this is a new level of abstraction! 

> 好了，回到施乐公司的Alto! 大约制造了2000台Altos，在施乐公司使用，并提供给大学实验室。它们从未进行过商业销售。相反，PARC团队不断完善硬件和软件，最终于1981年发布了**施乐之星系统**。施乐之星扩展了桌面的隐喻。现在，**文件看起来像纸片，它们可以被储存在小文件夹中，所有这些都可以放在你的桌面上，或者被收进数字文件柜。这是一个坐落在底层文件系统之上的隐喻**。从用户的角度来看，这是一个新的抽象层次！

Xerox, being in the printing machine business, also advanced text and graphics creation tools. For example, they introduced the terms: **cut**, **copy** and **paste**. This metaphor was drawn from how people dealt with making edits in documents written on typewriters. You’d literally cut text out with scissors, and then paste it, with glue, into the spot you wanted in another document. Then you’d photocopy the page to flatten it back down into a single layer, making the change invisible. Thank goodness for computers! This manual process was moot with the advent of **word processing software**, which existed on platforms like the Apple II and Commodore PET. But Xerox went way beyond the competition with the idea that whatever you made on the computer should look exactly like the real world version, if you printed it out. They dubbed this **What-You-See-Is-What-You-Get** or **WYSIWYG**. Unfortunately, like Engelbart’s oN-Line System, the Xerox Star was ahead of its time. Sales were sluggish because it had a price tag equivalent to nearly $200,000 today for an office setup. It also didn’t help that the IBM PC launched that same year, followed by a tsunami of cheap “IBM Compatible” PC Clones. But the great ideas that PARC researchers had been cultivating and building for almost a decade didn’t go to waste. 

> 施乐公司由于从事印刷机业务，也有先进的文字和图形创作工具。例如，他们引入了一些术语。**切割**，**复制**和**粘贴**。这个比喻来自于人们如何处理在打字机上书写的文件的编辑工作。你可以用剪刀把文字剪下来，然后用胶水把它粘贴到另一个文件中你想要的地方。然后你再把这一页复印下来，把它压成单层，使改动不可见。谢天谢地，有了电脑。随着**文字处理软件**的出现，这一手工操作过程变得毫无意义，这些软件存在于苹果II和Commodore PET等平台上。但施乐公司超越了竞争对手，他们认为无论你在电脑上做什么，如果你把它打印出来，看起来都应该和现实世界的版本一模一样。他们将此称为**你所见即所得**或**WYSIWYG**。不幸的是，就像恩格尔巴特的 "在线系统 "一样，施乐公司的 "明星 "也走在了时代的前列。由于它的价格相当于今天办公设备的近200,000美元，所以销售很不景气。同年推出的IBM个人电脑也无济于事，随后出现了廉价的 "IBM兼容 "个人电脑的海啸。但是，PARC研究人员近十年来一直在培养和建立的伟大想法并没有被浪费。

In December of 1979, a year and a half before the Xerox Star shipped, a guy you may have heard of visited: Steve Jobs. There’s a lot of lore surrounding this visit, with many suggesting that Steve Jobs and Apple stole Xerox’s ideas. But that simply isn’t true. In fact, Xerox approached Apple, hoping to partner with them. Ultimately, Xerox was able to buy a million dollar stake in Apple before its highly anticipated I.P.O. -but it came with an extra provision: “disclose everything cool going on at Xerox PARC". Steve knew they had some of the greatest minds in computing, but he wasn’t prepared for what he saw. There was a demonstration of Xerox’s graphical user interface, running on a crisp, bitmapped display, all driven with intuitive mouse input. Steve “later said, “It was like a veil being lifted from my eyes. I could see the future of what computing was destined to be.” Steve returned to Apple with his engineering entourage, and they got to work inventing new features, like the **menu bar** and a **trash can** to store files to be deleted; it would even bulge when full - again with the metaphors. Apple’s first product with a graphical user interface, and mouse, was the **Apple Lisa**, released in 1983. It was a super advanced machine, with a super advanced price – almost 25 thousand dollars today. That was significantly cheaper than the Xerox Star, but it turned out to be an equal flop in the market. Luckily, Apple had another project up its sleeve: The **Macintosh**, released a year later, in 1984. It had a price of around 6,000 dollars today – a quarter of the Lisa’s cost. And it hit the mark, selling 70,000 units in the first 100 days. But after the initial craze, sales started to falter, and Apple was selling more of its Apple II computers than **Macs**. 

> 1979年12月，在施乐之星发货的一年半前，一个你可能听说过的人来访。史蒂夫-乔布斯。围绕这次访问有很多传说，许多人认为史蒂夫-乔布斯和苹果偷了施乐公司的想法。但这根本不是事实。事实上，施乐公司与苹果公司接触，希望与他们合作。最终，施乐公司能够在苹果公司备受瞩目的I.P.O.之前购买其一百万美元的股份--但它有一个额外的条款。"披露施乐PARC的一切酷事"。史蒂夫知道他们拥有一些计算机领域最伟大的头脑，但他对他所看到的情况没有准备。有一个施乐公司的图形用户界面的演示，在一个清晰的位图显示器上运行，所有的驱动都是由直观的鼠标输入。史蒂夫 "后来说，"这就像从我的眼中揭开了一层面纱。我看到了计算机注定要成为的未来"。史蒂夫带着他的工程团队回到了苹果公司，他们开始发明新的功能，如**菜单栏**和**垃圾桶**来存储要删除的文件；当它装满时甚至会鼓起来--又是这样的比喻。苹果公司的第一款带有图形用户界面和鼠标的产品是1983年发布的**苹果丽莎**。这是一台超级先进的机器，有着超级先进的价格--今天几乎是2.5万美元。这比施乐之星要便宜得多，但它在市场上却同样失败了。幸运的是，苹果公司有另一个项目：**Macintosh**，一年后于1984年发布。今天，它的价格约为6000美元，是Lisa成本的四分之一。它一鸣惊人，在头100天就售出了7万台。但在最初的热潮之后，销售开始动摇，苹果公司销售的Apple II电脑比**Mac**更多。

A big problem was that no one was making software for this new machine with it’s new radical interface. And it got worse. The competition caught up fast. Soon, other personal computers had primitive, but usable graphical user interfaces on computers a fraction of the cost. Consumers ate it up, and so did PC software developers. With Apple’s finances looking increasingly dire, and tensions growing with Apple’s new CEO, John Sculley, Steve Jobs was ousted. A few months later, Microsoft released Windows 1.0. It may not have been as pretty as Mac OS, but it was the first salvo in what would become a bitter rivalry and near dominance of the industry by Microsoft. Within ten years, Microsoft Windows was running on almost 95% of personal computers. Initially, fans of Mac OS could rightly claim superior graphics and ease-of-use. Those early versions of Windows were all built on top of DOS, which was never designed to run GUIs. But, after Windows 3.1, Microsoft began to develop a new consumer-oriented OS with upgraded GUI called **Windows 95**. This was a significant rewrite that offered much more than just polished graphics. It also had advanced features Mac OS didn’t have, like **program multitasking** and **protected memory**. **Windows 95 introduced many GUI elements still seen in Windows versions today, like the Start menu, taskbar, and Windows Explorer file manager**. 

> 一个大问题是，没有人为这台新机器制作软件，它有新的激进的界面。而且情况越来越糟。竞争者迅速赶上了。很快，其他个人电脑也有了原始的、但可用的图形用户界面，而其成本仅是电脑的一小部分。消费者吃了这一套，个人电脑软件开发商也是如此。随着苹果公司的财务状况越来越糟糕，以及与苹果公司新任首席执行官约翰-斯库利的关系越来越紧张，史蒂夫-乔布斯被赶走了。几个月后，微软发布了Windows 1.0。它可能没有Mac OS那么漂亮，但它是微软后来成为激烈竞争和几乎主导该行业的第一炮。在十年内，微软的Windows几乎在95%的个人电脑上运行。最初，Mac OS的粉丝们可以理直气壮地宣称自己拥有卓越的图形和易用性。那些早期版本的Windows都是建立在DOS之上的，而DOS从来就不是为运行图形用户界面而设计的。但是，在Windows 3.1之后，微软开始开发一个新的面向消费者的操作系统，并升级了图形用户界面，称为**Windows 95**。这是一次重大的重写，它提供的不仅仅是精美的图形。它还具有Mac OS没有的先进功能，如**程序多任务处理**和**保护的内存**。**Windows 95引入了许多GUI元素，至今仍在Windows版本中看到，如开始菜单、任务栏和Windows Explorer文件管理器**。

Microsoft wasn’t infallible though. Looking to make the desktop metaphor even easier and friendlier, it worked on a product called Microsoft Bob, and it took the idea of using metaphors to an extreme. Now you had a whole virtual room on your screen, with applications embodied as objects that you could put on tables and shelves. It even came with a crackling fireplace and a virtual dog to offer assistance. And you see those doors on the sides? Yep, those went to different rooms in your computer where different applications were available. As you might have guessed, it was not a success. This is a great example of how the user interfaces we enjoy today are the product of what’s essentially **natural selection**. 

> 不过，微软并不是无懈可击的。为了使桌面隐喻变得更加简单和友好，它致力于开发一个名为Microsoft Bob的产品，并将使用隐喻的想法发挥到极致。现在你的屏幕上有一个完整的虚拟房间，应用程序体现为你可以放在桌子和架子上的物体。它甚至还有一个噼啪作响的壁炉和一只虚拟的狗来提供帮助。你看到边上的那些门了吗？是的，这些门通向你电脑中的不同房间，那里有不同的应用程序。正如你可能已经猜到的，它并不成功。这是一个很好的例子，说明我们今天所享受的用户界面是本质上**自然选择**的产物。

Whether you’re running Windows, Mac, Linux, or some other desktop GUI, it’s almost certainly an evolved version of the WIMP paradigm first introduced on the Xerox Alto. Along the way, a lot of bad ideas were tried, and failed. Everything had to be invented, tested, refined, adopted or dropped. Today, GUIs are everywhere and while they’re good, they are not always great. No doubt you’ve experienced design-related frustrations after downloading an application, used someone else’s phone, or visited a website. And for this reason, computer scientists and interface designers continue to work hard to craft computing experiences that are both easier and more powerful. Ultimately, working towards Engelbart's vision of augmenting human intellect. I’ll see you next week. 

> 无论你运行的是Windows、Mac、Linux，还是其他桌面图形用户界面，它几乎肯定是在施乐公司的Alto上首次引入的WIMP范式的进化版。在这一过程中，有很多糟糕的想法被尝试，但都失败了。一切都必须被发明、测试、完善、采用或放弃。今天，图形用户界面无处不在，虽然它们很好，但并不总是很好。毫无疑问，你在下载一个应用程序、使用别人的手机或访问一个网站后，经历过与设计有关的挫折。正因为如此，计算机科学家和界面设计师们继续努力工作，以打造既简单又强大的计算体验。最终，努力实现恩格尔巴特关于增强人类智力的愿景。下周见。



## #27 3D graphics

<iframe width="560" height="315" src="https://www.youtube.com/embed/TEAtmCYYKZA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science! Over the past five episodes, we’ve worked up from text-based teletype interfaces to pixelated bitmapped graphics. Then, last episode, we covered Graphical User Interfaces and all their “Ooey Gooey” richness. All of these examples have been 2D. But of course “we are living in a 3D world, and I’m a 3 dimensional girl!” So today, we're going to talk about some fundamental methods in **3D computer graphics** and how you render them onto a 2D screen. 

> 在过去的五期节目中，我们从基于文本的电传界面到像素化的点阵图形。然后，在上一集，我们介绍了图形用户界面和他们所有的 "Ooey Gooey "的丰富性。所有这些例子都是2D的。但当然，"我们生活在一个三维的世界，而我是一个三维的女孩！" 因此，今天我们要谈一谈**三维计算机图形**的一些基本方法，以及如何将它们渲染到二维屏幕上。
>

INTRO As we discussed in episode 24 we can write functions that draw a line between any two points like A and B. By manipulating the X and Y coordinates of points A and B, we can manipulate the line. In 3D graphics, points have not just two coordinates, but three -- X, Y and Z. Or “zee” but I’m going to say “zed”. Of course, we don’t have X/Y/Z coordinates on a 2D computer screen, so graphics algorithms are responsible for “flattening” 3D coordinates onto a 2D plane. This process is known as **3D Projection**. Once all of the points have been converted from 3D to 2D, we can use the regular 2D line drawing function to connect the dots… literally. This is called **Wireframe Rendering**. Imagine building a cube out of chopsticks, and shining a flashlight on it. The shadow it casts onto your wall – its projection – is flat. If you rotate the cube around, you can see it’s a 3D object, even though it’s a flat projection. This transformation from 3D to 2D is exactly what your computer is doing, just with a lot more math… and less chopsticks. There are several types of 3D Projection. What you’re seeing right now is an **Orthographic Projection**, where, for example, the parallel sides in the cube appear as parallel in the projection. In the real 3D world through, parallel lines converge as they get further from the viewer, like a road going to the horizon. This type of 3D projection is called **Perspective Projection**. It’s the same process, just with different math. Sometimes you want perspective and sometimes you don’t -- the choice is up to the developer. Simple shapes, like cubes, are easily defined by straight lines. But for more complex shapes, triangles are better -- what are called **polygons** in 3D graphics. Look at this beautiful teapot made out of polygons. A collection of polygons like this is a **mesh**. The denser the mesh, the smoother the curves and the finer the details. But, that also increases the polygon count, which means more work for the computer. 

> 正如我们在第24集讨论的那样，我们可以编写函数，在任何两点之间画一条线，如A和B。在三维图形中，点不仅仅有两个坐标，而是有三个坐标--X、Y和Z。或 "Zee"，但我要说的是 "Zed"。当然，我们在二维计算机屏幕上没有X/Y/Z坐标，所以图形算法负责将三维坐标 "压扁 "到二维平面上。这个过程被称为**3D投影**。一旦所有的点都从3D转换为2D，我们就可以使用常规的2D画线功能来连接这些点......从字面上看。这被称为**线框渲染**。想象一下，用筷子搭成一个立方体，然后用手电筒照着它。它投射到墙上的阴影--它的投影--是平的。如果你把这个立方体旋转一下，你可以看到它是一个三维物体，尽管它是一个平面投影。这种从3D到2D的转换正是你的电脑在做的事情，只是多了很多数学运算......少了很多筷子。有几种类型的3D投影。你现在看到的是**正投影**，例如，立方体的平行边在投影中显示为平行。在真实的三维世界中，当平行线离观看者越来越远时，它们就会汇聚在一起，就像一条通往地平线的道路。这种类型的3D投影被称为**透视投影**。这是同样的过程，只是用不同的数学方法。有时你需要透视，有时你不需要--选择权在开发者手中。简单的形状，如立方体，很容易用直线定义。但对于更复杂的形状，三角形更好--在3D图形中被称为**多边形**。看看这个由多边形组成的美丽茶壶。像这样的多边形的集合就是一个**网格**。网格越密集，曲线就越平滑，细节就越精细。但是，这也增加了多边形的数量，这意味着计算机要做更多的工作。
>

Game designers have to carefully balance **model fidelity** vs. **polygon count**, because if the count goes too high, the **framerate** of an animation drops below what users perceive as smooth. For this reason, there are algorithms for simplifying meshes. The reason triangles are used, and not squares, or polygons, or some other more complex shape is simplicity: **three points in space unambiguously define a plane**. If you give me three points in a 3D space, I can draw a plane through it - there is only one.. single.. answer. This isn’t guaranteed to be true for shapes with four or more points. Also, two points aren’t enough to define a plane, only a line, so three is the perfect and minimal number. Triangles for the win! Wireframe rendering is cool and all – sorta retro – but of course 3D graphics can also be filled. The classic algorithm for doing this is called **Scanline Rendering**, first developed in 1967 at the University of Utah. For a simple example, let’s consider just one polygon. Our job here is to figure out how this polygon translates to filled pixels on a computer screen, so let’s first overlay a grid of pixels to fill. The scanline algorithm starts by reading the three points that make up the polygon, and finding the lowest and highest Y values. It will only consider rows between these two points. Then, the algorithm works down one row at a time. In each row, it calculates where a line – running through the center of a row – intersects with the side of the polygon. Because polygons are triangles, if you intersect one line, you have to intersect with another. It’s guaranteed! The job of the scanline algorithm is to fill in the pixels between the two intersections. Let’s see how this works. On the first row we look at we intersect here and here. The algorithm then colors in all pixels between those two intersections. And this just continues, row by row, which is why it’s called Scan... Line... Rendering. When we hit the bottom of the polygon, we’re done. 

> 游戏设计者必须仔细平衡**模型的保真度**与**多边形数量**，因为如果数量太高，动画的**帧率**就会降到用户认为的流畅程度以下。出于这个原因，有一些简化网格的算法。之所以使用三角形，而不是正方形、多边形或其他更复杂的形状，是因为简单。**空间中的三个点毫不含糊地定义了一个平面**。如果你给我三维空间中的三个点，我就能画出一个穿过它的平面--答案只有一个。对于有四个或更多点的形状来说，这并不能保证是真的。另外，两个点不足以定义一个平面，只能定义一条线，所以三个点是最完美和最小的数字。三角形的胜利! 线框渲染很酷，有点复古，但当然也可以用3D图形来填充。做到这一点的经典算法被称为**扫描线渲染**，最早于1967年在犹他大学开发。对于一个简单的例子，让我们只考虑一个多边形。我们的工作是弄清楚这个多边形如何在电脑屏幕上转化为填充的像素，所以我们首先叠加一个要填充的像素网格。扫描线算法从读取组成多边形的三个点开始，并找到最低和最高的Y值。它将只考虑这两点之间的行。然后，该算法每次都会向下工作一行。在每一行中，它计算一条线--穿过一行的中心--与多边形的边相交的地方。因为多边形是三角形，如果你与一条线相交，你必须与另一条线相交。这是有保证的! 扫描线算法的工作是填补两个交叉点之间的像素。让我们看看这是如何工作的。在我们看的第一行，我们在这里和这里相交。然后，该算法对这两个交叉点之间的所有像素进行着色。这就是为什么它被称为 "扫描..."。线... 渲染。当我们到达多边形的底部时，我们就完成了。
>

The rate at which a computer fills in polygons is called the fillrate. Admittedly, this is a pretty ugly filled polygon. It has what are known as “Jaggies” -- those rough edges. This effect is less pronounced when using smaller pixels. But nonetheless, you see these in games all the time, especially on lower powered platforms. One method to soften this effect is **Antialiasing**. Instead of filling pixels in a polygon with the same color, we can adjust the color based on how much the polygon cuts through each pixel. If a pixel is entirely inside of a polygon, it gets fully colored. But if the polygon only grazes a pixel, it’ll get a lighter shade. This feathering of the edges is much more pleasant to the eyes. Antialiasing is used all over the place, including in 2D graphics, like fonts and icons. If you lean in real close to your monitor.. Closer… closer…. Closer! You’ll see all the fonts in your browser are Antialiased. So smooth! In a 3D scene, there are polygons that are part objects in the back, near the front, and just about everywhere. Only some are visible, because some objects are hidden behind other objects in the scene -- what’s called **occlusion**. The most straightforward way to handle this is to use a **sort algorithm**, and arrange all the polygons in the scene from farthest to nearest, then render them in that order. This is called the **Painter's Algorithm**, because painters also have to start with the background, and then increasingly work up to foreground elements. Consider this example scene with three overlapping polygons. To make things easier to follow, we’re going to color the polygons differently. Also for simplicity, we’ll assume these polygons are all parallel to the screen, but in a real program, like a game, the polygons can be tilted in 3D space. 

> 计算机填充多边形的速度被称为填充率。无可否认，这是一个非常丑陋的填充多边形。它有所谓的 "Jaggies" --那些粗糙的边缘。当使用较小的像素时，这种效果就不那么明显了。但尽管如此，你还是经常在游戏中看到这些，特别是在低功率平台上。一种软化这种效果的方法是**反锯齿**。我们可以根据多边形对每个像素的切割程度来调整颜色，而不是用相同的颜色填充多边形中的像素。如果一个像素完全在多边形的内部，它就会被完全着色。但如果多边形只擦过一个像素，它就会得到一个较浅的阴影。这种边缘的羽化对眼睛来说更令人愉快。抗锯齿被广泛使用，包括在2D图形中，如字体和图标。如果你真正靠近你的显示器... 更近...更近.... 更近！更近 你会看到你浏览器中的所有字体都是抗锯齿的。如此顺畅! 在一个三维场景中，有一些多边形是后面、前面附近以及几乎所有地方的部分物体。只有一些是可见的，因为有些物体隐藏在场景中其他物体的后面--这就是所谓的**遮挡**。处理这个问题的最直接的方法是使用**排序算法**，将场景中的所有多边形从最远到最近排列，然后按照这个顺序渲染它们。这被称为**画家算法**，因为画家也必须从背景开始，然后逐渐上升到前景元素。考虑一下这个有三个重叠的多边形的场景例子。为了使事情更容易理解，我们将对多边形进行不同的着色。同样为了简单起见，我们将假设这些多边形都与屏幕平行，但在真正的程序中，比如游戏，多边形可以在三维空间中倾斜。

Our three polygons, A B and C… are at distance 20, 12 and 14. The first thing the Painter’s Algorithm does is sort all the polygons, from farthest to nearest. Now that they’re in order, we can use scanline rendering to fill each polygon, one at a time. We start with Polygon A, the farthest one away. Then we repeat the process for the next farthest polygon, in this case, C. And then we repeat this again, for Polygon B. Now we’re all done, and you can see the ordering is correct. The polygons that are closer, are in front! An alternative method for handling occlusion is called **Z-Buffering**. It achieves the same output as before, but with a different algorithm. Let’s go back to our previous example, before it was sorted. That’s because this algorithm doesn’t need to sort any polygons, which makes it faster. In short, Z-buffering keeps track of the closest distance to a polygon for every pixel in the scene. It does this by maintaining a Z-Buffer, which is just a matrix of values that sits in memory. At first, every pixel is initialized to infinity. Then Z-buffering starts with the first polygon in its list. In this case, that’s A. It follows the same logic as the scanline algorithm, but instead of coloring in pixels, it checks the distance of the polygon versus what’s recorded in its Z-Buffer. It records the lower of the two values. For our Polygon A, with a distance of 20, it wins against infinity every time. When it’s done with Polygon A, it moves on to the next polygon in its list, and the same thing happens. Now, because we didn’t sort the polygons, it’s not always the case that later polygons overwrite high values. In the case of Polygon C, only some of the values in the Z-buffer get new minimum distances. This completed Z-buffer is used in conjunction with a fancier version of scanline rendering that not only tests for line intersection, but also does a lookup to see if that pixel will even be visible in the final scene. If it’s not, the algorithm skips it and moves on. 

> 我们的三个多边形，A、B和C......的距离分别是20、12和14。画家算法做的第一件事就是对所有的多边形进行排序，从最远的到最近的。现在，它们已经按顺序排列，我们可以使用扫描线渲染来填充每个多边形，一次一个。我们从最远的那个多边形A开始。然后我们对下一个最远的多边形重复这个过程，在这个例子中是C。距离较近的多边形在前面。另一种处理闭塞的方法叫做**Z缓冲**。它实现了与之前相同的输出，但采用了不同的算法。让我们回到我们之前的例子，在它被排序之前。这是因为这个算法不需要对任何多边形进行排序，这使得它的速度更快。简而言之，Z-buffering跟踪场景中每个像素与多边形的最近距离。它通过维护一个Z-缓冲区来做到这一点，Z-缓冲区只是一个位于内存中的数值矩阵。起初，每个像素都被初始化为无穷大。然后Z-buffering从其列表中的第一个多边形开始。它遵循与扫描线算法相同的逻辑，但不是给像素着色，而是检查多边形的距离与Z-缓冲区中的记录。它记录这两个值中较低的那个。对于我们的多边形A，其距离为20，它每次都能战胜无穷大。当它完成了对多边形A的处理后，它又转到列表中的下一个多边形，同样的事情发生了。现在，因为我们没有对多边形进行排序，所以后来的多边形并不总是覆盖高值。在多边形C的例子中，只有Z-缓冲区中的一些值得到了新的最小距离。这个完成的Z缓冲区与一个更高级的扫描线渲染结合使用，它不仅测试线的相交，而且还进行查找，看该像素是否在最终场景中可见。如果不可见，算法会跳过它，继续前进。

An interesting problem arises when two polygons have the same distance, like if Polygon A and B are both at a distance of 20. Which one do you draw on top? Polygons are constantly being shuffled around in memory and changing their access order. Plus, rounding errors are inherent in floating point computations. So, which one gets drawn on top is often unpredictable. The result is a flickering effect called Z-Fighting, which if you’ve played 3D games, you’ve no doubt encountered. Speaking of glitches, another common optimization in 3D graphics is called **Back-Face Culling**. If you think about it, a triangle has two sides, a front and a back. With something like the head of an avatar, or the ground in a game, you should only ever see one side -- the side facing outwards. So to save processing time, the back-side of polygons are often ignored in the rendering pipeline, which cuts the number of polygon faces to consider in half. This is great, except when there’s a bug that lets you get inside of those objects, and look outwards. Then the avatar head or ground becomes invisible. Moving on. We need to talk about lighting -- also known as **shading** -- because if it’s a 3D scene, the lighting should vary over the surface of objects. Let’s go back to our teapot mesh. With scanline rendering coloring in all the polygons, our teapot looks like this. Not very 3D. So, let’s add some lighting to enhance the realism! As an example, we’ll pick 3 polygons from different parts of our teapot. Unlike our previous examples, we’re now going to consider how these polygons are oriented in 3D space -- they’re no longer parallel to the screen, but rather tilted in different 3D directions. The direction they face is called the **Surface Normal**, and we can visualize that direction with a little 3D arrow that’s perpendicular to the polygon’s surface. Now let’s add a light source. Each polygon is going to be illuminated a different amount. Some will appear brighter, because their angle causes more light to be reflected towards the viewer. For example, the bottom-most polygon is tilted downwards, away from the light source, which means it’s going to be dark. In a similar way, the rightmost polygon is slightly facing away from the light, so it will be partially illuminated. And finally, there’s the upper-left polygon. Its angle means that it will reflect light from the light source towards our view. So, it’ll appear bright. If we do this for every polygon, our teapot looks like this which is much more realistic! This approach is called **Flat Shading**, and it’s the most basic **lighting algorithm**. 

> 当两个多边形有相同的距离时，会出现一个有趣的问题，比如多边形A和B的距离都是20。你要把哪一个画在上面？多边形在内存中不断被洗牌，改变其访问顺序。另外，四舍五入的误差是浮点计算中固有的。因此，哪一个被画在上面往往是不可预知的。其结果是一种叫做Z-Fighting的闪烁效果，如果你玩过3D游戏，你肯定会遇到这种情况。说到闪失，3D图形中另一个常见的优化被称为**背脸剔除**。如果你想一想，一个三角形有两个边，一个正面和一个背面。对于像头像的头部或游戏中的地面这样的东西，你应该只看到一个面--朝外的一面。因此，为了节省处理时间，多边形的背面通常在渲染管道中被忽略，这就将需要考虑的多边形面的数量减少了一半。这很好，除非有一个错误让你进入这些物体的内部，并向外看。然后，头像的头部或地面就变得不可见了。继续前进。我们需要谈谈光照--也被称为**阴影处理**--因为如果是3D场景，光照应该在物体的表面变化。让我们回到我们的茶壶网格。通过扫描线渲染给所有的多边形上色，我们的茶壶看起来像这样。不是很3D。所以，让我们添加一些灯光来增强真实感吧 作为一个例子，我们将从我们的茶壶的不同部分挑选3个多边形。与之前的例子不同，我们现在要考虑的是这些多边形在三维空间中的方向 -- 它们不再与屏幕平行，而是向不同的三维方向倾斜。它们所面对的方向被称为**表面法线**，我们可以用一个垂直于多边形表面的小的三维箭头来表示这个方向。现在我们来添加一个光源。每个多边形将被照亮不同的量。有些会显得更亮，因为它们的角度会使更多的光反射到观众面前。例如，最下面的多边形是向下倾斜的，远离光源，这意味着它将是黑暗的。以类似的方式，最右边的多边形略微远离光源，所以它将被部分照亮。最后，是左上角的多边形。它的角度意味着它将从光源处向我们的视线反射光线。因此，它将显得很亮。如果我们对每个多边形都这样做，我们的茶壶看起来就像这样，这就更真实了 这种方法被称为**平面着色**，它是最基本的**光照算法**。

Unfortunately, it also makes all those polygon boundaries really noticeable and the mesh doesn’t look smooth. For this reason, more advanced lighting algorithms were developed, such as **Gouraud Shading** and **Phong Shading**. Instead of coloring in polygons using just one colour, they vary the colour across the surface in clever ways, which results in much nicer output. We also need to talk about **textures**, which in graphics refers to the look of a surface, rather than its feel. Like with lighting, there are many algorithms with all sorts of fancy effects. The simplest is **texture mapping**. To visualize this process, let’s go back to our single polygon. When we’re filling this in, using scanline rendering, we can look up what color to use at every pixel according to a texture image saved in memory. To do this, we need a mapping between the polygon’s coordinates and the texture’s coordinates. Let’s jump to the first pixel that scanline rendering needs to fill in. The texturing algorithm will consult the texture in memory, take the average color from the corresponding region, and fill the polygon accordingly. This process repeats for all pixels in the polygon, and that’s how we get textures. If you combine all the techniques we’ve talked about this episode, you get a wonderfully funky little teapot. And this teapot can sit in an even bigger scene, comprised of millions of polygons. Rendering a scene like this takes a fair amount of computation. But importantly, it’s the same type of calculations being performed over and over and over again for many millions of polygons – scanline filling, antialiasing, lighting, and texturing. 

> 不幸的是，这也使得所有这些多边形的边界非常明显，而且网格看起来并不平滑。由于这个原因，更先进的照明算法被开发出来，例如**Gouraud Shading**和**Phong Shading**。它们不是只用一种颜色给多边形着色，而是以巧妙的方式在整个表面上改变颜色，从而获得更漂亮的输出。我们还需要谈谈**纹理**，在图形中，纹理指的是表面的外观，而不是它的感觉。就像照明一样，有很多算法都有各种花哨的效果。最简单的是**纹理映射**。为了直观地了解这个过程，让我们回到我们的单一多边形。当我们使用扫描线渲染来填充时，我们可以根据保存在内存中的纹理图像来查找每个像素点应该使用什么颜色。要做到这一点，我们需要在多边形的坐标和纹理的坐标之间建立一个映射。让我们跳到扫描线渲染需要填入的第一个像素。纹理算法将查阅内存中的纹理，从相应的区域中获取平均颜色，并相应地填充多边形。这个过程对多边形中的所有像素重复进行，这就是我们获得纹理的方式。如果你把我们这一集谈到的所有技术结合起来，你就会得到一个奇妙的时髦的小茶壶。而这个茶壶可以放在一个更大的场景中，由数百万个多边形组成。渲染这样的场景需要相当多的计算。但重要的是，这是对数百万个多边形反复进行的相同类型的计算--扫描线填充、抗锯齿、照明和纹理。
>

However there are a couple of ways to make this much faster! First off, we can speed things up by having special hardware with extra bells and whistles just for these specific types of computations, making them lightning fast. And secondly, we can divide up a 3D scene into many smaller parts, and then render all the pieces in parallel, rather than sequentially. CPU’s aren’t designed for this, so they aren’t particularly fast. So, computer engineers created special processors just for graphics – a **GPU**, or Graphics Processing Unit. These can be found on graphics cards inside of your computer, along with RAM reserved for graphics. This is where all the meshes and textures live, allowing them to be accessed super fast by many different cores of the GPU all at once. A modern graphics card, like a **GeForce GTX 1080 TI**, contains 3584 processing cores, offering massive parallelization. It can process hundreds of millions of polygons every second! Ok, that concludes our whistle stop tour of 3D graphics. Next week, we switch topics entirely. I’ll ping you then. 

> 然而，有几种方法可以使其更快！首先，我们可以通过为这些特定类型的计算提供额外的硬件来加速。首先，我们可以通过为这些特定类型的计算配备特殊的硬件来加快速度，使其快如闪电。其次，我们可以将一个3D场景分成许多小的部分，然后并行渲染所有的部分，而不是按顺序进行。CPU并不是为此而设计的，所以它们的速度并不特别快。因此，计算机工程师专门为图形创造了特殊的处理器--**GPU**，或图形处理单元。这些处理器可以在计算机内部的显卡上找到，同时还有为图形保留的内存。这是所有网格和纹理所在的地方，允许它们被GPU的许多不同核心同时超快访问。一个现代的显卡，如**GeForce GTX 1080 TI**，包含3584个处理核心，提供大规模的并行化。它每秒可以处理数以亿计的多边形! 好了，我们的3D图形之旅到此结束。下周，我们将完全转换话题。到时候我会给你发短信。



## #28 Computer networks

<iframe width="560" height="315" src="https://www.youtube.com/embed/3QhU9jd03a0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science! The internet is amazing. In just a few keystrokes, we can **stream videos on Youtube** -- Hello! -- **read articles on Wikipedia**, **order supplies on amazon**, **video chat with friends**, and **tweet about the weather**. Without a doubt, the ability for computers, and their users, to send and receive information over a global telecommunications network forever changed the world. 150 years ago, sending a letter from London to California would have taken two to three weeks, and that’s if you paid for express mail. Today, that email takes a fraction of a second. This million fold improvement in **latency**(the time it taks for a message to transfer), that’s the time it takes for a message to transfer, juiced up the global economy helping the modern world to move at the speed of light on **fiber optic cables** spanning the globe. You might think that computers and networks always went hand in hand, but actually most computers pre-1970 were humming away all alone. However, as big computers began popping up everywhere, and low cost machines started to show up on people’s desks, it became increasingly useful to share data and resources, and the first networks of computers appeared. Today, we’re going to start a three-episode arc on how computer networks came into being and the fundamental principles and techniques that power them. 

> 互联网是惊人的。只需敲几下键盘，我们就可以在Youtube上**看流视频**--你好！--**在维基百科上阅读文章**，**在亚马逊上订购用品**，**与朋友视频聊天**，以及**推特天气信息**。毫无疑问，计算机及其用户通过全球电信网络发送和接收信息的能力永远改变了世界。150年前，从伦敦寄信到加利福尼亚需要两到三周的时间，这还是在你支付快件费用的情况下。今天，这封电子邮件只需要几分之一秒。这种**延迟**（信息传输所需的时间）的百万倍改善，即信息传输所需的时间，为全球经济注入了活力，帮助现代世界以光速在**光缆**上移动，横跨全球。你可能认为计算机和网络总是携手并进的，但实际上，1970年以前的大多数计算机都是独自嗡嗡作响。然而，随着大型计算机开始到处出现，低成本的机器也开始出现在人们的办公桌上，分享数据和资源变得越来越有用，第一批计算机网络出现了。今天，我们将开始一个三集的跨度，介绍计算机网络是如何产生的，以及支持它们的基本原则和技术。

INTRO The first computer networks appeared in the 1950s and 60s. They were generally used within an organization – like a company or research lab – to facilitate the exchange of information between different people and computers. This was faster and more reliable than the previous method of having someone walk a pile of punch cards, or a reel of magnetic tape, to a computer on the other side of a building ‒ which was later dubbed a **sneakernet**. A second benefit of networks was the ability to share physical resources. For example, instead of each computer having its own printer, everyone could share one attached to the network. It was also common on early networks to have large, shared, storage drives, ones too expensive to have attached to every machine. These relatively small networks of close-by computers are called **Local Area Networks**, or **LANs**. A LAN could be as small as two machines in the same room, or as large as a university campus with thousands of computers. Although many LAN technologies were developed and deployed, the most famous and succesful was **Ethernet**, developed in the early 1970s at Xerox PARC, and still widely used today. In its simplest form, a series of computers are connected to a single, common ethernet cable. When a computer wants to transmit data to another computer, it writes the data, as an electrical signal, onto the cable. Of course, because the cable is shared, every computer plugged into the network sees the transmission, but doesn’t know if data is intended for them or another computer. To solve this problem, Ethernet requires that each computer has a unique **Media Access Control address**, or **MAC address**. This unique address is put into a header that prefixes any data sent over the network. So, computers simply listen to the ethernet cable, and only process data when they see their address in the header. This works really well; every computer made today comes with its own unique MAC address for both Ethernet and WiFi. The general term for this approach is **Carrier Sense Multiple Access**, or **CSMA** for short. The “carrier”, in this case, is any shared transmission medium that carries data – copper wire in the case of ethernet, and the air carrying radio waves for WiFi. Many computers can simultaneously sense the carrier, hence the “Sense” and “Multiple Access”, and the rate at which a carrier can transmit data is called its **Bandwidth**. 

> 第一个计算机网络出现在20世纪50年代和60年代。它们通常用于一个组织内部--如公司或研究实验室--以促进不同的人和计算机之间的信息交流。这比以前让人把一堆打孔卡或一卷磁带走到大楼另一边的计算机上的方法更快、更可靠--后来被称为**球鞋网络**。网络的第二个好处是能够共享物理资源。例如，不是每台计算机都有自己的打印机，而是每个人都可以共享一台连接到网络的打印机。在早期的网络中，拥有大型、共享的存储驱动器也是很常见的，这些驱动器太昂贵了，不可能连接到每台机器上。这些相对较小的邻近计算机的网络被称为**局域网**，或**LAN**。一个局域网可以小到同一房间里的两台机器，也可以大到一个拥有数千台计算机的大学校园。尽管开发和部署了许多局域网技术，但最著名和最成功的是**以太网**，它于20世纪70年代初在施乐PARC开发，至今仍被广泛使用。在其最简单的形式中，一系列的计算机被连接到一个单一的、通用的以太网电缆上。当一台计算机想要向另一台计算机传输数据时，它将数据作为电信号写到电缆上。当然，由于电缆是共享的，每台插入网络的计算机都能看到传输，但不知道数据是要给他们还是给另一台计算机。为了解决这个问题，以太网要求每台计算机都有一个独特的**媒体访问控制地址**，或**MAC地址**。这个唯一的地址被放在一个头里，作为通过网络发送的任何数据的前缀。因此，计算机只需监听以太网电缆，只有当它们看到头中的地址时才会处理数据。这种方法非常有效；今天的每台计算机都有自己独特的MAC地址，用于以太网和WiFi。这种方法的一般术语是**载波侦听多路访问**，或简称**CSMA**。在这种情况下，"载波 "是任何携带数据的共享传输介质--以太网是铜线，WiFi是携带无线电波的空气。许多计算机可以同时感应到载波，因此有 "感应 "和 "多路存取 "之说，而载波可以传输数据的速率被称为其**带宽**。

Unfortunately, using a shared carrier has one big drawback. When network traffic is light, computers can simply wait for silence on the carrier, and then transmit their data. But, as network traffic increases, the probability that two computers will attempt to write data at the same time also increases. This is called a **collision**, and the data gets all garbled up, like two people trying to talk on the phone at the same time. Fortunately, computers can detect these collisions by listening to the signal on the wire. The most obvious solution is for computers to stop transmitting, wait for silence, then try again. Problem is, the other computer is going to try that too, and other computers on the network that have been waiting for the carrier to go silent will try to jump in during any pause. This just leads to more and more collisions. Soon, everyone is talking over one another and has a backlog of things they need to say, like breaking up with a boyfriend over a family holiday dinner. Terrible idea! Ethernet had a surprisingly simple and effective fix. When transmitting computers detect a collision, they wait for a brief period before attempting to re-transmit. As an example, let’s say 1 second. Of course, this doesn’t work if all the computers use the same wait duration -- they’ll just collide again one second later. So, a random period is added: one computer might wait 1.3 seconds, while another waits 1.5 seconds. With any luck, the computer that waited 1.3 seconds will wake up, find the carrier to be silent, and start transmitting. When the 1.5 second computer wakes up a moment later, it’ll see the carrier is in use, and will wait for the other computer to finish. This definitely helps, but doesn’t totally solve the problem, so an extra trick is used. As I just explained, if a computer detects a collision while transmitting, it will wait 1 second, plus some random extra time. However, if it collides again, which suggests **network congestion**, instead of waiting another 1 second, this time it will wait 2 seconds. If it collides again, it’ll wait 4 seconds, and then 8, and then 16, and so on, until it’s successful. With computers backing off, the rate of collisions goes down, and data starts moving again, freeing up the network. Family dinner saved! 

> 不幸的是，使用共享载体有一个很大的缺点。当网络流量较少时，计算机可以简单地等待载体上的沉默，然后传输他们的数据。但是，随着网络流量的增加，两台计算机同时试图写入数据的概率也在增加。这被称为**冲突**，数据会被搅乱，就像两个人同时试图打电话一样。幸运的是，计算机可以通过监听电线上的信号来检测这些碰撞。最明显的解决方案是计算机停止传输，等待安静，然后再试。问题是，另一台计算机也会这样做，而且网络上其他一直在等待载波沉默的计算机也会在任何暂停期间尝试加入。这就导致了越来越多的碰撞。很快，每个人都在互相交谈，积压了很多需要说的话，比如在家庭节日晚宴上与男友分手。糟糕的想法! 以太网有一个令人惊讶的简单而有效的解决方案。当传输的计算机检测到碰撞时，它们会在尝试重新传输之前等待一个短暂的时间。作为一个例子，让我们说1秒。当然，如果所有的计算机都使用相同的等待时间，这是不可行的 -- 它们会在一秒钟后再次发生碰撞。因此，增加了一个随机的时间段：一台计算机可能等待1.3秒，而另一台则等待1.5秒。运气好的话，等了1.3秒的那台电脑会醒来，发现载波是静止的，然后开始传输。当1.5秒的计算机稍后醒来时，它将看到载体正在使用，并将等待另一台计算机完成。这无疑是有帮助的，但并不能完全解决这个问题，所以要使用一个额外的技巧。正如我刚才解释的，如果一台计算机在传输时检测到碰撞，它将等待1秒，再加上一些随机的额外时间。然而，如果它再次发生碰撞，这表明**网络拥堵**，而不是再等1秒，这一次它将等待2秒。如果它再次碰撞，它将等待4秒，然后是8秒，然后是16秒，以此类推，直到它成功。随着计算机的后退，碰撞率下降，数据又开始移动，解放了网络。家庭晚餐得救了! 

This “backing off” behavior using an exponentially growing wait time is called **Exponential Backoff**. Both Ethernet and WiFi use it, and so do many **transmission protocols**. But even with clever tricks like Exponential Backoff, you could never have an entire university’s worth of computers on one shared ethernet cable. To reduce collisions and improve efficiency, we need to shrink the number of devices on any given shared carrier -- what’s called the **Collision Domain**. Let go back to our earlier Ethernet example, where we had six computers on one shared cable, a.k.a. one collision domain. To reduce the likelihood of collisions, we can break this network into two collision domains by using a **Network Switch**. It sits between our two smaller networks, and only passes data between them if necessary. It does this by keeping a list of what MAC addresses are on what side of the network. So if A wants to transmit to C, the switch doesn’t forward the data to the other network – there’s no need. This means if E wants to transmit to F at the same time, the network is wide open, and two transmissions can happen at once. But, if F wants to send data to A, then the switch passes it through, and the two networks are both briefly occupied. This is essentially how big computer networks are constructed, including the biggest one of all – The **Internet** – which literally inter-connects a bunch of smaller networks, allowing inter-network communication. What’s interesting about these big networks, is that there’s often multiple paths to get data from one location to another. And this brings us to another fundamental networking topic, **routing**. The simplest way to connect two distant computers, or networks, is by allocating a communication line for their exclusive use. This is how early telephone systems worked. For example, there might be 5 telephone lines running between Indianapolis and Missoula. If John picked up the phone wanting to call Hank, in the 1910s, John would tell a human operator where he wanted to call, and they’d physically connect John’s phone line into an unused line running to Missoula. For the length of the call, that line was occupied, and if all 5 lines were already in use, John would have to wait for one to become free. This approach is called **Circuit Switching**, because you’re literally switching whole circuits to route traffic to the correct destination. It works fine, but it’s relatively inflexible and expensive, because there’s often unused capacity. On the upside, once you have a line to yourself – or if you have the money to buy one for your private use – you can use it to its full capacity, without having to share. For this reason, the military, banks and other high importance operations still buy dedicated circuits to connect their data centers. 

> 这种使用指数级增长的等待时间的 "回退 "行为被称为**指数退避**。以太网和WiFi都使用它，许多**传输协议**也是如此。但是，即使有像指数退避这样的聪明技巧，你也不可能在一条共享的以太网电缆上拥有整所大学的计算机。为了减少碰撞并提高效率，我们需要减少任何给定共享载体上的设备数量--这就是所谓的**冲突域**。让我们回到先前的以太网例子，我们在一条共享电缆上有六台计算机，也就是一个碰撞域。为了减少碰撞的可能性，我们可以通过使用一个**网络交换机**将这个网络分成两个碰撞域。它位于我们的两个小网络之间，只在必要时在它们之间传递数据。它通过保持一个列表，记录哪些MAC地址在网络的哪一边。因此，如果A想传输到C，交换机不会将数据转发到另一个网络--没有必要。这意味着如果E想同时向F传输，网络是开放的，两个传输可以同时发生。但是，如果F想向A发送数据，那么交换机就会将其通过，两个网络都会被短暂占用。这基本上就是大型计算机网络的构建方式，包括最大的网络--**互联网**--它实际上是将一堆较小的网络相互连接起来，允许网络间通信。这些大网络的有趣之处在于，通常有多条路径可以将数据从一个地方送到另一个地方。这就把我们带到了另一个基本的网络话题，**路由**。连接两台远距离计算机或网络的最简单方法是分配一条通信线路供其专用。这就是早期电话系统的工作方式。例如，印第安纳波利斯和米苏拉之间可能有5条电话线。如果约翰拿起电话想给汉克打电话，在1910年代，约翰会告诉人工操作员他想打电话的地方，然后他们会把约翰的电话线实际连接到一条未使用的线路上，然后再把它连接到米苏拉。在通话过程中，这条线路被占用，如果所有5条线路都已被使用，约翰将不得不等待一条空闲线路。这种方法被称为**电路切换**，因为你实际上是在切换整个电路，将流量导向正确的目的地。它工作得很好，但它相对不灵活，也很昂贵，因为经常有未使用的容量。从正面看，一旦你有一条属于自己的线路--或者如果你有钱买一条供你私人使用--你就可以使用它的全部容量，而不需要分享。由于这个原因，军队、银行和其他高度重要的业务仍然购买专用线路来连接他们的数据中心。

Another approach for getting data from one place to another is **Message Switching**, which is sort of like how the postal system works. Instead of dedicated route from A to B, messages are passed through several stops. So if John writes a letter to Hank, it might go from Indianapolis to Chicago, and then hop to Minneapolis, then Billings, and then finally make it to Missoula. Each stop knows where to send it next because they keep a table of where to pass letters given a destination address. What’s neat about Message Switching is that it can use different routes, making communication more reliable and **fault-tolerant**. Sticking with our mail example, if there’s a blizzard in Minneapolis grinding things to a halt, the Chicago mail hub can decide to route the letter through Omaha instead. In our example, cities are acting like **network routers**. The number of hops a message takes along a route is called the **hop count**. Keeping track of the hop count is useful because it can help identify routing problems. For example, let’s say Chicago thinks the fastest route to Missoula is through Omaha, but Omaha thinks the fastest route is through Chicago. That's bad, because both cities are going to look at the destination address, Missoula, and end up passing the message back and forth between them, endlessly. Not only is this wasting bandwidth, but it’s a routing error that needs to get fixed! This kind of error can be detected because the hop count is stored with the message and updated along its journey. If you start seeing messages with high hop counts, you can bet something has gone awry in the routing! This threshold is the **Hop Limit**. A downside to Message Switching is that messages are sometimes big. So, they can clog up the network, because the whole message has to be transmitted from one stop to the next before continuing on its way. While a big file is transferring, that whole link is tied up. Even if you have a tiny, one kilobyte email trying to get through, it either has to wait for the big file transfer to finish or take a less efficient route. That’s bad. 

> 将数据从一个地方送到另一个地方的另一种方法是**报文交换**，这有点像邮政系统的工作方式。信息不是从A到B的专用路线，而是通过几个站点传递。因此，如果约翰给汉克写了一封信，它可能从印第安纳波利斯到芝加哥，然后跳到明尼阿波利斯，然后到比林斯，最后到达米苏拉。每一站都知道下一步该把信送到哪里，因为他们有一张表格，上面记录了给定的目的地地址，要把信送到哪里。报文交换的好处是，它可以使用不同的路线，使通信更加可靠和**容错**。以我们的邮件为例，如果明尼阿波利斯发生暴风雪，事情就会陷入停顿，芝加哥的邮件中心可以决定将信件转到奥马哈。在我们的例子中，城市就像**网络路由器**。一条信息沿途所经过的跳数被称为**跳数**。追踪跳数很有用，因为它可以帮助识别路由问题。例如，假设芝加哥认为到米苏拉最快的路线是通过奥马哈，但奥马哈认为最快的路线是通过芝加哥。这很糟糕，因为这两个城市都要看目的地地址，即米苏拉，最终在它们之间无休止地来回传递信息。这不仅是在浪费带宽，而且是一个需要修复的路由错误 这种错误可以被发现，因为跳数是与信息一起存储的，并在其旅程中更新。如果你开始看到邮件的跳数很高，你可以肯定路由中出现了问题。这个阈值就是**跳数限制**。报文交换的一个缺点是，报文有时很大。因此，它们可能会堵塞网络，因为整个报文在继续前进之前必须从一个站点传输到下一个站点。当一个大文件在传输时，整个链接就被占用了。即使你有一个很小的、一公斤的电子邮件想通过，它也不得不等待大文件传输完成，或者采取一个效率较低的路线。这很糟糕。

The solution is to chop up big transmissions into many small pieces, called **packets**. Just like with Message Switching, each packet contains a destination address on the network, so routers know where to forward them. This format is defined by the “**Internet Protocol**”, or **IP** for short, a standard created in the 1970s. Every computer connected to a network gets an IP Address. You’ve probably seen these as four, 8-bit numbers written with dots in between. For example,172.217.7.238 is an IP Address for one of Google’s servers. With millions of computers online, all exchanging data, bottlenecks can appear and disappear in milliseconds. Network routers are constantly trying to balance the load across whatever routes they know to ensure speedy and reliable delivery, which is called congestion control. Sometimes different packets from the same message take different routes through a network. This opens the possibility of packets arriving at their destination out of order, which is a problem for some applications. Fortunately, there are protocols that run on top of IP, like **TCP/IP**, that handle this issue. We’ll talk more about that next week. Chopping up data into small packets, and passing these along flexible routes with spare capacity, is so efficient and fault-tolerant, it’s what the whole internet runs on today. This routing approach is called **Packet Switching**. It also has the nice property of being decentralized, with no central authority or single point of failure. In fact, the threat of nuclear attack is why packet switching was developed during the cold war! 

> 解决办法是将大的传输分成许多小的部分，称为**数据包**。就像报文交换一样，每个数据包都包含网络上的一个目标地址，所以路由器知道在哪里转发这些数据。这种格式是由 "**互联网协议**"定义的，简称**IP**，是20世纪70年代创建的一个标准。每台连接到网络的计算机都有一个IP地址。你可能已经看到这些数字是四个8位数字，中间有点。例如，172.217.7.238是谷歌的一个服务器的IP地址。随着数以百万计的计算机在线，所有的数据交换，瓶颈可能在几毫秒内出现和消失。网络路由器不断尝试在他们知道的任何路线上平衡负载，以确保快速和可靠的交付，这被称为**阻塞控制**。有时，同一消息的不同数据包在网络中采取不同的路线。这就为数据包不按顺序到达目的地提供了可能性，这对某些应用来说是个问题。幸运的是，有一些运行在IP之上的协议，如**TCP/IP**，可以处理这个问题。我们将在下周进一步讨论这个问题。将数据分割成小包，并沿着有余量的灵活路由传递，是非常高效和容错的，这也是今天整个互联网的运行基础。这种路由方法被称为**分组？交换**。它还有一个很好的特性，就是去中心化，没有中央机构或单一故障点。事实上，核攻击的威胁是数据包交换在冷战期间被开发的原因! 

Appanet:

<img src="../reference pics/appranet.jpg">

Today, routers all over the globe work cooperatively to find efficient routings, exchanging information with each other using special protocols, like the **Internet Control Message Protocol** (ICMP) and the **Border Gateway Protocol** (BGP). The world's first packet-switched network, and the ancestor to the modern internet, was the **ARPANET**, named after the US agency that funded it, the Advanced Research Projects Agency. Here’s what the entire ARPANET looked like in 1974. Each smaller circle is a location, like a university or research lab, that operated a router. They also plugged in one or more computers – you can see PDP-1’s, IBM System 360s, and even an ATLAS in London connected over a satellite link. Obviously the internet has grown by leaps and bounds in the decades since. Today, instead of a few dozen computers online, it’s estimated to be nearing 10 billion. And it continues to grow rapidly, especially with the advent of wifi-connected refrigerators and other smart appliances, forming an “**internet of things**”. So that’s part one – an overview of computer networks. Is it a series of tubes? Well, sort of. Next week we’ll tackle some higher-level transmission protocols, slowly working our way up to the World Wide Web. I’ll see you then! 

> 今天，全球各地的路由器合作寻找有效的路由，使用特殊协议相互交换信息，如**互联网控制消息协议**（ICMP）和**边界网关协议**（BGP）。世界上第一个分组交换网络，也是现代互联网的祖先，是**ARPANET**，以资助它的美国机构--高级研究计划局命名。下面是整个ARPANET在1974年的样子。每个较小的圆圈是一个地点，如大学或研究实验室，它们操作一个路由器。他们还插入了一台或多台计算机--你可以看到PDP-1、IBM系统360，甚至伦敦的ATLAS也通过卫星连接。显然，互联网在此后的几十年里有了飞跃式的发展。今天，在线的计算机不再是几十台，而是估计接近100亿台。而且它还在继续快速增长，特别是随着连接wifi的冰箱和其他智能电器的出现，形成了一个 "**物联网**"。所以这就是第一部分--计算机网络的概述。网络是一堆管子构成的吗？嗯，算是吧。下周我们将处理一些更高级别的传输协议，慢慢地将我们的方式提升到万维网。到时见! 
>



## #29 The Internet

<iframe width="560" height="315" src="https://www.youtube.com/embed/AEaKrq3SpW8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science! As we talked about last episode, your computer is connected to a large, distributed network, called The Internet. I know this because you’re watching a youtube video, which is being streamed over that very internet. It’s arranged as an ever-enlarging web of interconnected devices. For your computer to get this video, the first connection is to your local area network, or **LAN**, which might be every device in your house that’s connected to your **wifi router**. This then connects to a **Wide Area Network**, or **WAN**, which is likely to be a router run by your **Internet Service Provider**, or **ISP** – companies like Comcast, AT&T or Verizon. At first, this will be a regional router, like one for your neighborhood, and then that router connects to an even bigger WAN, maybe one for your whole city or town. There might be a couple more **hops**, but ultimately you’ll connect to the backbone of the internet made up of gigantic routers with super high-bandwidth connections running between them. To request this video file from youtube, a **packet** had to work its way up to the backbone, travel along that for a bit, and then work its way back down to a youtube server that had the file. That might be four hops up, two hops across the backbone, and four hops down, for a total of ten hops. If you’re running Windows, MacOS or Linux, you can see the route data takes to different places on the internet by using the **traceroute program** on your computer. Instructions in the Doobly Doo. For us here at the Chad & Stacey Emigholz Studio in Indianapolis, the route to the **DFTBA server** in California goes through 11 stops. We start at 192.168.0.1 -- thats the IP address for my computer on our LAN. Then there’s the wifi router here at the studio, then a series of regional routers, then we get onto the backbone, and then we start working back down to the computer hosting “DFTBA dot com”, which has the IP address 104.24.109.186. But how does a packet actually get there? What happens if a packet gets lost along the way? If I type “DFTBA dot com” into my web browser, how does it know the server’s address? Those are our topics for today! 

> 正如我们上一集所谈到的，你的电脑连接到一个大型的、分布式的网络，称为互联网。我知道这一点，因为你正在看一个youtube视频，这个视频正是通过这个互联网流传的。它被安排成一个不断扩大的相互连接的设备网络。你的电脑要获得这个视频，首先要连接到你的局域网，或者说是**局域网（LAN）**，这可能是你家里所有连接到**wifi路由器**的设备。然后连接到**广域网**，即**WAN**，这可能是由你的**互联网服务提供商**，即**ISP**--像Comcast、AT&T或Verizon等公司运行的路由器。起初，这将是一个区域性的路由器，如你的邻居，然后该路由器连接到一个更大的广域网，可能是一个为整个城市或城镇。可能还有几个**跳迁**，但最终你会连接到由巨大的路由器组成的互联网骨干网，它们之间有超高的带宽连接。为了从youtube上请求这个视频文件，一个**数据包**必须沿着骨干网运行，沿着骨干网运行一段时间，然后再向下运行到拥有该文件的youtube服务器。这可能是四个跳数，两个跳数穿过骨干网，四个跳数下来，总共十个跳数。如果你运行的是Windows、MacOS或Linux，你可以通过使用你电脑上的**traceroute程序**来查看数据到互联网上不同地方的路线。说明在Doobly Doo。对于我们在印第安纳波利斯的Chad & Stacey Emigholz工作室来说，通往加州的**DFTBA服务器**的路线要经过11个站点。我们从192.168.0.1开始 -- 这是我们局域网中我的电脑的IP地址。然后是工作室的WIFI路由器，然后是一系列的区域路由器，然后我们进入主干网，然后我们开始回落到托管 "DFTBA dot com "的计算机，它的IP地址是104.24.109.186。但是，一个数据包实际上是如何到达那里的？如果一个数据包在途中丢失会怎样？如果我在我的网络浏览器中输入 "DFTBA dot com"，它是如何知道服务器的地址的？这些都是我们今天的主题! 

**traceroute program** example: tracert dftba.com

<img src="../reference pics/tracert.jpg">

INTRO As we discussed last episode, the internet is a huge distributed network that sends data around as little **packets**. If your data is big enough, like an email attachment, it might get broken up into many packets. For example, this video stream is arriving to your computer right now as a series of packets, and not one gigantic file. Internet packets have to conform to a standard called the **Internet Protocol**, or **IP**. It’s a lot like sending physical mail through the postal system – every letter needs a unique and legible address written on it, and there are limits to the size and weight of packages. Violate this, and your letter won’t get through. IP packets are very similar. However, IP is a very low level protocol – there isn’t much more than a destination address in a packet’s header, which is the metadata that’s stored in front of the data payload. This means that a packet can show up at a computer, but the computer may not know which application to give the data to; Skype or Call of Duty. For this reason, more advanced protocols were developed that sit on top of IP. One of the simplest and most common is the **User Datagram Protocol**, or **UDP**. UDP has its own header, which sits inside the data payload. Inside of the UDP header is some useful, extra information. One of them is a **port number**. Every program wanting to access the internet will ask its host computer’s Operating System to be given a unique port. **Like Skype might ask for port number 3478**. When a packet arrives to the computer, the Operating System will look inside the UDP header and read the port number. Then, if it sees, for example, 3478, it will give the packet to Skype. So to review, **IP gets the packet to the right computer, but UDP gets the packet to the right program running on that computer**. UDP headers also include something called a **checksum**, which allows the data to be verified for correctness. As the name suggests, it does this by checking the sum of the data. Here’s a simplified version of how this works. 

> 正如我们上一集所讨论的，互联网是一个巨大的分布式网络，以小**数据包**的形式发送数据。如果你的数据足够大，比如一个电子邮件附件，它可能会被分割成许多数据包。例如，这个视频流现在正以一系列数据包的形式到达你的电脑，而不是一个巨大的文件。互联网数据包必须符合一个称为**互联网协议**，或**IP**的标准。这很像通过邮政系统发送实体邮件--每封信都需要写上一个独特的、可读的地址，而且包裹的大小和重量也有限制。违反这一点，你的信就无法通过。IP数据包非常类似。然而，IP是一个非常低级的协议--在一个数据包的头中除了一个目标地址外没有更多的东西，这是存储在数据有效载荷前面的元数据。这意味着，一个数据包可以出现在一台计算机上，但计算机可能不知道该把数据交给哪个应用程序；是Skype还是Call of Duty。由于这个原因，更先进的协议被开发出来，它们位于IP之上。其中最简单和最常见的是**用户数据报协议**，或**UDP**。UDP有自己的报头，位于数据有效载荷内。UDP头内有一些有用的、额外的信息。其中之一是一个**端口号**。每个想要访问互联网的程序都会要求其主机的操作系统提供一个独特的端口。**像Skype可能会要求端口号3478**。当一个数据包到达计算机时，操作系统将查看UDP头的内部并读取端口号。然后，如果它看到，例如，3478，它将把数据包给Skype。因此，回顾一下，**IP将数据包送到正确的计算机，但UDP将数据包送到该计算机上运行的正确程序**。UDP报头还包括一个叫做**校验和**的东西，它允许数据被验证是否正确。顾名思义，它通过检查数据的总和来实现这一目的。下面是一个简化版的工作方式。

**IP**(destination address in a packet’s header, which is the metadata that’s stored in front of the data payload)

<img src="../reference pics/IP.jpg">

**UDP**:

<img src="../reference pics/UDP.jpg">

Lets imagine the raw data in our UDP packet is 89 111 33 32 58 and 41. Before the packet is sent, the transmitting computer calculates the checksum by adding all the data together: 89 plus 111 plus 33 and so on. In our example, this adds up to a checksum of 364. In UDP, the checksum value is stored in 16 bits. If the sum exceeds the maximum possible value, the upper-most bits overflow, and only the lower bits are used. Now, when the receiving computer gets this packet, it repeats the process, adding up all the data. 89 plus 111 plus 33 and so on. If that sum is the same as the checksum sent in the header, all is well. But, if the numbers don’t match, you know that the data got corrupted at some point in transit, maybe because of a power fluctuation or faulty cable. Unfortunately, UDP doesn’t offer any mechanisms to fix the data, or request a new copy – receiving programs are alerted to the corruption, but typically just discard the packet. Also, UDP provides no mechanisms to know if packets are getting through – a sending computer shoots the UDP packet off, but has no confirmation it ever gets to its destination successfully. Both of these properties sound pretty catastrophic, but some applications are ok with this, because UDP is also really simple and fast. Skype, for example, which uses UDP for video chat, can handle corrupt or missing packets. That’s why sometimes if you’re on a bad internet connection, Skype gets all glitchy – only some of the UDP packets are making it to your computer. Skype does the best it can with the data it does receive correctly. But this approach doesn’t work for many other types of data transmission. Like, it doesn’t really work if you send an email, and it shows up with the middle missing. The whole message really needs to get there correctly! 

> 让我们想象一下，我们的UDP数据包中的原始数据是89 111 33 32 58和41。在数据包发送之前，发送的计算机通过将所有数据相加计算出校验和。89加111加33，以此类推。在我们的例子中，这加起来就是一个364的校验和。在UDP中，校验和值被存储在16位。如果总和超过了可能的最大值，最上面的比特就会溢出，而只使用较低的比特。现在，当接收计算机得到这个数据包时，它重复这个过程，把所有的数据加起来。89加111加33，以此类推。如果这个总和与报头中发送的校验和相同，那么一切都很好。但是，如果数字不匹配，你就知道数据在传输过程中的某个环节被破坏了，可能是因为电源波动或电缆故障。不幸的是，UDP没有提供任何机制来修复数据，或请求一个新的副本--接收程序被提醒有损坏，但通常只是丢弃数据包。此外，UDP没有提供任何机制来知道数据包是否通过--发送计算机将UDP数据包射出，但无法确认它是否成功到达目的地。这两种特性听起来都是灾难性的，但有些应用可以接受，因为UDP也非常简单和快速。例如，Skype使用UDP进行视频聊天，它可以处理损坏或丢失的数据包。这就是为什么有时如果你的网络连接不好，Skype就会出现故障--只有部分UDP数据包能够到达你的电脑。Skype会尽其所能正确接收数据。但这种方法对许多其他类型的数据传输不起作用。例如，如果你发送一封电子邮件，而它显示的中间部分不见了，那么它就不起作用了。整个信息真的需要正确到达那里！"。

**checksum** example:

<img src="../reference pics/checksum.png">

When it “absolutely, positively needs to get there”, programs use the **Transmission Control Protocol**, or **TCP**, which like UDP, rides inside the data payload of IP packets. For this reason, people refer to this combination of protocols as TCP/IP. Like UDP, the TCP header contains a destination port and checksum. But, it also contains fancier features, and we’ll focus on the key ones. First off, TCP packets are given sequential numbers. So packet 15 is followed by packet 16, which is followed by 17, and so on... for potentially millions of packets sent during that session. These sequence numbers allow a receiving computer to put the packets into the correct order, even if they arrive at different times across the network. So if an email comes in all scrambled, the TCP implementation in your computer’s operating system will piece it all together correctly. Second, TCP requires that once a computer has correctly received a packet – and the data passes the checksum – that it send back an **acknowledgement**, or “**ACK**” as the cool kids say, to the sending computer. Knowing the packet made it successfully, the sender can now transmit the next packet. But this time, let’s say, it waits, and doesn’t get an acknowledgement packet back. Something must be wrong If enough time elapses, the sender will go ahead and just retransmit the same packet. It’s worth noting that the original packet might have actually gotten there, but the acknowledgment is just really delayed. Or perhaps it was the acknowledgment that was lost. Either way, it doesn’t matter, because the receiver has those sequence numbers, and if a duplicate packet arrives, it can be discarded. Also, TCP isn’t limited to a back and forth conversation – it can send many packets, and have many outstanding ACKs, which increases bandwidth significantly, since you aren’t wasting time waiting for acknowledgment packets to return. Interestingly, the success rate of ACKs, and also the round trip time between sending and acknowledging, can be used to infer network congestion. TCP uses this information to adjust how aggressively it sends packets – a mechanism for congestion control. So, basically, **TCP can handle out-of-order packet delivery, dropped packets – including retransmission – and even throttle its transmission rate according to available bandwidth.** Pretty awesome! 

> 当它 "绝对需要到达那里 "时，程序使用**传输控制协议**，或**TCP**，它和UDP一样，在IP数据包的数据有效载荷中运行。由于这个原因，人们把这种协议组合称为TCP/IP。与UDP一样，TCP头包含一个目标端口和校验。但是，它也包含了更多的功能，我们将重点讨论其中的关键功能。首先，TCP数据包是按顺序编号的。因此，15号包之后是16号包，16号包之后是17号包，以此类推......在该会话期间可能有数百万个包被发送。这些序列号允许接收计算机将数据包放入正确的顺序，即使它们在网络上不同的时间到达。因此，如果一封电子邮件是乱七八糟的，你的计算机操作系统中的TCP实现将正确地把它们拼凑起来。第二，TCP要求，一旦计算机正确地接收了一个数据包--并且数据通过了校验--它就会向发送计算机发回一个**确认码**，或者用酷孩子的话说，是 "**ACK**"。知道数据包成功到达后，发送者现在可以传送下一个数据包。但是这一次，让我们说，它在等待，并没有得到一个确认的数据包回来。一定是出了问题。如果时间足够长，发件人就会继续下去，直接重发同一个数据包。值得注意的是，原来的数据包可能真的到达了那里，但确认只是真的延迟了。也可能是确认被丢失了。无论哪种情况，这都不重要，因为接收方有这些序列号，如果有重复的数据包到达，就可以丢弃了。另外，TCP并不局限于来回对话--它可以发送许多数据包，并有许多未完成的ACK，这大大增加了带宽，因为你没有浪费时间等待确认数据包的返回。有趣的是，ACK的成功率，以及发送和确认之间的往返时间，可以用来推断网络拥堵情况。TCP使用这些信息来调整它发送数据包的积极性--这是一种拥堵控制机制。因此，基本上，**TCP可以处理失序的数据包交付、丢弃的数据包--包括重传--甚至可以根据可用带宽节制其传输速率**。相当了不起！ 

**TCP/IP**:

<img src="../reference pics/tcpip.jpg">

**TCP/IP send many packets at the same time**:

<img src="../reference pics/send many.jpg">

You might wonder why anyone would use UDP when TCP has all these nifty features. The single biggest downside are all those acknowledgment packets – it doubles the number of messages on the network, and yet, you're not transmitting any more data. That overhead, including associated delays, is sometimes not worth the improved robustness, especially for time-critical applications, like **Multiplayer First Person Shooters**. And if it’s you getting **lag-fragged** you’ll definitely agree! **When your computer wants to make a connection to a website, you need two things - an IP address and a port. Like port 80, at 172.217.7.238**. This example is the IP address and port for the Google web server. In fact, you can enter this into your browser’s address bar, like so, and you’ll end up on the google homepage. This gets you to the right destination, but remembering that long string of digits would be really annoying. It’s much easier to remember: google.com. So the internet has a special service that **maps these domain names to addresses**. **It’s like the phone book for the internet**. And it’s called the **Domain Name System**, or **DNS** for short. You can probably guess how it works. When you type something like “youtube.com” into your web browser, it goes and asks a **DNS server** – **usually one provided by your ISP** – to lookup the address. DNS consults its huge registry, and replies with the address... if one exists. In fact, if you try mashing your keyboard, adding “.com”, and then hit enter in your browser, you’ll likely be presented with an error that says DNS failed. That’s because that site doesn’t exist, so DNS couldn’t give your browser an address. But, if DNS returns a valid address, which it should for “youtube.com”, then your browser shoots off a request over TCP for the website’s data. There’s over 300 million registered domain names, so to make that DNS Lookup a little more manageable, it’s not stored as one gigantically long list, but rather in a **tree data structure**. What are called **Top Level Domains**, or **TLDs**, are at the very top. These are huge categories like .com and .gov. Then, there are lower level domains that sit below that, called **second level domains**; Examples under .com include google.com and dftba.com. Then, there are even lower level domains, called **subdomains**, like images.google.com, store.dftba.com. And this tree is absolutely HUGE! Like I said, more than 300 million domain names, and that's just second level domain names, not all the sub domains. For this reason, this data is distributed across many DNS servers, which are authorities for different parts of the tree. Okay, I know you’ve been waiting for it... We’ve reached a new level of abstraction! 

> 你可能会想，既然TCP有这么多漂亮的功能，为什么还有人要使用UDP呢？最大的缺点是所有这些确认数据包--它使网络上的信息数量增加了一倍，然而，你并没有传输更多的数据。这种开销，包括相关的延迟，有时是不值得改进的，特别是对于时间紧迫的应用，如**多人第一人称射击游戏**。如果是你被**lag-fragged**，你一定会同意的。**当你的电脑想与一个网站建立连接时，你需要两样东西--一个IP地址和一个端口。比如80端口，在172.217.7.238**。这个例子是谷歌网络服务器的IP地址和端口。事实上，你可以在浏览器的地址栏中输入这个地址，就像这样，你最终会进入谷歌的主页。这可以让你到达正确的目的地，但记住那一长串数字会非常烦人。它更容易记住：google.com。所以互联网有一个特殊的服务，**将这些域名映射到地址上**。**这就像互联网上的电话簿**。它被称为 "**域名系统**"，简称 "**DNS**"。你可能能猜到它是如何工作的。当你在浏览器中输入 "youtube.com "这样的内容时，它就会去询问**DNS服务器**--**通常是由你的ISP提供的**--来查询这个地址。DNS查询其庞大的注册表，并回复该地址......如果存在的话。事实上，如果你尝试乱敲击键盘，添加".com"，然后在浏览器中点击回车键，你可能会遇到一个错误，说DNS失败。这是因为该网站不存在，所以DNS无法给你的浏览器一个地址。但是，如果DNS返回一个有效的地址，对于 "youtube.com "来说，它应该返回一个有效的地址，那么你的浏览器就会通过TCP发送请求，获取该网站的数据。有超过3亿个注册域名，所以为了使DNS查询更容易管理，它不是以一个巨大的长列表的形式存储，而是以一个**树数据结构**。所谓的**顶级域名**，或**TLD**，位于最上面。这些是巨大的类别，如.com和.gov。然后，下面是较低级别的域名，称为**二级域名**；.com下面的例子包括google.com和dftba.com。然后，还有更低级别的域名，称为**子域名**，如images.google.com、store.dftba.com。而这棵树绝对是巨大的! 就像我说的，超过3亿个域名，而且这只是二级域名，不是所有的子域名。由于这个原因，这些数据分布在许多DNS服务器上，这些服务器负责该树的不同部分。好了，我知道你一直在等待它... 我们已经达到了一个新的抽象水平! 

**Domain Name System**(DNS):

<img src="../reference pics/dns.jpg">

**Domain structure**:

<img src="../reference pics/structure.jpg">

Over the past two episodes, we’ve worked up from electrical signals on wires, or radio signals transmitted through the air in the case of **wireless networks**. This is called the **Physical Layer**. MAC addresses, collision detection, exponential backoff and similar low level protocols that mediate access to the physical layer are part of the **Data Link Layer**. Above this is the **Network Layer**, which is where all the switching and routing technologies that we discussed operate. And today, we mostly covered the **Transport layer**, protocols like UDP and TCP, which are responsible for point to point data transfer between computers, and also things like error detection and recovery when possible. We’ve also grazed the **Session Layer** – where protocols like TCP and UDP are used to **open a connection, pass information back and forth, and then close the connection when finished – what’s called a session**. This is exactly what happens when you, for example, do a DNS Lookup, or request a webpage. These are the bottom five layers of the **Open System Interconnection** (OSI) model, a conceptual framework for compartmentalizing all these different network processes. Each level has different things to worry about and solve, and it would be impossible to build one huge networking implementation. As we’ve talked about all series, abstraction allows computer scientists and engineers to be improving all these different levels of the stack simultaneously, without being overwhelmed by the full complexity. And amazingly, we’re not quite done yet… The OSI model has two more layers, the **Presentation Layer** and the **Application Layer**, which include things like web browsers, Skype, HTML decoding, streaming movies and more. Which we’ll talk about next week. See you then. 

> 在过去的两期节目中，我们已经从电线上的电信号，或在**无线网络**中通过空气传输的无线电信号开始工作。这就是所谓的**物理层**。MAC地址、冲突检测、指数回避和类似的低级协议，这些都属于**数据链路层**的一部分。在这之上是**网络层**，也就是我们讨论的所有报文交换和路由技术的操作场所。今天，我们主要讨论了**传输层**，像UDP和TCP这样的协议，它们负责计算机之间点对点的数据传输，还有像错误检测和可能的恢复。我们还掠过了**会话层**--像TCP和UDP这样的协议被用来**打开一个连接，来回传递信息，然后在完成后关闭连接--这就是所谓的会话**。这正是当你，例如，做一个DNS查询，或请求一个网页时所发生的事情。这些是**开放系统互连**（OSI）模型的底层五层，这是一个将所有这些不同的网络过程分门别类的概念性框架。每个层次都有不同的事情需要担心和解决，不可能建立一个巨大的网络实现。正如我们所谈到的所有系列，抽象化允许计算机科学家和工程师同时改进所有这些不同层次的堆栈，而不被全部的复杂性所淹没。令人惊讶的是，我们还没有完全完成......OSI模型还有两层，即**表现层**和**应用层**，其中包括像网络浏览器、Skype、HTML解码、流媒体电影等等。我们将在下周讨论这个问题。到时见。

**Layers of the OSI model**:

<img src="../reference pics/layers of osi.jpg">



## #30 The World Wide Web

<iframe width="560" height="315" src="https://www.youtube.com/embed/guvsH5OFizE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science. Over the past two episodes, **we’ve delved into the wires, signals, switches, packets, routers and protocols that make up the internet**. Today we’re going to move up yet another level of abstraction and talk about the World Wide Web.This is not the same thing as the Internet, even though people often use the two terms interchangeably in everyday language. The **World Wide Web** runs on top of the internet, in the same way that Skype, Minecraft or Instagram do. The Internet is the underlying plumbing that conveys the data for all these different applications. And The World Wide Web is the biggest of them all – a huge distributed application running on millions of servers worldwide, accessed using a special program called a web browser. We’re going to learn about that, and much more, in today’s episode. 

> 在过去的两期节目中，**我们已经深入研究了构成互联网的电线、信号、交换机、数据包、路由器和协议**。今天，我们要再提高一个抽象层次，谈谈万维网。这与互联网不是一回事，尽管人们在日常用语中经常交替使用这两个术语。**万维网**运行在互联网之上，就像Skype、Minecraft或Instagram一样。互联网是底层管道，为所有这些不同的应用程序传递数据。而万维网是其中最大的一个--一个巨大的分布式应用，运行在全球数以百万计的服务器上，使用一个称为网络浏览器的特殊程序访问。在今天的节目中，我们将了解到这一点，以及更多的信息。

INTRO The fundamental building block of the World Wide Web – or web for short – is a single page. This is a document, containing content, which can include links to other pages. These are called **hyperlinks**. You all know what these look like: text or images that you can click, and they jump you to another page. These hyperlinks form a huge web of interconnected information, which is where the whole thing gets its name. This seems like such an obvious idea. But before hyperlinks were implemented, every time you wanted to switch to another piece of information on a computer, you had to rummage through the file system to find it, or type it into a search box. With hyperlinks, you can easily flow from one related topic to another. The value of hyperlinked information was conceptualized by Vannevar Bush way back in 1945. He published an article describing a hypothetical machine called a Memex, which we discussed in Episode 24. Bush described it as "associative indexing ... whereby any item may be caused at will to select another immediately and automatically." He elaborated: "The process of tying two things together is the important thing...thereafter, at any time, when one of those items is in view, the other [item] can be instantly recalled merely by tapping a button." In 1945, computers didn’t even have screens, so this idea was way ahead of its time! Text containing hyperlinks is so powerful, it got an equally awesome name: **hypertext**! **Web pages are the most common type of hypertext document today**. They’re retrieved and rendered by web browsers which we'll get to in a few minutes. In order for pages to link to one another, each hypertext page needs a unique address. On the web, this is specified by a **Uniform Resource Locator**, or **URL** for short. An example web page URL is thecrashcourse.com/courses. Like we discussed last episode, when you request a site, the first thing your computer does is a DNS lookup. This takes a domain name as input – like “the crash course dot com” – and replies back with the corresponding computer’s IP address. Now, armed with the IP address of the computer you want, your web browser opens a TCP connection to a computer that’s running a special piece of software called a web server. **The standard port number for web servers is port 80**. At this point, all your computer has done is connect to the web server at the address thecrashcourse.com.

> 万维网--或简称网络--的基本构件是一个单一的页面。这是一个包含内容的文件，其中可以包括与其他页面的链接。这些链接被称为**超级链接**。你们都知道这些看起来像什么：你可以点击的文本或图像，它们会让你跳到另一个页面。这些超链接形成了一个巨大的相互连接的信息网络，这就是整个事情的名称来源。这似乎是一个如此明显的想法。但在超链接实现之前，每次你想切换到计算机上的另一条信息时，你必须在文件系统中翻找它，或在搜索框中输入它。有了超链接，你可以轻松地从一个相关的主题流向另一个。超链接信息的价值早在1945年就被Vannevar Bush概念化了。他发表了一篇文章，描述了一种叫做Memex的假想机器，我们在第24集讨论过。布什将其描述为 "关联索引......据此，任何项目都可以随心所欲地立即自动选择另一个项目"。他阐述道。"将两件事情联系在一起的过程是重要的......此后，在任何时候，当这些项目中的一个出现在视野中时，只需点击一个按钮，就可以立即召回另一个[项目]。" 在1945年，计算机甚至没有屏幕，所以这个想法在当时是非常超前的。包含超链接的文本是如此强大，它有一个同样令人敬畏的名字。**超文本**! **网页是当今最常见的超文本文件类型**。它们由网络浏览器检索和呈现，我们将在几分钟后讨论。为了使页面能够相互链接，每个超文本页面需要一个独特的地址。在网络上，这是由**统一资源定位器**，或简称为**URL**指定的。一个网页URL的例子是thecrashcourse.com/courses。就像我们上一集讨论的那样，当你请求一个网站时，你的计算机做的第一件事就是进行DNS查询。这需要一个域名作为输入--如 "the crash course dot com"--并回复相应的计算机的IP地址。现在，有了你想要的计算机的IP地址，你的网络浏览器就会打开一个TCP连接，连接到运行一个特殊软件的计算机，这个软件被称为网络服务器。**网络服务器的标准端口号是80端口**。在这一点上，你的电脑所做的就是连接到地址为thecrashcourse.com的网络服务器。

www is consisted of hyperlinks:

<img src="../reference pics/www build on hyperlinks.jpg">

The next step is to ask that web server for the “courses” hypertext page. To do this, it uses the aptly named **Hypertext Transfer Protocol**, or **HTTP**. The very first documented version of this spec, HTTP 0.9, created in 1991, only had one command – “GET”. Fortunately, that’s pretty much all you need. Because we’re trying to get the “courses” page, we send the server the following command – GET /courses. This command is sent as **raw ASCII text** to the web server, which then replies back with the web page hypertext we requested. This is interpreted by your computer's web browser and rendered to your screen. If the user follows a link to another page, the computer just issues another GET request. And this goes on and on as you surf around the website. In later versions, HTTP added **status codes**, which prefixed any hypertext that was sent following a GET request. For example, **status code 200 means OK** – I’ve got the page and here it is! Status codes in the four hundreds are for client errors. Like, if a user asks the web server for a page that doesn’t exist, that’s the dreaded 404 error! Web page hypertext is stored and sent as plain old text, for example, encoded in ASCII or UTF-16, which we talked about in Episodes 4 and 20. Because plain text files don’t have a way to specify what’s a link and what’s not, it was necessary to develop a way to “mark up” a text file with hypertext elements. For this, the **Hypertext Markup Language** was developed. The very first version of HTML version 0.a, created in 1990, provided 18 HTML commands to markup pages. That’s it! Let’s build a webpage with these! First, let’s give our web page a big heading. To do this, we type in the letters “H1”, which indicates the start of a first level heading, and we surround that in angle brackets. This is one example of an HTML tag. Then, we enter whatever heading text we want. We don’t want the whole page to be a heading. So, we need to “close” the “h1” tag like so, with a little slash in the front. Now lets add some content. Visitors may not know what Klingons are, so let’s make that word a hyperlink to the Klingon Language Institute for more information. We do this with an “A” tag, inside of which we include an attribute that specifies a hyperlink reference. That’s the page to jump to if the link is clicked. And finally, we need to close the A tag. Now lets add a second level heading, which uses an “h2” tag. HTML also provides tags to create lists. We start this by adding the tag for an ordered list. Then we can add as many items as we want, surrounded in “Li” tags, which stands for list item. People may not know what a bat'leth is, so let’s make that a hyperlink too. Lastly, for good form, we need to close the ordered list tag. And we’re done – that’s a very simple web page! If you save this text into notepad or textedit, and name it something like “test.html”, you should be able to open it by dragging it into your computer’s web browser. 

> 下一步是要求该网络服务器提供 "课程 "的超文本页面。为了做到这一点，它使用了被恰当地命名为**超文本传输协议**，或**HTTP**。这个规范的第一个记录版本，即1991年创建的HTTP 0.9，只有一个命令--"GET"。幸运的是，这几乎是你所需要的全部。因为我们试图获得 "课程 "页面，我们向服务器发送以下命令--GET /courses。这个命令是以**原始ASCII文本**的形式发送给网络服务器的，然后它以我们所要求的网页超文本进行回复。这被你的计算机的网络浏览器解释并呈现在你的屏幕上。如果用户跟随一个链接到另一个页面，计算机只是发出另一个GET请求。当你在网站上冲浪时，这样的情况会一直持续下去。在后来的版本中，HTTP增加了**状态代码**，它在GET请求后发送的任何超文本前缀。例如，**状态代码200表示OK**--我已经得到了这个页面，它就在这里! 四百位的状态代码是针对客户端错误的。比如，如果一个用户向网络服务器询问一个不存在的页面，这就是可怕的404错误！这也是一种错误。网页超文本是以纯文本形式存储和发送的，例如，以ASCII或UTF-16编码，我们在第4集和第20集谈到过。因为纯文本文件没有办法指定什么是链接，什么不是，所以有必要开发一种方法，用超文本元素 "标记 "一个文本文件。为此，**超文本标记语言**被开发出来。1990年创建的第一个HTML版本0.a，提供了18个HTML命令来标记页面。这就是了! 让我们用这些来建立一个网页吧! 首先，让我们给我们的网页一个大标题。要做到这一点，我们输入字母 "H1"，这表示一级标题的开始，我们用角括号将其包围。这是HTML标签的一个例子。然后，我们输入我们想要的任何标题文本。我们不希望整个页面都是一个标题。因此，我们需要像这样 "关闭""h1 "标签，在前面加上一个小斜线。现在让我们添加一些内容。访问者可能不知道克林贡人是什么，所以让我们把这个词变成一个超链接，指向克林贡语言研究所，以获取更多信息。我们用一个 "A "标签来做这件事，在这个标签中，我们包括一个指定超链接参考的属性。如果链接被点击，这就是要跳到的页面。最后，我们需要关闭A标签。现在让我们添加一个二级标题，它使用 "h2 "标签。HTML还提供了用于创建列表的标签。我们首先添加一个有序列表的标签。然后我们可以添加任意多的项目，用 "Li "标签包围，代表列表项目。人们可能不知道bat'leth是什么，所以我们也让它成为一个超链接。最后，为了保持良好的形式，我们需要关闭有序列表标签。这样我们就完成了--这就是一个非常简单的网页 如果你把这个文本保存在记事本或textedit中，并命名为 "test.html"，你应该可以把它拖到你的电脑的网络浏览器中打开它。

web render as status code 200:

<img src="../reference pics/web render as status code 200.jpg">

simple HTML webpage

<img src="../reference pics/simple HTML webpage.jpg">

Of course, today’s web pages are a tad more sophisticated. The newest version of HTML, version 5, **has over a hundred different tags** – for things like images, tables, forms and buttons. And there are other technologies we’re not going to discuss, like **Cascading Style Sheets** or CSS and **JavaScript**, which can be embedded into HTML pages and do even fancier things. That brings us back to **web browsers**. This is the application on your computer that lets you talk with all these web servers. Browsers not only request pages and media, but also render the content that’s being returned. **The first web browser, and web server, was written by (now Sir) Tim Berners-Lee over the course of two months in 1990**. At the time, he was working at CERN in Switzerland. To pull this feat off, **he simultaneously created several of the fundamental web standards we discussed today: URLs, HTML and HTTP**. Not bad for two months work! Although to be fair, he’d been researching hypertext systems for over a decade. After initially circulating his software amongst colleagues at CERN, it was released to the public in 1991. **The World Wide Web was born**. Importantly, the web was an open standard, making it possible for anyone to develop new web servers and browsers. This allowed a team at the University of Illinois at Urbana-Champaign to create the **Mosaic web browser** in 1993. It was the first browser that allowed graphics to be embedded alongside text; previous browsers displayed graphics in separate windows. It also introduced new features like bookmarks, and had a friendly GUI interface, which made it popular. Even though it looks pretty crusty, it’s recognizable as the web we know today! By the end of the 1990s, there were many web browsers in use, like **Netscape Navigator**, **Internet Explorer**, **Opera**, **OmniWeb** and **Mozilla**. Many **web servers** were also developed, like **Apache** and **Microsoft’s Internet Information Services** (IIS). New websites popped up daily, and web mainstays like **Amazon** and **eBay** were founded in the mid-1990s. A golden era! 

> 当然，今天的网页更复杂一些。最新版本的HTML，即第五版，**有一百多个不同的标签**--用于图像、表格、表单和按钮等。还有其他一些我们不打算讨论的技术，如**层叠样式表**或CSS和**JavaScript**，它们可以嵌入到HTML页面中，做更多的事情。这使我们回到了**网络浏览器**。这是你电脑上的应用程序，让你与所有这些网络服务器对话。浏览器不仅请求网页和媒体，而且还渲染被返回的内容。**第一个网络浏览器和网络服务器是由（现在的爵士）蒂姆-伯纳斯-李在1990年的两个月内编写的**。当时，他正在瑞士的欧洲核子研究中心工作。为了完成这一壮举，**他同时创造了我们今天讨论的几个基本网络标准。URLs、HTML和HTTP**。对于两个月的工作来说，这并不坏 尽管公平地说，他研究超文本系统已经超过十年了。在最初将他的软件在欧洲核子研究中心的同事中传阅后，于1991年向公众发布。**万维网诞生了**。重要的是，网络是一个开放的标准，使任何人都有可能开发新的网络服务器和浏览器。这使得伊利诺伊大学厄巴纳-香槟分校的一个团队在1993年创造了**Mosaic网络浏览器**。这是第一个允许图形与文本一起嵌入的浏览器；以前的浏览器在单独的窗口中显示图形。它还引入了书签等新功能，并有一个友好的图形用户界面，这使它很受欢迎。尽管它看起来很破旧，但它是我们今天所知道的网络的识别标志 到90年代末，有许多网络浏览器在使用，如**Netscape Navigator**、**Internet Explorer**、**Opera**、**OmniWeb**和**Mozilla**。许多**网络服务器**也被开发出来，如**Apache**和**微软的互联网信息服务**（IIS）。每天都有新的网站出现，像**亚马逊**和**eBay**这样的网络支柱在90年代中期成立。一个黄金时代! 

The web was flourishing and people increasingly needed ways to find things. If you knew the web address of where you wanted to go – like ebay.com – you could just type it into the browser. But what if you didn’t know where to go? Like, you only knew that you wanted pictures of cute cats. Right now! Where do you go? At first, people maintained web pages which served as directories hyperlinking to other websites. Most famous among these was "Jerry and David's guide to the World Wide Web", renamed **Yahoo** in 1994. As the web grew, these human-edited directories started to get unwieldy, and so search engines were developed. Let’s go to the thought bubble! The earliest **web search engine** that operated like the ones we use today, was **JumpStation**, created by Jonathon Fletcher in 1993 at the University of Stirling. This consisted of three pieces of software that worked together. The first was a web crawler, software that followed all the links it could find on the web; anytime it followed a link to a page that had new links, it would add those to its list. The second component was an ever enlarging index, recording what text terms appeared on what pages the crawler had visited. The final piece was a search algorithm that consulted the index; for example, if I typed the word “cat” into JumpStation, every webpage where the word “cat” appeared would come up in a list. Early search engines used very simple metrics to rank order their search results, most often just the number of times a search term appeared on a page. This worked okay, until people started gaming the system, like by writing “cat” hundreds of times on their web pages just to steer traffic their way. Google’s rise to fame was in large part due to a clever algorithm that sidestepped this issue. Instead of trusting the content on a web page, **they looked at how other websites linked to that page**. If it was a spam page with the word cat over and over again, no site would link to it. But if the webpage was an authority on cats, then other sites would likely link to it. So the number of what are called “backlinks”, especially from reputable sites, was often a good sign of quality. This started as a research project called BackRub at Stanford University in 1996, before being spun out, two years later, into the **Google** we know today. Thanks thought bubble! 

> 网络正在蓬勃发展，人们越来越需要寻找东西的方法。如果你知道你想去的地方的网址--比如ebay.com--你可以直接在浏览器中输入它。但如果你不知道要去哪里呢？比如，你只知道你想要可爱猫咪的图片。现在就去吧! 你要去哪里？起初，人们维护网页，作为超链接到其他网站的目录。其中最著名的是 "杰里和大卫的万维网指南"，1994年改名为**雅虎。随着网络的发展，这些由人类编辑的目录开始变得不方便了，因此搜索引擎被开发出来。让我们去看看思想泡影吧! 最早的**网络搜索引擎是**JumpStation**，由Jonathon Fletcher于1993年在斯特林大学创建。它由三个共同工作的软件组成。第一个是网络爬虫，这个软件跟踪它能在网上找到的所有链接；只要它跟踪一个有新链接的页面，它就会把这些链接添加到它的列表中。第二个组成部分是一个不断扩大的索引，记录爬虫访问过的页面上出现了哪些文本术语。最后一个部分是查询索引的搜索算法；例如，如果我在JumpStation中输入 "猫 "这个词，出现 "猫 "这个词的每个网页都会出现在一个列表中。早期的搜索引擎使用非常简单的指标对其搜索结果进行排序，最常见的就是一个搜索词在一个页面上出现的次数。这样做还行，直到人们开始玩弄这个系统，比如在他们的网页上写几百次 "猫"，以引导流量。谷歌的成名在很大程度上是由于一个巧妙的算法，该算法回避了这个问题。他们不相信网页上的内容，而是**看其他网站如何链接到该网页**。如果它是一个反复出现猫这个词的垃圾网页，没有网站会链接到它。但如果该网页是关于猫的权威，那么其他网站就可能会链接到它。因此，所谓的 "反向链接 "的数量，特别是来自知名网站的链接，往往是质量的一个好标志。这始于1996年斯坦福大学的一个名为BackRub的研究项目，两年后被剥离出来，成为我们今天所知的**谷歌**。谢谢你，思想泡影! 

Finally, I want to take a second to talk about a term you’ve probably heard a lot recently, “**Net Neutrality**”. Now that you’ve built an understanding of packets, internet routing, and the World Wide Web, you know enough to understand the essence – at least the technical essence – of this big debate. **In short, network neutrality is the principle that all packets on the internet should be treated equally**. It doesn’t matter if the packets are my email or you streaming this video, they should all chug along at the same speed and priority. But many companies would prefer that their data arrive to you preferentially. Take for example, **Comcast**, a large ISP that also owns many TV channels, like NBC and The Weather Channel, which are streamed online. Not to pick on Comcast, but in the absence of Net Neutrality rules, they could for example say that they want their content to be delivered silky smooth, with high priority… But other streaming videos are going to get throttled, that is, **intentionally given less bandwidth and lower priority**. Again I just want to reiterate here this is just conjecture. At a high level, Net Neutrality advocates argue that giving internet providers this ability to essentially set up tolls on the internet – to provide premium packet delivery – plants the seeds for an exploitative business model. ISPs could be gatekeepers to content, with strong incentives to not play nice with competitors. Also, if big companies like **Netflix** and Google can pay to get special treatment, small companies, like start-ups, will be at a disadvantage, stifling innovation. On the other hand, there are good technical reasons why you might want different types of data to flow at different speeds. That skype call needs high priority, but it’s not a big deal if an email comes in a few seconds late. Net-neutrality opponents also argue that market forces and competition would discourage bad behavior, because customers would leave ISPs that are throttling sites they like. This debate will rage on for a while yet, and as we always encourage on Crash Course, you should go out and learn more because the implications of Net Neutrality are complex and wide-reaching. I’ll see you next week. 

> 最后，我想花点时间谈谈你最近可能经常听到的一个术语，"**网络中立性**"。现在你已经建立了对数据包、互联网路由和万维网的理解，你知道的足够多，可以理解这场大辩论的本质--至少是技术本质。**简而言之，网络中立性是指互联网上的所有数据包应被平等对待的原则**。不管这些数据包是我的电子邮件还是你的流媒体视频，它们都应该以同样的速度和优先权前进。但许多公司希望他们的数据能优先到达你手中。以**康卡斯特**为例，这是一家大型的互联网服务提供商，也拥有许多电视频道，如NBC和气象频道，这些频道都是在线流媒体。不是要挑剔康卡斯特，但在没有网络中立规则的情况下，他们可以说，他们希望他们的内容能以高优先级的方式如丝般顺畅地传送......但其他流媒体视频将被扼杀，也就是说，（节流）**故意给予较少的带宽和较低的优先级**。我只想再次重申，这只是猜测。在高层次上，"网络中立 "的倡导者认为，给予互联网供应商这种能力，基本上是在互联网上设置收费--提供优质的数据包交付--为剥削性商业模式埋下了种子。互联网服务提供商可能成为内容的看门人，有强烈的动机不与竞争对手友好相处。另外，如果像**Netflix**和谷歌这样的大公司可以付费获得特殊待遇，那么小公司，如初创公司，将处于不利地位，扼杀了创新。另一方面，有很好的技术原因，你可能希望不同类型的数据以不同速度流动。那个Skype电话需要高优先级，但如果一封电子邮件晚来几秒也不是什么大事。网络中立性的反对者还认为，市场力量和竞争会阻止不良行为，因为客户会离开那些扼杀他们喜欢的网站的互联网服务供应商。这场辩论还将持续一段时间，正如我们在《速成班》中一直鼓励的那样，你应该出去了解更多，因为网络中立性的影响是复杂和广泛的。下周见。



## #31 Cybersecurity

<iframe width="560" height="315" src="https://www.youtube.com/embed/bPVaOlJ6ln0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science! Over the last three episodes, we’ve talked about how computers have become interconnected, allowing us to communicate near-instantly across the globe. But, not everyone who uses these networks is going to play by the rules, or have our best interests at heart. Just as how we have physical security like locks, fences and police officers to minimize crime in the real world, we need **cybersecurity** to minimize crime and harm in the virtual world. Computers don’t have ethics. Give them a formally specified problem and they’ll happily pump out an answer at lightning speed. Running code that takes down a hospital’s computer systems until a ransom is paid is no different to a computer than code that keeps a patient's heart beating. Like the Force, computers can be pulled to the light side or the dark side. Cybersecurity is like the Jedi Order, trying to bring peace and justice to the cyber-verse. 

> 在过去的三期节目中，我们谈到了计算机如何变得互联互通，使我们能够在全球范围内近乎即时地沟通。但是，并不是所有使用这些网络的人都会遵守规则，或将我们的最佳利益放在心上。就像我们有像锁、栅栏和警察这样的物理安全来减少现实世界的犯罪一样，我们需要**网络安全**来减少虚拟世界的犯罪和伤害。计算机没有伦理道德。给他们一个正式指定的问题，他们会很高兴地以闪电般的速度抽出一个答案。对计算机来说，运行代码使医院的计算机系统瘫痪，直到支付赎金为止，这与保持病人心脏跳动的代码没有区别。像原力一样，计算机可以被拉到光明面或黑暗面。网络安全就像绝地武士团，试图为网络世界带来和平和正义。

INTRO The scope of cybersecurity evolves as fast as the capabilities of computing, but we can think of it as a set of techniques to protect the **secrecy**, **integrity** and **availability** of computer systems and data against threats. Let’s unpack those three goals: Secrecy, or confidentiality, means that **only authorized people should be able to access or read specific computer systems and data**. Data breaches, where hackers reveal people’s credit card information, is an attack on secrecy. Integrity means that **only authorized people should have the ability to use or modify systems and data**. Hackers who learn your password and send e-mails masquerading as you, is an integrity attack. And availability means that **authorized people should always have access to their systems and data**. Think of [Denial of Service Attacks](https://en.wikipedia.org/wiki/Denial-of-service_attack)(DoS), where hackers overload a website with fake requests to make it slow or unreachable for others. That’s attacking the service’s availability. To achieve these three general goals, security experts start with a specification of who your “enemy” is, at an abstract level, called a **threat model**. This profiles attackers: their capabilities, goals, and probable means of attack – what’s called, awesomely enough, an **attack vector**. Threat models let you prepare against specific threats, rather than being overwhelmed by all the ways hackers could get to your systems and data. And there are many, many ways. Let’s say you want to “secure” physical access to your laptop. Your threat model is a nosy roommate. To preserve the secrecy, integrity and availability of your laptop, you could keep it hidden in your dirty laundry hamper. But, if your threat model is a mischievous younger sibling who knows your hiding spots, then you’ll need to do more: maybe lock it in a safe. In other words, how a system is secured depends heavily on who it’s being secured against. Of course, threat models are typically a bit more formally defined than just “nosy roommate”. Often you’ll see threat models specified in terms of technical capabilities. For example, “someone who has physical access to your laptop along with unlimited time”. With a given threat model, security architects need to come up with a solution that keeps a system secure – as long as certain assumptions are met, like no one reveals their password to the attacker. 

> 网络安全的范围与计算能力的发展一样快，但我们可以把它看作是一套保护计算机系统和数据免受威胁的**保密性**、**完整性**和**可用性**的技术。让我们来解读一下这三个目标。秘密，或保密性，意味着**只有被授权的人才能访问或阅读特定的计算机系统和数据**。数据泄露，即黑客泄露人们的信用卡信息，就是对保密性的攻击。完整性是指**只有被授权的人应该有能力使用或修改系统和数据**。黑客得知你的密码并伪装成你的样子发送电子邮件，就是对完整性的攻击。而可用性意味着**授权的人应该始终能够访问他们的系统和数据**。想一想[拒绝服务攻击](https://en.wikipedia.org/wiki/Denial-of-service_attack)（DoS），黑客用虚假的请求使一个网站超载，使它变得缓慢或对其他人来说无法访问。这就是攻击服务的可用性。为了实现这三个一般的目标，安全专家们从一个抽象的层面开始，说明你的 "敌人 "是谁，称为**威胁模型**。这个模型描述了攻击者：他们的能力、目标和可能的攻击手段--也就是所谓的**攻击矢量**，非常了不起。威胁模型让你准备好应对具体的威胁，而不是被黑客可能进入你的系统和数据的所有方式所淹没。而且有很多很多的方法。比方说，你想 "确保 "对你的笔记本电脑的物理访问。你的威胁模型是一个爱管闲事的室友。为了保护你的笔记本电脑的保密性、完整性和可用性，你可以把它藏在你的脏衣服篮子里。但是，如果你的威胁模型是一个调皮的弟弟妹妹，他知道你的藏身之处，那么你就需要做更多的事情：也许把它锁在一个保险箱里。换句话说，一个系统如何被保护，在很大程度上取决于它的保护对象是谁。当然，威胁模型的定义通常比 "爱管闲事的室友 "更正式一些。通常你会看到威胁模型在技术能力方面的规定。例如，"某人可以对你的笔记本电脑进行物理访问，并有无限的时间"。对于一个给定的威胁模型，安全架构师需要想出一个解决方案来保证系统的安全--只要满足某些假设，比如没有人向攻击者透露他们的密码。

There are many methods for protecting computer systems, networks and data. A lot of security boils down to two questions: **who are you**, and **what should you have access to**? Clearly, access should be given to the right people, but refused to the wrong people. Like, bank employees should be able to open ATMs to restock them, but not me… because I’d take it all... all of it! That ceramic cat collection doesn’t buy itself! So, to differentiate between right and wrong people, we use **authentication** - the process by which a computer understands who it’s interacting with. Generally, there are three types, each with their own pros and cons: **What you know**. **What you have**. And **what you are**. What you know authentication is based on knowledge of a secret that should be known only by the real user and the computer, for example, a username and password. This is the most widely used today because it’s the easiest to implement. But, it can be compromised if hackers guess or otherwise come to know your secret. Some passwords are easy for humans to figure out, like 12356 or q-w-e-r-t-y. But, there are also ones that are easy for computers. Consider the PIN(Personal identification number): 2580. This seems pretty difficult to guess – and it is – for a human. But there are only ten thousand possible combinations of 4-digit PINs. A computer can try entering 0000, then try 0001, and then 0002, all the way up to 9999... in a fraction of a second. This is called a **brute force attack**, because it just tries everything. There’s nothing clever to the algorithm. Some computer systems lock you out, or have you wait a little, after say three wrong attempts. That’s a common and reasonable strategy, and it does make it harder for less sophisticated attackers. But think about what happens if hackers have already taken over tens of thousands of computers, forming a **botnet**. Using all these computers, the same pin – 2580 – can be tried on many tens of thousands of bank accounts simultaneously. Even with just a single attempt per account, they’ll very likely get into one or more that just happen to use that PIN. In fact, we’ve probably guessed the pin of someone watching this video!

> 有许多保护计算机系统、网络和数据的方法。很多安全问题可以归结为两个问题：**你是谁**，以及**你应该访问什么**？显然，应该把访问权交给正确的人，但拒绝交给错误的人。就像，银行员工应该能够打开自动取款机来补货，但不是我......因为我会把它全部......全部拿走！"。那只陶瓷猫的收藏品可不会自己买! 因此，为了区分正确和错误的人，我们使用**认证**--计算机了解它与谁互动的过程。一般来说，**有三种认证类型**，每一种都有各自的优点和缺点。**你所知道的**。**你所拥有的**。和**你是什么人**。你所知道的认证是基于对只有真正的用户和计算机才知道的秘密的了解，例如，一个用户名和密码。这是目前使用最广泛的，因为它是最容易实现的。但是，如果黑客猜到或以其他方式知道了你的秘密，它就会受到影响。有些密码对人类来说很容易猜出来，如12356或q-w-e-r-t-y。但是，也有一些密码对计算机来说很容易。考虑一下密码：2580。对于人类来说，这似乎很难猜到--而且确实如此。但是4位数的PIN码只有一万种可能的组合。计算机可以尝试输入0000，然后尝试0001，然后0002，一直到9999......在几分之一秒内。这被称为**暴力攻击**，因为它只是尝试一切。这个算法并不高明。有些计算机系统会将你锁定，或者让你在三次错误的尝试后等待一段时间。这是一个常见和合理的策略，而且它确实使不太复杂的攻击者更难。但是请想一想，如果黑客已经占领了数以万计的电脑，形成了一个**机器人网络**，会发生什么？利用所有这些电脑，同一个密码--2580--可以同时在数万个银行账户上尝试。即使每个账户只尝试一次，他们也很可能进入一个或多个恰好使用该密码的账户。事实上，我们可能已经猜到了正在观看这段视频的人的密码。

Increasing the length of PINs and passwords can help, but even 8 digit PINs are pretty easily cracked. This is why so many websites now require you to use a mix of upper and lowercase letters, special symbols, and so on – it explodes the number of possible password combinations. An 8-digit numerical PIN only has a hundred million combinations – computers eat that for breakfast! But an 8-character password with all those funky things mixed in has more than 600 trillion combinations. Of course, these passwords are hard for us mere humans to remember, so a better approach is for websites to let us pick something more memorable, like three words joined together: “green brothers rock” or “pizza tasty yum”. English has around 100,000 words in use, so putting three together would give you roughly 1 quadrillion possible passwords. Good luck trying to guess that! I should also note here that using **non-dictionary words** is even better against more sophisticated kinds of attacks, but we don’t have time to get into that here. Computerphile has a great video on choosing a password - link in the dooblydoo. What you have authentication, on the other hand, is based on possession of a secret token that only the real user has. An example is a physical key and lock. You can only unlock the door if you have the key. This escapes this problem of being “guessable”. And they typically require physical presence, so it’s much harder for remote attackers to gain access. Someone in another country can’t gain access to your front door in Florida without getting to Florida first. But, what you have authentication can be compromised if an attacker is physically close. Keys can be copied, smartphones stolen, and locks picked. Finally, what you are authentication is based on... you! You authenticate by presenting yourself to the computer. **Biometric authenticators**, like **fingerprint readers** and **iris scanners** are classic examples. These can be very secure, but the best technologies are still quite expensive. Furthermore, data from sensors varies over time. What you know and what you have authentication have the nice property of being deterministic – either correct or incorrect. If you know the secret, or have the key, you’re granted access 100% of the time. If you don’t, you get access zero percent of the time. Biometric authentication, however, is probabilistic.There’s some chance the system won’t recognize you… maybe you’re wearing a hat or the lighting is bad. Worse, there’s some chance the system will recognize the wrong person as you – like your evil twin! 

> 增加PIN和密码的长度会有帮助，但即使是8位数的PIN也很容易被破解。这就是为什么现在许多网站要求你混合使用大写字母和小写字母、特殊符号等--它使可能的密码组合数量激增。一个8位数的数字密码只有一亿种组合--计算机把它当早餐吃了！但一个8位数的密码加上一个特殊符号，就有可能被破解。但一个8位数的密码加上所有这些古怪的东西，就有超过600万亿的组合。当然，这些密码对我们普通人来说很难记住，所以更好的方法是网站让我们选择一些更容易记住的东西，比如三个词连在一起。"绿色兄弟岩 "或 "比萨饼好吃"。英语中大约有10万个词在使用，所以把三个词放在一起就可以得到大约1万亿个可能的密码。要想猜出这一点，祝你好运! 我还应该注意到，使用**非字典中的单词**能更好地对付更复杂的攻击，但我们没有时间在这里讨论这个问题。Computerphile有一个关于选择密码的伟大视频--链接在doblydoo。另一方面，你所拥有的认证是基于拥有一个只有真正的用户拥有的秘密信物。一个例子是物理钥匙和锁。只有当你拥有钥匙时，你才能开锁。这就摆脱了 "可猜测 "的问题。而且它们通常需要物理存在，所以远程攻击者更难获得访问权。如果不先到佛罗里达州，另一个国家的人就无法进入你在佛罗里达的前门。但是，如果攻击者离你很近，你所拥有的认证就会受到影响。钥匙可以被复制，智能手机可以被偷，锁也可以被撬。最后，你的认证是基于......你！你是什么？你通过向计算机展示自己来进行认证。**生物识别认证器**，如**指纹阅读器**和**虹膜扫描仪**是典型的例子。这些可以是非常安全的，但最好的技术仍然相当昂贵。此外，来自传感器的数据会随时间变化。你所知道的和你所认证的都有一个很好的特性，那就是确定性--要么正确，要么不正确。如果你知道秘密，或者有钥匙，你就能100%获得访问权。如果你不知道，你就会在百分之零的时间内获得访问权。然而，生物识别认证是概率性的。系统有一些机会无法识别你......也许你戴着帽子或光线不好。更糟的是，系统有可能会识别出错误的人--比如你的邪恶双胞胎！"。

Of course, in production systems, these chances are low, but not zero. Another issue with biometric authentication is it can’t be reset. You only have so many fingers, so what happens if an attacker compromises your fingerprint data? This could be a big problem for life. And, recently, researchers showed it’s possible to forge your iris just by capturing a photo of you, so that’s not promising either. Basically, all forms of authentication have strengths and weaknesses, and all can be compromised in one way or another. So, **security experts suggest using two or more forms of authentication for important accounts**. This is known as **two-factor** or **multi-factor authentication**. An attacker may be able to guess your password or steal your phone: but it’s much harder to do both. After authentication comes **Access Control**. Once a system knows who you are, it needs to know what you should be able to access, and for that there’s a specification of who should be able to see, modify and use what. This is done through Permissions or **Access Control Lists** (ACL), which describe what access each user has for every file, folder and program on a computer. “Read” permission allows a user to see the contents of a file, “write” permission allows a user to modify the contents, and “execute” permission allows a user to run a file, like a program. For organizations with users at different levels of access privilege – like a spy agency – it’s especially important for Access Control Lists to be configured correctly to ensure secrecy, integrity and availability. Let’s say we have three levels of access: public, secret and top secret. The first general rule of thumb is that people shouldn’t be able to “read up”. If a user is only cleared to read secret files, they shouldn’t be able to read top secret files, but should be able to access secret and public ones. The second general rule of thumb is that people shouldn’t be able to “write down”. If a member has top secret clearance, then they should be able to write or modify top secret files, but not secret or public files. It may seem weird that even with the highest clearance, you can’t modify less secret files. But, it guarantees that there’s no accidental leakage of top secret information into secret or public files. This “no read up, no write down” approach is called the **Bell-LaPadula model**. It was formulated for the U.S. Department of Defense’s Multi-Level Security policy. 

> 当然，在生产系统中，这些几率很低，但不是零。生物识别认证的另一个问题是它不能被重置。你只有那么多手指，如果攻击者泄露了你的指纹数据会怎样？这可能是生活中的一个大问题。而且，最近，研究人员表明，只要拍下你的照片，就可以伪造你的虹膜，所以这也是不乐观的。基本上，所有的认证形式都有优势和劣势，而且都可以通过这种或那种方式被破坏。因此，**安全专家建议对重要账户使用两种或更多形式的认证**。这被称为**双因素**或**多因素认证**。攻击者可能会猜到你的密码或偷走你的手机：但要同时做到这两点就难多了。认证之后是**访问控制**。一旦系统知道你是谁，它就需要知道你应该能够访问什么，为此，需要对谁应该能够看到、修改和使用什么做出规定。这是通过权限或**访问控制列表**（ACL）完成的，它描述了每个用户对计算机上的每个文件、文件夹和程序的访问权限。"读 "的权限允许用户查看文件的内容，"写 "的权限允许用户修改内容，"执行 "的权限允许用户运行一个文件，比如一个程序。对于有不同级别访问权限的用户的组织--如间谍机构--正确配置访问控制列表以确保保密性、完整性和可用性尤为重要。假设我们有三个级别的访问：公开、秘密和最高机密。第一条一般经验法则是，人们不应该能够 "向上阅读"。如果一个用户只被允许阅读秘密文件，他们就不应该能够阅读绝密文件，但应该能够访问秘密和公开文件。第二条一般经验法则是，人们不应该能够 "写下去"。如果一个成员拥有绝密权限，那么他们应该能够写入或修改绝密文件，但不能写入秘密或公共文件。即使拥有最高权限，也不能修改不那么机密的文件，这可能看起来很奇怪。但是，这保证了绝密信息不会意外地泄漏到秘密或公共文件中。这种 "不读就不写 "的方法被称为**贝尔-拉帕杜拉模式**。它是为美国国防部的多层次安全政策制定的。

There are many other models for access control – like the **Chinese Wall model** and **Biba model**. Which model is best depends on your use-case. Authentication and access control help a computer determine who you are and what you should access, but depend on being able to trust the hardware and software that run the authentication and access control programs. That’s a big dependence. If an attacker installs malicious software – called **malware** – compromising the host computer’s operating system, how can we be sure security programs don’t have a backdoor that let attackers in? The short answer is… we can’t. We still have no way to guarantee the security of a program or computing system. That’s because even while security software might be “secure” in theory, implementation bugs can still result in vulnerabilities. But, we do have techniques to reduce the likelihood of bugs, quickly find and patch bugs when they do occur, and mitigate damage when a program is compromised. Most security errors come from implementation error. To reduce implementation error, reduce implementation. One of the holy grails of system level security is a “**security kernel**” or a “**trusted computing base**”: a minimal set of operating system software that’s close to provably secure. A challenge in constructing these security kernels is deciding what should go into it. **Remember, the less code, the better**! Even after minimizing code bloat, it would be great to “guarantee” that code as written is secure. Formally verifying the security of code is an active area of research. The best we have right now is a process called **Independent Verification and Validation**. This works by having code audited by a crowd of security-minded developers. This is why security code is almost always open-sourced. It’s often difficult for people who wrote the original code to find bugs, but external developers, with fresh eyes and different expertise, can spot problems. There are also conferences where like-minded hackers and security experts can mingle and share ideas, the biggest of which is DEF CON, held annually in Las Vegas. 

> 有许多其他的门禁模型--如**中国墙模型**和**Biba模型**。哪种模式最好，取决于你的使用情况。认证和访问控制帮助计算机确定你是谁以及你应该访问什么，但取决于能否信任运行认证和访问控制程序的硬件和软件。这是一个很大的依赖性。如果攻击者安装了恶意软件--称为**恶意软件**--破坏了主机的操作系统，我们怎么能确保安全程序没有后门，让攻击者进入呢？简短的答案是...我们不能。我们仍然没有办法保证一个程序或计算系统的安全。这是因为，即使安全软件在理论上可能是 "安全的"，但执行错误仍然可能导致漏洞。但是，我们确实有一些技术来减少错误的可能性，在错误发生时快速发现并修补错误，并在程序被破坏时减轻损失。大多数安全错误来自于执行错误。要减少执行错误，就要减少执行。系统级安全的圣杯之一是 "**安全内核**"或 "**信任的计算基础**"：一个最小的操作系统软件集，接近于可证明的安全。构建这些安全内核的一个挑战是决定什么应该进入它。**记住，代码越少越好**! 即使在最大限度地减少代码膨胀之后，能 "保证 "所写的代码是安全的，那就更好了。正式验证代码的安全性是一个活跃的研究领域。我们现在拥有的最好的方法是一个叫做**独立安全检查和质量验证**的过程。它的工作原理是由一群有安全意识的开发者对代码进行审核。这就是为什么安全代码几乎都是开源的。编写原始代码的人通常很难发现错误，但外部开发者，以新的眼光和不同的专业知识，可以发现问题。还有一些会议，志同道合的黑客和安全专家可以混在一起并分享想法，其中最大的是每年在拉斯维加斯举行的DEF CON。

Finally, even after reducing code and auditing it, clever attackers are bound to find tricks that let them in. With this in mind, good developers should take the approach that, not if, but when their programs are compromised, the damage should be limited and contained, and not let it compromise other things running on the computer. This principle is called **isolation**. To achieve isolation, we can “**sandbox**” applications. This is like placing an angry kid in a sandbox; when the kid goes ballistic, they only destroy the sandcastle in their own box, but other kids in the playground continue having fun. Operating Systems attempt to sandbox applications by giving each their own block of memory that others programs can’t touch. It’s also possible for a single computer to run **multiple Virtual Machines**, essentially simulated computers, that each live in their own sandbox. If a program goes awry, worst case is that it crashes or compromises only the virtual machine on which it’s running. All other Virtual Machines running on the computer are isolated and unaffected. Ok, that’s a broad overview of some key computer security topics. And I didn’t even get to n**etwork security**, like **firewalls**. Next episode, we’ll discuss some specific example methods hackers use to get into computer systems. After that, we’ll touch on **encryption**. Until then, make your passwords stronger, turn on 2-factor authentication, and NEVER click links in unsolicited emails! I’ll see you next week. 

> 最后，即使在减少代码并对其进行审计之后，聪明的攻击者也一定会找到让他们进入的窍门。考虑到这一点，好的开发者应该采取这样的方法：不是如果，而是当他们的程序被破坏时，损害应该被限制和控制，而不是让它影响到计算机上运行的其他东西。这一原则被称为**隔离**。为了实现隔离，我们可以 "**沙盒**"应用程序。这就像把一个愤怒的孩子放在一个沙箱里；当这个孩子发飙时，他们只破坏了自己箱子里的沙堡，但操场上的其他孩子却在继续玩耍。操作系统试图通过给每个应用程序提供他们自己的内存块，使其他程序无法触及，来实现沙盒化。一台计算机也可以运行**多个虚拟机**，本质上是模拟的计算机，它们各自生活在自己的沙盒中。如果一个程序出了问题，最坏的情况是，它只崩溃或损害了它所运行的虚拟机。计算机上运行的所有其他虚拟机都被隔离，不受影响。好了，这就是对一些关键计算机安全主题的广泛概述。而我甚至还没有涉及到**网络安全**，比如**防火墙**。下一集，我们将讨论一些黑客用来进入计算机系统的具体方法。之后，我们将讨论**加密**。在那之前，让你的密码更强大，开启双因素认证，并且永远不要点击未经请求的电子邮件中的链接! 下周见。



## #32 Hcakers & Cyber Attacks

<iframe width="560" height="315" src="https://www.youtube.com/embed/_GzE99AmAQU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science! Last episode, we talked about the basics of computer security, principles and techniques used to keep computer systems safe and sound. But, despite our best efforts, the news is full of stories of individuals, companies and governments getting cyberattacked by hackers, people who, with their technical knowledge, break into computer systems. (The term "hacking" is also commonly used in more general sense to mean developing creative solutions to problems)Not all hackers are bad though. There are hackers who hunt for bugs and try to close security holes in software to make systems safer and more resilient. They’re often hired by companies and governments to perform security evaluations.(full disclosure: [Marcus Hutchins](https://en.wikipedia.org/wiki/Marcus_Hutchins) - the "white hat" that helped stop the Wannacry ransomware attack. He's currently being prosecuted in the U.S. for some very "black hat" activities rendering these distinction a little erm...gray) These hackers are called **White Hats**, they’re the good guys. On the flip side, there are **Black Hats**, malicious hackers with intentions to steal, exploit and sell computer vulnerabilities and data. Hackers’ motivations also differ wildly. Some hack for amusement and curiosity, while cybercriminals hack most often for monetary gain. And then there are hacktivists, who use their skills to promote a social or political goal. And that’s just the tip of the iceberg. Basically, the stereotypical view of a hacker as some unpopular kid sitting in a dark room full of discarded pizza boxes probably better describes John Green in college than it does hackers. Today, we’re not going to teach you how to be a hacker. Instead, we’ll discuss some classic examples of how hackers break into computer systems to give you an idea of how it’s done. 

> 上一集，我们谈到了计算机安全的基本知识，用于保持计算机系统安全和健全的原则和技术。但是，尽管我们尽了最大努力，新闻中充满了个人、公司和政府被黑客攻击的故事，这些人凭借其技术知识，闯入计算机系统。 ("黑客 "一词通常也被用于更广泛的意义上，指的是为问题制定创造性的解决方案)不过，并非所有的黑客都是坏人。有些黑客会寻找漏洞，并试图弥补软件中的安全漏洞，使系统更安全、更有弹性。他们经常被公司和政府雇用来进行安全评估。(全面披露：[马库斯-哈钦斯](https://en.wikipedia.org/wiki/Marcus_Hutchins)--帮助阻止Wannacry勒索软件攻击的 "白帽子"。他目前因一些非常 "黑帽 "的活动在美国被起诉，使得这些区别有点......灰色)这些黑客被称为 "**白帽子**"，他们是好人。另一方面，也有**黑帽**，即恶意的黑客，他们的目的是窃取、利用和出售计算机漏洞和数据。黑客的动机也大不相同。有些黑客是为了娱乐和好奇心，而网络犯罪分子的黑客行为通常是为了金钱利益。还有一些黑客活动家，他们利用自己的技能来促进社会或政治目标。而这只是冰山一角。基本上，人们对黑客的刻板印象是一些不受欢迎的孩子，坐在充满废弃比萨饼盒的黑暗房间里，这可能比描述大学时的约翰-格林更好。今天，我们不打算教你如何成为一名黑客。相反，我们将讨论一些黑客如何闯入计算机系统的经典案例，让你了解它是如何做到的。

INTRO The most common way hackers get into computer systems isn’t by hacking at all; it’s by tricking users into letting them in. This is called **social engineering**, where a person is manipulated into divulging confidential information, or configuring a computer system so that it permits entry by attackers. The most common type of attack is **phishing**, which you most often encounter as an email asking you to login to an account on a website, say your bank. You’ll be asked to click a link in the email, which takes you to a site that looks legit to the casual observer, but is really an evil clone. When you enter your username and password, that information goes straight to the hackers, who then can login to the real website as you. Bad news! Even with a 1/10th of one percent success rate, a million phishing emails might yield a thousand compromised accounts. Another social engineering attack is **pretexting**, where attackers call up, let's say a company, and then confidently pretend to be from their IT department. Often attackers will call a first number, and then ask to be transferred to a second, so that the phone number appears to be internal to the company. Then, the attacker can instruct an unwitting user to configure their computer in a compromising way, or get them to reveal confidential details, like passwords or network configurations. Sorry, one sec… Oh. Hey, it's Susan from It. We’re having some network issues down here, can you go ahead and check a setting for me?” ... and it begins. Attackers can be very convincing, especially with a little bit of research beforehand to find things like key employees’ names. It might take ten phone calls to find an victim, but you only need one to get in. Emails are also a common delivery mechanism for **trojan horses**, programs that masquerade as harmless attachments, like a photo or invoice, but actually contain malicious software, called **malware**. Malware can take many forms. Some might steal your data, like your banking credentials. Others might encrypt your files and demand a ransom, what's known as **ransomware**. If they can’t run malware or get a user to let them in, attackers have to force their way in through other means. One method, which we briefly discussed last episode, is to **brute force** a password, try every combination of password until you gain entry. Most modern systems defend against this type of attack by having you wait incrementally longer periods of time following each failed attempt, or even lock you out entirely after a certain number of tries. 

> 黑客进入计算机系统最常见的方式根本不是通过黑客攻击，而是通过欺骗用户让他们进入。这被称为**社会工程**，即操纵一个人泄露机密信息，或配置一个计算机系统，使其允许攻击者进入。最常见的攻击类型是**钓鱼**，你最常遇到的是一封电子邮件，要求你登录一个网站的账户，例如你的银行。你会被要求点击电子邮件中的一个链接，该链接将你带到一个在旁观者看来合法的网站，但实际上是一个邪恶的克隆。当你输入你的用户名和密码时，这些信息会直接传给黑客，然后他们就能以你的身份登录到真正的网站。坏消息! 即使有百分之一的成功率，一百万封钓鱼邮件也可能产生一千个受损的账户。另一种社会工程攻击是**假托**，即攻击者打电话给，比方说一家公司，然后自信地假装是来自他们的IT部门。攻击者通常会拨打第一个号码，然后要求转到第二个号码，这样一来，电话号码看起来就是公司的内部号码。然后，攻击者可以指示一个不知情的用户以妥协的方式配置他们的计算机，或让他们透露机密细节，如密码或网络配置。对不起，稍等......哦。嘿，这是苏珊从它。我们这里有一些网络问题，你能为我检查一个设置吗？"......然后就开始了。攻击者可以非常有说服力，特别是事先做了一点研究，找到关键员工的名字等东西。可能需要打十个电话才能找到一个受害者，但你只需要一个就可以进去了。电子邮件也是**木马**的常见传递机制，这些程序伪装成无害的附件，如照片或发票，但实际上包含恶意软件，称为**恶意软件**。恶意软件可以有多种形式。有些可能窃取你的数据，如你的银行凭证。其他人可能会加密你的文件并要求赎金，即所谓的**勒索软件**。如果他们不能运行恶意软件或让用户让他们进入，攻击者必须通过其他手段强行进入。有一种方法，我们在上一集简单讨论过，就是**强行**输入密码（暴力破解），尝试各种密码组合，直到你获得进入。大多数现代系统通过让你在每次尝试失败后等待更长的时间来防御这种类型的攻击，甚至在尝试了一定数量后将你完全锁定。

One recent hack to get around this is called **NAND Mirroring**, where if you have physical access to the computer, you can attach wires to the device's memory chip and make a perfect copy of its contents. With this setup, you can try a series of passwords, until the device starts making you wait. When this happens, you just reflash the memory with the original copy you made, essentially resetting it, allowing you to try more passwords immediately, with no waiting. This technique was shown to be successful on an iPhone 5C, but many newer devices include mechanisms to thwart this type of attack. If you don’t have physical access to a device, you have to find a way to hack it remotely, like over the internet. In general, this requires an attacker to find and take advantage of a bug in a system, and successfully utilizing a bug to gain capabilities or access is called an **exploit**. One common type of exploit is a **buffer overflow**. **Buffers are a general term for a block of memory reserved for storing data**. We talked about video buffers for storing pixel data in Episode 23. As a simple example, we can imagine an operating system’s login prompt, which has fields for a username and password. Behind the scenes, this operating system uses buffers for storing the text values that are entered. For illustration, let's say these buffers were specified to be of size ten. In memory, the two text buffers would look something like this: Of course, the operating system is keeping track of a lot more than just a username and password, so there’s going to be data stored both before and after in memory. When a user enters a username and password, the values are copied into the buffers, where they can be verified. A **buffer overflow attack** does exactly what the name suggests: **overflows the buffer**. In this case, any password longer than ten characters will overwrite adjacent data in memory. Sometimes this will just cause a program or operating system to crash, because important values are overwritten with gobbledygook. Crashing a system is bad, and maybe that’s all that a mischievous hacker wants to do, be a nuisance. But attackers can also exploit this bug more cleverly by injecting purposeful new values into a program’s memory, for example, setting an “is admin” variable to true. With the ability to arbitrarily manipulate a program’s memory, hackers can bypass things like login prompts, and sometimes even use that program to hijack the whole system. There are many methods to combat buffer overflow attacks. The easiest is to always test the length of input before copying it into a buffer, called **bounds checking**. Many modern programming languages implement bounds checking automatically. Programs can also randomize the memory location of variables, like our hypothetical “is admin” flag, so that hackers don’t know what memory location to overwrite, and are more likely to crash the program than gain access. Programs can also leave unused space after buffers, and keep an eye on those values to see if they change; if they do, they know an attacker is monkeying around with memory. These regions are called **canaries**, named after the small birds miners used to take underground to warn them of dangerous conditions. 

> 最近一个绕过这个问题的黑客被称为**NAND镜像**，如果你有对电脑的物理访问权，你可以将电线连接到设备的存储芯片上，并对其内容进行完美复制。通过这种设置，你可以尝试一系列的密码，直到设备开始让你等待。当这种情况发生时，你只需用你制作的原始拷贝重新刷新内存，基本上可以重置它，让你立即尝试更多密码，无需等待。这种技术在iPhone 5C上显示是成功的，但许多较新的设备包括挫败这种类型攻击的机制。如果你没有对设备进行物理访问，你必须找到一种方法来远程入侵它，比如通过互联网。一般来说，这需要攻击者找到并利用系统中的一个错误，成功利用一个错误来获得能力或访问权被称为**漏洞利用**。一种常见的利用方式是**缓冲区溢出**。**缓冲区是为存储数据而保留的内存块的一般术语**。我们在第23集谈到了用于存储像素数据的视频缓冲区。作为一个简单的例子，我们可以想象一个操作系统的登录提示，它有用户名和密码的字段。在幕后，这个操作系统使用缓冲区来存储输入的文本值。为了说明问题，我们假设这些缓冲区被指定为大小为10。在内存中，这两个文本缓冲区看起来会是这样的。当然，操作系统要记录的不仅仅是用户名和密码，所以在内存中，前后都会有数据存储。当用户输入用户名和密码时，这些值会被复制到缓冲区，在那里它们可以被验证。一个**缓冲区溢出攻击**正是如其名。**溢出缓冲区**。在这种情况下，任何超过十个字符的密码都会覆盖内存中的相邻数据。有时，这将导致程序或操作系统崩溃，因为重要的数值被胡乱覆盖了。崩溃系统是不好的，也许这就是一个调皮的黑客想要做的，成为一个讨厌的人。但是，攻击者也可以更巧妙地利用这个错误，在程序的内存中注入有目的的新值，例如，将 "is admin "变量设置为真。有了任意操纵程序内存的能力，黑客就可以绕过登录提示等事项，有时甚至利用该程序劫持整个系统。有许多方法可以对抗缓冲区溢出攻击。最简单的是在把输入的东西复制到缓冲区之前，总是测试输入的长度，称为**边界检查**。许多现代编程语言自动实现了边界检查。程序也可以随机化变量的内存位置，比如我们假设的 "is admin "标志，这样黑客就不知道要覆盖什么内存位置，更有可能使程序崩溃而不是获得访问权。程序也可以在缓冲区后留下未使用的空间，并留意这些值是否有变化；如果有变化，他们就知道攻击者在对内存进行捣乱。这些区域被称为**金丝雀**，以矿工在地下用来警告他们危险情况的小鸟命名。

Another classic hack is **code injection**. It’s most commonly used to attack websites that use databases, which pretty much all big websites do. We won’t be covering databases in this series, so here’s a simple example to illustrate this type of attack. We’ll use **Structured Query Language**, S-Q-L, also called sequel, a popular database API. Let’s imagine our login prompt is now running on a webpage. When a user clicks “login”, the text values are sent to a server, which executes code that checks if that username exists, and if it does, verifies the password matches. To do this, the server will execute code, known as a SQL query, that looks something like this. First, it needs to specify what data we’re retrieving from the database. In this case, we want to fetch the password. The server also needs to specify from what place in the database to retrieve the value from. In this case, let’s imagine all the users’ data is stored in a data structure called a table labeled “users”. Finally, the server doesn’t want to get back a giant list of passwords for every user in the database, so it specifies that it only wants data for the account whose username equals a certain value. That value is copied into the SQL query by the server, based on what the user typed in, so the actual command that’s sent to the SQL database would look something like this, where username equals philbin. Note also that SQL commands end with a semicolon. So how does someone hack this? By sending in a malicious username, with embedded SQL commands! Like, we could send the server this funky username: When the server copies this text into the SQL Query, it ends up looking like this: As I mentioned before, semicolons are used to separate commands, so the first command that gets executed is this: If there is a user named ‘whatever’, the database will return the password. Of course, we have no idea what ‘whatever’s’ password is, so we’ll get it wrong and the server will reject us. If there’s no user named ‘whatever’, the database will return no password or provide an error, and the server will again reject us. Either way, we don’t care, because it’s the next SQL command we’re interested in: “drop table users” – a command that we injected by manipulating the username field. This command instructs the SQL database to delete the table containing all user data. Wiped clean! Which would cause a lot of headaches at a place like a bank... or really anywhere. 

> 另一个经典的黑客是**代码注入**。它最常被用来攻击使用数据库的网站，几乎所有的大网站都是如此。在这个系列中，我们不会涉及数据库，所以这里有一个简单的例子来说明这种类型的攻击。我们将使用**结构化查询语言**，S-Q-L，也叫sequel，是一种流行的数据库API。让我们想象一下，我们的登录提示现在是在一个网页上运行。当用户点击 "登录 "时，文本值被发送到服务器，服务器执行代码，检查该用户名是否存在，如果存在，则验证密码是否匹配。要做到这一点，服务器将执行代码，即所谓的SQL查询，它看起来像这样。首先，它需要指定我们要从数据库中检索什么数据。在这种情况下，我们要取回密码。服务器还需要指定从数据库中的什么地方获取该值。在这种情况下，让我们想象一下，所有用户的数据都存储在一个叫做 "用户 "表的数据结构中。最后，服务器不想为数据库中的每个用户找回一个巨大的密码列表，所以它指定它只想要用户名等于某个值的账户的数据。这个值由服务器根据用户输入的内容复制到SQL查询中，所以发送到SQL数据库的实际命令看起来像这样，其中用户名等于philbin。还请注意，SQL命令以分号结束。那么，有人是如何入侵的呢？通过发送一个恶意的用户名，并嵌入SQL命令 比如，我们可以向服务器发送这个古怪的用户名：当服务器将这个文本复制到SQL查询中时，它最终看起来像这样。正如我之前提到的，分号是用来分隔命令的，所以第一个被执行的命令是这样的。如果有一个名为 "什么 "的用户，数据库将返回密码。当然，我们不知道'什么'的密码是什么，所以我们会弄错，服务器会拒绝我们。如果没有名为 "什么 "的用户，数据库将不返回密码或提供一个错误，服务器将再次拒绝我们。无论怎样，我们都不在乎，因为我们感兴趣的是下一个SQL命令。"drop table users"--这是我们通过操纵用户名字段注入的命令。这条命令指示SQL数据库删除包含所有用户数据的表。擦拭干净! 这将在像银行这样的地方造成很多麻烦......或者真的是任何地方。

**SQL query** example:

<img src="../reference pics/sql.jpg">

And notice that we didn’t even break into the system – it’s not like we correctly guessed a username and password. Even with no formal access, we were able to create mayhem by exploiting a bug. This is a very simple example of code injection, which almost all servers today have defenses against. With more sophisticated attacks, it’s possible to add records to the database – like a new administrator account – or even get the database to reveal data, allowing hackers to steal things like credit card numbers, social security numbers and all sorts of nefarious goodies. But we’re not going to teach you how to do that. As with buffer overflows, programmers should always assume input coming from the outside to be potentially dangerous, and examine it carefully. Most username and password forms on the web don’t let you include special symbols like semicolons or quotes as a first level of defense. Good servers also sanitize input by removing or modifying special characters before running database queries. Working exploits are often sold or shared online. The more prevalent the bug, or the more damaging the exploit, the higher the price or prestige it commands. Even governments sometimes buy exploits, which allow them to compromise computers for purposes like spying. When a new exploitable bug is discovered that the software creators weren’t aware of, it’s called a **zero day vulnerability**. Black Hat Hackers rush to use the exploit for maximum benefit before white hat programmers release a patch for the bug. This is why it’s so important to keep your computer’s software up to date; a lot of those downloads are security patches. If bugs are left open on enough systems, it allows hackers to write a program that jump from computer to computer automatically which are called **worms**. If a hacker can take over a large number of computers, they can be used together, to form what’s called a **botnet**. This can have many purposes, like sending huge volumes of spam, mining bitcoins using other people's computing power and electricity, and launching **Distributed Denial of Service** or **DDoS** attacks against servers. 

> 请注意，我们甚至没有闯入系统--这并不是说我们正确地猜到了一个用户名和密码。即使没有正式的访问权限，我们也能通过利用一个错误来制造混乱。这是一个非常简单的代码注入的例子，今天几乎所有的服务器都有防御措施。通过更复杂的攻击，有可能向数据库添加记录--比如一个新的管理员账户--甚至让数据库泄露数据，让黑客窃取诸如信用卡号码、社会安全号码和各种邪恶的好东西。但我们不打算教你如何做到这一点。与缓冲区溢出一样，程序员应该始终认为来自外部的输入有潜在的危险，并仔细检查。作为第一道防线，网络上的大多数用户名和密码表格都不允许你加入特殊符号，如分号或引号。好的服务器在运行数据库查询之前也会通过删除或修改特殊字符来净化输入。工作中的漏洞经常在网上出售或分享。漏洞越普遍，或者漏洞的破坏性越大，它的价格或声望就越高。甚至政府有时也会购买漏洞，这使他们能够为间谍活动等目的破坏计算机。当一个新的可利用的漏洞被发现，而软件创造者没有意识到，它被称为**零日漏洞**。黑帽黑客急于在白帽程序员发布漏洞补丁之前利用该漏洞获取最大利益。这就是为什么保持你的计算机软件的更新是如此重要；这些下载的软件有很多是安全补丁。如果在足够多的系统上留下漏洞，就会让黑客编写一个程序，自动从一台电脑跳到另一台电脑，这些程序被称为**蠕虫**。如果黑客能够接管大量的计算机，他们可以一起使用，形成所谓的**机器人网络**。这可以有很多目的，比如发送大量的垃圾邮件，利用其他人的计算能力和电力开采比特币，以及对服务器发起**分布式拒绝服务**或**DDoS攻击**。

DDoS is where all the computers in the botnet send a flood of dummy messages. This can knock services offline, either to force owners to pay a ransom or just to be evil. Despite all of the hard working white hats, exploits documented online, and software engineering best practices, cyberattacks happen on a daily basis. They cost the global economy roughly half a trillion dollars annually, and that figure will only increase as we become more reliant on computing systems. This is especially worrying to governments, as infrastructure is increasingly computer-driven, like powerplants, the electrical grid, traffic lights, water treatment plants, oil refineries, air traffic control, and lots of other key systems. Many experts predict that the next major war will be fought in **cyberspace**, where nations are brought to their knees not by physical attack, but rather crippled economically and infrastructurally through **cyberwarfare**. There may not be any bullets fired, but the potential for lives lost is still very high... maybe even higher than conventional warfare. So, **we should all adopt good cybersecurity practices**. And, as a community interconnected over the internet, we should ensure our computers are secured against those who wish to use their great potential for harm. So maybe stop ignoring that update notification? I’ll see you next week. 

> DDoS是指僵尸网络中的所有计算机发送大量的虚假信息。这可以使服务脱机，要么迫使所有者支付赎金，要么只是为了作恶。尽管有所有努力工作的白帽子，网上记录的漏洞，以及软件工程的最佳实践，网络攻击每天都在发生。它们每年给全球经济带来大约5万亿美元的损失，而且这个数字只会随着我们对计算系统的依赖而增加。这对政府来说尤其令人担忧，因为基础设施越来越由计算机驱动，如发电厂、电网、交通灯、水处理厂、炼油厂、空中交通管制和许多其他关键系统。许多专家预测，下一场重大战争将在**网络空间**进行，国家不是通过物理攻击，而是通过**网络战**在经济和基础设施上瘫痪，使其屈服。也许没有任何子弹发射，但生命损失的可能性仍然很高......甚至可能比常规战争更高。因此，**我们都应该采取良好的网络安全做法**。而且，作为一个通过互联网相互连接的社区，我们应该确保我们的计算机是安全的，以防止那些希望利用其巨大潜力进行伤害的人。因此，也许不要再忽视那个更新通知了？下周见。



## #33 Crytography

<iframe width="560" height="315" src="https://www.youtube.com/embed/jhXCTbFnK8o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science! Over the past two episodes, we’ve talked a lot about computer security. But the fact is, there’s no such thing as a perfectly, 100% secure, computer system. There will always be bugs and security experts know that. So system architects employ a strategy called **defence in depth**, which uses many layers of varying security mechanisms to frustrate attackers. It’s a bit like how castles are designed – first you’ve got to dodge the archers, then cross the moat, scale the walls, avoid the hot oil, get over the ramparts, and defeat the guards before you get to the throne room, but in this case we’re talking about one of the most common forms of computer security - **[Cryptography](https://en.wikipedia.org/wiki/Cryptography)**.

> 在过去的两期节目中，我们已经谈了很多关于计算机安全的问题。但事实是，没有完美的、100%安全的计算机系统这回事。总会有一些漏洞，安全专家也知道这一点。因此，系统架构师采用了一种叫做 "**深度（多层）防御** "的策略，即使用多层不同的安全机制来挫败攻击者。这有点像城堡的设计--首先你得躲避弓箭手，然后穿过护城河，爬上城墙，避开热油，越过城墙，在你到达王室之前击败守卫，但在这种情况下，我们谈论的是计算机安全的最常见形式之一--**[密码学](https://en.wikipedia.org/wiki/Cryptography)**。

INTRO The word cryptography comes from the roots ‘crypto’ and ‘graphy’, roughly translating to “**secret writing**”. In order to make information secret, you use a **cipher** – **an algorithm that converts plain text into ciphertext**, which is gibberish unless you have a key that lets you undo the cipher. The process of making text secret is called **encryption**, and the reverse process is called **decryption**. Ciphers have been used long before computers showed up. Julius Caesar used what’s now called a **Caesar cipher**, to encrypt private correspondence. He would shift the letters in a message forward by three places. So, A became D, and the word "brutus" became this: "euxwxv". To decipher the message, recipients had to know both the algorithm and the number to shift by, which acted as the **key**(the decryption algorithm). The Caesar cipher is one example of a larger class of techniques called **substitution ciphers**. These replace every letter in a message with something else according to a translation. A big drawback of basic substitution ciphers is that letter frequencies are preserved. For example, E is the most common letter in English, so if your cipher translates E to an X, then X will show up the most frequently in the ciphertext. A skilled **cryptanalyst** can work backwards from these kinds of statistics to figure out the message. Indeed, it was the breaking of a substitution cipher that led to the execution of Mary, Queen of Scots, in 1587 for plotting to kill Queen Elizabeth. Another fundamental class of techniques are **permutation ciphers**. Let’s look at a simple example, called a **columnar transposition cipher**. Here, we take a message, and fill the letters into a grid. In this case, we’ve chosen 5 by 5. To encrypt our message, we read out the characters in a different order, let’s say from the bottom left, working upwards, one column at a time. The new letter ordering, what’s called a **permutation**, is the encrypted message. The ordering direction, as well as the 5 by 5 grid size, serves as the key. Like before, if the cipher and key are known, a recipient can reverse the process to reveal the original message. 

> 密码学一词源于 "crypto "和 "graphy"，大致翻译为 "**秘密写作**"。为了使信息保密，你使用**加密算法**--**一种将明文转换为密文的算法**，除非你有一个能让你撤销密码的钥匙，否则密文就是胡言乱语。使文本成为秘密的过程称为**加密**，相反的过程称为**解密**。早在计算机出现之前，密码就已经被使用了。凯撒大帝使用现在被称为**凯撒密码**的东西来加密私人信件。他将信息中的字母向前移动三个位置。因此，A变成了D，而 "brutus "这个词变成了这样。"euxwxv"。为了破译信息，收信人必须知道算法和要移动的数字，这就是**密钥**（解密算法）。凯撒密码是被称为**替换加密**的较大一类技术中的一个例子。这些技术根据翻译将信息中的每一个字母替换成其他的东西。基本替代密码的一个很大的缺点是，字母的频率被保留下来。例如，E是英语中最常见的字母，所以如果你的密码将E翻译成X，那么X将在密码文本中出现的频率最高。一个熟练的**密码学家**可以从这些类型的统计数字中逆向思维，找出信息。事实上，正是因为破解了一个替换密码，才导致了苏格兰女王玛丽在1587年因密谋杀害伊丽莎白女王而被处决。另一类基本技术是**换位密码**。让我们看一个简单的例子，叫做**列式换位密码**。在这里，我们采取一个信息，并将字母填入一个网格。在这个例子中，我们选择了5乘5。为了加密我们的信息，我们以不同的顺序读出这些字符，比如从左下角开始，向上读，一次读一列。新的字母顺序，也就是所谓的**变位**，就是加密的信息。排序方向，以及5乘5的网格大小，作为密钥。像以前一样，如果知道密码和钥匙，收件人可以逆转这个过程以显示原始信息。

By the 1900s, cryptography was mechanized in the form of encryption machines. The most famous was the German **Enigma**, used by the Nazis to encrypt their wartime communications. As we discussed back in Episode 15, the Enigma was a typewriter-like machine, with a keyboard and lampboard, both showing the full alphabet. Above that, there was a series of configurable rotors that were the key to the Enigma’s encryption capability. First, let’s look at just one rotor. One side had electrical contacts for all 26 letters. These connected to the other side of the rotor using cross-crossing wires that swapped one letter for another. If ‘H’ went in, ‘K’ might come out the other side. If “K’ went in, ‘F’ might come out, and so on. This letter swapping behavior should sound familiar: it’s a **substitution cipher**! But, the Enigma was more sophisticated because it used three or more rotors in a row, each feeding into the next. Rotors could also be rotated to one of 26 possible starting positions, and they could be inserted in different orders, providing a lot of different substitution mappings. Following the rotors was a special circuit called a **reflector**. Instead of passing the signal on to another rotor, it connected every pin to another, and sent the electrical signal back through the rotors. Finally, there was a plugboard at the front of the machine that allowed letters coming from the keyboard to be optionally swapped, adding another level of complexity. With our simplified circuit, let’s encrypt a letter on this example enigma configuration. If we press the ‘H’ key, electricity flows through the plugboard, then the rotors, hits the reflector, comes back through the rotors and plugboard, and illuminates the letter ‘L’ on the lampboard. So H is encrypted to L. Note that the circuit can flow both ways – so if we typed the letter ‘L’, ‘H’ would light up. In other words, it’s the same process for encrypting and decrypting; you just have to make sure the sending and receiving machines have the same initial configuration. If you look carefully at this circuit, you’ll notice it’s impossible for a letter to be encrypted as itself, which turned out to be a fatal cryptographic weakness. Finally, to prevent the Enigma from being a simple substitution cipher, every single time a letter was entered, the rotors advanced by one spot, sort of like an odometer in a car. So if you entered the text A-A-A, it might come out as B-D-K, where the substitution mapping changed with every key press. The Enigma was a tough cookie to crack, for sure. But as we discussed in Episode 15, Alan Turing and his colleagues at Bletchley Park were able to break Enigma codes and largely automate the process. 

> 到了1900年代，密码学以加密机的形式被机械化。最著名的是德国的**Enigma**，被纳粹用来加密他们的战时通信。正如我们在第15集所讨论的，英格玛是一台类似打字机的机器，有一个键盘和灯板，都显示完整的字母表。在这上面，有一系列可配置的转子，是英格玛加密能力的关键。首先，让我们看一下一个转子。一边是所有26个字母的电触点。这些触点与转子的另一面连接，使用交叉的电线将一个字母换成另一个。如果 "H "进去了，"K "可能会从另一边出来。如果 "K "进去，"F "可能出来，以此类推。这种交换字母的行为听起来应该很熟悉：这是一个**替换密码**！但是，英格玛更复杂。但是，英格玛更复杂，因为它在一排使用三个或更多的转子，每个转子进入下一个。转子也可以旋转到26个可能的起始位置之一，它们可以以不同的顺序插入，提供许多不同的替换映射。在转子之后是一个被称为**反射器**的特殊电路。它不是将信号传递给另一个转子，而是将每个针脚连接到另一个针脚，并通过转子将电信号送回去。最后，在机器的前面有一个插板，允许来自键盘的字母可以选择交换，这又增加了一层复杂性。用我们的简化电路，让我们在这个示例的英格玛配置上加密一个字母。如果我们按下 "H "键，电流流经插板，然后流经转子，击中反射器，再通过转子和插板回来，照亮灯板上的字母 "L"。所以H被加密成了L。请注意，电路可以双向流动--所以如果我们输入字母'L'，'H'就会亮起来。换句话说，加密和解密的过程是一样的；你只需要确保发送和接收机器有相同的初始配置。如果你仔细观察这个电路，你会发现一个字母不可能被加密成它本身，这被证明是一个致命的密码学弱点。最后，为了防止英格玛成为一个简单的替换密码，每输入一个字母，转子就前进一个点，有点像汽车的里程表。因此，如果你输入的文字是A-A-A，它可能会显示为B-D-K，每按一次键，替换的映射就会改变。英格玛是一块难以破解的饼干，这是肯定的。但正如我们在第15集所讨论的，阿兰-图灵和他在布莱切利公园的同事能够破解英格玛密码，并在很大程度上实现了自动化。

But with the advent of computers, cryptography moved from hardware into software. One of the earliest **software ciphers** to become widespread was the **Data Encryption Standard** developed by IBM and the NSA in 1977. **DES**, as it was known, originally used binary keys that were 56 bits long, which means that there are 2 to the 56, or about 72 quadrillion different keys. Back in 1977, that meant that nobody – except perhaps the NSA – had enough computing power to brute-force all possible keys. But, by 1999, a quarter-million dollar computer could try every possible DES key in just two days, rendering the cipher insecure. So, in 2001, the **Advanced Encryption Standard** (AES) was finalized and published. AES is designed to use much bigger keys – 128, 192 or 256 bits in size – making brute force attacks much, much harder. For a 128-bit keys, you'd need trillions of years to try every combination, even if you used every single computer on the planet today. So you better get started! AES chops data up into 16-byte blocks, and then applies a series of substitutions and permutations, based on the key value, plus some other operations to obscure the message, and this process is repeated ten or more times for each block. You might be wondering: why only ten rounds? Or why only 128 bit keys, and not ten thousand bit keys? Well, it’s a performance tradeoff. If it took hours to encrypt and send an email, or minutes to connect to a secure website, people wouldn't use it. AES balances performance and security to provide practical cryptography. Today, AES is used everywhere, from encrypting files on iPhones and transmitting data over WiFi with **WPA2**, to accessing websites using **HTTPS**. 

> 但随着计算机的出现，密码学从硬件进入软件。最早普及的**软件密码**之一是1977年由IBM和NSA开发的**数据加密标准**。**DES**，正如它所知道的那样，最初使用的二进制密钥为56位，这意味着56位中有2位，或大约72万亿个不同的密钥。在1977年，这意味着没有人--也许除了NSA--有足够的计算能力来破解所有可能的密钥。但是，到了1999年，一台价值25万美元的计算机可以在短短两天内尝试所有可能的DES密钥，使该密码变得不安全。因此，在2001年，**高级加密标准**（AES）被最终确定并公布。AES被设计为使用更大的密钥--128、192或256比特大小--使暴力攻击变得更加困难。对于一个128位的密钥，你需要数万亿年的时间来尝试每一个组合，即使你使用今天地球上的每一台计算机。所以你最好开始吧 AES将数据分割成16字节的块，然后根据密钥值进行一系列的替换和排列，再加上一些其他的操作来掩盖信息，这个过程对每个块重复十次或更多次。你可能想知道：为什么只有十轮？或者为什么只有128位的密钥，而不是一万位的密钥？嗯，这是一个性能的权衡。如果加密和发送电子邮件需要几个小时，或者连接到一个安全网站需要几分钟，人们就不会使用它。AES平衡了性能和安全，提供了实用的密码学。今天，AES的应用无处不在，从为iPhone手机的文件加密，用**WPA2**在WiFi上传输数据，到用**HTTPS**访问网站。

So far, the cryptographic techniques we’ve discussed rely on keys that are known by both sender and recipient. The sender encrypts a message using a key, and the recipient decrypts it using the same key. In the old days, keys would be shared by voice, or physically; for example, the Germans distributed codebooks with daily settings for their Enigma machines. But this strategy could never work in the internet era. Imagine having to crack open a codebook to connect to youtube! What’s needed is a way for a server to send a secret key over the public internet to a user wishing to connect securely. It seems like that wouldn’t be secure, because if the key is sent in the open and intercepted by a hacker, couldn’t they use that to decrypt all communication between the two? The solution is **key exchange**! – An algorithm that lets two computers agree on a key without ever sending one. We can do this with one-way functions – mathematical operations that are very easy to do in one direction, but hard to reverse. To show you how one-way functions work, let’s use paint colors as an analogy. It’s easy to mix paint colors together, but it’s not so easy to figure out the constituent colors that were used to make a mixed paint color. You’d have to test a lot of possibilities to figure it out. In this metaphor, our secret key is a unique shade of paint. First, there’s a public paint color that everyone can see. Then, John and I each pick a secret paint color. To exchange keys, I mix my secret paint color with the public paint color. Then, I send that mixed color to John by any means – mail, carrier pigeon, whatever. John does the same – mixing his secret paint color with the public color, then sending that to me. When I receive John’s color, I simply add my private color to create a blend of all three paints. John does the same with my mixed color. And Voila! We both end up with the same paint color! We can use this as a **shared secret**, even though we never sent each other our individual secret colors. A snooping outside observer would know partial information, but they’d find it very difficult to figure out our shared secret color. 

> 到目前为止，我们所讨论的加密技术都依赖于发送方和接收方都知道的密钥。发送方使用密钥对信息进行加密，而接收方则使用相同的密钥对信息进行解密。在过去，钥匙是通过声音或实物来分享的；例如，德国人分发密码本，上面有他们的英格玛机器的日常设置。但这种策略在互联网时代是行不通的。想象一下，要打开密码本才能连接到youtube! 我们需要的是一种方法，让服务器通过公共互联网向希望安全连接的用户发送一个秘密密钥。这似乎是不安全的，因为如果密钥在公开场合发送并被黑客截获，他们就不能用它来解密两者之间的所有通信吗？解决方案是**密钥交换**! - 一种让两台计算机在不发送密钥的情况下达成协议的算法。我们可以用单向函数来做这件事--在一个方向上非常容易做的数学运算，但是很难逆转。为了告诉你单向函数是如何工作的，让我们用油漆的颜色来做个比喻。把油漆的颜色混合在一起很容易，但要弄清楚用来制造混合油漆颜色的组成颜色就不那么容易了。你必须测试很多的可能性才能弄清楚。在这个比喻中，我们的秘钥是一种独特的油漆色泽。首先，有一个大家都能看到的公共油漆颜色。然后，约翰和我各自选择一种秘密的油漆颜色。为了交换钥匙，我将我的秘密油漆颜色与公共油漆颜色混合。然后，我通过任何方式--邮件、信鸽等--将混合后的颜色发送给约翰。约翰也做同样的事情--将他的秘密油漆颜色与公共颜色混合，然后将其发送给我。当我收到约翰的颜色时，我只需加入我的私人颜色，创造出三种油漆的混合色。约翰用我的混合颜色做同样的事情。然后就可以了。我们俩最终都得到了相同的油漆颜色! 我们可以把这作为一个**共享的秘密**，尽管我们从未向对方发送过我们各自的秘密颜色。一个窥探的外部观察者会知道部分信息，但他们会发现很难弄清我们共享的秘密颜色。

Of course, sending and mixing paint colors isn’t going to work well for transmitting computer data. But luckily, mathematical one-way functions are perfect, and this is what **Diffie-Hellman Key Exchange** uses. In Diffie-Hellman, the one-way function is modular exponentiation. This means taking one number, the base, to the power of another number, the exponent, and taking the remainder when dividing by a third number, the modulus. So, for example, if we wanted to calculate 3 to the 5th power, modulo 31, we would calculate 3 to the 5th, which is 243, then take the remainder when divided by 31, which is 26. The hard part is figuring out the exponent given only the result and the base. If I tell you I raised 3 to some secret number, modulo 31, and got 7 as the remainder, you'd have to test a lot of exponents to know which one I picked. If we make these numbers big, say hundreds of digits long, then finding the secret exponent is nearly impossible. Now let’s talk about how Diffie-Hellman uses modular exponentiation to calculate a shared key. First, there's a set of public values – the base and the modulus, that, like our public paint color, everyone gets to know... even the bad guys! To send a message securely to John, I would pick a secret exponent: X. Then, I’d calculate B to the power of X, modulo M. I send this big number over to John. John does the same, picking a secret exponent Y, and sending me B to the Y modulo M. To create a shared secret key, I take what John sent me, and take it to the power of X, my secret exponent. This is mathematically equivalent to B to the XY modulus M. John does the same, taking what I sent to him to the power of Y, and we both end up with the exact same number! It’s a secret shared key, even though we never sent each other our secret number. We can use this big number as a shared key for encrypted communication, using something like AES for encryption. **Diffie-Hellman key exchange is one method for establishing a shared key**. These keys that can be used by both sender and receiver, to encrypt and decrypt messages, are called **symmetric keys** because the key is the same on both sides. The Caesar Cipher, Enigma and AES are all **symmetric encryption**. 

> 当然，发送和混合颜料的颜色对于传输计算机数据来说并不顺利。但幸运的是，数学上的单向函数是完美的，这就是**Diffie-Hellman密钥交换**使用的东西。在Diffie-Hellman中，单向函数是模块化指数化。这意味着将一个数字（基数）与另一个数字（指数）相乘，并在除以第三个数字（模数）时取其余数。因此，例如，如果我们想计算3的5次方，模数为31，我们将计算3的5次方，也就是243，然后在除以31时取余数，也就是26。困难的部分是在只有结果和基数的情况下计算出指数的大小。如果我告诉你，我把3提高到某个秘密数字，再调制到31，得到的余数是7，你必须测试很多指数才能知道我选的是哪一个。如果我们把这些数字弄得很大，比如说有几百位数，那么找到秘密指数几乎是不可能的。现在我们来谈谈Diffie-Hellman是如何使用模块化指数来计算共享密钥的。首先，有一组公共值--基数和模数，就像我们的公共油漆颜色一样，每个人都能知道......甚至是坏人！为了安全地将信息发送给约翰，我们必须要有一组公共值。为了向约翰安全地发送一个信息，我会选择一个秘密指数。然后，我计算出B是X的幂，是M的模数，我把这个大数字发给约翰。约翰也是这样做的，他选择了一个秘密指数Y，并给我发送了B到Y的模数M。为了创建一个共享的秘密密钥，我把约翰发给我的东西，拿给我的秘密指数X的幂。这在数学上等同于B到XY的模数M。约翰也是这样做的，把我发给他的东西拿到Y的幂上，最后我们都得到了完全相同的数字！这就是秘密共享密钥。这是一个秘密共享密钥，尽管我们从未向对方发送过我们的秘密数字。我们可以用这个大数字作为加密通信的共享密钥，使用类似AES的东西进行加密。**Diffie-Hellman密钥交换是建立共享密钥的一种方法**。这些可由发送方和接收方使用的密钥，用于加密和解密信息，被称为**对称密钥**，因为双方的密钥都是相同的。凯撒密码、Enigma和AES都是**对称加密**。

There’s also **asymmetric encryption**, where there are two different keys, most often one that’s public and another that’s private. So, people can encrypt a message using a public key that only the recipient, with their private key, can decrypt. In other words, knowing the public key only lets you encrypt, but not decrypt – it’s **asymmetric**! So, think about boxes with padlocks that you can open with a key. To receive a secure message, I can give a sender a box and padlock. They put their message in it and lock it shut. Now, they can send that box back to me and only I can open it, with my private key. After locking the box, neither the sender, nor anyone else who finds the box, can open it without brute force. In the same way, a digital public key can encrypt something that can only be decrypted with a private key. The reverse is possible too: encrypting something with a private key that can be decrypted with a public key. This is used for signing, where a server encrypts data using their private key. Anyone can decrypt it using the server's public key. This acts like an unforgeable signature, as only the owner, using their private key, can encrypt. It proves that you're getting data from the right server or person, and not an imposter. The most popular asymmetric encryption technique used today is **RSA**, named after its inventors: Rivest, Shamir and Adleman. So, now you know all the “key” parts of modern cryptography: symmetric encryption, key exchange and public-key cryptography. When you connect to a secure website, like your bank, that little padlock icon means that your computer has used public key cryptography to verify the server, key exchange to establish a secret temporary key, and symmetric encryption to protect all the back-and-forth communication from prying eyes. Whether you're buying something online, sending emails to BFFs, or just browsing cat videos, cryptography keeps all that safe, private and secure. Thanks cryptography! 

> 还有一种是**非对称加密**，即有两个不同的密钥，最常见的是一个是公开的，另一个是私人的。因此，人们可以用公钥加密信息，而只有收件人用他们的私钥可以解密。换句话说，知道公钥只能让你加密，但不能解密--这是**不对称的**！所以，想想那些有密码锁的盒子吧。所以，想想有挂锁的盒子，你可以用钥匙打开。为了接收一个安全的信息，我可以给发件人一个盒子和挂锁。他们把他们的信息放在里面，然后锁上它。现在，他们可以把这个盒子寄回给我，只有我可以用我的私人钥匙打开它。锁上盒子后，无论是发件人，还是其他发现盒子的人，都不能用蛮力打开它。以同样的方式，数字公钥可以加密一些只能用私钥解密的东西。反过来也是可以的：用私钥加密的东西，可以用公钥解密。这用于签名，即服务器使用其私钥对数据进行加密。任何人都可以用服务器的公钥来解密它。这就像一个不可伪造的签名，因为只有所有者，使用他们的私钥，可以加密。它证明你从正确的服务器或个人那里获得数据，而不是一个冒牌货。今天最流行的非对称加密技术是**RSA**，以其发明者命名。Rivest、Shamir和Adleman。所以，现在你知道了现代密码学的所有 "关键 "部分：对称加密、密钥交换和公钥密码学。当你连接到一个安全的网站时，比如你的银行，那个小挂锁图标意味着你的电脑已经使用了公钥密码学来验证服务器，密钥交换来建立一个秘密的临时密钥，以及对称加密来保护所有来回的通信不被窥视。无论你是在网上买东西，给闺蜜发邮件，还是只是浏览猫咪视频，密码学都能保证所有的安全、隐私和保障。谢谢密码学! 



## #34 Machine Learning & Artificial Intelligence

<iframe width="560" height="315" src="https://www.youtube.com/embed/z-EtmaFJieY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to Crash Course Computer Science! As we’ve touched on many times in this series, computers are incredible at storing, organizing, fetching and processing huge volumes of data. That’s perfect for things like e-commerce websites with millions of items for sale, and for storing billions of health records for quick access by doctors. But what if we want to use computers not just to fetch and display data, but to actually make decisions about data? This is the essence of machine learning – algorithms that give computers the ability to learn from data, and then make predictions and decisions. Computer programs with this ability are extremely useful in answering questions like Is an email spam? Does a person’s heart have arrhythmia? What video should youtube recommend after this one? While useful, we probably wouldn’t describe these programs as “intelligent” in the same way we think of human intelligence. So, even though the terms are often interchanged, most computer scientists would say that machine learning is a set of techniques that sits inside the even more ambitious goal of Artificial Intelligence, or AI for short. INTRO Machine Learning and AI algorithms tend to be pretty sophisticated. So rather than wading into the mechanics of how they work, we're going to focus on what the algorithms do conceptually. Let’s start with a simple example: deciding if a moth is a Luna Moth or an Emperor Moth. This decision process is called classification, and an algorithm that does it is called a classifier. Although there are techniques that can use raw data for training – like photos and sounds – many algorithms reduce the complexity of real world objects and phenomena into what are called features. Features are values that usefully characterize the things we wish to classify. For our moth example, we’re going to use two features: “wingspan” and “mass”. In order to train our machine learning classifier to make good predictions, we’re going to need training data. To get that, we’d send an entomologist out into a forest to collect data for both luna and emperor moths. These experts can recognize different moths, so they not only record the feature values, but also label that data with the actual moth species. This is called labeled data. Because we only have two features, it’s easy to visualize this data in a scatterplot. Here, I’ve plotted data for 100 Emperor Moths in red and 100 Luna Moths in blue. We can see that the species make two groupings, but…. there’s some overlap in the middle… so it’s not entirely obvious how to best separate the two. That’s what machine learning algorithms do – find optimal separations! I’m just going to eyeball it and say anything less than 45 millimeters in wingspan is likely to be an Emperor Moth. We can add another division that says additionally mass must be less than .75 in order for our guess to be Emperor Moth. These lines that chop up the decision space are called decision boundaries. If we look closely at our data, we can see that 86 emperor moths would correctly end up inside the emperor decision region, but 14 would end up incorrectly in luna moth territory. On the other hand, 82 luna moths would be correct, with 18 falling onto the wrong side. A table, like this, showing where a classifier gets things right and wrong is called a confusion matrix... which probably should have also been the title of the last two movies in the Matrix Trilogy! Notice that there’s no way for us to draw lines that give us 100% accuracy. If we lower our wingspan decision boundary, we misclassify more Emperor moths as Lunas. If we raise it, we misclassify more Luna moths. The job of machine learning algorithms, at a high level, is to maximize correct classifications while minimizing errors On our training data, we get 168 moths correct, and 32 moths wrong, for an average classification accuracy of 84%. Now, using these decision boundaries, if we go out into the forest and encounter an unknown moth, we can measure its features and plot it onto our decision space. This is unlabeled data. Our decision boundaries offer a guess as to what species the moth is. In this case, we’d predict it’s a Luna Moth. This simple approach, of dividing the decision space up into boxes, can be represented by what’s called a decision tree, which would look like this pictorially or could be written in code using If-Statements, like this. A machine learning algorithm that produces decision trees needs to choose what features to divide on…and then for each of those features, what values to use for the division. Decision Trees are just one basic example of a machine learning technique. There are hundreds of algorithms in computer science literature today. And more are being published all the time. A few algorithms even use many decision trees working together to make a prediction. Computer scientists smugly call those Forests… because they contain lots of trees. There are also non-tree-based approaches, like Support Vector Machines, which essentially slice up the decision space using arbitrary lines. And these don’t have to be straight lines; they can be polynomials or some other fancy mathematical function. Like before, it’s the machine learning algorithm's job to figure out the best lines to provide the most accurate decision boundaries. So far, my examples have only had two features, which is easy enough for a human to figure out. If we add a third feature, let’s say, length of antennae, then our 2D lines become 3D planes, creating decision boundaries in three dimensions. These planes don’t have to be straight either. Plus, a truly useful classifier would contend with many different moth species. Now I think you’d agree this is getting too complicated to figure out by hand… But even this is a very basic example – just three features and five moth species. We can still show it in this 3D scatter plot. Unfortunately, there’s no good way to visualize four features at once, or twenty features, let alone hundreds or even thousands of features. But that’s what many real-world machine learning problems face. Can YOU imagine trying to figure out the equation for a hyperplane rippling through a thousand-dimensional decision space? Probably not, but computers, with clever machine learning algorithms can… and they do, all day long, on computers at places like Google, Facebook, Microsoft and Amazon. Techniques like Decision Trees and Support Vector Machines are strongly rooted in the field of statistics, which has dealt with making confident decisions, using data, long before computers ever existed. There’s a very large class of widely used statistical machine learning techniques, but there are also some approaches with no origins in statistics. Most notable are artificial neural networks, which were inspired by neurons in our brains! For a primer of biological neurons, check out our three-part overview here, but basically neurons are cells that process and transmit messages using electrical and chemical signals. They take one or more inputs from other cells, process those signals, and then emit their own signal. These form into huge interconnected networks that are able to process complex information. Just like your brain watching this youtube video. Artificial Neurons are very similar. Each takes a series of inputs, combines them, and emits a signal. Rather than being electrical or chemical signals, artificial neurons take numbers in, and spit numbers out. They are organized into layers that are connected by links, forming a network of neurons, hence the name. Let’s return to our moth example to see how neural nets can be used for classification. Our first layer – the input layer – provides data from a single moth needing classification. Again, we’ll use mass and wingspan. At the other end, we have an output layer, with two neurons: one for Emperor Moth and another for Luna Moth. The most excited neuron will be our classification decision. In between, we have a hidden layer, that transforms our inputs into outputs, and does the hard work of classification. To see how this is done, let’s zoom into one neuron in the hidden layer. The first thing a neuron does is multiply each of its inputs by a specific weight, let’s say 2.8 for its first input, and .1 for it’s second input. Then, it sums these weighted inputs together, which is in this case, is a grand total of 9.74. The neuron then applies a bias to this result - in other words, it adds or subtracts a fixed value, for example, minus six, for a new value of 3.74. These bias and inputs weights are initially set to random values when a neural network is created. Then, an algorithm goes in, and starts tweaking all those values to train the neural network, using labeled data for training and testing. This happens over many interactions, gradually improving accuracy – a process very much like human learning. Finally, neurons have an activation function, also called a transfer function, that gets applied to the output, performing a final mathematical modification to the result. For example, limiting the value to a range from negative one and positive one, or setting any negative values to 0. We’ll use a linear transfer function that passes the value through unchanged, so 3.74 stays as 3.74. So for our example neuron, given the inputs .55 and 82, the output would be 3.74. This is just one neuron, but this process of weighting, summing, biasing and applying an activation function is computed for all neurons in a layer, and the values propagate forward in the network, one layer at a time. In this example, the output neuron with the highest value is our decision: Luna Moth. Importantly, the hidden layer doesn’t have to be just one layer… it can be many layers deep. This is where the term deep learning comes from. Training these more complicated networks takes a lot more computation and data. Despite the fact that neural networks were invented over fifty years ago, deep neural nets have only been practical very recently, thanks to powerful processors, but even more so, wicked fast GPUs. So, thank you gamers for being so demanding about silky smooth framerates! A couple of years ago, Google and Facebook demonstrated deep neural nets that could find faces in photos as well as humans – and humans are really good at this! It was a huge milestone. Now deep neural nets are driving cars, translating human speech, diagnosing medical conditions and much more. These algorithms are very sophisticated, but it’s less clear if they should be described as “intelligent”. They can really only do one thing like classify moths, find faces, or translate languages. This type of AI is called Weak AI or Narrow AI. It’s only intelligent at specific tasks. But that doesn’t mean it’s not useful; I mean medical devices that can make diagnoses, and cars that can drive themselves are amazing! But do we need those computers to compose music and look up delicious recipes in their free time? Probably not. Although that would be kinda cool. Truly general-purpose AI, one as smart and well-rounded as a human, is called Strong AI. No one has demonstrated anything close to human-level artificial intelligence yet. Some argue it’s impossible, but many people point to the explosion of digitized knowledge – like Wikipedia articles, web pages, and Youtube videos – as the perfect kindling for Strong AI. Although you can only watch a maximum of 24 hours of youtube a day, a computer can watch millions of hours. For example, IBM’s Watson consults and synthesizes information from 200 million pages of content, including the full text of Wikipedia. While not a Strong AI, Watson is pretty smart, and it crushed its human competition in Jeopardy way back in 2011. Not only can AIs gobble up huge volumes of information, but they can also learn over time, often much faster than humans. In 2016, Google debuted AlphaGo, a Narrow AI that plays the fiendishly complicated board game Go. One of the ways it got so good and able to beat the very best human players, was by playing clones of itself millions and millions of times. It learned what worked and what didn’t, and along the way, discovered successful strategies all by itself. This is called Reinforcement Learning, and it’s a super powerful approach. In fact, it’s very similar to how humans learn. People don’t just magically acquire the ability to walk... it takes thousands of hours of trial and error to figure it out. Computers are now on the cusp of learning by trial and error, and for many narrow problems, reinforcement learning is already widely used. What will be interesting to see, is if these types of learning techniques can be applied more broadly, to create human-like, Strong AIs that learn much like how kids learn, but at super accelerated rates. If that happens, there are some pretty big changes in store for humanity – a topic we’ll revisit later. Thanks for watching. See you next week. 



## #35 Computer vision

<iframe width="560" height="315" src="https://www.youtube.com/embed/-4E2-0sxVUM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to Crash Course Computer Science! Today, let’s start by thinking about how important vision can be. Most people rely on it to prepare food, walk around obstacles, read street signs, watch videos like this, and do hundreds of other tasks. Vision is the highest bandwidth sense, and it provides a firehose of information about the state of the world and how to act on it. For this reason, computer scientists have been trying to give computers vision for half a century, birthing the sub-field of computer vision. Its goal is to give computers the ability to extract high-level understanding from digital images and videos. As everyone with a digital camera or smartphone knows, computers are already really good at capturing photos with incredible fidelity and detail – much better than humans in fact. But as computer vision professor Fei-Fei Li recently said, “Just like to hear is the not the same as to listen. To take pictures is not the same as to see.” INTRO As a refresher, images on computers are most often stored as big grids of pixels. Each pixel is defined by a color, stored as a combination of three additive primary colors: red, green and blue. By combining different intensities of these three colors, what’s called a RGB value, we can represent any color. Perhaps the simplest computer vision algorithm – and a good place to start – is to track a colored object, like a bright pink ball. The first thing we need to do is record the ball’s color. For that, we’ll take the RGB value of the centermost pixel. With that value saved, we can give a computer program an image, and ask it to find the pixel with the closest color match. An algorithm like this might start in the upper right corner, and check each pixel, one at time, calculating the difference from our target color. Now, having looked at every pixel, the best match is very likely a pixel from our ball. We’re not limited to running this algorithm on a single photo; we can do it for every frame in a video, allowing us to track the ball over time. Of course, due to variations in lighting, shadows, and other effects, the ball on the field is almost certainly not going to be the exact same RGB value as our target color, but merely the closest match. In more extreme cases, like at a game at night, the tracking might be poor. And if one of the team's jerseys used the same color as the ball, our algorithm would get totally confused. For these reasons, color marker tracking and similar algorithms are rarely used, unless the environment can be tightly controlled. This color tracking example was able to search pixel-by-pixel, because colors are stored inside of single pixels. But this approach doesn’t work for features larger than a single pixel, like edges of objects, which are inherently made up of many pixels. To identify these types of features in images, computer vision algorithms have to consider small regions of pixels, called patches. As an example, let’s talk about an algorithm that finds vertical edges in a scene, let’s say to help a drone navigate safely through a field of obstacles. To keep things simple, we’re going to convert our image into grayscale, although most algorithms can handle color. Now let’s zoom into one of these poles to see what an edge looks like up close. We can easily see where the left edge of the pole starts, because there’s a change in color that persists across many pixels vertically. We can define this behavior more formally by creating a rule that says the likelihood of a pixel being a vertical edge is the magnitude of the difference in color between some pixels to its left and some pixels to its right. The bigger the color difference between these two sets of pixels, the more likely the pixel is on an edge. If the color difference is small, it’s probably not an edge at all. The mathematical notation for this operation looks like this – it’s called a kernel or filter. It contains the values for a pixel-wise multiplication, the sum of which is saved into the center pixel. Let’s see how this works for our example pixel. I’ve gone ahead and labeled all of the pixels with their grayscale values. Now, we take our kernel, and center it over our pixel of interest. This specifies what each pixel value underneath should be multiplied by. Then, we just add up all those numbers. In this example, that gives us 147. That becomes our new pixel value. This operation, of applying a kernel to a patch of pixels, is call a convolution. Now let’s apply our kernel to another pixel. In this case, the result is 1. Just 1. In other words, it’s a very small color difference, and not an edge. If we apply our kernel to every pixel in the photo, the result looks like this, where the highest pixel values are where there are strong vertical edges. Note that horizontal edges, like those platforms in the background, are almost invisible. If we wanted to highlight those features, we’d have to use a different kernel – one that’s sensitive to horizontal edges. Both of these edge enhancing kernels are called Prewitt Operators, named after their inventor. These are just two examples of a huge variety of kernels, able to perform many different image transformations. For example, here’s a kernel that sharpens images. And here’s a kernel that blurs them. Kernels can also be used like little image cookie cutters that match only certain shapes. So, our edge kernels looked for image patches with strong differences from right to left or up and down. But we could also make kernels that are good at finding lines, with edges on both sides. And even islands of pixels surrounded by contrasting colors. These types of kernels can begin to characterize simple shapes. For example, on faces, the bridge of the nose tends to be brighter than the sides of the nose, resulting in higher values for line-sensitive kernels. Eyes are also distinctive – a dark circle sounded by lighter pixels – a pattern other kernels are sensitive to. When a computer scans through an image, most often by sliding around a search window, it can look for combinations of features indicative of a human face. Although each kernel is a weak face detector by itself, combined, they can be quite accurate. It’s unlikely that a bunch of face-like features will cluster together if they’re not a face. This was the basis of an early and influential algorithm called Viola-Jones Face Detection. Today, the hot new algorithms on the block are Convolutional Neural Networks. We talked about neural nets last episode, if you need a primer. In short, an artificial neuron – which is the building block of a neural network – takes a series of inputs, and multiplies each by a specified weight, and then sums those values all together. This should sound vaguely familiar, because it’s a lot like a convolution. In fact, if we pass a neuron 2D pixel data, rather than a one-dimensional list of inputs, it’s exactly like a convolution. The input weights are equivalent to kernel values, but unlike a predefined kernel, neural networks can learn their own useful kernels that are able to recognize interesting features in images. Convolutional Neural Networks use banks of these neurons to process image data, each outputting a new image, essentially digested by different learned kernels. These outputs are then processed by subsequent layers of neurons, allowing for convolutions on convolutions on convolutions. The very first convolutional layer might find things like edges, as that’s what a single convolution can recognize, as we’ve already discussed. The next layer might have neurons that convolve on those edge features to recognize simple shapes, comprised of edges, like corners. A layer beyond that might convolve on those corner features, and contain neurons that can recognize simple objects, like mouths and eyebrows. And this keeps going, building up in complexity, until there’s a layer that does a convolution that puts it together: eyes, ears, mouth, nose, the whole nine yards, and says “ah ha, it’s a face!” Convolutional neural networks aren’t required to be many layers deep, but they usually are, in order to recognize complex objects and scenes. That’s why the technique is considered deep learning. Both Viola-Jones and Convolutional Neural Networks can be applied to many image recognition problems, beyond faces, like recognizing handwritten text, spotting tumors in CT scans and monitoring traffic flow on roads. But we’re going to stick with faces. Regardless of what algorithm was used, once we’ve isolated a face in a photo, we can apply more specialized computer vision algorithms to pinpoint facial landmarks, like the tip of the nose and corners of the mouth. This data can be used for determining things like if the eyes are open, which is pretty easy once you have the landmarks – it’s just the distance between points. We can also track the position of the eyebrows; their relative position to the eyes can be an indicator of surprise, or delight. Smiles are also pretty straightforward to detect based on the shape of mouth landmarks. All of this information can be interpreted by emotion recognition algorithms, giving computers the ability to infer when you’re happy, sad, frustrated, confused and so on. In turn, that could allow computers to intelligently adapt their behavior... maybe offer tips when you’re confused, and not ask to install updates when you’re frustrated. This is just one example of how vision can give computers the ability to be context sensitive, that is, aware of their surroundings. And not just the physical surroundings – like if you're at work or on a train – but also your social surroundings – like if you’re in a formal business meeting versus a friend’s birthday party. You behave differently in those surroundings, and so should computing devices, if they’re smart. Facial landmarks also capture the geometry of your face, like the distance between your eyes and the height of your forehead. This is one form of biometric data, and it allows computers with cameras to recognize you. Whether it’s your smartphone automatically unlocking itself when it sees you, or governments tracking people using CCTV cameras, the applications of face recognition seem limitless. There have also been recent breakthroughs in landmark tracking for hands and whole bodies, giving computers the ability to interpret a user’s body language, and what hand gestures they’re frantically waving at their internet connected microwave. As we’ve talked about many times in this series, abstraction is the key to building complex systems, and the same is true in computer vision. At the hardware level, you have engineers building better and better cameras, giving computers improved sight with each passing year, which I can’t say for myself. Using that camera data, you have computer vision algorithms crunching pixels to find things like faces and hands. And then, using output from those algorithms, you have even more specialized algorithms for interpreting things like user facial expression and hand gestures. On top of that, there are people building novel interactive experiences, like smart TVs and intelligent tutoring systems, that respond to hand gestures and emotion. Each of these levels are active areas of research, with breakthroughs happening every year. And that’s just the tip of the iceberg. Today, computer vision is everywhere – whether it’s barcodes being scanned at stores, self-driving cars waiting at red lights, or snapchat filters superimposing mustaches. And, the most exciting thing is that computer scientists are really just getting started, enabled by recent advances in computing, like super fast GPUs. Computers with human-like ability to see is going to totally change how we interact with them. Of course, it’d also be nice if they could hear and speak, which we’ll discuss next week. I’ll see you then. 



## #36 Natural Language Processing

<iframe width="560" height="315" src="https://www.youtube.com/embed/fOvTtapxa9c" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to Crash Course Computer Science! Last episode we talked about computer vision – giving computers the ability to see and understand visual information. Today we’re going to talk about how to give computers the ability to understand language. You might argue they’ve always had this capability. Back in Episodes 9 and 12, we talked about machine language instructions, as well as higher-level programming languages. While these certainly meet the definition of a language, they also tend to have small vocabularies and follow highly structured conventions. Code will only compile and run if it’s 100 percent free of spelling and syntactic errors. Of course, this is quite different from human languages – what are called natural languages – containing large, diverse vocabularies, words with several different meanings, speakers with different accents, and all sorts of interesting word play. People also make linguistic faux pas when writing and speaking, like slurring words together, leaving out key details so things are ambiguous, and mispronouncing things. But, for the most part, humans can roll right through these challenges. The skillful use of language is a major part of what makes us human. And for this reason, the desire for computers to understand and speak our language has been around since they were first conceived. This led to the creation of Natural Language Processing, or NLP, an interdisciplinary field combining computer science and linguistics. INTRO There’s an essentially infinite number of ways to arrange words in a sentence. We can’t give computers a dictionary of all possible sentences to help them understand what humans are blabbing on about. So an early and fundamental NLP problem was deconstructing sentences into bite-sized pieces, which could be more easily processed. In school, you learned about nine fundamental types of English words: nouns, pronouns, articles, verbs, adjectives, adverbs, prepositions, conjunctions, and interjections. These are called parts of speech. There are all sorts of subcategories too, like singular vs. plural nouns and superlative vs. comparative adverbs, but we’re not going to get into that. Knowing a word’s type is definitely useful, but unfortunately, there are a lot words that have multiple meanings – like “rose” and “leaves”, which can be used as nouns or verbs. A digital dictionary alone isn’t enough to resolve this ambiguity, so computers also need to know some grammar. For this, phrase structure rules were developed, which encapsulate the grammar of a language. For example, in English there’s a rule that says a sentence can be comprised of a noun phrase followed by a verb phrase. Noun phrases can be an article, like “the”, followed by a noun or they can be an adjective followed by a noun. And you can make rules like this for an entire language. Then, using these rules, it’s fairly easy to construct what’s called a parse tree, which not only tags every word with a likely part of speech, but also reveals how the sentence is constructed. We now know, for example, that the noun focus of this sentence is “the mongols”, and we know it’s about them doing the action of “rising” from something, in this case, “leaves”. These smaller chunks of data allow computers to more easily access, process and respond to information. Equivalent processes are happening every time you do a voice search, like: “where’s the nearest pizza”. The computer can recognize that this is a “where” question, knows you want the noun “pizza”, and the dimension you care about is “nearest”. The same process applies to “what is the biggest giraffe?” or “who sang thriller?” By treating language almost like lego, computers can be quite adept at natural language tasks. They can answer questions and also process commands, like “set an alarm for 2:20” or “play T-Swizzle on spotify”. But, as you’ve probably experienced, they fail when you start getting too fancy, and they can no longer parse the sentence correctly, or capture your intent. Hey Siri... methinks the mongols doth roam too much, what think ye on this most gentle mid-summer’s day? Siri: I’m not sure I got that. I should also note that phrase structure rules, and similar methods that codify language, can be used by computers to generate natural language text. This works particularly well when data is stored in a web of semantic information, where entities are linked to one another in meaningful relationships, providing all the ingredients you need to craft informational sentences. Siri: Thriller was released in 1983 and sung by Michael Jackson Google’s version of this is called Knowledge Graph. At the end of 2016, it contained roughly seventy billion facts about, and relationships between, different entities. These two processes, parsing and generating text, are fundamental components of natural language chatbots - computer programs that chat with you. Early chatbots were primarily rule-based, where experts would encode hundreds of rules mapping what a user might say, to how a program should reply. Obviously this was unwieldy to maintain and limited the possible sophistication. A famous early example was ELIZA, created in the mid-1960s at MIT. This was a chatbot that took on the role of a therapist, and used basic syntactic rules to identify content in written exchanges, which it would turn around and ask the user about. Sometimes, it felt very much like human-human communication, but other times it would make simple and even comical mistakes. Chatbots, and more advanced dialog systems, have come a long way in the last fifty years, and can be quite convincing today! Modern approaches are based on machine learning, where gigabytes of real human-to-human chats are used to train chatbots. Today, the technology is finding use in customer service applications, where there’s already heaps of example conversations to learn from. People have also been getting chatbots to talk with one another, and in a Facebook experiment, chatbots even started to evolve their own language. This experiment got a bunch of scary-sounding press, but it was just the computers crafting a simplified protocol to negotiate with one another. It wasn’t evil, it’s was efficient. But what about if something is spoken – how does a computer get words from the sound? That’s the domain of speech recognition, which has been the focus of research for many decades. Bell Labs debuted the first speech recognition system in 1952, nicknamed Audrey – the automatic digit recognizer. It could recognize all ten numerical digits, if you said them slowly enough. 5… 9… 7? The project didn’t go anywhere because it was much faster to enter telephone numbers with a finger. Ten years later, at the 1962 World's Fair, IBM demonstrated a shoebox-sized machine capable of recognizing sixteen words. To boost research in the area, DARPA kicked off an ambitious five-year funding initiative in 1971, which led to the development of Harpy at Carnegie Mellon University. Harpy was the first system to recognize over a thousand words. But, on computers of the era, transcription was often ten or more times slower than the rate of natural speech. Fortunately, thanks to huge advances in computing performance in the 1980s and 90s, continuous, real-time speech recognition became practical. There was simultaneous innovation in the algorithms for processing natural language, moving from hand-crafted rules, to machine learning techniques that could learn automatically from existing datasets of human language. Today, the speech recognition systems with the best accuracy are using deep neural networks, which we touched on in Episode 34. To get a sense of how these techniques work, let’s look at some speech, specifically, the acoustic signal. Let’s start by looking at vowel sounds, like aaaaa…and Eeeeeee. These are the waveforms of those two sounds, as captured by a computer’s microphone. As we discussed in Episode 21 – on Files and File Formats – this signal is the magnitude of displacement, of a diaphragm inside of a microphone, as sound waves cause it to oscillate. In this view of sound data, the horizontal axis is time, and the vertical axis is the magnitude of displacement, or amplitude. Although we can see there are differences between the waveforms, it’s not super obvious what you would point at to say, “ah ha! this is definitely an eeee sound”. To really make this pop out, we need to view the data in a totally different way: a spectrogram. In this view of the data, we still have time along the horizontal axis, but now instead of amplitude on the vertical axis, we plot the magnitude of the different frequencies that make up each sound. The brighter the color, the louder that frequency component. This conversion from waveform to frequencies is done with a very cool algorithm called a Fast Fourier Transform. If you’ve ever stared at a stereo system’s EQ visualizer, it’s pretty much the same thing. A spectrogram is plotting that information over time. You might have noticed that the signals have a sort of ribbed pattern to them – that’s all the resonances of my vocal tract. To make different sounds, I squeeze my vocal chords, mouth and tongue into different shapes, which amplifies or dampens different resonances. We can see this in the signal, with areas that are brighter, and areas that are darker. If we work our way up from the bottom, labeling where we see peaks in the spectrum – what are called formants – we can see the two sounds have quite different arrangements. And this is true for all vowel sounds. It’s exactly this type of information that lets computers recognize spoken vowels, and indeed, whole words. Let’s see a more complicated example, like when I say: “she.. was.. happy” We can see our “eee” sound here, and “aaa” sound here. We can also see a bunch of other distinctive sounds, like the “shh” sound in “she”, the “wah” and “sss” in “was”, and so on. These sound pieces, that make up words, are called phonemes. Speech recognition software knows what all these phonemes look like. In English, there are roughly forty-four, so it mostly boils down to fancy pattern matching. Then you have to separate words from one another, figure out when sentences begin and end... and ultimately, you end up with speech converted into text, allowing for techniques like we discussed at the beginning of the episode. Because people say words in slightly different ways, due to things like accents and mispronunciations, transcription accuracy is greatly improved when combined with a language model, which contains statistics about sequences of words. For example “she was” is most likely to be followed by an adjective, like “happy”. It’s uncommon for “she was” to be followed immediately by a noun. So if the speech recognizer was unsure between, “happy” and “harpy”, it’d pick “happy”, since the language model would report that as a more likely choice. Finally, we need to talk about Speech Synthesis, that is, giving computers the ability to output speech. This is very much like speech recognition, but in reverse. We can take a sentence of text, and break it down into its phonetic components, and then play those sounds back to back, out of a computer speaker. You can hear this chaining of phonemes very clearly with older speech synthesis technologies, like this 1937, hand-operated machine from Bell Labs. Say, "she saw me" with no expression. She saw me. Now say it in answer to these questions. Who saw you? She saw me. Who did she see? She saw me. Did she see you or hear you? She saw me. By the 1980s, this had improved a lot, but that discontinuous and awkward blending of phonemes still created that signature, robotic sound. Thriller was released in 1983 and sung by Michael Jackson. Today, synthesized computer voices, like Siri, Cortana and Alexa, have gotten much better, but they’re still not quite human. But we’re soo soo close, and it’s likely to be a solved problem pretty soon. Especially because we’re now seeing an explosion of voice user interfaces on our phones, in our cars and homes, and maybe soon, plugged right into our ears. This ubiquity is creating a positive feedback loop, where people are using voice interaction more often, which in turn, is giving companies like Google, Amazon and Microsoft more data to train their systems on... Which is enabling better accuracy, which is leading to people using voice more, which is enabling even better accuracy… and the loop continues! Many predict that speech technologies will become as common a form of interaction as screens, keyboards, trackpads and other physical input-output devices that we use today. That’s particularly good news for robots, who don’t want to have to walk around with keyboards in order to communicate with humans. But, we’ll talk more about them next week. See you then. 



## #37 Robots

<iframe width="560" height="315" src="https://www.youtube.com/embed/3XkL0qQ21Oo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Hi, I’m Carrie Anne, and welcome to CrashCourse Computer Science! Today we’re going to talk about robots. The first image that jumps to your mind is probably a humanoid robot, like we usually see in shows or movies. Sometimes they’re our friends and colleagues, but more often, they’re sinister, apathetic and battle-hardened. We also tend to think of robots as a technology of the future. But the reality is: they’re already here – by the millions – and they’re our workmates, helping us to do things harder, better, faster, and stronger. INTRO There are many definitions for robots, but in general, these are machines capable of carrying out a series of actions automatically, guided by computer control. How they look isn’t part of the equation – robots can be industrial arms that spray paint cars, drones that fly, snake-like medical robots that assist surgeons, as well as humanoid robotic assistants. Although the term “robot” is sometimes applied to interactive virtual characters, it’s more appropriate to call these “bots”, or even better, “agents.” That’s because the term “robot” carries a physical connotation a machine that lives in and acts on the real world. The word “robot” was first used in a 1920 Czech play to denote artificial, humanoid characters. The word was derived from “robota”, the slavic-language word for a forced laborer, indicating peasants in compulsory service in feudal, nineteenth century Europe. The play didn’t go too much into technological details. But, even a century later, it’s still a common portrayal: mass-produced, efficient, tireless creatures that look human-esque, but are emotionless, indifferent to self-preservation and lack creativity. The more general idea of self-operating machines goes back even further than the 1920s. Many ancient inventors created mechanical devices that performed functions automatically, like keeping the time and striking bells on the hour. There are plenty of examples of automated animal and humanoid figures, that would perform dances, sing songs, strike drums and do other physical actions. These non-electrical and certainly non-electronic machines were called automatons. For instance, an early automaton created in 1739 by the Frenchman Jacques de Vaucanson was the Canard Digerateur or Digesting Duck, a machine in the shape of a duck that appeared to eat grain and then defecate. In 1739 Voltaire wrote, “Without the voice of le Maure and Vaucanson’s duck, you would have nothing to remind you of the glory of France.” One of the most infamous examples was the “Mechanical Turk”: a chess-playing, humanoid automaton. After construction in 1770, it toured all over Europe, wowing audiences with its surprisingly good chess-playing. It appeared to be a mechanical, artificial intelligence. Unfortunately, it was a hoax – there was a dainty human stuffed inside the machine. The first machines controlled by computers emerged in the late 1940s. These Computer Numerical Control, or CNC, machines, could run programs that instructed a machine to perform a series of operations. This level of control also enabled the creation of new manufactured goods, like milling a complex propellor design out of a block of aluminum – something that was difficult to do using standard machine tools, and with tolerances too small to be done by hand. CNC machines were a huge boon to industry, not just due to increased capability and precision, but also in terms of reducing labor costs by automating human jobs – a topic we’ll revisit in a later episode. The first commercial deployment was a programmable industrial robot called the Unimate, sold to General Motors in 1960 to lift hot pieces of metal from a die casting machine and stack them. This was the start of the robotics industry. Soon, robots were stacking pallets, welding parts, painting cars and much more. For simple motions – like a robotic gripper that moves back and forth on a track – a robot can be instructed to move to a particular position, and it’ll keep moving in that direction until the desired position is reached, at which point it’ll stop. This behavior can be achieved through a simple control loop. First, sense the robot position. Are we there yet? Nope. So keep moving. Now sense position again. Are we there yet? Nope, so keep moving. Are we there yet? Yes! So we can stop moving, and also please be quiet! Because we’re trying to minimize the distance between the sensed position and the desired position, this control loop is, more specifically, a negative feedback loop. A negative feedback control loop has three key pieces. There’s a sensor, that measures things in the real world, like water pressure, motor position, air temperature, or whatever you’re trying to control. From this measurement, we calculate how far we are from where we want to be – the error. The error is then interpreted by a controller, which decides how to instruct the system to minimize that error. Then, the system acts on the world though pumps, motors, heating elements, and other physical actuators. In tightly controlled environments, simple control loops, like this, work OK. But in many real world applications, things are a tad more complicated. Imagine that our gripper is really heavy, and even when the control loop says to stop, momentum causes the gripper to overshoot the desired position. That would cause the control loop to take over again, this time backing the gripper up. A badly tuned control loop might overshoot and overshoot and overshoot, and maybe even wobble forever. To make matters worse, in real world settings, there are typically external and variable forces acting on a robot, like friction, wind and items of different weight. To handle this gracefully, more sophisticated control logic is needed. A widely used control-loop, feedback mechanism is a proportional–integral–derivative cotnroller. That’s a bit of a mouthful, so people call them PID controllers. These used to be mechanical devices, but now it’s all done in software. Let’s imagine a robot that delivers coffee. Its goal is to travel between customers at two meters per second, which has been determined to be the ideal speed that’s both safe and expedient. Of course, the environment doesn’t always cooperate. Sometimes there’s wind, and sometimes there's uphills and downhills and all sorts of things that affect the speed of the robot. So, it’s going to have to increase and decrease power to its motors to maintain the desired speed. Using the robot's speed sensor, we can keep track of its actual speed and plot that alongside its desired speed. PID controllers calculate three values from this data. First is the proportional value, which is the difference between the desired value and the actual value at the most recent instant in time or the present. This is what our simpler control loop used before. The bigger the gap between actual and desired, the harder you'll push towards your target. In other words, it’s proportional control. Next, the integral value is computed, which is the sum of error over a window of time, like the last few seconds. This look back helps compensate for steady state errors, resulting from things like motoring up a long hill. If this value is large, it means proportional control is not enough, and we have to push harder still. Finally, there’s the derivative value, which is the rate of change between the desired and actual values. This helps account for possible future error, and is sometimes called "anticipatory control". For example, if you are screaming in towards your goal too fast, you’ll need to ease up a little to prevent overshoot. These three values are summed together, with different relative weights, to produce a controller output that’s passed to the system. PID controllers are everywhere, from the cruise control in your car, to drones that automatically adjust their rotor speeds to maintain level flight, as well as more exotic robots, like this one that balances on a ball to move around. Advanced robots often require many control loops running in parallel, working together, managing everything from robot balance to limb position. As we’ve discussed, control loops are responsible for getting robot attributes like location to desired values. So, you may be wondering where these values come from. This is the responsibility of higher-level robot software, which plans and executes robot actions, like plotting a path around sensed obstacles, or breaking down physical tasks, like picking up a ball, into simple, sequential motions. Using these techniques, robots have racked up some impressive achievements – they’ve been to the deepest depths of Earth’s oceans and roved around on Mars for over a decade. But interestingly, lots of problems that are trivial for many humans have turned out to be devilishly difficult for robots: like walking on two legs, opening a door, picking up objects without crushing them, putting on a t-shirt, or petting a dog. These are tasks you may be able to do without thinking, but a supercomputer-powered robot fails at spectacularly. These sorts of tasks are all active areas of robotics research. Artificial intelligence techniques, which we discussed a few episodes ago, are perhaps the most promising avenue to overcome these challenges. For example, Google has been running an experiment with a series of robotic arms that spend their days moving miscellaneous objects from one box to another, learning from trial and error. After thousands of hours of practice, the robots had cut their error rate in half. Of course, unlike humans, robots can run twenty-four hours a day and practice with many arms at the same time. So, it may just be a matter of time until they become adept at grasping things. But, for the time being, toddlers can out-grasp them. One of the biggest and most visible robotic breakthrough in recent years has been self-driving, autonomous cars. If you think about it, cars don’t have too many system inputs – you can speed up or slow down, and you can steer left or right. The tough part is sensing lanes, reading signs, and anticipating and navigating traffic, pedestrians, bicyclists, and a whole host of obstacles. In addition to being studded with proximity sensors, these robotic vehicles heavily rely on Computer Vision algorithms, which we discussed in Episode 35. We’re also seeing the emergence of very primitive androids – robots that look and act like humans. Arguably, we’re not close on either of those goals, as they tend to look pretty weird and act even weirder. At least we’ll always have Westworld. But anyway, these remain a tantalizing goal for roboticists that combine many computer science topics we’ve touched on over the last few episodes, like artificial intelligence, computer vision and natural language processing. As for why humans are so fascinated by creating artificial embodiments of ourselves...you’ll have to go to Crash Course Philosophy for that. And for the foreseeable future, realistic androids will continue to be the stuff of science fiction. Militaries also have a great interest in robots – they’re not only replaceable, but can surpass humans in attributes like strength, endurance, attention, and accuracy. Bomb disposal robots and reconnaissance drones are fairly common today. But fully autonomous, armed-to-the-teeth robots are slowly appearing, like the Samsung SGR-A1 sentry gun deployed by South Korea. Robots with the intelligence and capability to take human lives are called lethal autonomous weapons. And they’re widely considered a complex and thorny issue. Without doubt, these systems could save soldiers lives by taking them off the battlefield and out of harm’s way. It might even discourage war all together. Though it’s worth noting that people said the same thing about dynamite and nuclear weapons. On the flip side, we might be creating ruthlessly efficient killing machines that don’t apply human judgment or compassion to complex situations. And the fog of war is about as complex and murky as they come. These robots would be taking orders and executing them as efficiently as they can and sometimes human orders turn out to be really bad. This debate is going to continue for a long time, and pundits on both sides will grow louder as robotic technology improves. It’s also an old debate – the danger was obvious to science fiction writer Isaac Asimov, who introduced a fictional “Three Laws of Robotics” in his 1942 short story "Runaround". And then, later he added a zeroth rule. In short, it’s a code of conduct or moral compass for robots – guiding them to do no harm, especially to humans. It’s pretty inadequate for practical application and it leaves plenty of room for equivocation. But still, Asimov’s laws inspired a ton of science fiction and academic discussion, and today there are whole conferences on robot ethics. Importantly, Asimov crafted his fictional rules as a way to push back on “Robot as a Menace” memes common in fiction from his childhood. These were stories where robots went off the rails, harming or even destroying their creators in the process. Asimov, on the other hand, envisioned robots as useful, reliable, and even loveable machines. And it’s this duality I want to leave you thinking about today. Like many of the technologies we’ve discussed throughout this series, there are benevolent and malicious uses. Our job is to carefully reflect on computing's potential and peril, and wield our inventive talents to improve the state of the world. And robots are one of the most potent reminders of this responsibility. I’ll see you next week. 
